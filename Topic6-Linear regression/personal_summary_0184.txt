Topic 5 Notes    Forms of Supervised Learning  Regression problems  o  Linear Regression (linear model) o  Logistic Regression (linear model)  Classification problems  o  Support Vector Machines (both linear and nonlinear) o  Decision Trees (nonlinear) o  Random Forest (nonlinear) o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear) In supervised learning, the data used to train the algorithm is already labelled with correct answers. Instead of finding patterns based on similarity only, learn a direct mapping or function between feature vector xi and the output (target or label) yi such that, yi = h(xi).      Supervised learning is the task of estimating a function from labelled training data. How can we measure the quality of function h? How can we understand how accurately h can map X to the target Y? The loss function is really a measure of accuracy,    Examples of loss functions:  o  Square loss o  Absolute loss o  0 -1 loss (useful for classification) o  Logistic loss, Hinge loss    Empirical risk by averaging the results of the loss function. The lower the empirical risk based on the training data, the closer the function represents the true relationship between the pair of values xi and yi.    Different models in terms of complexity:   o  o o  If we choose higher complexity than necessary, we would be over-fitting the data (you will review over-fitting later in this course). If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalisation.    Structural Risk minimisation seeks to prevent over-fitting by incorporating a penalty on the  model complexity that prefers simpler functions over more complex ones    Classification metric  o  A confusion matrix is a summary of prediction results on a classification problem  o  Receiver Operating Characteristics (ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channel  There are useful statistics that can be calculated via ROC curve, like the Area Under the Curve (AUC) and the Youden Index. These tell you how well the model predicts and the optimal cut point for any given model (under specific circumstances). AUC is used to summarize the ROC curve using a single number. The higher the value of AUC, better performing is the classifier! A random classifier has an AUC of 0.5.  o  F1 -measure is a metric that combines both Precision and Recall in a single number.    Regression metrics  o  Mean Square Error o  Root mean square error o  Mean absolute error o  Explained Variance (R2) /  R-square/ Coefficient of determination (between 0 and 100%)  0 % represents a model that does not explain any of the variation in the response variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model. 100% represents a model that explains all of the variation in the response variable around its mean.    3 methods for splitting data:   random subsampling, repeatedly partitions the data into random training and test sets in a specified ratio  o  stratified sampling, divide the entire data into different subgroups or strata, o  cross validation / k-fold cross-validation, partitioning the original sample into a training set to train the model, and a test set to evaluate it. The main idea is to partition training data into equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples.    hyperparameter is a parameter whose value is set before the learning process begins.  In Kmeans, the number of clusters is a hyperparameter.    3 possible ways to navigate the hyperparameter space:  o  Grid-search (not so efficient) o  Random search (efficient in certain scenarios) o  Bayesian optimization (efficient in general)  