The majority of practical machine learning applications use supervised learning. In supervised learning, the data used to train the algorithm is already labeled with correct answers. In supervised learning, the data used to train the algorithm is already labeled with correct answers. In other words, you make an algorithm based on the known relationship between the input and output. From this, you develop a mapping function from the input variable x to the output variable y.  Instead of finding patterns based on similarity only, we can learn a direct mapping or function between feature vector xi and the output (target or label) yi. Thus, supervised learning is the task of estimating a function from labelled training data.  Supervised learning can appear in many forms:  Regression problems (look to find relationships among feature variables)  Linear Regression (linear model) Logistic Regression (linear model)  Classification problems  Support Vector Machines (both linear and nonlinear) Decision Trees (nonlinear) Random Forest (nonlinear) Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  Ranking problems  Hypothesis space  We will name a hypothesis function, ℎ, as an element of a range of possible functions H,  usually called the hypothesis space. We’ll select a hypothesis function that we think is similar to the true function behind the data e.g space of all linear functions in d−dimensions space of all polynomial functions up to degree p.  Finding a function  In supervised learning, given the training data, the learning algorithm seeks a function on ℎ:X→Y where X is the input space and Y is the output space.  how can we measure the quality of function ℎ? How can we understand how accurately  ℎ can map X to the target Y? We need to introduce a new function called the loss function.  Loss function  The loss function is really a measure of accuracy. How accurately does your ℎ function describe the relationship between X to the target Y?  A loss function is a mathematical function that is used to measure the difference or distance between the predicted values and the actual values in a machine learning algorithm. The goal of the loss function is to provide a single number that can be used to optimize the model's parameters to minimize this difference.    Mean Squared Error (MSE): This loss function is commonly used for regression problems. It  measures the average squared difference between the predicted and actual values.    Binary Cross-Entropy: This loss function is used for binary classification problems. It measures  the difference between the predicted probability and the actual class label.    Categorical Cross-Entropy: This loss function is used for multi-class classification problems. It measures the difference between the predicted probability distribution and the actual probability distribution over multiple classes.    Hinge Loss: This loss function is used for binary classification problems where the output is a  binary decision (i.e., a single class). It measures the difference between the predicted score and the actual label, but only penalizes errors that exceed a certain threshold.    KL Divergence: This loss function is used in generative models to measure how well the  generated distribution matches the actual distribution. It measures the difference between two probability distributions.    Huber Loss: This loss function is used in regression problems, especially when there are outliers in the data. It combines the advantages of mean squared error and mean absolute error, by being less sensitive to outliers than MSE.  Empirical risk  Similar to the loss function, we can define a factor called empirical risk. Empirical risk is a concept in machine learning and statistics that refers to the average loss or error of a model on a given set of training data. It is a measure of how well the model fits the training data and is used to estimate how well the model will perform on new, unseen data. The lower the empirical risk based on the training data, the closer the function represents the true relationship between the pair of values  Model complexity  Let’s first examine the effects of selecting different models in terms of complexity:  If we choose higher complexity than necessary, we would be over-fitting the data (you will review over-fitting later in this course). If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalisation.  What is generalisation? It is prediction on unseen data, that is, the data, which is not part of our training set.  Model complexity and Occam's razor  Occam’s Razor, a famous problem-solving principle, is used as a heuristic guide in the development of theoretical models. This principle often paraphrased as: All other things being equal, the simplest solution is the best.  It also addresses the problem of Which hypothesis to choose if there are multiple hypothesis with similar fit?  In other words, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity.  Structural risk minimisation  So based on Occam’s razor and its simplistic principle, we define another risk value which is called Structural Risk.  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. So the general idea is to minimise both Structural Riskand Empirical Risk  Classification metrics  The metrics that you choose to evaluate your machine learning model are very important. The choice of evaluation metrics influences how performance is measured and compared.  The most common type of machine learning applications are classification problems. There are myriad metrics that can be used to evaluate predictions for these types of problems.  Confusion Matrix  A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class. Confusion matrices are a way to understand the types of errors made by a model. Confusion matrices are also called contingency tables.  One of the reasons for using a confusion matrix is that, accuracy is not a reliable metric for the real performance of a classifier.  Other evaluation metrics based on a confusion matrix:    True Positive Rate (TPR) or Recall or Sensitivity: that measures the proportion of actual positive cases that are correctly identified by the model. In other words, TPR measures the model's ability to correctly identify positive cases out of all the actual positive cases.    False positive rate (FPR): False Positive Rate (FPR) is a metric used in binary classification that measures the proportion of negative cases that are incorrectly classified as positive by the model. In other words, FPR measures the model's ability to correctly identify negative cases out of all the actual negative cases.  ROC Curve  Receiver Operating Characteristics (ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels.  The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. This has to be done to depict relative trade-offs between benefits (true positives) and costs (false positives).  Area Under the Curve (AUC) is used to summarize the ROC curve using a single number. The higher the value of AUC, better performing is the classifier! A random classifier has an AUC of 0.5.  F-1 Measure  Another useful metric could be the combination of Precision and Recall.  F1-measure is a metric that combines both Precision and Recall in a single number.  Regression Metrics  Regression measures how far the expected value is from the actual value:  Measuring regression performance    Mean Square Error  It measures the average squared difference between the predicted and actual values. To calculate MSE, the difference between the predicted value and the actual value is squared for each example in the training set, and the average of these squared differences is taken as the final loss value.    Explained Variance (R2)  This measure is known by many names including:  R-square Explained variance the coefficient of determination  R-square is measured as the percentage of target variation that is explained by the model. For linear regression with bias term, R-square is the square of the correlation between the target values and the predicted target values. Unlike the other introduced metrics, the higher the R- square of a model, the better its performance.  R-squared is always between 0 and 100%:  0% represents a model that does not explain any of the variation in the response  variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model.  100% represents a model that explains all of the variation in the response variable  around its mean.  Let’s now investigate the details on model selection. The first question that might come to mind might be the limitations of using only a single training/testing set.  A single training set may be affected by some outlier instances (i.e. noisy observations). To get a reliable estimate of model performance (accuracy), we need a large test set. Why?  Because variance of such an estimate is low.  However, we know that the larger the size of the training set, the more accurately the model  can be learnt.  Multiple training/test splits allow us to re-use same data for both training and evaluation in  different splits.  We usually work with 3 methods for splitting data:  random subsampling  o stratified sampling cross validation.  Lets start with Random sub-sampling.   Instead of using a single split, a more reliable estimate of model performance can be obtained by random sub-sampling. Random sub-sampling repeatedly partitions the data into random training and test sets in a specified ratio.  Stratified sampling is a probability sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata. When using randomly selecting training (or validation) sets, class proportions may differ between training and test splits.  Stratified sampling ensures that class proportions are maintained in each random set. The figure  below shows how Stratified Sampling works. As you can see it first separates (stratifies) instances by class label, then randomly selects instances from each class.  Another method for partitioning data which is even more popular among researchers is Cross- validation. This is a technique to evaluate models by partitioning the original sample into a training set to train the model, and a test set to evaluate it.  The main idea is to partition training data into k equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples. The following figure, illustrates this process.  To conclude:  In special cases, when k is equal to the number of instances n,  When using subsamples, we call it k-fold cross-validation. o we call it as leave-one-out cross validation scheme. Cross-validation makes efficient use of the available data for testing.  Finding the best hyperparameters  In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.  This means the value of a hyperparameter in a model cannot be estimated from data. They are often used in processes to help estimate model parameters.  A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.  The validation set is used to evaluate a given model and also to fine-tune the model hyperparameters. So, given a choice of hyperparameter values, you use the training set to train the model. But, how do you set the values for the hyperparameters? That’s what the validation set is for. Remember Kmeans in course 2? The number of clusters (k), is a hyperparameter. Because this value is set before the learning begins.  Common methods for hyperparameters:  Grid Search: Grid Search is a brute-force approach that involves defining a grid of  hyperparameters to search over. The model is trained and evaluated for each combination of hyperparameters in the grid, and the best performing set of hyperparameters is selected. Random Search: Random Search is a more efficient alternative to Grid Search. Instead of  searching over a grid of hyperparameters, it randomly samples hyperparameters from a defined range of values. This approach can be more effective for high-dimensional hyperparameter spaces.  Bayesian Optimization: Bayesian Optimization is a probabilistic approach that models the  distribution of the objective function (i.e., the model performance) using a Gaussian process. It iteratively samples hyperparameters from the distribution and updates the model performance estimate to find the optimal set of hyperparameters.  Evolutionary Algorithms: Evolutionary Algorithms, such as Genetic Algorithms or Particle Swarm Optimization, are optimization techniques inspired by biological evolution. They involve creating a population of candidate solutions (i.e., sets of hyperparameters), evaluating their fitness (i.e., model performance), and selecting the best solutions for reproduction and mutation to create the next generation of candidate solutions.  Gradient-based Optimization: Gradient-based Optimization techniques, such as Gradient Descent or Adam, are typically used to optimize the model parameters, but can also be used to optimize the  hyperparameters by treating them as additional parameters to optimize. This approach can be computationally expensive and requires a differentiable objective function.  Internal cross-validation  All the techniques that we previously discussed for model assessment are applicable for training/validation set splitting:  Random subsampling Stratified subsampling Cross-validation  We are still assessing how a particular hyperparameter is doing on the validation set. Remember, this step is internal to the learning process and different from model assessment on the test data.  Let us examine how an internal cross-validation works. Instead of using a single validation set, we can use cross-validation within a training set to select the best set of hyperparameters. So basically it is exactly the same as the one we saw for test/train partitioning. However, in here we partition the data into training/validation sets. The following figure illustrates this process.  Effect of imbalanced classes  One problem that can occur in machine learning is datasets where the total number of one class of data (i.e. positive outcomes) is far less than the total number of another class of data (i.e. negative) outcomes.  This problem is very common in practice and can be detected in various disciplines including fraud detection, anomaly detection, medical diagnosis, etc.  As you know most machine learning algorithms work best when the number of instances of each classes are roughly equal. When the number of instances of one class far exceeds the other, problems arise.  Solutions  At the data level: (Re-Sampling)  over-sampling the data from minority class under-sampling the data from majority class.  Two obvious solutions based on data manipulation which suggests that we can sample more data points from the minority class in order to cover the difference. Or we can under-sample the majority class in order to make them have an equal effect on the algorithm.  At the algorithmic level:  adjusting the costs adjusting the decision threshold.  From an algorithmic point of view, we may want to adjust some costs on the points we are observing from the majority class in order to dampen their effect. Also we can manually define some thresholds to cope with the unbalanced data.  But always remember:  Any pre-processing over an entire data set (e.g. feature selection, or feature extraction) must  not use the information that you are trying to predict (e.g. labels).  During the training process, you must not use any information that is not available during the  training process.  How can we measure the performance of our learning model? For linear regression, we measure the deviation in prediction from the truth using ‘mean square error’ (MSE).  