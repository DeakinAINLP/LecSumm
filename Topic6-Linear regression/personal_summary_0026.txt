Topic 5 provided a solid synopsis on linear regression and other modelling techniques.  This topic introduced the concepts of a training dataset and test datasets. Segregation of these and the percentages they are separated (if using real world data) is important to not overfit models but also allow a high accuracy predictions.  A  challenge  is  to  measure  the  effectivity  of  these  predictions,  some  tools  that  can  aid  are  the determination of “hyperparameters” which may help confirm the correct classification of data.  This is particularly important where there is an imbalance in the training data. E.g if we are looking for a flower in a forest, but it is only in one of 30 images in the training dataset it will be under-represented. The  machine  learning  models  generate  better  predictions  when  there  is  a  balance  in  the  “desired outcome” vs the “non-desired” outcome. This is an important concept when generating training datasets.  Training datasets can be artificially increase (duplicating data/re-sampling) but this may lead to overfitting the data.  Models can also be run iteratively to improve the fidelity of the output, especially when combined with hyperparameters  and  other  guiding  methods  to  improve  the  predictions  of  the  models.  This  is computationally intensive and may not be feasible on large datasets however.  Higher complexity models = overfitting  Lower complexity models = underfitting  Structural Risk mitigation on the modelling can be conducted by incorporating penalties to induce the simplest method that achieves accurate prediction results.  Measuring regression performance can be conducted by using MSE (Mean Square Error) with a end result of 0 being a perfect match (prediction), there is no limit to the error that can be achieved.  There are also randomised, stratified and cross-validation methods of sampling when making test and training data sets to improve and better utilise the available data to generate a more accurate model… at the cost of complexity!  