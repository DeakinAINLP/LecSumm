In this topic mainly focus on Fundamentals of Supervised Learning, such as model complexity, model assessment, as well as model selection. Most practical machine learning applications use supervised learning.  In supervised learning, the data used to train the algorithm is already labeled with correct answers.  Supervised learning can appear in many forms:    Regression problems   Linear Regression (linear model)   Logistic Regression (linear model)  Classification problems    Support Vector Machines (both linear and nonlinear)   Decision Trees (nonlinear)   Random Forest (nonlinear)   Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  Ranking problems  In supervised learning, model complexity refers to the degree of flexibility or expressiveness of the model used to predict the output or response variable based on the input or predictor variables. A more complex model is generally able to fit the training data better, but may also overfit the data, leading to poor performance on new, unseen data.  Model complexity in supervised learning is often characterized by the number of parameters or features used in the model. For example, in linear regression, model complexity is determined by the number of regression coefficients, while in decision trees, it is determined by the number of decision nodes and branches. In more complex models such as neural networks, the number of layers and neurons also contribute to the model's complexity.  Regression metrics are used to evaluate the performance of a regression model, which is a type of supervised learning model that predicts a continuous output variable based on one or more input variables. The most commonly used regression metrics include:  Mean Squared Error (MSE): This metric measures the average squared difference between the predicted values and the true values. A lower MSE indicates better model performance.     Root Mean Squared Error (RMSE): This is the square root of the MSE, which is a more interpretable metric as it is expressed in the same units as the output variable. A lower RMSE also indicates better model performance.  Mean Absolute Error (MAE): This metric measures the average absolute difference between the predicted values and the true values. Unlike MSE and RMSE, MAE is less sensitive to outliers in the data.  R-squared (R^2): This metric measures the proportion of variance in the output variable that is explained by the input variables. R-squared ranges from 0 to 1, with a higher value indicating better model performance.  Adjusted R-squared: This is a modified version of R-squared that penalizes for the number of input variables used in the model, which helps to prevent overfitting.  Mean Absolute Percentage Error (MAPE): This metric measures the average percentage difference between the predicted values and the true values. MAPE is commonly used in forecasting applications.  The choice of metric depends on the specific problem and the goals of the analysis. For example, if the goal is to minimize the prediction error, then MSE or RMSE may be more appropriate, while if the goal is to ensure that the model is accurate within a certain percentage, then MAPE may be a better choice.  In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.  This means the value of a hyperparameter in a model cannot be estimated from data. They are often used in processes to help estimate model parameters.            In this lesson we are addressing the following questions:  What is hyperparameter?  Why do we need to have hyperparameters?  How to find the best hyperparameter for a specific model?  Hyperparameters can often be set using heuristics Often they are tuned for a given predictive modelling problem. To search for the best hyperparameters, we need to partition training data into separate training and validation sets.  Finally, we learnt how to build the algorithm using python commands.       