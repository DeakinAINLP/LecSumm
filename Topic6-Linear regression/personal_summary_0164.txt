Supervised learning is a form of machine learning technique in which a system trains to make predictions or judgements from labelled training data. The method used in supervised learning is given input information along with the right output labels, and its purpose is to discover the underlying patterns or correlations between both the input and the output variables.  Model complexity refers to a machine learning model's level of sophistication or flexibility in identifying the fundamental trends or connections in the data. A complicated model has a greater capability for learning difficult patterns and can operate higher on training data. However, there is a trade-off among the complexity of the model and overfitting risk.  When a model grows too complicated, it begins to memorise the noise or quirks in the training data rather than learning generalizable patterns. As a result, the overfitted model underperforms on fresh, previously unknown data. However, if a model is overly simplistic and unable to grasp the underlying intricacies, it may suffer from underestimating and fail to understand the crucial correlations in the information being analysed.  Model complexity and Occam's razor are ideas that are closely connected in machine learning and choosing a model. Occam's razor is a philosophical and scientific idea that states that when deciding among various explanations or models, the one that comes with the fewest assumptions or complications is likely to be the best option.  In the setting of machine learning, Occam's razor instructs us to choose simpler algorithms over more complicated ones if their performance is equivalent. In this sense, simplicity means models with fewer parameters, beliefs, or complexity in their structure.  Structural Risk Minimisation (SRM) is a machine learning and model selection principle that tackles the trade-off between model complexity and data availability. It seeks to strike a compromise between accurately fitting the training data and avoiding overfitting.  SRM is founded on the assumption that simpler models are preferred over complex ones, however the quantity of data available for training should also be considered while choosing model complexity. Vladimir Vapnik and Alexey Chervonenkis proposed the idea in the framework of statistical learning theory.  Classification metrics are assessments that are used to evaluate the performance of a machine learning model that is employed to perform classification tasks. These metrics give information on how successfully the model classifies or predicts distinct categories or groups based on the provided data.   