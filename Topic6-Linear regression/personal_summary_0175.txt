 This topic we started of  with Fundamentals of Supervised Learning. It was about differentiate supervised learning from unsupervised learning , estimate the performance of different supervised learning models and implement model selection and compute relevant evaluation measures. It was about forms of supervised learning and application of it. Linear and nonlinear boundary decision making and also about the supervised learning algorithm. Model of complexity , The training data may not always be visible in high dimensions. Therefore, we might not be aware of the linearity or non-linearity of the regression problem. Similar to this, we might not be able to determine if the classification problem can be solved linearly or not.  We would be over-fitting the data if we chose a higher level of complexity than is necessary. We would be under-fitting the data if we choose a lower level of complexity than is necessary. Gaining the optimum fit is crucial for accurate generalisation.  Model complexity and Occam's razor, During the creation of theoretical models, Occam's Razor, a well-known problem-solving principle, is used as a heuristic guide.  We studied about Structural risk minimisation, Therefore, we define a new risk value called Structural Risk based on Occam's razor and its straightforward principle.  Structural risk minimisation employs a penalty on the complexity of the model that favours simpler functions over more complex ones in an effort to avoid over-fitting. Therefore, the main goal is to reduce both structural risk and empirical risk.   Classification metrics, It's crucial to carefully consider the metrics you use to assess your machine learning model. The evaluation measures selected affect how performance is assessed and contrasted.  Classification issues are the most typical applications of machine learning. Predictions for these kinds of issues can be assessed using a wide range of measures.  Regression Metrics, it was about ,How far the expected value is from the actual value as measured using regression? Regression is not typically associated with plotting a collection of numbers against a line.  Partitioning data for training and testing, there were we few limitations which understood that is Some outlier instances (i.e., noisy observations) may have an impact on a single training set. We require a sizable test set in order to obtain an accurate estimate of model performance (accuracy). Why? due to the minimal variance of such an estimate. However, we are aware that the training set's size affects how accurately the model can be learned. We can reuse the same data for both training and assessment in various splits by using multiple training/test splits.  We got to know about finding best hyperparameters and effect of imbalanced classes and we even explored the packages for supervised learning in python and also about fitting the regression line and at last we got to know about the evaluating the model and the data size and regression error.  