Topic 5 Key Learning Points Supervised Learning: In supervised learning, the data used to train the algorithm is already labeled with correct answers.  Instead of finding patterns based on similarity only, we can learn a direct mapping or function between feature vector and the output (target or label).  Supervised learning can appear in many forms:    Regression problems  o  Linear Regression (linear model) o  Logistic Regression (linear model)    Classification problems  o  Support Vector Machines (both linear and nonlinear) o  Decision Trees (nonlinear) o  Random Forest (nonlinear) o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)    Ranking problems  Hypothesis space  We will name a hypothesis function, ℎ, as an element of a range of possible functions H.  Let us come back to our main problem. We would like to find the function ℎ which can map the input to the corresponding output ℎ:x→y accurately, to take the values from set x to set y  Loss Function:  The loss function is really a measure of accuracy. How accurately does your ℎ function describe the relationship between X to the target Y ?    Square Loss, penalizes larger differences, so it would be more sensitive for outlier   Absolute Loss would treat both large and minor differences more consistently  Empirical risk  Like the loss function, we can define empirical risk.  Model Complexity Concept       If we choose higher complexity than necessary, we would be over-fitting the data (you will review over-fitting later in this course). If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalisation.  Structural risk minimisation So, based on Occam’s razor and its simplistic principle, we define another risk value, Structural Risk.  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. So the general idea is to minimise both Structural Risk and Empirical Risk which we introduced before:     Where C(ℎ) is the complexity of hypothesis function ℎ and lambda is a penalty parameter.  Confusion Matrix A confusion matrix is a summary of prediction results on a classification problem.    The number of correct and incorrect predictions are summarized with count  values and divided down by each class.    Confusion matrices are a way to understand the types of errors made by a model.  Confusion matrices are also called contingency tables.    True Positive Rate (TPR) or Recall or Sensitivity:  is the fraction of true positive  samples that have been predicted positive over the total amount of positive samples (TP+FN).    False positive rate (FPR):  is the fraction of false predicted positive samples over  the total amount of negative samples ).        ROC Curve Receiver Operating Characteristics (ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels. Recent years have seen an increase in ROC graph use in the machine learning community. ROC curve is especially useful for domains with imbalanced class distribution and unequal classification error costs.       F-1 Measure Another useful metric could be the combination of Precision and Recall.  F1-measure is a metric that combines both Precision and Recall in a single number.  F1-measure is defined as:  Regression Metrics Regression is a strange word for a simple concept.  Regression measures how far the expected value is from the actual value  Data partitioning for Train and Test  We usually work with 3 methods for splitting data:  random subsampling     stratified sampling   cross validation.            Imbalanced classes  Solutions We have two approaches to follow. First, we can perform some actions on the data itself. Alternatively, we can improve our algorithm to be able to handle such phenomenon. Let’s look at these two approaches  At the data level: (Re-Sampling)    over-sampling the data from minority class   under-sampling the data from majority class.     