Lesson Review: Supervised Learning Module Learning Objectives  1.  Differentiate supervised learning from unsupervised learning. 2.  Estimate the performance of different supervised learning models. 3.  Implement model selection and compute relevant evaluation measures.  Summarising the content: Supervised Learning – is a method of training an algorithm using known relationships between the inputs and outputs. The data used to train is already labelled with the correct answers, allowing to develop a mapping function from the input to the output.  Hypothesis  space  –  a  range  of  possible  hypothesis  function  that  we  think  is  similar  to  the  true function behind the data.  Loss  Function  (measure  of  accuracy)  –  how  accurately  does  the  hypothesis  function  describe  the relationship between the result and the target. A loss function is used to compute the error between the actual result of yi, and the calculated result as yi.  Empirical Risk – Among all functions in hypothesis space, the function is selected that minimises the empirical risk. This can be calculated by averaging the results of the loss function. The lower the risk based on the training data, the closer the function represents the true relationship between the pair.  Model complexity – is a measure of how accurately a machine learning model can predict unseen data, as well as how much data the model needs to see in order to make good predictions. Choosing  higher  complexity  than  necessary  can  introduce  overfitting,  whilst  having  a  lower complexity can cause under-fitting of the data.  Occam’s razor – a heuristic guide that recommends selecting the theory that introduces the fewest assumptions and has the least complexity.  Structural Risk minimisation – seeks to prevent over-fitting by incorporating a penalty on the model complexity so that it prefers simpler functions over more complex ones.  Confusion  Matrix  also  called  contingency  tables  –  are  a  summary  of  prediction  results  on  a classification problem. The number of correct and incorrect predictions.  Accuracy – is the measure or amount of correct predictions in the samples.  True Positive Rate (Recall) – The amount of true positive samples in the total positive samples.  False positive Rate (FPR) – amount of falsely predicted samples in the total negative samples.  Receiver Operating Characteristics (ROC) curve – used in signal detection theory to depict the trade- off between the true positive rate and false positive rate over noisy channels.  Tom O'Connor [SID: 217 602 149]  Regression Metrics – Mean Square Error – used to measure how close the prediction are to the true target values;  Explained  Variance,  or  Coefficient  of  determination  (R2)  –  is  measured  as  the  percent  of  target variation that is explained by the model. Where 0% represents a model that does not explain any of the variation in the response variable around it mean. 100% represents a model that explains all of the variation in the response variable around its mean.  Hyperparameter  –  are  parameters  whose  values  control  the  learning  process  and  determine  the values  of  model  parameter  that  a  learning  algorithm  end  up  learning.  Finding  the  best hyperparameters  can  be  done  through  cross  validation  of  a  subset  of  the  training  data.  Methods include, grid-search, random search, Bayesian optimisation.  