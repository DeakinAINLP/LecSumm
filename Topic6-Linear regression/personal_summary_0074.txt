Evidence of learning Summary of this topic Supervised learning Supervised learning is the task of estimating a function from labelled training data instead of finding patterns based on similarity only (as in unsupervised learning).  Forms of supervised learning:    Regression problems  o  Linear Regression (linear model) o  Logistic Regression (linear model)    Classification problems  o  Support Vector Machines (both linear and nonlinear) o  Decision Trees (nonlinear) o  Random Forest (nonlinear) o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)    Ranking problems  The goal of supervised learning is to find a function, h, that‚Äôs as close as possible to the unknown function which maps the values from training data sets X to Y.  Loss, complexity, and risk Loss function Loss is a function that measures how accurately a function, h, is in producing an output from a given output.  Here are some examples of loss functions:  -  Square loss (useful in regression):  -  Absolute loss (useful in regression): - - -  Hinge loss (useful for classification)  0-1 loss (useful for classification): 0 if both inputs are equal, otherwise 1 Logistic loss (useful for classification)  Empirical risk Empirical risk is defined as the average result of the loss function across all values in the training data:  The lower the empirical risk, the better the function being analysed represents the true relationship between two values in the training data.  A supervised learning algorithm is often trained through an optimization algorithm, which aims to find the function with the minimum empirical risk:     Model complexity  Since it is not possible to visualize data in higher dimensions, we may not know if the ideal decision boundary is linear or non-linear. Similarly in other supervised learning problems, we may not know if it would be better to use a complex or simple model.  It is important to choose a model with the best possible fit:  -  -  If we choose a model with a higher complexity than necessary, we would be overfitting the data. If we choose a model with a lower complexity than necessary, we would be underfitting the data  -  We need to choose a model with the best possible fit to get a good generalisation.  Occam‚Äôs razor ‚ÄúAll other things being equal, the simplest solution is the best.‚Äù  If there are multiple solutions available that are equal, the simplest solution is best. In machine learning this means selecting the least complex model if multiple models are equal to each other.  Structural risk Structural risk is a function that aims to prevent over-fitting by introducing a penalty for complex functions:  Where:  -  Rstr(h) is the structural risk -  Remp(h) is the empirical risk -  h is the hypothesis function     -  C(h) is the complexity of the hypothesis function -  Œª is the penalty parameter  The ideal hypothesis function is one that minimizes this definition of structural risk.  Classification metrics Confusion matrix/contingency table A summary of prediction results on a classification problem. The elements on the main diagonal of the confusion matrix represent the correct prediction for the given problem and any element on the off-diagonals represents cases where the classifier made a mistake.  We use confusion matrices because accuracy is not always a reliable metric for the real performance of a classifier.  For example, if the training data was unbalanced where there are significantly more observations for some classes over others such as 90 apples and 10 oranges, then the classifier may simply classify all objects as apples.  Two class confusion matrix    In this case we define accuracy as the sum of true positives and true negatives over the sum of all observations:  Since accuracy is not a useful metric for imbalanced class problems and for problems where some prediction classes may be less ideal than others (e.g.: incorrect medical diagnosis), we define additional metrics for classification:  True positive rate (TPR)/recall/sensitivity: the number of predicted true positive samples over the total number of positive samples.  False positive rate (FPR): the number of predicted false positive samples over the total number of negative samples.  Precision: the number of predicted true positive samples over the total number of predicted samples.  ùíëùíìùíÜùíÑùíäùíîùíäùíêùíè =  ùëªùë∑ ùëªùë∑ + ùë≠ùëµ  Receiver Operating Characteristics (ROC) curve In signals theory ROC has been used to depict the trade-off between the true positive rate and the false positive rate over noisy channels.  Useful for datasets with unbalanced class distribution or unequal classification error costs.  The ROC curve is created by plotting the true positive rate against the false positive rate at various threshold settings to depict the trade-offs between the benefits (true positives) and the costs (false positives).  Examples of ROC curves:      The closer the algorithm is to having 1.0 TPR and 0 FPR, the more ideal it is. Random guessing on average should result in the same FPR and TPR, which is depicted by the blue curve on the graph.  The tolerance towards false negatives to false positives will be different depending on our use case. For example, a medical diagnosis model cannot tolerate false negatives (i.e.: failing to diagnose), but false positives are ok. In justice, it is worse to wrongfully convict an innocent person (false positive) than it is to wrongfully declare a criminal innocent (false negative).  Statistics that use ROC:    -  Area under curve (AUC) -  Youden index  F-1 measure A combination of precision and recall.  Regression metrics Mean square error Measures how close each prediction is to the target value.  Root mean square error  Mean absolute error Is resilient to outliers in the test set due to using 1-norm of the error (i.e.: Manhattan distance).  Explained variance (R2) Known by several names including:  -  R-square - -  Explained variance The coefficient of determination  R-square is measured as the percentage of dependent variable variation that is explained by the model. Higher percentages indicate better performance.  It is defined as:  When the regression model accounts for more of the variance, the data points are closer to the regression line and the R2 value for the model increases.       An R2 score of 0% would mean the regression model does not explain any of the variance of the data around its mean while an R2 score of 100% means the regression model explains all the variance of the data around its mean.  Partitioning data for training and testing A limitation of a single training/test set is that it may be affected by outliers and noise.  Large training/testing sets are ideal, because it lowers the variance of the model‚Äôs estimated output and increases its accuracy.  Multiple training/testing sets allow us to re-use the same data for both training and evaluation but in different splits.  Methods for splitting data Sub-sampling  The data is randomly partitioned into random training test sets in a specified ratio. This is one approach  Stratified sampling  Used to ensure that the proportion of the training and test datasets are representative of the original dataset.  Stratified sampling works by first dividing the dataset into strata where metrics such as class distribution, mean, standard deviation are maintained. The specific metrics used to create strata depends on the problem being solved, but usually class distribution is used.    Samples are randomly taken from the strata to create the final training/test datasets.  Cross-validation  Cross validation works by taking k samples out of the dataset, training the model on all but one of the samples, then evaluating it on the sample that was left out. This process is repeated k times until each sample has been used as the evaluation sample once.  This form of sampling is called k-fold cross validation.    Finding the best hyperparameters Hyperparameters are values that are set before the machine learning process begins. For example, the k number of clusters chosen in the k-means clustering algorithm. Hyperparameters can significantly affect the performance of the model so finding the optimal hyperparameters is important.  Often, the most optimal hyperparameter are not known ahead of time and must be found using an optimization algorithm.  Some examples of this are:  -  Grid-search (not efficient): An exhaustive search that iteratively tries different possible  hyperparameter combinations from the space of possible hyperparameters. The number of combinations to try increases exponentially as the precision of the grid increases.  -  Random search (efficient under certain conditions): A variant of the grid search where the  hyperparameters are chosen at random.    -  Bayesian optimization (efficient in general): Uses Bayes theorem to help find the optimal  hyperparameters.  When training models the data is partitioned into a training and testing set. To find the optimal hyperparameters the training set is further split into a training and validation set, where the model‚Äôs hyperparameters are selected using the training set and evaluated using the validation set.  Effects of imbalanced classes Ideally the number of samples belonging to each class should be equal. When they aren‚Äôt equal some problems can occur such as:  -  A classifier always classifying everything to the majority class to achieve a high accuracy. This  can be solved by using other evaluation metrics instead of accuracy  -  When using random sub-sampling, it is possible that the class proportion will not be  maintained in an individual partition or the minority class could be excluded. This can be solved by using stratified sampling.  Solutions:  -  Under-sampling the data from the majority class -  Over-sampling the data from the minority class -  Adjust the costs of the majority class to mitigate their effect -  Adjust the decision threshold to cope with unbalanced data  