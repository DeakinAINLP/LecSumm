During this lesson forms of Supervised Learning algorithms, Examples, Model Complexity, Data partitioning schemes, and how to find the best hyperparameter and effect of imbalance class distribution.  Supervised Learning  Supervised learning is used with already labelled data. The model is trained with labelled data and tries to find a function that can map the features (input data) to the class label (output data).  yi =h (xi )  Forms of supervised learning:  Regression Problem  Classification Problem  Ranking Problem  Regression problems and Classification problems can be linear or nonlinear.  We can select whether to use linear or nonlinear regression based on our data.  When the number of features increases, dimensionality increases and the pattern gets away from the linear pattern.  Depending on the data we have, we have to find a linear or nonlinear decision boundary.  We get a lot of functions to map features to class labels. We need to find the best function which is accurate from those functions.  Loss function:  We have a data set. We divide the data set into a training data set and a test data set. Then we train the model with the training data set and evaluate that model with the testing data set. As we already know the output of the data in the testing data set, we can calculate the difference between the generated output and the true output with a LOSS Function.  There are several Loss functions such as Square loss, Absolute loss, 0-1 loss logistic loss and hinge loss.  The average of the loss function is the average error of a model on a training data set. It measures how well the model fits the data and how the model is trained on it. We are trying to minimize the empirical risk and find the best model. Sometimes we increase complexity and this can cause overfitting. This may not be the best solution for all solutions. We need to find the function which has the minimum Empirical risk.  Model Complexity  Model complexity differs with the number of features we select and with dimensionality. Let’s see how model complexity affects when selecting different models.  Higher complexity than necessary → Overfitting the data  It shows higher accuracy on the training data when trained. But when evaluated with the test data, the model is not performing well.  Lower Complexity than necessary → Underfitting the data  It is not giving good results when the model is trained with training data and when it is evaluated with testing data.  Good mode → Generalisation  Gives good results (model is working well) with training data and testing data.  “All other things being equal, the simplest solution is the best”  We must need fewer parameters as low as we can with the model working properly, it reduces the risk of overfitting.  Structural Risk  Minimizing the structural risk means minimizing the empirical risk and minimizing the model complexity.  Model Evaluation Metrics  Evaluating the model on different metrics is important. There are two types of matrices.  1. Classification metrics  a. Confusion Matrix b. ROC Curve c. F-1 Measure  2. Regression metrics  a. Mean Square Error b. Explained Variance  Partitioning Data  Data can be partitioned using the following methods  ● Random sub-sampling  ○ Doesn’t consider the pattern of data. ○ Put all data together and divide ○ Challenge- Training and testing data sets can be imbalanced.  ● Stratified Sampling  ○ This method can be used for imbalance data ○ The original pattern of data can be found in Training and Testing data sets. ○ Partition based on the proportions of data in the original set.  ● Cross Validation  ○ Can use even when data is biased ○ Iteratively selecting training data set and testing data set. ○ Get average performance ○ The model has a chance to get tested on the whole data set.  Finding Best hyper-parameters  Hyper-parameter is a parameter whose value is set before the learning process begins. This goes with trial and error. We need to assign a value and then train the model. We need to find the parameter which gives the accurate model. This is time-consuming and hard.  Hyper-parameter tuning methods can be used for this.  The validation set - is used to evaluate the performance of the model with different parameters.  Now we have a Training set, Validation set and Test set.  Decide possible hyper-parameters, Define a search grid within the specific range.  We train the model using each hyper-parameter value from the search grid.  We use the validation set to assess the performance of the model.  Finally, select the hyper-parameter with the best performance.  