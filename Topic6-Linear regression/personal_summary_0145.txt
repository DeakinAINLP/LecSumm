 In topic 5, we focused on some key concepts important for supervised learning including model complexity, model assessment, as well as model selection.  Supervised learning can appear in many forms:  ▪  Regression problems  o  Linear Regression (linear model)  o  Logistic Regression (linear model)  ▪  Classification problems  o  Support Vector Machines (both linear and nonlinear)  o  Decision Trees (nonlinear)  o  Random Forest (nonlinear)  o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  ▪  Ranking problems  In supervised learning to figure out the relationship between the pairs of numbers we can use hypothesis space. Moreover, we learnt that linear regression's complexity is determined by the number of regression coefficients, while decision trees' complexity is determined by the number of decision nodes and branches.  Another famous problem-solving principle that is used as a heuristic guide in the development of theoretical models is Occam’s Razor. Also we learnt about confusion matrix which is  a metrics that choose to evaluate machine learning model.  To evaluate the performance of a regression model we can use Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), R-squared (R^2), Adjusted R-squared, and Mean Absolute Percentage Error (MAPE).  A hyperparameter is a parameter whose value is set before the learning process begins. Here, the model cannot be estimated from data. They are often used in processes to help estimate model parameters.    