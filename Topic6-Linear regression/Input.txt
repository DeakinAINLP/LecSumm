Evaluating our model We can see that there is some discrepancy between the truth and the predictions. How can we measure this discrepancy? Can we get a numerical estimate for this discrepancy? Code example #1 Feature correlation between the features and classes using Heatmap How features correlated with each other and with the target. For example target/class Sales correlation between different feature sets. sns.set(rc={'figure.figsize':(8,8)}) sns.heatmap(df_adver.iloc[:,0:-1].corr().round(2), square=True, cmap='RdYlGn', annot=True) The output would look like this: One measure that gives the deviation from the truth is the mean square error. As we have introduced this topic, Mean Square Error (MSE) measures how well our prediction fits the true data. If the prediction = truth,  MSE=0{"version":"1.1","math":"\(MSE=0\)"}. Code example #2 Following on from the advertising python code that you have tested and saved, let’s find the MSE of this model. mse = np.mean((ypredicts - ytest) ** 2) print ("MSE: {}".format(mse)) The output would look like this: MSE: 2.55796592546 Code example #3 The model weights and intercepts can also be retrieved: print (my_model.intercept_) print (my_model.coef_) zipped_list =  (zip(feature_cols, my_model.coef_)) print(list(zipped_list)) The output would look like this: 3.0451422090371167 [ 0.04704868  0.17968299 -0.00300557] Data size and regression error What is the effect of data size on prediction performance? Lets find out. To make life easier, lets define a function which will take the following input: model to use: Linear regression in our case training data testing data training sizes: the different sizes of training data to use The model is trained on different sizes of training data, and each time it’s tested on the fixed testing data. Model weights and MSE for each training size is recorded and returned. Code example#1 Following on from the previous code from the previous step, let’s add some code to plot the change in MSE with training data of different sizes. def getErrorwithSize(model, train_sizes, X, y):     model_mse   = np.zeros(len(train_sizes))  # storing model accuracy     model_wts   = np.zeros([len(train_sizes), 4]) # storing model weights     #Train our model with increasing data for each iteration     for size in train_sizes:            Xsubtrain = X[0:size,:]         ysubtrain = y[0:size]         model.fit(Xsubtrain, ysubtrain)         Xtest=X[size:,:]         ytest=y[size:]         # Test our model on fixed test set         ypredicts = model.predict(Xtest)             index              = (size//10)-1                 model_mse[index]  = np.mean((ypredicts - ytest)**2)         model_wts[index,:] = np.append(model.intercept_, model.coef_)       return model_mse, model_wts There should be no output from this piece of code. Code example#2 We can now plot the change in model mse with respect to the increasing size of training data. data_size = np.arange(10,df_adver.shape[0]-10,10) # training size from 10 to number of samples-10 my_model = LinearRegression() model_mse, model_wts=getErrorwithSize(my_model, data_size,df_adver.iloc[:,0:-2].values, df_adver.iloc[:,-2].values) print(f"MSE ={model_mse.min():.2f}") plt.plot(data_size,model_mse,'o-') plt.ylabel("MSE") plt.xlabel("Data size") plt.show() The output would look like this: SEE ALSO Classification metrics The metrics that you choose to evaluate your machine learning model are very important. The choice of evaluation metrics influences how performance is measured and compared. The most common type of machine learning applications are classification problems. There are myriad metrics that can be used to evaluate predictions for these types of problems. Confusion Matrix A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class. Confusion matrices are a way to understand the types of errors made by a model. Confusion matrices are also called contingency tables. One of the reasons for using a confusion matrix is that, accuracy is not a reliable metric for the real performance of a classifier. If the data set is unbalanced (i.e. when the numbers of observations in different classes vary greatly), it will yield misleading results. For example, if there were 90 apples and only 10 oranges in the data set, a particular classifier might classify all the observations as apples. But is this wise? The following figure illustrates a sample confusion matrix for a classification problem with 11{"version":"1.1","math":"\(11\)"} classes. The diagonal values represents the elements where the predicted classes were equal to the expected classes, and the off-diagonal values represent the elements where the classifier got the prediction wrong. The higher the proportion of values on the diagonal of the matrix in relation to values off of the diagonal, the better the classifier is (why?). Figure. Confusion matrix for a classification problem. Now that we understand the fundamentals around confusion matrices, let’s explore more detailed concepts. Consider the following figure as a confusion matrix for only two classes. Figure. Confusion matrix for two classes. You could represent the positive class as class 1 and the negative class as class 0. For the acronyms used in the table (i.e. TP), the second letter (e.g. letter P in TP) says what we predicted and the first letter (e.g. letter T in TP) says whether it was true or false. In this case we define the accuracy as: accuracy=TP + TNTP + FP + FN + TN{"version":"1.1","math":"accuracy = \frac{TP \ + \ TN}{TP \ + \ FP \ + \ FN \ + \ TN}"} But as we have said before, accuracy may not be a useful metric for imbalanced class problems. Also There may be differential costs of making errors for different classes. For example, an incorrect medical diagnosis may be more costly than a false positive! So we need high confidence predictions only. Therefore, we can define other evaluation metrics based on a confusion matrix: True Positive Rate (TPR) or Recall or Sensitivity:is the fraction of true positive  (TP{"version":"1.1","math":"\(TP\)"}) samples that have been predicted positive over the total amount of positive samples (TP+FN). recall=TPTP + FN{"version":"1.1","math":"recall= \frac{TP}{TP \ + \ FN}"} False positive rate (FPR):  is the fraction of false predicted positive FP{"version":"1.1","math":"\(FP\)"} samples over the total amount of negative samples  (TN + FP{"version":"1.1","math":"\({TN \ + \ FP}\)"}). FPR=FPTN + FP{"version":"1.1","math":"FPR= \frac{FP}{TN \ + \ FP}"} Reporting all these metrics in a machine learning classifier, can shed some lights on the overall performance of the classifier. The next metric we are going to cover for classifiers, is ROC curve. ROC Curve Receiver Operating Characteristics (ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels. Recent years have seen an increase in the use of ROC graphs in the machine learning community. ROC curve is especially useful for domains with imbalanced class distribution and unequal classification error costs. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. This has to be done to depict relative trade-offs between benefits (true positives) and costs (false positives). As you can see in the figure below, different methods can work better in different parts of ROC space. Lets say there are two algorithms like Alg 1and Alg 2 in the figure. The Alg 2 is good in the sense that it can give you high true positive rate while keeping the false positive rate low. whereas in Alg 1, if it is allowed to incur more false positive rate, then Alg 1 can give us better higher true positive rate too. Figure. Sample ROC curve Obviously it depends on the specification of the problem. how much can we afford false positive rate. if we can afford higher false positive rate, we can have higher true positive rate too. A model that predicts at chance (random guessing) will have an ROC curve that looks like the diagonal dashed line in the figure above. That is not a discriminating model. The further the curve is from the diagonal line, the better the model is at discriminating between positives and negatives in general. Consider the following figure as another example. Lets say we are designing a classifier for a medical diagnosis. In this case we probably do not mind false positives since missing  positive occurrence in detection of diseases are extremely costly. Figure. Illustration of different scenarios in ROC curve. But, there can be situations where we do mind the false positive rate. A good example of that could be in conviction for a crime. You do not want to waste someones life with a false positive decision! There are useful statistics that can be calculated via ROC curve, like the Area Under the Curve (AUC) and the Youden Index. These tell you how well the model predicts and the optimal cut point for any given model (under specific circumstances). AUC is used to summarize the ROC curve using a single number. The higher the value of AUC, better performing is the classifier! A random classifier has an AUC of 0.5. F-1 Measure Another useful metric could be the combination of Precision and Recall.  F1{"version":"1.1","math":"\(F_1\)"}-measure is a metric that combines both Precision and Recall in a single number.  F1{"version":"1.1","math":"\(F_1\)"}-measure is defined as: F1=2×Precision×RecallPrecision+Recall{"version":"1.1","math":"\(F_1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}\)"} Activity How comprehensive is a confusion matrix for evaluating a model? Can you think of an example where a confusion matrix is appropriate? Regression Metrics Regression is a strange word for a simple concept. Regression measures how far the expected value is from the actual value? The word, ‘regression’ doesn’t really bring to mind measuring a set of values against a line. However the term was applied to the concept by a 19th century scientist, Sir Francis Galton. He was working on how tall children were in relation to their parents and how closely their heights ‘regressed’ towards the average height at the time. The term stuck. Read the story if you’re interested. Now for the mathematical aspect. Measuring regression performance What are the ways of measuring regression performance? Mean Square Error To measure how close the predictions are to the true target values, Mean Square Error (MSE) is a popular measure. MSE is defined as: MSE=1n∑i=1n(yi−y^i)2{"version":"1.1","math":"MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2"} Derived from MSE, Root Mean Square Error (RMSE) is also popular and is computed as: RMSE=1n∑i=1n(yi−y^i)2{"version":"1.1","math":"RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}"} Clearly, the lower the MSE of a model, the better its performance. Similar to MSE, Mean Absolute Error (MAE) is defined as: MAE=1n∑i=1n|yi−y^i|{"version":"1.1","math":"MAE = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i|"} Due to using 1−{"version":"1.1","math":"\(1-\)"}norm of the error, MAE is robust to outliers in the test set. Same as MSE and RMSE, the lower the MAE of a model, the better its performance. Explained Variance (R2{"version":"1.1","math":"\(R^2\)"}) This measure is known by many names including: R-square Explained variance the coefficient of determination R-square is measured as the percentage of target variation that is explained by the model. For linear regression with bias term, R-square is the square of the correlation between the target values and the predicted target values. Unlike the other introduced metrics, the higher the R-square of a model, the better its performance. As we said, R-squared is the percentage of the dependent variable variation that a linear model explains: R2=Variance Explained by the modelTotal variance{"version":"1.1","math":"R^2 = \frac{Variance\ Explained\ by\ the\ model}{Total\ variance}"} R-squared is always between 0{"version":"1.1","math":"\(0\)"} and 100%{"version":"1.1","math":"\(100\%\)"}: 0%{"version":"1.1","math":"\(0\%\)"} represents a model that does not explain any of the variation in the response variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model. 100%{"version":"1.1","math":"\(100\%\)"} represents a model that explains all of the variation in the response variable around its mean. Figure. Illustration of regression for two different test cases.(Frost 2017) Consider the above figure as illustration of regression in two cases. The R-squared for the regression model on the left is ≤20%{"version":"1.1","math":"\(\leq 20\%\)"}, and for the model on the right it is≥80%{"version":"1.1","math":"\(\geq 80\%\)"}. When a regression model accounts for more of the variance, the data points are closer to the regression line. Activity Check out the following interesting example of MSE. Share your thoughts in the discussion forum. This is an additional video, hosted on YouTube. Finding the best hyperparameters In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. This means the value of a hyperparameter in a model cannot be estimated from data. They are often used in processes to help estimate model parameters. In this lesson we are addressing the following questions: What is hyperparameter? Why do we need to have hyperparameters? How to find the best hyperparameter for a specific model? Hyperparameters can often be set using heuristics Often they are tuned for a given predictive modelling problem. To search for the best hyperparameters, we need to partition training data into separate trainingand validation sets. Figure. Training set, validation set and test set. We already know about training and test data. But what is a validation set? A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The validation set is used to evaluate a given model and also to fine-tune the model hyperparameters. So, given a choice of hyperparameter values, you use the training set to train the model. But, how do you set the values for the hyperparameters? That’s what the validation set is for. You can use it to evaluate the performance of your model for different combinations of hyperparameter values (e.g. by means of a grid search process) and keep the best trained model. However the test set allows you to compare different models in an unbiased way, by basing your comparisons in data that were not use in any part of your training/hyperparameter selection process. Here is another example. Remember Kmeans in course 2? The number of clusters (k{"version":"1.1","math":"\(k\)"}), is a hyperparameter. Because this value is set before the learning begins. But, how can we find the best hyperparameter? First, we need to decide a possible range for hyperparameters. For example, a bounded interval such as [0,1]{"version":"1.1","math":"\([0,1]\)"} We then define a search grid within the specified range.For example, we might like to select these values {0,10−3,10−2,10−1,1}{"version":"1.1","math":"\(\{0,10^{-3}, 10^{-2}, 10^{-1}, 1\}\)"} for as hyperparameters in order to evaluate the model with them. Next, we train a model using each hyperparameter value from the search grid and assess its performance on a validation set (separated from the training set). Finally, we compute the performance on the validation set for each hyperparameter value and select the one with the best performance. Once the model is working with the best hyperparameter we defined it’s ready to be tested on separate test data. Note: In the above example we only considered 4 cases to evaluate as a hyperparameter in the [0,1]{"version":"1.1","math":"\([0,1]\)"} interval. In this continuous space, we may lose many other good options by restricting the search to this degree. We can extend the grids for search to very small continuous values such as {0,10−5,2×10−5,3×10−5,...,0.99999,1}{"version":"1.1","math":"\(\{0,10^{-5},2\times10^{-5},3\times 10^{-5},...,0.99999,1\}\)"}.  This will result in 100000{"version":"1.1","math":"\(100000\)"} possible values for the hyperparameter. It is obvious in this case we will be more accurate in finding the best value of the hyperparamter but this kind of grid-searching can be extremely computationally expensive (i.e. may take your machine a long time to run). A grid-search will build a model on each possible value for hyperparameter. Internal cross-validation All the techniques that we previously discussed for model assessment are applicable for training/validation set splitting: Random subsampling Stratified subsampling Cross-validation We are still assessing how a particular hyperparameter is doing on the validation set. Remember, this step is internal to the learning process and different from model assessment on the test data. Let us examine how an internal cross-validation works. Instead of using a single validation set, we can use cross-validation within a training set to select the best set of hyperparameters. So basically it is exactly the same as the one we saw for test/train partitioning. However, in here we partition the data into training/validation sets. The following figure illustrates this process. Figure. Internal cross-validation within a training set. Lets work on another example of that. Say, we want to do 10{"version":"1.1","math":"\(10\)"}-fold Cross-validation to estimate the model performance of Elastic Net model. We can divide the data into 10{"version":"1.1","math":"\(10\)"} equal subsamples and then train the model using 9 subsamples and test the model using the 10th{"version":"1.1","math":"\(10th\)"} subsample. We repeat this  10{"version":"1.1","math":"\(10\)"}-times using each subsample for the test purpose and all other subsamples for the training. In the above train the model step, best hyperparameter can be selected using an internal cross-validation. If we want to use 5{"version":"1.1","math":"\(5\)"}-fold cross-validation for this. Then for each possible hyperparameter set, we compute 5{"version":"1.1","math":"\(5\)"}-fold cross validation (CV) accuracy and select the best hyperparameter set. So in this example, we have an external 10{"version":"1.1","math":"\(10\)"} -fold cross-validation for partitioning training/testing. Also we ran a5{"version":"1.1","math":"\(5\)"}-fold cross-validation for partitioning training/validation inside the training set for finding the best hyperparameters. Remember that we can select the best hyperparameter set by searching/or optimizing over all possible values. Let us show you 3 possible ways to navigate the hyperparameter space: Grid-search (not so efficient). This is what we are using and explaining! Random search (efficient in certain scenarios) [Bergstra et al. (JMLR 2012) Bayesian optimization (efficient in general) [Snoek et al. (2012)] For better understanding, you can read this article on hyperparameter tuning. Activity For any given application, list the criteria that will help you to decide whether you will use Grid-search or go for an alternative (e.g. Bayesian optimization) Share in the discussion forum. Effect of imbalanced classes One problem that can occur in machine learning is datasets where the total number of one class of data (i.e. positive outcomes) is far less than the total number of another class of data (i.e. negative) outcomes. This problem is very common in practice and can be detected in various disciplines including fraud detection, anomaly detection, medical diagnosis, etc. As you know most machine learning algorithms work best when the number of instances of each classes are roughly equal. When the number of instances of one class far exceeds the other, problems arise. Example: When developing a breast cancer diagnosis model, imbalanced class problem is encountered because the risk of a female being diagnosed with breast cancer (by their 85th{"version":"1.1","math":"\(85th\)"} birthday) is 1{"version":"1.1","math":"\(1\)"} in 8{"version":"1.1","math":"\(8\)"}. This means that a representative training set will have 7{"version":"1.1","math":"\(7\)"}  times more instances in negative class than the positive class (see figure below). Figure. Sample imbalance class distribution. Almost 900{"version":"1.1","math":"\(900\)"} negative samples vs. almost 100{"version":"1.1","math":"\(100\)"} positive samples. So what are the possible solutions to overcome this problem? Solutions We have two approaches to follow. First, we can perform some actions on the data itself. Alternatively, we can improve our algorithm to be able to handle such phenomenon. Let’s look at these two approaches At the data level: (Re-Sampling) over-sampling the data from minority class under-sampling the data from majority class. Two obvious solutions based on data manipulation which suggests that we can sample more data points from the minority class in order to cover the difference. Or we can under-sample the majority class in order to make them have an equal effect on the algorithm. At the algorithmic level: adjusting the costs adjusting the decision threshold. From an algorithmic point of view, we may want to adjust some costs on the points we are observing from the majority class in order to dampen their effect. Also we can manually define some thresholds to cope with the unbalanced data. Issues of imbalanced classes Now, let us have a close look on possible issues of imbalanced classes. Problem-1: Since the test data contains only few samples from the minority class, even a dumb classifier that always classifies an instance to the majority class will get very high accuracy! This problem is dealt with by using other evaluation metrics in place of accuracy. Problem-2: When doing random subsampling, it is possible that class proportion is not maintained in an individual partition. In fact, we may not sample even one instance from the minority class. This problem can be solved using Stratified Sampling. But always remember: Any pre-processing over an entire data set (e.g. feature selection, or feature extraction) must not use the information that you are trying to predict (e.g. labels). During the training process, you must not use any information that is not available during the training process. Example: I was building a cancer prognosis model to predict whether a patient will survive 1{"version":"1.1","math":"\(1\)"} year from diagnosis or not? By mistake, I used the cause of death field as one of the features. Clearly, this information is not available at the prediction time. If you modify your model again and again by looking at how it performs on a specified test set, then you may be overfitting on the test set. Activity You have covered a lot of ground on how to assess a trained model. Let’s now investigate the details on model selection. The first question that might come to mind might be the limitations of using only a single training/testing set. A single training set may be affected by some outlier instances (i.e. noisy observations). To get a reliable estimate of model performance (accuracy), we need a large test set. Why? Because variance of such an estimate is low. However, we know that the larger the size of the training set, the more accurately the model can be learnt. Multiple training/test splits allow us to re-use same data for both training and evaluation in different splits. We usually work with 3 methods for splitting data: random subsampling stratified sampling cross validation. Lets start with Random sub-sampling. Sub-sampling Instead of using a single split, a more reliable estimate of model performance can be obtained by random sub-sampling. Random sub-sampling repeatedly partitions the data into random training and test sets in a specified ratio. Figure. Sub-sampling. As you can see in the figure, We train the model with each training set and estimate an accuracy using the corresponding test set. We finally average the accuracies to get an averaged estimate. Stratified Sampling Stratified sampling is a probability sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata. When using randomly selecting training (or validation) sets, class proportions may differ between training and test splits. Stratified sampling ensures that class proportions are maintained in each random set. The figure below shows how Stratified Sampling works. As you can see it first separates (stratifies) instances by class label, then randomly selects instances from each class. Figure. Stratified Sampling by 50/50 stratified split. The figure below illustrates another example of Class-wise random selection in a specified split ratio. In the left image we can see 2 samples are selected from the red class and 1 from blue class. Same as the right image which shows a ratio of 1:2:1{"version":"1.1","math":"\(1:2:1\)"}  for blue, red, and green respectively. Figure. Class-wise random selection in specified split ratio. Cross-validation Another method for partitioning data which is even more popular among researchers is Cross-validation. This is a technique to evaluate models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. The main idea is to partition training data into k{"version":"1.1","math":"\(k\)"} equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples. The following figure, illustrates this process. Figure. Cross validation, partitioning data into k=5{"version":"1.1","math":"\(k=5\)"} equal sized subsamples. Now suppose we have 1000{"version":"1.1","math":"\(1000\)"} instances, and we want to estimate accuracy using cross-validation. Lets define k{"version":"1.1","math":"\(k\)"} to be k=5{"version":"1.1","math":"\(k=5\)"}. At first iteration, we are going to leave out the first sub-sample (S1{"version":"1.1","math":"\(S_1\)"}) for testing, so we will use S2,S3,S4,S5{"version":"1.1","math":"\(S_2,S_3,S_4,S_5\)"} for training the model. After training the model with S2,S3,S4,S5{"version":"1.1","math":"\(S_2,S_3,S_4,S_5\)"}, S1{"version":"1.1","math":"\(S_1\)"} will be used to calculate the accuracy of the trained model. We save this accuracy for the first iteration. In the second iterationS2{"version":"1.1","math":"\(S_2\)"} will be used for testing and S1,S3,S4,S5{"version":"1.1","math":"\(S_1,S_3,S_4,S_5\)"} sub-samples are used for training the model. Again the accuracy on test data which is S2{"version":"1.1","math":"\(S_2\)"},  will be saved. We will keep doing this for k=5{"version":"1.1","math":"\(k=5\)"} times. The final accuracy will be the average of these 5 obtained accuracies. We call this way of data partitioning, k-fold cross-validation. In k-fold cross-validation, the original sample is randomly partitioned into k{"version":"1.1","math":"\(k\)"} equal size sub-samples. Of the k{"version":"1.1","math":"\(k\)"} sub-samples, a single sub-sample is retained as the validation data for testing the model, and the remaining k−1{"version":"1.1","math":"\(k-1\)"} sub-samples are used as training data. The following figure illustrates how this procedure works. Figure. 5-fold cross validation. So, to conclude: When using subsamples, we call it k-fold cross-validation. In special cases, when k{"version":"1.1","math":"\(k\)"} is equal to the number of instances n{"version":"1.1","math":"\(n\)"},   we call it as leave-one-out cross validation scheme. Cross-validation makes efficient use of the available data for testing. View transcript SPEAKER: In this tutorial, we're going to show you different methods of partitioning for test and train data. The first method is random sampling. In this method, you should repeatedly partition the data into random training and test sets in a specified ratio. As you can see in this figure, we are randomly separating or partitioning this data into train and test sets. Also, we are always following a specified ratio. Then we train the model with each training set and estimate an accuracy using the corresponding test set. And finally, we average the accuracies to get an average estimate. The next method is stratified sampling. In a stratified sampling, we are randomly selecting training or validation sets. Class properties may differ between training and test [? displays. ?] Stratified sampling ensures that class proportions are maintained in each random set. As you can see in here, we have three data points. Of course, there are training data points in class blue and six in class red. So as we sample these data points for training set, you choose one from the blue set and two from the red set, and this selection is based on the ratio in the data set. Consider this three-class problem. You're choosing one from the blue class and two from the red class and one from the green class. Again, the reason is the stratified sampling ensures that the class proportions are maintained in each random set. The last method is called cross-validation, which is kind of the more popular method in machine learning. In cross-validation, we are partitioning data into equal size sub-samples. Then iteratively, we leave one sample out for the test set and we train on the rest of the data. As you can see in here, this is our train data. We partitioned this into five equal sized partitions. Then we are selecting each of them for the test data at each iteration, and the rest is going to be used for training data. As you can see in here, here's the number of iteration. First, the train data from S2 to S5, and the test, that is S1, which has been left out, and the accuracy is 110 over 200. In the next one, we'll leave out the partition number 2, and we are training with partition number 1, 3, 4, and 5. Again, the accuracy is 170 over 200. If you keep doing this until the last iteration, we leave the last partition as 5 if you're going to train with S1 to S4, and then we find the accuracy. Now by averaging on the value of accuracies, we're going to have the final value of accuracy for our model. Activity © Getty Images A supervised learning algorithm Let’s take a closer look at a supervised learning example. This is an important concept so it’s stated in several different ways. If you understand this concept you can skip ahead. Consider a supervised learning algorithm with n{"version":"1.1","math":"\(n\)"} training data {xi,yi}{"version":"1.1","math":"\(\{x_i,y_i\}\)"} where I=1,2,3,...,n{"version":"1.1","math":"\(\quad I=1,2,3,...,n\)"}. The aim is to find a function that’s as close as possible to the unknown function, to determine the existing relationship between xi{"version":"1.1","math":"\(x_i\)"} and yi{"version":"1.1","math":"\(y_i\)"}. In other words you have two sets of data: the input and the output. The output set is obtained by applying the function to the input set. This means for each element in the input set there is a corresponding element in the output set. You are trying to figure out the relationship between the pairs of numbers. The relationship between the two is the function. Hypothesis space We will name a hypothesis function, h{"version":"1.1","math":"\(h\)"}, as an element of a range of possible functions H{"version":"1.1","math":"\(H\)"},  usually called the hypothesis space. We’ll select a hypothesis function that we think is similar to the true function behind the data. Some examples of hypothesis space are: space of all linear functions in d−{"version":"1.1","math":"\(d-\)"}dimensions space of all polynomial functions up to degree p{"version":"1.1","math":"\(p\)"}. Finding a function Let us come back to our main problem. We would like to find the function h{"version":"1.1","math":"\(h\)"} which can map the input to the corresponding output h:X→Y{"version":"1.1","math":"\(h: X\rightarrow Y\)"} accurately, to take the values from set X{"version":"1.1","math":"\(X\)"} to set Y{"version":"1.1","math":"\(Y\)"}. In supervised learning, given the training data, the learning algorithm seeks a function on h:X→Y{"version":"1.1","math":"\(h: X\rightarrow Y\)"} where X{"version":"1.1","math":"\(X\)"} is the input space and Y{"version":"1.1","math":"\(Y\)"} is the output space. The question which arises here is: how can we measure the quality of function h{"version":"1.1","math":"\(h\)"}? How can we understand how accurately  h{"version":"1.1","math":"\(h\)"} can map X{"version":"1.1","math":"\(X\)"} to the target Y{"version":"1.1","math":"\(Y\)"}? To answer this question, we need to introduce a new function called the loss function. Loss function The loss function is really a measure of accuracy. How accurately does your h{"version":"1.1","math":"\(h\)"} function describe the relationship between X{"version":"1.1","math":"\(X\)"} to the target Y{"version":"1.1","math":"\(Y\)"}? A function h{"version":"1.1","math":"\(h\)"} is applied to a training instance xi{"version":"1.1","math":"\(x_i\)"} and it gives the output h(xi){"version":"1.1","math":"\(h(x_i)\)"}. Let’s denote the function as y^i=h(xi){"version":"1.1","math":"\(\hat{y}_i = h(x_i)\)"} However, since we are dealing with a supervised problem we know that the true output is yi{"version":"1.1","math":"\(y_i\)"}. n order to measure how well function h{"version":"1.1","math":"\(h\)"} fits the training data, we need to find the difference between yi{"version":"1.1","math":"\(y_i\)"} and  y^i{"version":"1.1","math":"\(\hat{y}_i\)"}. To measure the difference we define a different equation, a loss function L(yi,y^i){"version":"1.1","math":"\(L(y_i,\hat{y}_i)\)"}. Examples You should be familiar with some of these examples of loss functions: Square loss: L(yi,y^i)=(yi−y^i)2{"version":"1.1","math":"\(L(y_i,\hat{y}_i) = (y_i-\hat{y}_i)^2\)"}  (useful for regression) Absolute loss:  L(yi,y^i)={"version":"1.1","math":"\(L(y_i,\hat{y}_i) =\)"}yi−y^i{"version":"1.1","math":"\(y_i - \hat{y}_i\)"} (useful for regression) 0−1{"version":"1.1","math":"\(0-1\)"} loss  L(yi,y^i)=1(yi,y^i){"version":"1.1","math":"\(L(y_i,\hat{y}_i) = 1(y_i,\hat{y}_i)\)"} which is equal to 0{"version":"1.1","math":"\(0\)"} if  yi=y^i{"version":"1.1","math":"\(y_i = \hat{y}_i\)"} and 1{"version":"1.1","math":"\(1\)"} otherwise (useful for classification) Other loss functions for classification problem: e.g. Logistic loss, Hinge loss The loss function is used to compute the error between the actual result of  yi{"version":"1.1","math":"\(y_i\)"} and what we calculated as y^i{"version":"1.1","math":"\(\hat{y}_i\)"}. Empirical risk Similar to the loss function, we can define a factor called empirical risk. Among all functions in hypothesis space, that is, h∈H{"version":"1.1","math":"\(h \in H\)"},  we select the function h{"version":"1.1","math":"\(h\)"}, which minimises the empirical risk. You can calculate the empirical risk by averaging the results of the loss function. The lower the empirical risk based on the training data, the closer the function represents the true relationship between the pair of values  xi{"version":"1.1","math":"\(x_i\)"} and yi{"version":"1.1","math":"\(y_i\)"}.  But how can achieve this? In theory, the answer is simple! We just need to minimize the risk of loss. In other words, we select a function h{"version":"1.1","math":"\(h\)"} that achieves minimum risk: minh∈H1n∑i=1nL(yi,h(xi)){"version":"1.1","math":"\min_{h \in H} \frac{1}{n} \sum_{i=1}^{n} L(y_i,h(x_i))"} This simple equation is the sum of all losses for all n{"version":"1.1","math":"\(n\)"} training points which can be calculated by  ∑i=1nL(yi,h(xi)){"version":"1.1","math":"\(\sum_{i=1}^{n} L(y_i,h(x_i))\)"}.  Then we divide this value by n{"version":"1.1","math":"\(n\)"} to find this average loss or the risk. This equation states that by finding a function likeh{"version":"1.1","math":"\(h\)"} which minimizes the risk as mentioned in the above formula, one can find the solution of the learning problem. Hence a supervised learning algorithm is often trained or learnt through an optimization algorithm. Activity Have you ever thought about the true application of 0 − 1 loss? Write and share a simple explanation of the 0 - 1 loss function and its use? Exploring Python packages for supervised learning Let’s move from the theoretical to the practical and see how we can work with supervised learning packages of Python. In this example, you will learn how to fit a linear regression model to a regression dataset. You will also study the behaviour of the linear regression model. Linear Regression Our initial objective is: write a program to read the provided regression dataset and split it into training and test sets in ratio of 70:30{"version":"1.1","math":"\(70:30\)"}  write code for a linear regression model to estimate regression weights and visualise it. For this exercise, we will use linear regression to predict profits from a food truck franchise in cities. The data is stored in ‘data/data.xlsx’. Download the data file data.xlsx and add it to your data store and rename it. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.You are encouraged to take a look at the raw data. Our objective is to predict the profit, given the population of a city. A natural question is: how can we measure the performance of our learning model? For linear regression, we measure the deviation in prediction from the truth using ‘mean square error’ (MSE). We shall see how the size of data affects MSE. Code example #1 Let’s begin by importing the necessary libraries. We’re going to use lumpy, pandas and matplotlib for this exercise. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns df=pd.read_excel("data/data.xlsx") df.head() The output would look like this: Code example #2 Data type and null value checking df.info() The output would look like this: Code example #3 Feature vs class/target Data distribution and density. f = plt.figure(figsize=(14,6)) ax = f.add_subplot(221) sns.violinplot(x='X1', y='Y1',data=df,palette='Wistia',ax=ax) ax.set_title('Violin plot of Y1 vs X1') ax = f.add_subplot(222) sns.violinplot(x='X2', y='Y1',data=df,palette='magma',ax=ax) ax.set_title('Violin plot of Y1 vs X2'); ax = f.add_subplot(223) sns.violinplot(x='X3', y='Y1',data=df,palette='Wistia',ax=ax) ax.set_title('Violin plot of Y1 vs X3') ax = f.add_subplot(224) sns.violinplot(x='X4', y='Y1',data=df,palette='magma',ax=ax) ax.set_title('Violin plot of Y1 vs X4'); plt.tight_layout() The output would look like this: Code example #4 Scatter plot X1 vs Y1 (feature-1 vs class/target) plt.scatter(df["X1"],df["Y1"]) plt.xlabel("X1") plt.ylabel("Y1") Fitting a regression line A simple regression line takes the form: y=wo+w1x1{"version":"1.1","math":"\(y=w_o+w_1x_1\)"} The closed form solution is: W=(XTX)−1XTy{"version":"1.1","math":"\(W=(X^{T}X)^{-1}X^{T}y\)"} Where W= [wo,w1{"version":"1.1","math":"\(w_{o}, w_{1}\)"}], X=[1,x1{"version":"1.1","math":"\(1,x_{1}\)"}] Let’s extend the code we started previously to fit a regression line. Code example #1 We’ll calculate the value of y{"version":"1.1","math":"\(y\)"} where x=0{"version":"1.1","math":"\(x = 0\)"}  and set the intercept value. If you’d like to know more about constants check out this article on Y intercepts. import numpy as np import pandas as pd import matplotlib.pyplot as plt # use the correct path to where you saved the foodtruck data file data = pd.read_csv('data/foodtruck_profits.txt', delimiter=',', header=None).values print("Data shape: {}".format(data.shape)) print('The first few lines of data: {}'.format(data[0:5,:])) The output would look like this: Data shape: (100, 2)The first few lines of data: [[ 6.1101 17.592 ] [ 5.5277  9.1302] [ 8.5186 13.662 ] [ 7.0032 11.854 ] [ 5.8598  6.8233]] # add a variable and find the length of the data m = len(data) # we change the one-dimensional data into a matrix form  X = np.matrix(data[:,0]).T y = np.matrix(data[:,1]).T print("Number of examples: {}".format(m)) print("Shape of data     : {}".format(X.shape)) print("Shape of labels   : {}".format(y.shape)) The output would look like this: Number of examples: 100 Shape of data     : (100, 1) Shape of labels   : (100, 1) # add intercept term to data X X = np.c_[np.ones(m), X] #np.c_ helps in appending columns print("New shape of data: {}".format(X.shape)) The output would look like this: New shape of data: (100, 2) Code example #2 # Closed form solution #Compute inverse of (Xtranspose * X) temp1 = np.linalg.pinv(np.dot(X.T,X)) #Computer Xtranspose * y temp2 = np.dot(X.T,y) W = np.dot(temp1,temp2) print (W.shape) print (W) The output would look like this: (2, 1) [[-3.95911545]  [ 1.19673809]] Code example #3 plt.scatter(data[:,0],data[:,1], color="red", marker="x",) plt.plot(X[:,1], np.dot(X,W)) #regression line plt.xlabel('Population of City in 10,000s') plt.ylabel('Profit in 10,000s') The output would look like this: Text(0,0.5,'Profit in 10,000s') Code example #4 We can use this fitted regression model to predict profit for population of size 175,000. Population in 10000{"version":"1.1","math":"\(10000\)"} is 17.5{"version":"1.1","math":"\(17.5\)"}. Note that we have to add the intercept term to this. # 17.5 x 10,000 = 175,000 population so we use x = 17.5 predicted_profit = np.dot([1,17.5], W) # Predicted profit needs to be multiplied by y units, 10,000, and rounded to be currency print ("Start by finding the y value at x = 17.5: {}".format(predicted_profit)) # multiply by 10,000 because that's the y unit multiplied_profit = predicted_profit * 10000            # round to two decimal places so it looks like money final_profit = np.around(multiplied_profit, decimals = 2) print ("For a population of 175,000, we predict a profit of: {}".format(final_profit)) The output would look like this: Start by finding the y value at x = 17.5: [[16.98380112]] For a population of 175,000, we predict a profit of: [[169838.01]] In the next lesson we will look at a multivariate example. Activity Make sure that you have experimented with all the individual Python coding examples that we have in this lesson, and that you are confident about how they work. Discuss any issues or questions in the discussion forum. See if you can help someone else. © Getty Images Forms of Supervised Learning The majority of practical machine learning applications use supervised learning. In supervised learning, the data used to train the algorithm is already labeled with correct answers. In other words, you make an algorithm based on the known relationship between the input and output. From this, you develop a mapping function from the input variable x{"version":"1.1","math":"\(x\)"} to the output variable y{"version":"1.1","math":"\(y\)"}. Instead of finding patterns based on similarity only, we can learn a direct mapping or function between feature vector xi{"version":"1.1","math":"\(x_i\)"} and the output (target or label)yi{"version":"1.1","math":"\(y_i\)"} such that, yi=h(xi){"version":"1.1","math":"\(y_i=h(x_i)\)"}. Thus, supervised learning is the task of estimating a function from labelled training data. Training data for supervised learning is arranged in the following form: {xi,yi}{"version":"1.1","math":"\(\{{x}_i, y_i\}\)"}, i=1,...,n{"version":"1.1","math":"\(i=1,...,n\)"}. For each data point xi{"version":"1.1","math":"\(x_i\)"} there is an output yi{"version":"1.1","math":"\(y_i\)"}.This means there is a significant advantage over unsupervised learning because you already know what you’re going to get. This means you can test your algorithm to make sure it’s giving you usable outputs. Supervised learning can appear in many forms: Regression problems Linear Regression (linear model) Logistic Regression (linear model) Classification problems Support Vector Machines (both linear and nonlinear) Decision Trees (nonlinear) Random Forest (nonlinear) Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear) Ranking problems Let’s see some examples. Example 1 The following figure illustrates a regression problem about the sale of Yogurt with seasonal temperature. Let’s estimating the relationships among the feature variables (e.g. the sale of frozen yogurt and its temperature). Figure. Yogurt sale with season temperature. Example 2 The following is another example of regression, where we look to find relationships among feature variables. The figure illustrates sample data for GoPro stock price against date. Imagine the amount of money you could earn by intelligently predicting prices in the stock market! Figure. goPro stock price with time. Example 3 The following familiar figure illustrates the classic classification problem for classifying two types of data. As you can see, sometimes we can successfully find a linear boundary and sometimes we have to search for a more complex boundary. Figure. Linear boundary vs nonlinear boundary for classification. Activity Is it possible to generate classification output from regression output? How? What about generated regression value from a classification model? Linear regression model from sci-kit learn To fit the multivariate regression, we use LinearRegression() from scikit learn package as: This again is an extension of the code you started in the previous step. Download the data Advertising.csv for this experiment Code example 4 from sklearn.linear_model import LinearRegression my_model = LinearRegression() #fit the model using our data my_model.fit(Xtrain, ytrain) The output would look like this: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) Code example 5 Now that we have built our model, lets see how it performs with our test dataset ypredicts = my_model.predict(Xtest) print("The predicted Y1:") print(ypredicts) print("The true Y1:") print(ytest) The output would look like this: The predicted sales: [ 9.5143544  18.29202877 19.27094064  8.88723582 10.11361706  9.96041972  15.62706788 23.1587013  12.03845031  9.72212471 18.63809455 10.10099867  16.48589616 18.12468609 15.64364266  5.30523275 15.12744231 10.25358725  10.09006252 12.44362791 14.3210309  13.3616908  15.08443331 17.32769433  11.18435749 14.4341081  10.57847377 13.65085298 17.24682604 18.30648623   7.42659652 14.39756088  7.52782982 12.20541805 14.08031808 24.73378746  20.09744782 12.3485591  16.40555117 12.5803357  10.85519521 14.21321462   6.62420582 24.08691176 18.72317811 20.7349155   9.90581943 17.13834561  18.98753788  6.02878648 12.27110367  8.5198605   4.49610391 18.42872767  16.46701074  5.46575208  8.33322951 13.02457489 23.73586521 15.48426632] The true sales: [10.9 19.2 20.1 10.4 11.4 10.3 13.2 25.4 10.9 10.1 16.1 11.6 16.6 19.  15.6  3.2 15.3 10.1  7.3 12.9 14.4 13.3 14.9 18.  11.9 11.9  8.  12.2  17.1 15.   8.4 14.5  7.6 11.7 11.5 27.  20.2 11.7 11.8 12.6 10.5 12.2   8.7 26.2 17.6 22.6 10.3 17.3 15.9  6.7 10.8  9.9  5.9 19.6 17.3  7.6   9.7 12.8 25.5 13.4] SEE ALSO TUTORIAL: LINEAR REGRESSION © Getty Images Model complexity and Occam's razor Occam’s Razor, a famous problem-solving principle, is used as a heuristic guide in the development of theoretical models. This principle often paraphrased as: All other things being equal, the simplest solution is the best. It also addresses the problem of Which hypothesis to choose if there are multiple hypothesis with similar fit? In other words, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity. Figure. William of Ockham (Wikipedia contributors 2018) Activity Check out this interesting tutorial How to Use Occam’s Razor Without Getting Cut! . Please share your thoughts about Occam’s Razor in the discussion forum. Why do you think this concept is useful for us? Where else have you seen this term used? References Multivariate regression Now, lets look at a multivariate example. In this example, we look at the X1 vs Y1 (feature vs class/target). Our data is stored in a csv file. You are encouraged to open this file in Excel and look at it. Download data file data.xsxl add it your data store and rename it. Open a new Jupyter notebook and save your new .py file in the same directory. We can now build our regression model. We proceed by dividing this original dataset into two parts: Training dataset: containing 77% of original data Testing dataset: containing  33% of original data Why do we do this? So that we can effectively judge our model. We build our model using training data. Then we test its performance on the unseen(by the model) test data. For now, lets manually create the train/test split using 77% training data and 33% testing data. Code example# 1 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:-1], df.iloc[:,-1], test_size=0.33, random_state=42) print("Train shape: ",X_train.shape) print("Test shape: ",,X_test.shape) The output would look like this: Train shape:  (514, 8) Test shape:  (254, 8) Code example# 2 print(type(X)) print(type(y)) The output would look like this: <type 'numpy.ndarray'> © Getty Images Structural risk minimisation So based on Occam’s razor and its simplistic principle, we define another risk value which is called Structural Risk. Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. So the general idea is to minimise both Structural Riskand Empirical Risk which we introduced before: Rstr(h)=Remp(h)+λC(h){"version":"1.1","math":"R_{str}(h) = R_{emp}(h) + \lambda C(h)"} Where C(h){"version":"1.1","math":"\(C(h)\)"} is the complexity of hypothesis function h{"version":"1.1","math":"\(h\)"} and λ{"version":"1.1","math":"\(\lambda\)"} is a penalty parameter. So a comprehensive definition of risk is made of a loss function for over y^i{"version":"1.1","math":"\(\hat{y}_i\)"} and yi{"version":"1.1","math":"\(y_i\)"} for all training points, in addition to the complexity of the proposed model as a penalty. Activity As you know, C(h){"version":"1.1","math":"\(C(h)\)"} is the complexity of hypothesis function h{"version":"1.1","math":"\(h\)"}.  How do you think we should calculate this value? © Getty Images The concept of model complexity How complex should a machine learning model be? What are the costs when a complex model is used? When is it necessary to use a complex model? Consider the following figure as an example of a classification problem. Which boundary line you think is more appropriate for this problem? Figure. Two different boundary lines for a classification problem. We may not always be able to visualise the training data in high dimensions. So, we may not know whether the regression problem is linear or non-linear. Similarly, we may not know if the classification problem is linearly separable or non-linearly separable. So, the big question is: what should be the right complexity of the model that we use to fit the given data? Let’s first examine the effects of selecting different models in terms of complexity: If we choose higher complexity than necessary, we would be over-fitting the data (you will review over-fitting later in this course). If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalisation. What is generalisation? It is prediction on unseen data, that is, the data, which is not part of our training set. We’ll discuss these concepts in detail later in this course. Activity As a starting point, watch the following video about model complexity. Share your thoughts and questions about model complexity in machine learning in the discussion forum. Can you help another student to understand? 