Topic 5: Fundamentals of Supervised Learning  1.  Forms of Supervised Learning  a.  Labeled data: Supervised learning relies on labeled training data, which includes input- output pairs. This enables the algorithm to learn the relationship between input features and output variables.  b.  Function estimation: The goal of supervised learning is to estimate a function that maps input variables to output variables. This function can be used to make predictions on new, unseen data.  c.  Types  of  problems:  Supervised  learning  can  be  applied  to  various  types  of  problems,  including regression, classification, and ranking problems.  d.  Regression  problems:  These  involve  predicting  a  continuous  output  variable  based  on input  features.  Examples  include  predicting  the  sale  of  yogurt  based  on  seasonal temperature or predicting stock prices over time.  e.  Classification problems: These involve predicting a discrete output variable or class based on input features. Algorithms for classification problems include support vector machines, decision trees, random forests, and neural networks. An example is classifying data into two or more categories using linear or nonlinear boundaries.  f.  Ranking problems: These involve predicting the relative ranking or order of items based  on input features.  g.  Model  evaluation:  Supervised  learning  allows  for  model  evaluation  and  testing,  as  the correct output labels are already known. This enables the assessment of the algorithm's performance and usability.  2.  A supervised learning algorithm  a.  Goal  of  supervised  learning:  The  aim  is  to  find  a  function  that  accurately  maps  input  variables to output variables, approximating the true unknown function.  b.  Hypothesis space: The set of possible functions from which the learning algorithm selects a hypothesis function, denoted as h(x). Examples of hypothesis spaces include the space of all linear functions and the space of all polynomial functions up to a certain degree. c.  Loss function: A measure of how accurately the hypothesis function h(x) represents the relationship  between  input  and  output  variables.  The  loss  function  calculates  the difference between the predicted output and the true output. Examples of loss functions include square loss, absolute loss, and 0-1 loss.  d.  Empirical risk: A measure of the average error between the true output and the predicted output for all training instances, using the chosen loss function. The goal is to minimize the empirical risk to select the best hypothesis function from the hypothesis space. e.  Optimization  algorithm:  Supervised  learning  algorithms  are  often  trained  through optimization algorithms that seek to minimize the empirical risk. This allows the algorithm to find a function that best  represents the true relationship between input and output variables.  3.  Classification metrics  a.  Confusion Matrix: A summary of prediction results, showing the number of correct and     incorrect predictions for each class. It helps to understand the types of errors made by a model.  b.  Accuracy:  The  proportion  of  correct  predictions  out  of  all  predictions  made.  However, accuracy  may  not  be  a  reliable  metric  for  imbalanced  datasets  or  cases  with  unequal classification error costs.  c.  True Positive Rate (TPR), Recall, or Sensitivity: The fraction of true positive predictions out  of all actual positive samples.  d.  False Positive Rate (FPR): The fraction of false positive predictions out of all actual negative  samples.  e.  ROC Curve: A graphical representation of the trade-off between the true positive rate and the  false  positive  rate  at  various  threshold  settings.  It  is  useful  for  imbalanced  class distributions and unequal classification error costs.  f.  Area Under the Curve (AUC): A single number summarizing the performance of a classifier based on the ROC curve. A higher AUC value indicates a better-performing classifier. g.  F-1  Measure:  A  metric  that  combines  both  Precision  and  Recall  in  a  single  number,  providing a balance between these two evaluation measures.  4.  Regression Metrics  a.  Mean  Square  Error  (MSE):  Measures  the  average  squared  difference  between  the  predicted and actual values. Lower MSE values indicate better performance.  b.  Root Mean Square Error (RMSE): Derived from MSE, it calculates the square root of the MSE. RMSE values are easier to interpret as they are on the same scale as the target values. Lower RMSE values indicate better performance.  c.  Mean  Absolute  Error  (MAE):  Measures  the  average  absolute  difference  between  the predicted and actual values. It is robust to outliers and, similar to MSE and RMSE, lower MAE values indicate better performance.  d.  Explained  Variance  (R-squared):  Also  known  as  the  coefficient  of  determination,  it measures  the  proportion  of  target  variation  explained  by  the  model.  R-squared  values range from 0 to 1, with higher values indicating better performance.  5.  Partitioning data for training and testing  a.  Random sub-sampling: Repeatedly partitions the data into random training and test sets in  a  specified  ratio,  averaging  the  accuracies  to  get  a  more  reliable  estimate  of  model performance.  b.  Stratified  Sampling:  Divides  the  data  into  different  subgroups  or  strata,  then  randomly selects  subjects  proportionally  from  the  strata,  maintaining  class  proportions  in  each random set.  c.  Cross-validation:  A  popular  method  that  partitions  the  original  sample  into  equal-sized sub-samples, iteratively leaving one sub-sample out for the test set and training on the rest.  This  process  is  known  as  k-fold  cross-validation,  where  k  is  the  number  of  sub- samples.  d.  Leave-one-out cross-validation: A special case of cross-validation where k is equal to the  number of instances, meaning each instance is used once as a test set.     6.  Finding the best hyperparameters  a.  Hyperparameters  are  used  to  help  estimate  model  parameters  and  are  often  set  using  heuristics or tuned for a given predictive modeling problem.  b.  To  find  the  best  hyperparameters,  partition  training  data  into  separate  training  and validation  sets,  with  the  validation  set  used  for  unbiased  evaluation  and  fine-tuning  of hyperparameters.  c.  A search grid is defined within the specified range of possible hyperparameter values, and a model is trained using each value from the grid to assess its performance on a validation set. The best performing hyperparameter is selected. Internal  cross-validation  can  be  applied  to  select  the  best  set  of  hyperparameters. Techniques such as random subsampling, stratified subsampling, and cross-validation can be used for training/validation set splitting.  d.  e.  Three ways to navigate the hyperparameter space include grid search, random search, and  Bayesian optimization.  7.  Effect of imbalanced classes  a.  Possible solutions to address imbalanced classes are:  b.  At the data level (Re-sampling): a. Over-sampling the minority class b. Under-sampling the majority class At the algorithmic level: a. Adjusting the costs b. Adjusting the decision threshold Issues of imbalanced classes include: High accuracy for a naive classifier that always predicts the majority class. The solution is to use alternative evaluation metrics instead of accuracy. Imbalanced  class  proportions  in  random  subsampling,  which  can  be  addressed  using stratified sampling.  