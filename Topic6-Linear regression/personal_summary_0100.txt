Topic 5 & 6  learning goals  -  Differentiate supervised learning from unsupervised learning -  Estimate the performance of different supervised learning models -  Implement model selection and compute relevant evaluation measures  Forms of supervised learning  -  In supervised learning the data used to train the algorithm is already labeled with correct answers; an algorithm is made based on the known relationship between the input and output, a mapping function is made from the input variable to the output variable  -  We can learn a direct mapping or function between feature vector X and the output Y such that Y  = h(x)  -  Supervised learning is the task of estimating a function from labelled training data -  Supervised learning has many forms: o  Regression problems  ▪  Linear regression(linear model) ▪  Logistic Regresion (linear model)  o  Classification problems  ▪  Support bector Machines (both linear and nonlinear) ▪  Decision trees (nonlinear) ▪  Random forest (nonlinear) ▪  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  o  Ranking problems   Loss function  -  Measure of accuracy, how accurately does your H function describe the relationship between X  to the target Y In order to measure how well function h hits the training data, we need to find the difference  between To measure the distance we define a different equation, a loss function Examples  -  - -  o  Square loss o  Absolute loss o  0 – 1 loss o  Logistic loss o  Hinge loss  -  The loss function is used to compute the error between the actual result and what we calculated  Empirical risk  -  Calculate by averaging the results of the loss function, the lower the empirical risk based on the training data, the closer the function represents the true relationship between the pair of values x and y   Concept of model complexity  -  How complex should a learning model be, and what are the costs when a complex model is  used?  -  Selecting different models in terms of complexity  - - -  If we choose higher complexity than necessary, we would be over-fitting the data If we choose lower complexity than necessary, we would be under-fitting the data It is important to get the best possible fit for good generalization  Generalization = prediction on unseen data, the data that is not apart of our training set  Occam’s razor – simplest explanation is preferable to one that is more complex  Structural risk minimization  -  Seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones  o  The idea is to minimize both structural risk and empirical risk  Classification metrics  -  The metric that is chosen to evaluate the machine learning model is important, the choice of evaluation metric influces how performance is measured and compared; the most common type of machine learning applications are classification problems, there are myriad metrics used to evaluate predictions for these types of problems   Confusion matrix  Ricky Boeing  - -  Summary of predction results on a classification problem The number of correct and incorrect predictions are summarized with count values and divided down by each class  -  A way to understand the types of errors made by a model -  Also called contingency tables -  A reason for using a confusion matrix; accuracy is not a reliable metric for the real performance of a classifier, if the data set is unbalanced (no. Of observations in different classes vary grealty) it will yield misleading results ( 90 apples 10 oranges a classifier may classify all observations as apples)  -  Receiver Operating Characteristics (ROC) curve  -  Used in signal detection to theory to depict the trade-off between the true positive rate and  false positive rate over noisy channels  -  Useful for domains with imbalanced class distribution and unequal classification error costs - Created by plotting the true positive rate TPR against the false positive rate FPR at various threshold settings, this is done to depict relative trade-offs between benefits (True positives) and costs (false positives)   F-1 measure  - combination of precision and recall, f1 measure is a metric that combines both precision and recall in a single number. Defined as:  Regression Metrics  -  Measures how far the expected value is from the actual value  The ways of measuring regression performance:  Mean square error   Explained Variance (R^2)  Also known as:  -  R-square -  The coefficient of determination  R-square is measured as the percentage of target variation that is explained by the model  -  -  R squared for the regression model on the left is <= 20% and the model on the right is >= 80% -  When a regresssion model accounts for more of the  variance, the data points are closer to the  regression line  partitioning data for training and testing  -  A single training set may be affected by some outlier instances (I.e., noisy observations) -  To get a reliable estimate of model performance, we need a large test set because variance of such an estimate is low   -  We know that the larger the size of the training set, the more accurately the model can be  learnt  -  Multiple training/test splits allow us to re-use same data for both training and evaluation in  different splits  Usually work with three methods for splitting data  -  Random subsampling - Stratified sampling -  Cross validation  Subsampling  -  Instead of using single split, a more reliable estimate of model performance can be obtained by random sub-sampling  - -  We train the model with each training set and estimate an accuracy using the corresponding  test set, we finally average the accuracies to get an averaged estimate  Stratified sampling  -  -  Probability sampling technique in which we divide the entire data into different subgroups or strata, then randomyl select the final subjects proportionally from the different strata Ensures that class proportions are maintained in each random set  Cross validation  -  -  Technique to evaluate models by partitioning the original sample into a training set to train the model and a test set to evaluate it Partition training data into k equal sized sub-samples, then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples  In conclusion  -  When using subsamples, we call it k-fold cross validation -  In special cases, when k is equal to the number of instances n o  We call it as leave-one-out cross validation scheme o  Cross-validation makes efficient use of the available data for testing  -  Finding the best hyperparameters  -  Hyperparameter is a parameter whose value is set before the learning process begins -  The value of a hyperparamter in a model cannot be estimated from data, they are often used in processes to help estimate model parameters  -  Hyperparamteres can often be set using heuristics, they are tuned for a given predictive  modelling problem, to search for the best hyperparameters we need to partition training data into separate training and validation sets  Validation set  -  A sample of data used to provide an unbiased evaluation of a model fit on the training dataset  while tuning model hyperparameters  -  Internal cross-validation  Techniques that we have discussed for model assessment are applicable for training/validation set splitting:  -  Random subsampling - -  Cross-validation  Stratified subsampling Still assessing how a particular hyperparameter is doing on the validation set, this step is internal to the learning process and different from model assessment on the test data  How internal cross-validation works  -  Instead of using a single validation set, we can use cross-validation within a training set to select the best set of hyperparameters  -  We select the best hyperparameter set by searching/or optimizing over all possible values, three ways to navigate the hyperparameter space:  -  Grid-search (not so efficient -  Random search -  Bayesian optimization  Effect of imbalanced classes  -  A problem that can occur in machine learning is datasets where the total number of one class of data (positive outcomes) is far less than the total number of another class of data (negative) outcomes This problem is very common in practice and can be detected in various disciplines including:  -  o  Fraud detection o  Anomaly detection    o  Medical diagnosis  -  Most machine learning algorithms work best when the number of instances of each classes are roughly equal, when the number of instances of one class far exceeds the other this may cause problems  -  Solutions  -  Two approaches  o  Perform actions o the data itself o  Improve our algorithm to be able to handle such phenomenon  At the data level: Re-sampling  -  Over-sampling the data from minority class -  Under-sampling the data from majority class  We can sample more data points from the minority class in order to over the difference, or we can under-sample the majority class in order to make them have an equal effect  on the algorithm  At the algorithmic level:  -  Adjusting the costs -  Adjusting the decision threshold  We may want to adjust some costs on the points we are observing from the majority class in order to dampen their effect, we can manually define some thresholds to cope with the unbalanced data  Issues of imbalanced classes  -  -  Problem 1: since the test data contains only few samples from the minority class, even a dumb classifier that always classifies an instance to the majority class will get very high accuracy o  This problem is dealt with by using over evaluation metrics in place of accuracy  Problem 2: when doing random subsampling, it is possible that class proportion is not maintained in an individual partition, in fact, we may not sample even one instance from the minority class  o  This problem can be solved using Stratified Sampling  Things to remember:  -  Any pre-processing over an entire data set must not use the information that you are trying to  predict  -  During the training process, you must not use any information that is not avaliable during the  -  training process If you modify your model repeatedly by looking at how it performs on a specific test, then you may be overfitting on the test set.  Python packages for supervised learning  Data type and null value checking  Feature vs class/target data distribution and density  Scatter plot X1 vs Y1 (feature-1 vs class/target)  Fitting a regression line  