The module this topic covered the fundamentals of supervised learning, different forms of supervised learning, some common pitfalls, as well as methods to test the quality and performance of supervised learning models.  The task this topic required us to train a number of multivariate regression models differentiated by the method of segmenting the training and testing data. Overall, the leave-one-out method performed the best, but only slightly. The standard random data selection method based on a defined percentage split achieved similar results with less computation time. I did try my own method of train/test data segmentation, it performed terribly so I won‚Äôt be trying that again anytime soon.  Supervised Learning  Up until now we have been deploying unsupervised learning techniques where we leave it up to the algorithm to identify the patterns within the data. With supervised learning, we provide the labeled target data to the algorithm once the models trained, so we can assess its accuracy.  In a nutshell, supervised learning uses training data that includes inputs, and correct outputs, to train models over time to yield the desired output. The algorithms measure accuracy via a loss function, adjusting itself with each iteration until the error has been minimized to a satisfactory level.  Supervised learning is commonly used to solve two main types of problems, namely classification and regression. Classification problems as the name suggests aims to assign test data into specific categories. Common classification algorithms include linear classifiers, decision trees, k-nearest neighbor, and random forest. Regression problems are all about understanding the relationship between dependent and independent variables and are commonly used to make forecasts and predictions. Common regression algorithms include linear regression, logistical regression, and polynomial regression.  Model Complexity  Model complexity is a key consideration in supervised learning. Put simply, it refers to the number of independent variables or features that a model needs to consider in order to make an accurate prediction. For example, a linear regression problem with one independent variable is relatively simple. A model with multiple variables with a non-linear relationship is far more complex. The later will be harder to train and the issue of over fitting raises its head.  Over fitting occurs when a model becomes too complex and captures noise in the training data instead of the underlying signal. The result is a model that may perform great on the test data but does not generalize well so past that data is unable to provide accurate predictions.  Underfitting is essentially the opposite, as an example, imagine fitting a wave function to a straight line. There is a balance to strike.   Classification and Regression Metrics  Once we‚Äôve trained our models, we need tools to measure the model‚Äôs effectiveness and accuracy, as it turns out, there are a range of performance metrics which suit different machine learning algorithms we can deploy.  Classification problems commonly assessed with:    Confusion Matrix (contingency table): A n x n matrix that compares the classes being  predicted. The higher the proportion of values on the diagonal of the matrix the better the model.    ROC Curve: The Receiver Operating Characteristics (ROC) curve is one of the more  important evaluation metrics for classifiers. It‚Äôs used when we need to check or visualize the performance of the multi-class classification problem. The ROC scores range for 0 to 1 with the higher value indicating a better fit.    Recall & Precision: The Recall and Precision metrics. Recall is defined as the number of true positives divided by the number of false negatives plus the number of true positives. It tells us what proportion of predicted positives are actually positives in the original dataset. Precision is defined as the number of true positives divided by the number of false positives plus true positives. This tells us what proportion of predictions which are predicted as positives that are actual positives to the total predicted positives.    F-1: The F1 score combines the results from the above recall and precision metrics. It  conveys the balance between precision and recall and if there is an uneven class distribution. F1 score reaches its best value at 1 and worst at 0.  Regression problems are often assessed with:    Mean Square Error (MSE): MSE is one of many loss functions but the default loss function  for regression problems. It‚Äôs simply the sum of the squared differences between the predicted values and the actual values in a model.    Mean Absolute Error (MAE): MAE, on the other hand, is measured as the average sum of  absolute difference between predictions and actuals. Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. MAE is more robust to outliers since it does not make use of square.    Explained Variance (ùëÖ2): R squared measures the variation that is explained by the model  as a percentage. The higher the percentage the better the model.   Partitioning Data  The idea of partitioning data into training and testing essentially boils down to being able to test you model on data it has never seen. If you train a model on all the available data, it becomes difficult to determine how it will perform out in the wild.  Methods for partitioning a dataset include sub-sampling where we create partitions of the data that are later split into training and testing. Stratified sampling which is a probability sampling technique where we divide the entire data into different subgroups, then randomly select the final subjects proportionally from the different groups. Lastly, cross-validation which is an iterative method leaving out a different test sample with each iteration while training the model on the remaining data. The results are then aggregated. Cross-validation is popular with researchers.  