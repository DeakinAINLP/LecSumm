Fundamentals of supervised  learning  Most practical  machine learning applications use supervised learning.  In supervised learning, the data used to train the algorithm is already labelled with correct answers. In other words, you make an algorithm based on the known relationship between the input and output. Thus, supervised learning is the task of estimating a function from labelled training data.  Supervised learning can appear in many forms:   Regression problems  Linear regression Logistic regression   Classification problems  Support vector machines Decision trees Random forest Neural networks   Ranking problems  Model Complexity Concept How complex should a machine learning model be? What are the costs when a complex model is used? When is it necessary to use a complex model?  We may not always be able to visualise the training data in high dimensions. So, we may not know whether the regression problem is linear or non-linear. Similarly, we may not know if the classification problem is linearly separable or non-linearly separable. So, the big question is:  what should be the right complexity of the model that we use to fit the given data?  Let’s first examine the effects of selecting different models in terms of complexity:    If we choose higher complexity than necessary, we would be over-fitting the data (you will review over-fitting later in this course). If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalisation.  Structural risk minimisation So based on Occam’s razor and its simplistic principle, we define another risk value which is called Structural Risk.  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. So the general idea is to minimise both Structural Riskand Empirical Risk which we introduced before:    Classification metrics Confusion matrix A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class. Confusion matrices are a way to understand the types of errors made by a model.  One of the reasons for using a confusion matrix is that accuracy is not a reliable metric for the real performance of a classifier. If the data set is unbalanced  Regressions Metrics Regression measures how far the expected value is from the actual value  Partitioning data for training and testing The first question that might come to mind might be the limitations of using only a single training/testing set.   A single training set may be affected by some outlier instances (i.e. noisy observations).  To get a reliable estimate of model performance (accuracy), we need a large test set. Why?  Because variance of such an estimate is low.   However, we know that the larger the size of the training set, the more accurately the model  can be learnt.   Multiple training/test splits allow us to re-use same data for both training and evaluation in  different splits.  We usually work with 3 methods for splitting data:   random subsampling stratified sampling cross validation.  Sub-sampling Instead of using a single split, a more reliable estimate of model performance can be obtained by random sub-sampling. Random sub-sampling repeatedly partitions the data into random training and test sets in a specified ratio. We train the model with each training set and estimate an accuracy using the corresponding test set. We finally average the accuracies to get an averaged estimate.  Stratified sampling Stratified sampling is a probability sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata. Stratified sampling ensures that class proportions are maintained in each random set.  Cross-validation This is a technique to evaluate models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. The main idea is to partition training data into k equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples.  Finding the best hyperparameters A hyperparameter is a parameter whose value is set before the learning process begins. This means this value cannot be estimated from data. They are often used in the process to help estimate model parameters.  hyperparameters can often be set using heuristics Often they are tuned for a given predictive modelling problem. To search for the best hyperparameters, we need to partition training data into separate training and validation sets.  A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.  As an example, the number of clusters used in kmeans is a type of hyper parameter  How can we find the best hyperparameter?   First, we need to decide a possible range for hyperparameters.  We then define a search grid within the specified range.  Next, we train a model using each hyperparameter value from the search grid and assess its  performance on a validation set (separated from the training set).   Finally, we compute the performance on the validation set for each hyperparameter value and select the one with the best performance. Once the model is working with the best hyperparameter we defined it’s ready to be tested on separate test data.  Effect of imbalanced classes One problem that can occur in machine learning is datasets where the total number of one class of data (i.e. positive outcomes) is far less than the total number of another class of data (i.e. negative) outcomes. This problem is very common in practice and can be detected in various disciplines including fraud detection, anomaly detection, medical diagnosis, etc.      Solutions At the data level: (Re-Sampling)   over-sampling the data from minority class  under-sampling the data from majority class.  Two obvious solutions based on data manipulation which suggests that we can sample more data points from the minority class in order to cover the difference. Or we can under-sample the majority class in order to make them have an equal effect on the algorithm.  At the algorithmic level:   adjusting the costs  adjusting the decision threshold.  From an algorithmic point of view, we may want to adjust some costs on the points we are observing from the majority class in order to dampen their effect. Also we can manually define some thresholds to cope with the unbalanced data.      