Supervised learning  -  Data used to train algorithms already has the correct labels. Therefore, the algorithm is trained using known relationships between the input and output.  -  For unsupervised learning, finding a pattern is based on similarity only. For supervised learning, a pattern is derived from the direct mapping between the target and label. -  Training data for supervised learning is: [𝑥𝑖, 𝑦𝑖], 𝑖 = 1, … , 𝑛. Where 𝑥𝑖 is the input and 𝑦𝑖 is the output. Knowing the correct output means that you can evaluate the algorithm by comparing the output and known output.  -  Supervised learning has many forms: o  Regression problems o  Classification problems o  Ranking problems  Finding the function Since the expected output is known, the objective of supervised learning is to find the function that will give an output as close to the known output as possible. Hypothesis space: to select a hypothesized function that is believed to be most similar to the unknown function. ℎ ∶ 𝑋 → 𝑌 where X is the input space and Y is the output space. Loss function: measures the accuracy of the function by checking the difference between the expected output and the actual output. Complexity: the complexity of the algorithm must be taken into consideration. Too high or lack of complexity can lead to overfitted or underfitted data.  Overfitted data  means that the error in training data is low but the error for test data is too high. Underfitted data would be the error for training and test data is too high. The ideal medium must be found where errors for both types of data is low. Occam’s  razor:  a  problem-solving  principle  –  All  other  things  being  equal,  the  simplest solution is the best. This means for example, if there are multiple hypothesized functions with a similar fit, the best would be the one that introduces the fewest assumptions and has the least complexity. Structural risk minimisation: used to prevent overfitting by adding a penalty to the function when the training data is fitted too closely.  Classification metrics Confusion matrix: Also known as a contingency table, the matrix summarises the number of correct  and  incorrect  predictions.  Accuracy  is  not  a  reliable  metric  when  checking  real performance.  This  is  because  an  unbalanced  data  set  can  skew  the  yielded  results  thus  a confusion matrix should be used. True positive rate: can also be referred to as recall or sensitivity.  𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑒𝑥𝑝𝑙𝑎𝑖𝑛𝑒𝑑 𝑏𝑦 𝑡ℎ𝑒 𝑚𝑜𝑑𝑒𝑙  𝑇𝑜𝑡𝑎𝑙 𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒  When plotting this on a graph, the expected values are represented as a ‘regression line’ and the actual values are plotted around this. The closer the values are to the line, the closer the similarity is.  Partitioning To gain a true estimated of the model’s accuracy, a large test set should be used to ensure a variety  of  scenarios  are  covered.  Partitioning  this  data  for  training  and  testing  increases reusability of the data. There are three methods to split data:  1.  Random  sampling:  partitions  the  data  randomly  into  a  training  or  testing  set  at  a specified ratio. The training set is used to train the algorithm and the testing set is used to measure performance.  2.  Stratified sampling: entire data set is partitioned into different classes and ensures that the  class  proportions  are  equal  and  maintained  in  each  random  set.  The  values  for sampling are selected randomly from the classes.  3.  Cross validation: the data is split into equal sized sub samples. Each testing round, one sub  sample  is  left  out  and  the  remaining  will  be  used  for  training.  The  accuracy  of testing the singled-out sub will be recorded. Each round will have a different sub sample left out of training. If the number of sub samples is equal to the number of instances, then this is called leave-one-out cross validation scheme.  Hyperparameters Parameters  that  are  set  before  the  learning  process  begins  therefore  the  value  cannot  be estimated from data (e.g., setting the number of clusters in kmeans). To find the best suited hyperparameter, the training data is partitioned into training and validation sets. Validation sets help fine-tune the hyperparameters of a model without over fitting the training data. These sets are also used to provide an unbiased evaluation of a model fit. To find the best hyperparameter, the following steps are completed:  1.  Decide a range of possible hyperparameters. 2.  Define a search grid within the specified range. 3.  Train  the  model  using  the  search  grid  values  and  assess  the  performance  using  the  validation set.  4.  Calculate the performance on the validation set for each hyperparameter value and pick  the algorithm with the best performance.  Internal  cross-validation  is  a  method  used  within  training  sets  to  select  the  best  set  of hyperparameters. The data is split into training/validation sets instead of test/training sets.  Imbalanced classes Imbalanced classes are when there is a higher chance of one class (e.g., positive outcome) over another (e.g., negative outcome). There are two issues that can arise from this:  -  The accuracy of identifying the majority outcome will be higher even if the classifier is inefficient. Thus, the function will be more skewed towards majority outcomes and the perceived accuracy of the function will be greater than it actually is.  -  When using randomized subsampling, it is possible that the minority sample will not be sample for some instances. For example, women only make up 27% of the workforce across all STEM industries. Therefore, when subsampling different experiences of individuals across the industry, there could be an instance where only the experience of men is taken into consideration.  There are a few solutions for over sampling:  -  At a data level, oversampling the data minority or under sampling the data majority. -  At an algorithm level, adjust the cost and decision threshold. -  Utilizing evaluation metrics over accuracy readings to determine the efficiency of an algorithm.  -  Stratified sampling.  