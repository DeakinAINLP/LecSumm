 In this module we learned about supervised learning. Most practical machine learning applications are supervised learning. In supervised learning the data used to train the model is already correctly labelled. For this we develop a mapping function between the feature vector x and the output y.  In a supervised learning algorithm, we have 2 sets of data, input x and output y. We need to just create a function which when applied on x gives us y. We measure how accurately the function has output y. We create a function, a loss function L(output y, actual y). It gives the difference between the two. Empirical risk is another factor, we choose the model with the lowest amount of empirical risk.  The concept of model complexity states that the model should be somewhere between high complexity and low complexity. If the model is low complexity, it results in high error, if it is high complexity, it will result in zero error but the error on unseen data would be high, this phenomenon is also known as overfitting of data. So, the best model to choose is a model which lands somewhere in the middle of the two.  Classification metrics, these are the metrics we use to evaluate a machine learning algorithm. The most common machine learning algorithms are classification algorithms. A confusion matrix is a metric, it is a summary of prediction results in a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class. These are also called contingency matrix.  Regression metric is a measure of how far the expected value is from the actual value. They can be calculated by MSE or mean squared error. The lower the MSE the better the performance. Another one is explained variance or R squared. Simply put, it is variance explained by the model / total variance.   Then, we learned about partitioning of data between training and testing datasets. Multiple test / train splits allows us to re-use same data for both training and Evalution in different splits.  Three methods of splitting data: -    Random sub sampling:  it partitions data into random training and test sets in  a specified ratio.    Stratifies sampling: it is a probability sampling technique where we divide the data into different subgroups then randomly select the final subjects proportionally from different strata.    Cross- validation: in this technique we divide the data into k number of  groups, then we take one group out in each iteration, the taken out group is the test and the rest is training data. We do this in iterations.  Then, we learnt about hyperparameters. In machine learning a hyperparameter us a parameter whose value is set before the learning process begins, a good example of it is kmeans k(value) which was a hyperparameter. They are set to tune a given predictive model for partitioning training and validation datasets. To find the best hyperparameter, we can take values between a set and run the model using these hyperparameter values, we evaluate all the results to find the best hyperparameter. Then, we learned about internal cross validation which is cross validation upon training dataset to partition it into training and validation data. Then we also learned about effects of unbalanced data and techniques to mitigate imbalanced data. Then finally we explored the practical side by exploring python packages to apply our knowledge practically.   