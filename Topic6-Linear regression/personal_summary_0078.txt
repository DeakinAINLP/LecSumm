Summary and Reflection of Topic 5:  Fundamentals of Supervised Learning  Most of practical machine learning applications use supervised learning – data used to train the algorithm is  already  labeled  with  correct  answers.  That  is  an  algorithm  is  made  based  on  a  known  relationship between input and output.  Supervised learning – estimating a function from labelled training data.  Forms of supervised learning include:    Regression problems  o  Linear Regression (linear model) o  Logistic Regression (linear model)    Classification problems  o  Support Vector Machines (both linear and nonlinear) o  Decision Trees (nonlinear) o  Random Forest (nonlinear) o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)    Ranking problems  Loss function – a measure of accuracy. It is used to compute the error between the actual result and what we calculated as the result to be.  Examples of loss functions include square loss, absolute loss (both useful for regression) and logistic and hinge loss (useful for classification).  Empirical risk - You can calculate the empirical risk by averaging the results of the loss function. The lower the empirical risk based on the training data, the closer the function represents the true relationship      If we choose higher complexity than necessary, we would be over-fitting the data If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalisation.  What is generalisation? It is prediction on unseen data, that is, the data, which is not part of our training set.  Occam’s Razo - This principle often paraphrased as:  All other things being equal, the simplest solution is the best.  Structural  risk  minimisation seeks  to  prevent  over-fitting  by  incorporating a  penalty on  the  model complexity that prefers simpler functions over more complex ones.  General idea is to minimise both Structural Risk and Empirical Risk.  Classification Metrics (also called a contingency table)  A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class. Confusion matrices are a way to understand the types of errors made by a model.   Accuracy may not be a useful metric for imbalanced class problems.  F-1 Measure  Another  useful  metric  could  be  the  combination  of Precision and Recall.  This  metric  combines  both Precision and Recall in a single number.  Receiver Operating Characteristics (ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels. Recent years have seen an increase in the use of ROC graphs in the machine learning community. ROC curve is especially useful for domains with imbalanced class distribution and unequal classification error costs.  Regression Performance Metrics  Mean Square Error - To measure how close the predictions are to the true target values.  R-squared - Percentage of the dependent variable variation that a linear model explains:  R-squared is always between 0 and 100%:      o  0% represents a model that does not explain any of the variation in the response variable around its mean. The mean of the dependent variable predicts the dependent variable as well as the regression model.  o  100% represents a model that explains all of the variation in the response variable  around its mean.  Multiple training/test splits allow us to re-use same data for both training and evaluation in different splits.  We usually work with 3 methods for splitting data:  1)  Random subsampling – repeatedly partitions the data into random training and test sets in a specified ratio.  We train the model with each training set and estimate an accuracy using the corresponding test set. We finally average the accuracies to get an averaged estimate.  2)  Stratified sampling - Stratified sampling is a probability sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata.  3)  Cross validation. Another method for partitioning data which is even more popular among researchers is Cross-validation.  This  is  a  technique  to  evaluate  models  by  partitioning  the  original  sample  into a training set to train the model, and a test set to evaluate it.  The main idea is to partition training data into equal sized sub-samples. Then iteratively leave one sub- sample out for the test set, train on the rest of the sub-samples.  Hyperparameter – is a parameter whose value is set before the learning process begins. Given a choice of hyperparameter values, you use the training set to train the model.  Validation set - A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.  Finding the best hyperparameter    Decide a possible range   Define a search grid within the specified range   Train a model using each hyperparameter value from the search grid and assess its performance  on a validation set (separated from the training set)    Compute the performance on the validation set for each hyperparameter value and select the one with the best performance. Once the model is working with the best hyperparameter we defined it’s ready to be tested on separate test data.  Note: If there are only a few cases to evaluate as a hyperparameter – may lose good options by restricting the search to this degree. However, extending the grids to search for very small continuous values, although can be more accurate in finding the best value of the hyperparameter, it can be extremely computationally expensive (i.e. may take your machine a long time to run). A grid-search will build a model on each possible value for hyperparameter.  All the techniques for model assessment are applicable for training/validation set splitting:    Random subsampling   Stratified subsampling   Cross-validation    We are still assessing how a particular hyperparameter is doing on the validation set. This step is internal to the learning process and different from model assessment on the test data.  We can select the best hyperparameter set by searching/or optimizing over all possible values. Let us show you 3 possible ways to navigate the hyperparameter space:    Grid-search (not so efficient).   Random search (efficient in certain scenarios)   Bayesian optimization (efficient in general)  Effect of imbalanced classes  One problem that can occur in machine learning is datasets where the total number of one class of data (i.e. positive outcomes) is far less than the total number of another class of data (i.e. negative) outcomes.  Most machine learning algorithms work best when the number of instances of each classes are roughly equal. When the number of instances of one class far exceeds the other, problems arise.  Solutions – effect of imbalance classes  First, we can perform some actions on the data itself.  Alternatively, we can improve our algorithm to be able to handle such phenomenon.  At the data level: (Re-Sampling)    over-sampling the data from minority class   under-sampling the data from majority class.  Two obvious solutions based on data manipulation:  -  Sample more data points from the minority class to cover the difference. -  Under-sample the majority class to make them have an equal effect on the algorithm.  At the algorithmic level:    adjusting the costs   adjusting the decision threshold.  From an algorithmic point of view, we may want to adjust some costs on the points we are observing from the majority class to dampen their effect. Also, we can manually define some thresholds to cope with the unbalanced data.  The  above  summary  has  been  created  using  the  full  course  material  for  Topic  5  (including  the relevant YouTube clips)     