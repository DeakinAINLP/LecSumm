Overview In topic 5, we focused on the following learning goals:    Differentiate supervised learning from unsupervised learning.   Estimate the performance of different supervised learning  models.    Implement model selection and computer relevant evaluation  measures.  Forms of Supervised Learning The majority of machine learning applications use supervised learning. In supervised learning, the data used to train the algorithm is already labeled with correct answers. In other words, you make an algorithm based on the known relationship between the input and output.  Supervised learning can appear in many forms:    Regression Problems  o  Linear regression (linear model) o  Logistic regression (linear model)    Classification Problems  o  Support vector machines (both linear and nonlinear) o  Decision trees (nonlinear) o  Random forest (nonlinear) o  Neural networks: Perceptron and Multi-layer perceptron  (nonlinear)    Ranking Problems  Supervised Learning Algorithm Finding a Function  In supervised learning, given the training data, the learning algorithm seeks a function h where X is the input space and Y is the output space. And the questions is how can we measure the quality of function h? how can we understand how accurately h can map X to the target Y?  Loss Function The loss function is really a measure of accuracy. How accurately does your h function describe the relationship between X to the target Y?  Empirical Risk Similar to the loss function, we can define a factor called empirical risk. Among all functions in hypothesis space, we select the function h, which minimizes the empirical risk. You can calculate the empirical risk by averaging the results of the loss function. The lower the empirical risk the closer the function to represent the true relationship between the pair of values xi and yi.  The Concept of Model Complexity  Sometimes, we may not know if the classification problem is linearly separable or non-linearly separable. And the questions is: what should be the right complexity of the model that we use to fit the given data?  Let’s examine the effects of selecting different models in terms of complexity:  -  If we choose higher complexity than necessary, we would be  over-fitting the data  -  If we pick lower complexity, we would be under-fitting the data. -  It is important to get the best possible fir for good  generalization  Generalization: prediction on unseen data, that is, the data which is not part of our training set.  Occam’s Razor A famous problem-solving principle used as a heuristic guide in the development of theoretical models.  This principle is paraphrased as: all other things being equal, the simplest solution is the best.  In other words, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity.  Structural Risk Minimization This seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. So the general idea is to minimize both structural risk and empirical risk.  Classification Metrics The choice of evaluation metrics influences how performance is measured and compared.  Confusion Matrix  Is a summary of prediction results on a classification problem. Confusion matrices are also called contingency tables.  One of the reasons to use confusion matrix is that accuracy is not a reliable metric for the real performance of a classifier. If the data set is unbalanced (i.e. when the numbers of observations in different classes vary greatly), it will yield misleading results.  ROC Curve Receiver Operating Characteristics (ROC) curve has been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels.  F-1 Measure Another useful metric could be the combination of precision and recall. F1-measure is a metric that combines both precision and recall in a single number.  Regression Metrics Regression measures how fat the expected value is from the actual value. The ways of measuring regression performance:  -  Mean square error (MSE)  -  Root mean square error (RMSE)  -  Mean Absolute Error (MAE)  Explained Variance (R-square) r-square is measured as the percentage of target variation that is explained by the model.  Partitioning Data for Training and Testing  -  A single training set may be affected by outlier instances -  To get a reliable estimate of model performance, we need a large test set because variance of such an estimate is low -  We know the larger the size of the training set, the more  accurately the model can be learnt  -  Multiple training/test splits allow us to re-use same data for  both training and evaluation in different splits.  We usually work with 3 methods for splitting data:  -  Random subsampling -  Stratified sampling -  Cross validation  Random Sampling  Stratified Sampling  Cross Validation   Finding the Best Hyperparameters In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. This means the value of a hyperparameter in a model cannot be estimated from data. They are often used in processes to help estimate model parameters.  To find the best hyperparameter, we partition training data into separate training and validation sets  To find the best hyperparameter:  1. Decide a possible range for hyperparameters, for example a  bounded interval such as [0,1]  2. We then define a search grid within the specified range. 3. Train a model using each hyperparameter value from the search  grid and assess its performance on a validation set.  4. Finally, we compute the performance on the validation set for  each hypermeter value and select the one with the best performance. Once the model is working with the best hyperparameter we defined its ready to be tested on separate test data.   Internal Cross-Validation  Effects of Imbalance Classes A problem where the total number of one class of data is fat less than the total number of another class of data outcomes.  Example:  When developing a breast cancer diagnosis model, imbalanced class problem is encountered because the risk of a female being diagnosed with breast cancer is 1 in 8.  Solutions  We have two approaches: first we can perform some actions on the data itself. Alternatively, we can improve our algorithm to be able to handle such phenomenon.  