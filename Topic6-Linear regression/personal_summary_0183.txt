Review of topic 5  Introduction  During this topic, we started to examine supervised learning.  In supervised learning, the model is trained, using labelled data containing the answers. By doing this a mapping between the input data and the desired answers is created. Then the trained model can be fed new unseen data and predict the output answers.  One the issues when training is that the model can be ‘over-fitted’: in other words it can very accurately predict the training data, but fail badly on non-training data.   Forms of supervised learning  Supervised learning is a type of machine learning in which a dataset is labelled, with each data point being an input vector of features, and a corresponding target The goal is for the model to learn a mapping between the input features and the output target from the labelled data, so that the model can predict the output targets for new unseen data.  When training a model a loss function is used to measure the difference between the predicted outputs and the actual outputs. The model’s parameters are iteratively tuned to minimise this difference. The process is stopped when the desired level of accuracy is reached. This accuracy is typically evaluated on a set of test data, which has been kept separate from the training data. This ensures that the model is able to generalise results from new data. This testing gives supervised learning an advantage over unsupervised learning, as we have a fairly good idea of how our model is going to perform.  There are several types of supervised learning:   Regression: used to predict a continuous target. Regression can break down further into:   Linear regression   Logistic regression   Classification: used to predict a discrete target. Classification can be broken down down  further:   Support Vector Machines   Decision Trees   Random Forest   Neural Networks   Ranking: used to rank items based on their relevance.  It is possible to generate classification output from regression output, but this requires the definition of threshold values to convert the continuous output of the regression model into discrete classes. For example, let's say there is a regression model that predicts the likelihood of a customer buying a product, and the output ranges from 0 to 1. A threshold of 0.5 could be set, such that any predicted value above 0.5 is classified as a "buy" and any predicted value below 0.5 is classified as a "not buy". This is known as thresholding.  Generating regression output from classification output is harder, as the classification model predicts discrete class labels rather than continuous values. One technique is to feed the output from a classification model into a regression model. As an example, let’s say there is a classification model that predicts if a patient has a certain disease or not: this prediction can be fed into a regression model to predict the severity of the disease. This is known as probability calibration.   Examples of supervised learning include image recognition, speech recognition, language translation and credit risk assessments.  A supervised learning algorithm  The goal in supervised learning is to find a function that accurately maps the input vector to the desired target output. In order to find this function, we need to be able to measure the quality of the functions that we are trying. This is done via the loss function, which measures the difference between the predicted target and the actual value. There are different loss functions that can be used. By example:    Square loss, commonly used in regression problems: it measures the squared difference between the predicted output and the true output. L(Y,f(x)) = (Y – f(x))2 where Y is the true output, f(x) is the predicted output, and L is the loss function. It penalises larger differences more than smaller differences.   Absolute loss, commonly used in regression problems: it measures the absolute difference  between the predicted output and the true output. L(Y,f(x)) = |Y – f(x)| where Y is the true output, f(x) is the predicted output, and L is the loss function. It is less sensitive to outliers and large errors in the predictions    0 – 1 loss, commonly used in binary classification problems, where the goal is used to predict one of two possible outcomes, often represented as a 0 or a 1. The loss is 0 if the predicted output is the same as the true output, and 1 if they are different. L(Y,f(x)) = { 0 if Y = f(x), 1 if Y ≠ f(x) where Y is the true output, f(x) is the predicted output, and L is the loss function. The 0 - 1 loss function is not differentiable, making it unsuitable for many optimisation algorithms.   Logistic loss, commonly used in binary classification problems: it measures the difference between the predicted probability and the true probability, using logarithmic loss. It is both differentiable and convex.   Hinge loss, commonly used in classification problems: it measures the difference between  the predicted output and the true output, subject to a margin parameter. It is not differentiable at 0, making it unsuitable for many optimisation algorithms.  In addition to the loss function, a factor termed empirical risk is defined. This is a measure of how well a model fits the training data, and is used to optimise the model parameters to minimise the expected risk on future unseen data. It is calculated by averaging the results of the selected loss function.  Model complexity  A big question is what is the correct level of complexity for our model? If too high, then we run the risk of over-fitting the data (capturing the noise in the training data). If too low, then we run the risk of under-fitting the data, and not being able to make accurate predictions.   Occam’s razor  Occam's razor is a philosophical and scientific principle that suggests that, when presented with multiple explanations for a phenomenon, the simplest explanation is usually the correct one.  In the context of machine learning, Occam's razor suggests that, when given two models that perform equally well on a given task, the simpler model is likely to be a better choice, because it is less likely to over fit the data and more likely to generalise well to new, unseen data.  In practice, this means that, when developing a machine learning model, it is important to balance model complexity with performance. A complex model may be able to fit the training data better, but it is also more likely to over-fit and perform poorly on new data. On the other hand, a simpler model may not fit the training data as well, but it is more likely to generalise well and perform better on new data.  Structural risk  Using Occam’s razor as a guide, structural risk describes the act of putting a penalty on model complexity: this leads to a preference for simpler models. The goal is minimise both empirical risk and structural risk.  A more complex hypothesis is likely to lead to over-fitting the model to the training data. The complexity of the hypothesis depends on the model in use, and so can be measured in several different ways. For example, in a linear regression, it can be measured by the number of features used. In a neural network it can be measured by the number of layers, nodes and weights.  The complexity of a hypothesis is closely related to structural risk. Reducing both is the goal.  Classification metrics  The way in which we measure the performance of our model are important.  Confusion Matrix  A confusion matrix is a table that is used to evaluate the performance of a classification model.  In a binary classification problem, a confusion matrix will contains the the true positive, false positive, true negative and false negative values. For example:  Actual/Predicted Positive Negative  Positive True Positive (TP) False Positive (FP)  Negative False Negative (FN) True Negative (TN)  The columns represent the predicted classes, and the rows represent the actual classes (this is the format used by the Python sklearn library). The diagonal from top left to bottom right represents the number of correct predictions. The off diagonal entries represent the number of incorrect predictions.   Confusion matrices can grow in size to match the number of classes. For example, consider the following three classes: Ferrari’s, Tesla’s and Morgans.  Actual/Predicted Ferrari Tesla Morgan Out of 100 samples: 25 Ferrari’s, 30 Teslas and 45 Morgans.  Ferrari 10 0 5  Tesla 0 30 0  Morgan 10 0 35  Again, the diagonal from top left to bottom right represents the number of correct predictions, and the off diagonal entries represent the number of incorrect predictions. From the above we can see that our model is really good at predicting Teslas, is okay for Morgans, but really bad at predicting Ferrari’s.  As can be seen from this example, the higher the proportion of values on the diagonal of the matrix, the better the classifier is. Also, a confusion matrix is a useful tool for evaluating the performance of a classification model and gaining insights into its strengths and weaknesses  ROC Curve  A ROC (Receiver Operating Characteristics) Curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR).  The TPR is defined as TP/(TP +FN) and represents the proportion of true positives that are correctly classified as positive.  The FPR is defined as FP/(FP +TN) and represents the proportion of false positives that are incorrectly classified as positive.  The ROC Curve is plotted at different classification thresholds. Lowering the threshold will classify more items as positive. To avoid computing the points in a ROC Curve many times with different classification thresholds, we can instead use the Area Under the Curve (AUC). The AUC provides a method to measure the performance across all of the possible classification thresholds. AUC will range in value from 0 to 1. A model that has predictions 100% wrong will have an AUC value of 0.0; one that has predictions 100% correct will have an AUC value of 1.0  F-1 Measure  The F-1 Measure is also known as the F1 Score or F-Score, is the harmonic mean of the precision and recall, where precision is the proportion of true positives amongst all predicted positives, and recall is the proportion of true positives amongst all actual positives.  The formula is:  F1 = 2 * (precision * recall)/(precision + recall)  The F1-Measure is a very useful metric in those cases where the classes are imbalanced, meaning that one class is much more common than the other.   The F1-Measure ranges from 0 to 1, with a higher value indicating better performance.  Regression metrics  Through an accident of history, we use the term “regression” to describe the distance between the expected value and the actual value. There are several ways of measuring regression performance.  Mean Squared Error  Mean Squared Error (MSE) gives an idea as to how close the predictions are to the actual values. The smaller the MSE, the smaller the errors. MSE is calculated as follows:  The Root Mean Square Error (RMSE) is calculated as follows:  Again, the lower the RMSE, the better the performance of the model.  Finally, Mean Absolute Error (MAE) is calculated as follows:  The lower the MAE, the better the performance of the model.  Explained Variance (R2)  Explained variance measures the proportion to which a mathematical model explains the variation in a given data set. In simpler terms: it is the difference between the expected value and the predicted value.  Partitioning data for training and testing  Partitioning data for training and testing is a common practice when performing supervised learning. The data is split into two disjoint subsets: one to train the model, the other to test it.  The training set is used to fit the model to the data, and the testing set is used to evaluate the performance of the model on new and unseen data.  Some points to bear in mind:   A single training set might be affected by outlier instances.   To get a more accurate picture of the performance of the model a larger test set is good.    The larger the training set, the more accurate the model’s learning.   Multiple training/testing splits allow us to use the same data for both training and  evaluation.  There are different ways to partition the data, depending on the size of the dataset and the goals of the analysis.  Sub-sampling: One common approach is to randomly divide the data into two sets: a training set, which contains a randomly selected portion of the data (usually around 70-80%), and a testing set, which contains the remaining portion of the data. We can do this repeatedly, and average the results to get our estimated accuracy.  Stratified-sampling: the population is divided into non-overlapping subgroups, then a random sample is drawn from each subgroup in proportion to its size in the overall population. In this way in can be ensured that the class proportions can be maintained across each random set. For example, if a population contains 60% females and 40% males, then a simple random sample may not be representative of the population, as it may contain more of one gender than the other. In this case, stratified sampling would be used to ensure that the sample contains 60% females and 40% males.  Cross-validation: In this approach, we divide the data into multiple subsets, training the model on some subsets and testing it on the others, and averaging the performance across all subsets.  The choice of sampling technique used depends on the question being answered, the type of data to hand, and the goals of the analysis. Cross-validation is useful if the data is limited. Stratified- sampling is excellent if the data has subgroups that differ significantly from each other.  Finding the best hyperparameters  “Hyperparameters” is the term used to describe the parameters whose value we set before the training process is started. Although we often use heuristics when selecting appropriate values for our hyperparameters, there are several methods that we can use to help us make informed selections:  1. Grid search: we define a range of hyperparameter values for each parameter, and the  algorithm tries all possible combinations to find the best combination.  2. Random search: the algorithm randomly selects parameter combinations to try.  3. Bayesian optimisation: a probabilistic model is used to select the values, based on previous  evaluation results.  4. Manual tuning: we tinker and experiment with different values, based on domain knowledge  and experience.  With all of these it is important that we use a validation set to test the model performance for each combination of hyperparameter values. A validation data set is reserved from the labelled data when we create the training and testing sets. It is important to note that the validation set should be used  for model evaluation and tuning during the training process, but it should not be used for final model evaluation or hyperparameter tuning, as this can lead to over-fitting on the validation set.  The effect of imbalanced classes  Imbalanced classes can have a big impact on the performance of a model in supervised learning. Imbalanced class occur when one class (termed the ‘minority’ class) has significantly fewer samples than the other class(es) (termed the ‘majority’ class(es)).  When there are imbalanced classes the model can become biased towards the majority class(es), as it can achieve high accuracy by predicting the majority class(es) for all samples. This can result in poor performance on the minority class: which could be class of interest in many applications, such as medical diagnosis or fraudulent payments detection.  There are several ways in which we can address the issue of imbalanced classes. They include:  1. Resampling at the data level: Modifying the class distribution in the training set by either  oversampling the minority class or under-sampling the majority class. Oversampling of the minority class can be achieved by duplicating samples from the minority class, or by generating synthetic samples. Under-sampling the majority class will reduce the bias, and can be done by simply removing samples from the majority class, or by only selecting smaller representative samples.  2. Adjusting the costs at the algorithmic level: Assigning different misclassification costs to the different classes, based on their importance. For example, misclassifying a sample from the minority class can be made far more expensive than misclassifying a sample from the majority class – and this will be reflected by the cost function used to train the model.  3. Adjusting the decision thresholds at the algorithmic level: The decision threshold can be  adjusted to achieve better performance for the minority class. Note that we should check the effect on both the positive and negative class predictions when we adjust the decision thresholds.  Some real world examples of imbalanced classes are the aforementioned fraud detection and medical diagnosis. Recommendation systems are another example, as some products or services may be so much more popular than others, leading to an imbalanced distribution of ratings or recommendations.  Fitting a regression line  Linear regression uses the relationship between variables to draw a straight line through them. That line can then be used to predict future values.  The formula for a simple regression line is: y = wo + w1x1  A closed form solution is simply an equation that can be solved in terms of maths operations and functions (Weisstein n.d.). Linear regressions have a closed form solution. This means that we can solve linear regressions without the need to iteratively improve our optimisation functions.  