Form of Supervised Learning  Most practical machine learning application use supervised learning, meaning they use data to train the algorithm that is already labelled with correct answers. The algorithm is based on the known relationship between the input and the output from which a mapping function can be developed from the input variable to the output variable.  Patterns aren’t only based on similarity, direct mapping or function between a feature vector and the output are also utilised in supervised learning, meaning that supervised learning is the task of estimating a function from labelled training data.  For each input there is a matching output that you know should occur, this gives supervised learning a significant advantage over unsupervised learning as you already know and can test whether suitable outputs are being provided by the application.  Regression problems appear as Linear Regression (linear model) or Logistic Regression (linear model). Classification problems can appear as Support Vector Machines (linear or nonlinear), Decision Trees (nonlinear), Random Forest (nonlinear) or Neural Networks (nonlinear).  A Supervised Learning Algorithm  An example supervised learning algorithm has n training data [xi, yi] with a goal of finding the function that’s as close as possible to the unknown function to determine the relationship between x and y. The output set should be obtained by applying the function to the input set, meaning that for each element in the input set there is a corresponding element in the output set. Figuring out the relationship between the elements is the goal, this relationship is the function.  Hypothesis space:  A hypothesis function (h) is a range of possible functions usually called the hypothesis space (H). The hypothesis function is selected based on what we think is similar to the actual function behind the data.  Finding a function:  We want a function h that can determine the correct output based on a given input which can be represented as h : X →Y. To measure how well function h and map X to the target Y we need to introduce the loss function.  Loss function:  Loss function is a measure of the accuracy of h, how accurate is h in describing the relationship between X and Y. Applying h to training instance x results in an output of h(xi), the function can be represented as ~yi = h(xi). Since this is supervised, we already know what the output is supposed to be, its supposed to be yi meaning that we can measure the quality of h by measuring the difference between ~yi and yi. That is what the loss equation is, L(~yi, yi).  Empirical Risk:  Among all the functions in hypothesis space we want to select the function h that minimises empirical risk. Empirical risk is calculated by averaging the results of the loss function, meaning that lower risk indicates a function closer to representing the true relationship between x (input) and y (output).   The Concept of Model Complexity  The complexity of them model we use to fit a given set of data can be difficult to determine as we may not know much about the data, different levels of complexity have different advantages and disadvantages depending on their suitability to the data. A model that is of a higher than necessary complexity will result in over-fitting the data. A model that is of a lower than necessary complexity would result in under-fitting the data.  Model Complexity and Occam’s razor  Occam’s razor can be used as a heuristic guide in the development of theoretical models, it involves the simplest solution most often being the best. Thus, if there are multiple hypothesis with a similar fit it recommends, we choose the hypothesis that is the least complex and involves the fewest assumptions.  Structural Risk Minimisation  Structural risk minimisation aims to prevent over-fitting as it incorporates as penalty on model complexity that prefers simpler functions over more complex ones. We aim to minimise both the empirical risk and structural risk which were introduced before. Risk can be determined by the loss function of ~yi and yi for all training points in conjunction with the complexity of the proposed model as a penalty.  Classification Metrics  The metrics chosen to evaluate your machine learning model are important as they influence how performance is measured and compared. The most common type of machine learning applications are classification problems which have many different metrics for evaluation.  Confusion Matrix:  A confusion matrix is the summary of the prediction results from a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class, this helps us determine the type of error made by the model. Confusion matrices can also be referred to as contingency tables.  Confusions matrices are used since accuracy alone isn’t reliable for determining the actual performance of a classifier. An unbalanced data set (when the amount of observations for each class vary greatly) it will have misleading results eg. if there are 90 apples and 10 oranges a classifier may just classify everything as apples which will be mostly accurate with that specific data set but will not actually train the classifier to determine the difference between apples and oranges and will not be useful with other data sets.  Depending on the classes and what is being classified the importance of different errors can vary, in some situations a false negative can be far more detrimental than a false positive. We can define other evaluation metrics based on a confusion matrix.  True Positive Rate (also called Recall or Sensitivity) is the fraction of true positive samples that have been predicted over the total amount of positive samples.  False Positive Rate is the fraction of false positive samples over the total amount of negative samples.  ROC Curve:  Receiver Operating Characteristics (ROC) curve depicts the trade-off between the true positive rate and false positive rate over noisy channels. ROC curve can be particularly useful when dealing with imbalanced class distribution and unequal classification error costs. The ROC curve is created by  plotting the true positive rate against the false positive rate at various different threshold settings, this depicts the relative trade-offs between benefits (true positives) and costs (false positives).  F1 Measure:  Another useful metric could be the combination of Precision and Recall. F1 measure combines both Precision and Recall in a single number, it is 2 x (Precison x Recall) / (Precision + Recall)  Regression Metrics  Regression measures how far an expected value is from the actual value. There are multiple different metrics for measuring regression performance. Mean Square Error (MSE) measures how close predictors are to true target values, the lower the MSE of a model the better its performance. Explained Variance (R2) is the percentage of target variation that is explained by the model, the higher the R2 of a model the better its performance.  Partitioning Data for Training and Testing  Using only a single training/testing set limitations:  a single set may be affected by outliers.  - -  A reliable estimate needs a large test set so that variance is low. -  The larger the size of the set the more accurately the model may learn. -  Multiple training sets allows for the re-use of data for training and evaluating the different  splits.  Sub-sampling  Instead of using a single split, it involves repeatedly partitioning the data into random training and test sets in a specific ratio.  Stratified Sampling  Is a probability sampling technique that involves dividing data into different subgroups (or strata) and then randomly selecting the final subjects proportionally from the different subgroups. When using randomly selected training sets it may result in differing class proportions between the training and test splits. Stratified sampling ensures that class proportions are maintained in each random set.  Cross-validation:  Involves evaluating models by partitioning the original sample into a training set to train the model and a test set to evaluate it. In other words, data is partitioned into k equal sized sub-samples, after we iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples.  For examples, when split into 5 sub-samples we use the sample 1 for testing and then use the samples 2,3,4,5 as training and store the accuracy, then we use sample 2 for testing and sample 1,3,4,5 for training and store the accuracy and so on. Once we have gone through them all we then get the average accuracy.  Finding the best hyperparameters  A hyperparameter is a parameter whose value is set before the learning process begins.  Hyperparameters can frequently be set using heuristics and tuned for a specific predictive modelling problem. To find the best hyperparameters the training data will be partitioned into separate training and validation sets.  A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.  The validation set helps us set the values for the hyperparameters as we can use it to evaluate the performance of our model when using differing combinations of hyperparameters, allowing us to then keep the best trained model.  Internal Cross-Validation  Random subsampling, stratified subsampling and cross-validation are all applicable for training/validation set splitting. However, we are still assessing how a particular hyperparameter performs on the validation set. Instead of using a single validation set, cross-validation can be used within a training set to select the best hyperparameters.  Effect of Imbalanced Classes  One problem that may occur during machine learning is where the total number of one class of data is far less than the total number of another class, machine learning algorithms work best when the number of instances of each class are roughly equal. We can try to fix this by either acting on the data itself or acting on the algorithm itself. At the data level we can either over-sample the data from the smaller class or under-sample the data from the larger class. At the algorithmic level we can either adjust the costs or adjust the deciding threshold.  Issues of imbalanced classes:  -  If only a few samples of a minority class exist even a bad classifier will inevitably get a high accuracy. This is dealt with by using other evaluation metrics as opposed to just accuracy.  -  When doing random subsampling it is possible that the class proportion isn’t maintained in  some partitions, it is possible to not even sample one instance from the minority class. This is addressed by using Stratified Sampling.  Reflection  Going over this topic was most useful in developing my understanding of supervised learning in a more in-depth manner. This was because supervised learning algorithms were broken down to how they work fundamentally to find the function that explains the input and output, from this perspective I feel I better understand the fundamental of supervised learning and the associated risks. Also through this module I was introduced to the importance of model complexity, how model complexity affects different types of data and how we can choose a suitable model complexity. I was also newly introduced to the different methods that can be used for data partitioning and testing along why and when specific metrics and techniques should ne used.  