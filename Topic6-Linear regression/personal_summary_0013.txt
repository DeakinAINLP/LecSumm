This topic’s content shifts our focus from unsupervised learning to supervised learning. The topic’s content takes us through the key concepts of supervised learning as well as a Python practical for implementing a supervised learning model. Supervised Learning As mentioned, this topic’s content focuses on supervised learning – so what exactly is supervised learning? Supervised learning is a type of machine learning in which models are trained using data that is already labeled with correct answers. Essentially, supervised machine learning works through the use of an algorithm based on the already known input and output relationship from which, a function can be mapped between the input variable and the output variable. Given this, supervised learning works through a direct mapping or function that works to calculate a given output, or target, from a given input or feature vector, as opposed to finding patterns based purely on similarity. Supervised learning is used in the majority of practical machine learning applications and as such is very important. Forms of Supervised Learning Supervised learning is seen in many forms, some of these include: -  Regression problems such as Linear Regression or Logistic Regression -  Classification problems such as Support Vector Machines, Decision Trees, Random Forest and Neural Networks -  Ranking problems Complexity in Supervised Learning Complexity refers to how flexible the model is in terms of fitting itself to the training data. A higher complexity model makes use of more parameters to be able to capture more detailed patterns from the training data. This can be useful but can also lead to overfitting, which is when the model follows the training data too closely and fits the noise and random variations as opposed to fitting the general patterns. In a case of overfitting, the model is bad at making predictions on unseen data as it cannot adapt the patters it has learnt to fit this new data. A model that is too low in complexity can also be problematic as in this case, the model has not learnt the patterns from the training data well enough and as such cannot apply these patterns to unseen data. Given these possibilities, it is important to fit a model in a way that attains good generalization of the training data’s patterns. How to Choose a Model To choose a model that works best based on the training data, there are several metrics that can be measured to rate the model’s performance. These include: -  Confusion Matrix: This is a summary in matrix form of the models predictions and allows the reader to see the amount of true positives, true negatives, false positive and false negatives. This also allows the reader to see where the model is performing poorly. -  Accuracy, True Positive Rate and False Positive Rate: these are several equations that measure the performance of the model. Accuracy simply measures the percentage of correct predictions compared to all predictions made, whilst True Positive Rate measures the percentage of true predictions compared to the True Positive and False Negative counts. Similarly, the False Positive Rate measures the number of False Positives predicted compared to the number of True Negatives and False Positives. -  ROC Curve: this measurement allows for the true positive and false positive rates over noisy channels to be compared which allows for the optimal threshold for noise to be determined. F-1 Score: this is an equation that combines the previously mentioned accuracy as well as precision. Precision is a score that measures the number of true positives predicted compared to the total number of true and false positive predictions. - Regression Specific Metrics When evaluating regression tasks, there are a handful of metrics that can be measured specifically for these. These include: -  Mean Square Error (MSE): This metric is calculated by using the average of the squared differences between the values predicted by the model and the true values. In other words, it is the average of the predicted value subtracted from the actual value and squared for all predictions. -  Root Mean Square Error (RMSE): as the name suggests, this is a metric that is largely similar to MSE with one difference in that it is the square root of the MSE. RMSE is easier to read as it is in the same unit structure as the actual and predicted values making it easier to understand what the model was incorrect by on average. -  Mean Absolute Error (MAE): MAE is very similar to MSE in that it calculates the average of - the differences between true and predicted values. The biggest difference is that MAE is less sensitive to outlier values. Given that MSE squares the false predictions, it gives these predictions more weight and thus can skew the results. MAE on the other hand does not square the differences and as such is less sensitive to outliers. Explained Variance (aka. R-Square, the coefficient of determination): this is the percentage of the target variation that is actually explained by the model. The variation refers to the differences in values of the training data. A model with a high R-Square score will have data points closer to the regression line whilst poorly scoring models will have data points far away. Data Partitioning in Supervised Learning An important part of training and evaluating a supervised machine learning model is data partitioning. This involves splitting the training data into two sets of data. One for training and one for testing. This allows the model to be trained and then evaluated using separate sample sets from the same initial set. Imbalanced Classes An issue that can arise in machine learning is datasets with classes that are heavily disproportionate. For example, a dataset where the amount of data for one class is only 5% of that of another class. This will lead to the model heavily favouring the class with more data even if this is not the true scenario. To address this, over and under sampling can be added to the model in which it will over sample data from the smaller of the classes and under sample data from the larger to help balance the classes out. 