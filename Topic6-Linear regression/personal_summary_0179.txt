In this topic  we learnt about  Supervised  Learning. Data will be already labelled in Supervised Learning. We are using an estimation function that can map input data to a class label. We estimate the function from the labelled training data. Different types of supervised learning algorithms are Regression  problem,  Classification  problem  and  Ranking  problem.  For  Regression  and Classification problem, we will have both linear and non-linear form of supervised learning. Based on the type of dataset we have we need to decide on whether to use linear or non-linear data. For creating a model to predict stock market, we need to use non-linear form as the amount of data will be enormous and fluctuating.  In supervised learning we train the model with training data and start with a hypothesis function ‘H’ from hypothesis space that is similar to the true function behind the data and then we make the model as accurate as possible to the main unknown function. A loss function is a mathematical function that quantifies  the difference between the predicted output and the actual output for  a given input. The aim of the training process is to find the values of the model's parameters that minimize the loss function over the entire training dataset. Examples of loss function are Square loss, Absolute loss, 0-1 loss, Logistic loss, Hinge loss etcetera Empirical risk is used to evaluate how well the model fits the data.  Model Complexity depends upon linear or nonlinear problems in machine learning. High complexity  and  lower  complexity  can  lead  to  over-fitting  and  under-fitting  of  the  data.  While considering model complexity we can use Structural risk instead of Empirical risk which considers both  Model  Complexity  and  Empirical  Risk.  In  terms  of  model  evaluation  we  Classification metrics and Regression metrics. Confusion matrix is commonly used in evaluating performance of models in classification problems. It is used to evaluate the performance of a machine learning algorithm on a set of test data for which the true values are known. It is used instead of accuracy metric  as  it  is  not  always  reliable.  Receiver  Operating  Characteristic  (ROC)  curve  is  another important classification metric and is utilized for domains with imbalanced class distribution and unequal classification costs. In this curve the True Positive Rate is plotted against False Positive rate various threshold levels. Area Under Curve and Youden Index can be calculated using ROC Curve. For Regression problems we use Regression metrics such as Mean Square Error (MSE), Root  Mean  Square  Error  (MSE),  Mean  Absolute  Error  (MAE),  Variance  (R-square)  etcetera. Higher value of variance depicts better performance of a model and its value ranges between 0 and 100%.  For machine learning we divide dataset into train data to train model and test data to evaluate the model.  Different  techniques  for  partitioning  the  data  are  Random  Sub-sampling,  Stratified Sampling and Cross validation. Random sub-sampling partitions the data into random training and test sets in a specified ratio. In stratified sampling, the dataset is divided into subgroups or strata based  on  some  characteristics  or  attributes,  and  then  samples  are  taken  from  each  stratum  in proportion to its size in the dataset. Cross validation partition the dataset into k equal subsets.  Hyper parameter is a parameter that are set before the learning process of models starts. It have a significant  effect  on  the  performance  of  model.  Number  of  hidden  layers  in  Neural  Network,   number  of  trees  in  Random  are  examples  of  hyper  parameters.  Methods  such  as  Grid  Search, Random Search, Bayesian Optimization etcetera can be used to find best hyper parameters. Dataset can  imbalanced  and  it  can  lead  to  difficulty  in  developing  models  due  to  unequal  cost  of misclassification  errors,  data  size,  data  noise  and  data  distribution.  Algorithms  needs  to  be improved  to  be  able  to  handle  such  phenomenon.  Re-Sampling,  adjusting  costs  and  decision thresholds can be used to overcome this imbalanced data problem.  