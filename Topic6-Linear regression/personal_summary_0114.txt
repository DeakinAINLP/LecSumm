Topic 5 summary  This topic we talk about types of supervised learning.  Types of Supervised Learning :    Regression problems  o o  Linear Regression (linear model) Logistic Regression (linear model)    Classification problems  Support Vector Machines (both linear and nonlinear)  o o  Decision Trees (nonlinear) o  Random Forest (nonlinear) o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)    Ranking problems  Structural risk minimization: It is technique that is used to prevent overfitting by applying a penalty to the model complexity that prefers simple functions over complex ones.  Classification metrics:    Confusion Matrix: A confusion matrix is a summary of prediction results on a classiﬁcation problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class.  o  Recall/ Sensitivity: The fraction of true positive samples that have been predicted positive over  the total amount of positive samples. FPR: The fraction of false predicted positive samples over the total amount of negative samples  o o  Precision: It is calculated as the ratio of true positives to the sum of true positives and false  positives    ROC curve: The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate  (FPR) at various threshold settings. F1-Measure: F1-Measure is a metric that combines both Precision and Recall in a single number.    Regression Metrics:  o  Mean Square Error (MSE) o  Root Mean Square Error (RMSE) o  Mean Absolute Error (MAE) o  R-squared: R-square is measured as the percentage of target variation that is explained by the model.  Partitioning data into training and test sets:  o  Random sub-sampling: Samples the data randomly o  Stratiﬁed sampling: Stratiﬁed sampling is a probability sampling technique in which we divide the entire data into diﬀerent subgroups or strata, then randomly select the ﬁnal subjects proportionally from the diﬀerent strata.     o  Cross-Validation: This is a technique to evaluate models by partitioning the original sample into a training  o  set to train the model, and a test set to evaluate it. Internal cross validation: it is a technique used to ﬁnd the best hyperparameters by testing on validation samples instead of the test set samples.     