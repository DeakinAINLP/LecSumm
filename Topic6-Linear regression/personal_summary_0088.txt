In this topic we have revised the concepts of Fundamentals of supervised learning.  At the start we did go through types and foundations of Supervised Learning. In this topic, we are looking  closely at Linear Regression and what are the depending on functions and features and uses of Linear  Regression Model.  Linear Regression Model is mainly useful to check on overfitting and underfitting scenario of the dataset. The  uses of Linear Regression Model are as follows –  1.  Predictive modeling – Linear Regression is widely used in predictive analysis as it can determine  accurate predictions of the dependent variable with known values of independent variable.  2.  Feature Engineering – Linear Regression can also be used to identify the most important features or  independent variables/features that affect the dependent variable. This can be used to improve the  performance and accuracy of the machine learning model.  3.  Hypothesis Testing – Linear Regression can be used to test hypothesis about the relationship between  the independent and dependent variable.  4.  Interpretation – Linear Regression provides interpretable results, which makes the relationship  between the independent and dependent variables.  The Model Complexity  Model complexity refers to the degree to which a model can capture the intricacies and nuances of a given  dataset. A complex model is one that can capture more subtle relationships and interactions between  variables in the data, while a simpler model may only capture the most basic relationships.  The complexity of a model can be influenced by a variety of factors, including the number of input features,  the number and type of parameters, and the complexity of the algorithm used to fit the model to the data. In  general, increasing the complexity of a model can improve its ability to accurately predict outcomes on new  data, but this improvement may come at the cost of increased computational resources, longer training times,  and a greater risk of overfitting (i.e., fitting the model too closely to the training data and not generalizing well  to new data).  It is important to balance the complexity of a model with its performance on both the training data and new,  unseen data. This is known as the bias-variance tradeoff, where bias refers to the error introduced by  simplifying assumptions in a model, and variance refers to the sensitivity of the model to small fluctuations in  the training data. Finding the right level of complexity for a given dataset and problem is a crucial step in  building effective machine learning models.  1. 2. 3.  If we choose higher complexity than necessary, we would be over-fitting the data. If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalisation.  To solve the issues, we use Occam’s razor and its simplistic principle, we define another risk value which is  called Structural Risk. Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on  the model complexity that prefers simpler functions over more complex ones.  The ways to measure regression are as follows-    MSE    RMSE    MAE  Linear Regression makes the data simpler and easier to read, because of this we can solve the problem  and predicting the solutions from the given dataset.  