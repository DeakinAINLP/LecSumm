Main points of this module:    Forms of supervised learning can be regression problems (linear/logistic), classification  problems (support vector machines, decision trees, random forests, and neural networks), and ranking problems. In supervised learning, the algorithm seeks a function given the training data.     A loss function is a measure of accuracy, empirical risk is the average of the loss function.   Over fitting data is when a higher complexity is chosen than necessary, under fitting is when  a lower complexity is chosen than necessary.    Occam’s razor is “all other things being equal, the simplest solution is the best”.   Structural risk minimisation aims to prevent over-fitting by introducing a penalty for higher  model complexity.    A confusion matrix is a summary of prediction results on a classification problem, it is also  known as a contingency table.    A true positive rate (TPR) or recall/sensitivity is the faction of true positive samples predicted  as positive over the total sample amount.    A false positive rate (FPR is the fraction of false positive samples predicted as positive over  the total negative sample amount.    A receiver operating characteristics (ROC) curve is created by plotting TPR against FPR.   The F-1 measure is another useful metric that combines precision and recall.   Mean square error (MSE) is used to measure how close predictions are to true target values.   Explained variance (also known as R-square or the coefficient of determination) is a measure  given by the percentage of target variation that is explained by the model.    Sub-sampling is a more reliable estimate of model performance than a single split and can  be obtained by randomly partitioning repeatedly.    Stratified sampling is a probability sampling technique in which the entire data is divided into different subgroups (strata) then final subjects and randomly selected proportionally from different strata.    Cross-validation is another method which involves partitioning the original sample into a  training set to train the model and a test set to evaluate it.    Hyperparameters can often be set using heuristics and are tuned for a given predictive  modelling problem.    A validation set is a sample of data used to provide an unbiased evaluation of a model fit on  training data whilst model hyperparameters are tuned.    The effect of imbalanced classes is a problem that can occur in machine learning when  datasets which have a total number of one class of data (such as positive outcomes) is much less than the total number of another class (negative outcomes).    To solve imbalanced classes, it is possible to re-sample (over-sample the data from minority class or under-sample from the majority class) or adjust the costs or decision threshold.  