 In this topic, we discussed about supervised learning and its forms, Model complexity, its concepts and how to do data partitioning. Supervised learning is the algorithm where we have the data that is labeled or is categorized. Whereas it can appear in various forms it can be either regression problem or classification problem. In regression problem we have data that is linear or non-linear. In classification if we can categorize the data into two different sets than that is linear problem. For instance, if there are cats and dogs, we can differentiate their categories whereas if there are cats, dogs, people, furniture, or any other things, we would not be able to categorize them into different data easily that would be non-linear dataset. The supervised learning function has various functions as well to be able to categorize them.  Learning algorithm are used to test the performance of model by inputs different data. There are many different loss functions available, for instance, square loss function which includes measuring the distance between predicted and actual value. The other is absolute loss function and 0-1 function Empirical risk minimization is a concept that verifies how well model will perform with training data. It is described as an average of loss function with training data. Whereas it has some limitations as well.  In machine learning, model of complexity is referred to predicting independent variables or features that the model wants to take into account in order to produce accurate predictions.  For instance, a linear regression model with just one independent variable is relatively simple, while the model with multiple variable is more complex. A model that huge amount of data with different variable is hard to train. we have to partition data, then train the data then test the data. We have to train our model and then evaluate the performance of our model. To partition the data, we will randomly partition the data into subgroups, and leave 20%of data to test, this is known as sub-sampling. The other technique that we can use is stratified sampling, which is also known as probability sampling technique, it divides the entire data into different groups or subgroups and then randomly selects sample proportionality from the different data. The third method is cross-validation, which is evaluating the model by partitioning the original sample into a training set to train the model, test set to evaluate it. After partitioning the data, then we train the data, and after training data with test it.  