Summary  I.  Forms of Supervised Learning: In supervised learning, the data used to train the algorithm is already labelled with correct answers which helps us to conduct an algorithm based on the relationship between input and output.  Forms of supervised learning: 1. Regression problems, such as Linear Regression and Logistic Regression (both are linear model).  2. Classification problems, such as Support Vector Machines (linear and nonlinear), Decision Trees (nonlinear), Random Forest (nonlinear), Neural Networks Perceptron and Multi-layer Perceptron (nonlinear).  3. Ranking problems  II.  A Supervised Learning Algorithm: The relationship between inputs and outputs is the function. In mathematical terms, given the training data, the learning algorithm seeks a function on h :X → Y where X is the input space and Y is the output space.  In order to determine the true h function that maps X to Y, two concepts for measuring accuracy has been introduced, Loss function and Empirical Risk. The Loss functions, such as square loss, absolute loss, 0-1 loss measure how well the h function performs by comparing the difference between h(xi) and the true yi value. In terms of empirical risk, lower the empirical risk based on the training data, the closer the function represents the true relationship between the pair of value xi and yi.  III.  The Concept of Model Complexity: When choosing the function to represent the relationship of the train data, we must choose the right complexity of the model to avoid over-fitting (due to higher complexity) and under-fitting (due to lower complexity). In other words, it is important to get the best possible fit for a good generalisation.  IV.  Model Complexity and Occam’s razor:  When multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and         has the least complexity.  V.  Structural Risk Minimisation:  VI.  Classification Metrics: The choice of evaluation metrics influences how performance is measured and compared. The most common type of machine learning applications are classification problems. There are myriad metrics that can be used to evaluate predictions for these types of problems. 1. Confusion Matrix: A confusion matrix is a summary of prediction results on a classification problem. And the correct and incorrect predictions are summarized with count values and go to each class. Confusion matrices shows the types errors made by a model and is called contingency tables.  ROC Curve: Receiver Operating Characteristics (ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels. It depends on the specification of the problem. how much can we afford false positive rate. if we can afford higher false positive rate, we can have higher true positive rate too.  F-1 Measure:  VII.  Regression Metrics:  Measuring Regression Performance: 1. Mean Square Error:        2. Explained Variance (R2):      VIII.  Partitioning Data for Training and Testing:  1. Sub-sampling: this method repeatedly partitions the data into training test sets in specified ratio  2. Stratified Sampling: Stratified sampling is a probability sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata.  3. Cross-validation: This is a technique to evaluate models by partitioning the     original sample into a training set to train the model, and a test set to evaluate it.  IX.  Finding The Best Hyperparameters: We need hyperparameters to partition training data into separate training and validation sets. A validation set is “a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters”.  To find the best hyperparameters, we must determine a possible range. Then, we define a grid within that range. Next, we train a model using each hyperparameter value from the search grid and assess its performance on a validation set. Finally, we compute the performance on the validation set for each hyperparameter value and select the one with the best performance.  Internal cross-validation: is used to find the best set of hyperparameters and the     process is exactly the same as for test/train partitioning.  