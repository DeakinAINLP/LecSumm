In topic 5 , we learnt about the fundamentals of supervised learning. We learnt about the basics in topic 1 about both supervised and unsupervised learning, however this topic expanded more on how supervised learning works within a practical environment. The topics that we covered followed:  -  The forms of supervised learning; and that supervised learning is the majority of  practical ML, and the different forms that SL can appear as, such as:  -  Regression problems  o  Linear regression (linear) o  Logistic Regression (linear)  -  Classification problems  o  SVM (linear/nonlinear) o  Decision tree (nonlinear) o  Random forest (nonlinear) o  Neural Networks (nonlinear)  -  Ranking problems -  Supervised learning algorithms, and concepts such as hypothesis space, finding a  function and loss function.  -  The concept of model complexity, and the effects of selecting different models (high  complexity could mean over-fitting the data, lower complexity could under-fit the data)  -  Occam’s razor: a problem-solving principle, which is used as a heuristic guide in the  development of theoretical models.  -  Structural risk minimization, a risk value based on Occam’s razor, which seeks to prevent over fitting by penalizing the model complexity if it doesn’t need to include complex functions.  -  Classification metrics, which are a method of evaluating the metrics of the MLM. These  include:  o  Confusion matrix; summary of predictions on a classification problem o  ROC curve: Receiver Operating Characteristic curve, used in signal detection  theory to determine trade-off between true positive and false positive rates over noisy channels.  o  F-1 Measure: combination of both precision and recall in a single number.  -  Regression metrics: it measures how far the expected value is from the actual value.  There are a few ways of measuring regression metrics, such as:  o  Mean Square error. o  Explained variance (R2)  -  Partitioning data for training and testing and different ways of doing this, such as:  o  Sub-sampling: repeatedly partitioning data into random training and test sets in a  specific ration  SIT307, Lachlan Connor Patrick Geary, 221260728  o  Stratified sampling: divide the entire data into different subgroups (strata) and then randomly selecting the final subjects proportionally from the different strata.  o  Cross validation: A method to evaluate models by partitioning the original  sample into a training set so that it can train the model, and then including a test to evaluate it.  o  Hyperparameters, which is a parameter that has their value set before the  learning process begins. We also learnt about internal cross-validation, which is a method that can be used to select the best set of hyperparameters in a set of data.  -  The effect of imbalanced classes, which can derive from the total number of one class of data is less than the total number of other class of data outcomes. This is detected through fraud detection, anomaly detection and medical diagnosis. There are also some solutions at specific levels to combat this issue, such as:  -  Re-sampling at the data level:  o  Over-sampling from minority class o  Under-sampling from majority class  -  Algorithmic level:  o  Adjust costs. o  Adjust decision threshold.  -  The issues of imbalanced classes, and methods to fix this, such as not using information that is unavailable during the training process, and if you are to modify the model over and over due to its performance on a specific test, then its possible you are overfitting the test set.  