Topic 5 focused on supervised learning, model complexity, data partitioning, hyperparameters and the effects of an imbalanced distribution.  Supervised learning is an area of machine learning that focuses on learning from already labeled data with the goal of creating a model that can map input data to categories accurately. When analyzing the training data, the boundary separating categories can be linear or non-linear. Once the model has been trained it can be evaluated using a loss function, which can include square loss for regression (sensitive to outliers), absolute loss for regression (robust to outliers), or 0-1 loss for classification, as well as other loss functions like logistic loss and hinge loss.  Model complexity is the term used to describe the complexity of the algorithm that defines the decision boundary (i.e., the equation for a linear boundary is simpler than the equation for a non-linear boundary). When deciding which model to apply to a dataset to most accurately categorize the data it is important to ensure that the simplest implementation is used. When training a model, if a overly complex model is used to get an extremely accurate result this can lead to overfitting (where the result cannot be replicated with test / real data). Alternatively, if a non-complex model is used and does not produce a good result in training, and also does not categorize test / real data well, this shows that the model is underfitted.  Model evaluation metrics are used to evaluate a trained model with the testing set. For classification problems, confusion matrix, F-1 measure, and ROC curve are commonly used. For regression problems, mean square error, root mean square error, mean absolute error, and R2 (explained variance) are common metrics. Precision, accuracy, sensitivity, and specificity are other metrics, but their reliability can depend on the data and data imbalances.  Partitioning data is the process of dividing the dataset into two groups of data, one to train the learning model and one to evaluation the model. There are several data portioning schemes, including random sub-sampling, stratified sampling, cross-validation.    Random sub-sampling involves randomly choosing entries to be in each set.   Stratified sampling is used when the dataset is imbalanced. The dataset is divided into two groups ensuring that the categories of the subsets are the same as the original dataset.   Cross validation divides the dataset into several subsets. One subset is used as the testing  dataset, the other sets are used as the training set. This process is repeated, using each dataset as the test dataset. The average of all these iterations is used.  Hyperparameters are the parameters of the model that are set before the learning process begins, for example, the number of nodes, trees and hidden layers. There are various ways to tune (find the best settings) the parameters. This can be very time consuming when there are a lot of parameters. Alternative methods to find the optimal settings include grid search, random search, and Bayesian optimization. Bayesian optimization is more efficient when there are a large number of hyperparameters. A subset of data, referred to as validation set, is used to validate the model when tuning hyperparameters.  An Imbalanced distribution is a set of data where each attribute does not have a consistent number of entries for each attribute. Attributes with the majority of entries are known as the majority class, while entries with minority of entries are known as the minority class. Several compounding factors, such as data size, label noise, and data distribution, can worsen the imbalance. To address this, techniques such as data augmentation, over-sampling, under-sampling, adjusting the class, and adjusting decision threshold can be used. During training, it's important to only use information that's available at that time.  