 In supervised learning, we essentially have data that is already labelled, and we want to  try and create or learn a mapping function that is capable of mapping input data (features) to the output  variable (which would be our label). We have numerous types of supervised learning. One of which is the  regression problem which utilizes linear models and examples of regression problems can include Linear  Regression and Logistic Regression.  Part of how supervised learning works was already mentioned earlier. However, we need to be  able to figure out what is the best (or most accurate ) function, and we can do this by using the loss  function which essentially, helps measure the quality of the mapping function. Here, we have a training  set which is used to create/learn the mapping function and then we can use the testing set to measure  the quality of the function by looking at the labels from the mapping function and the true values (all this  quality checking done by the loss function). Some examples of loss function include square loss, absolute  loss and 0-1 loss etc which can be beneficial to certain supervised learning procedures (regression or  classification). Ultimately, we want a mapping function that minimizes the loss function or empirical risk.  Something to consider is the model complexity. The reason why we need to concern ourselves  with this is because, if we have a model that is highly complex, we run the risk of over-fitting the data (fit  well in training but performed poorly with test set). However, if we have lower complexity, then we  would be under-fitting the data (performs poorly with both training and testing). We want to try get the  right fit where we can get good results for both training and testing. From Occamâ€™s razor where we want  to get the simplest model, we need to define another risk value in addition to the empirical risk, known  as the structural risk. This risk helps avoid over-fitting by including a penalty on the model complexity  where we have a higher penalty for higher complexity with the goal to minimize the structural risk,  meaning a less complex model.  The next topic to cover here is model evaluation metrics. For classification metrics, we can have  the Confusion Matrix, ROC Curve and F-1 Measure. The confusion matrix measures different metrics such  as accuracy, sensitivity, specificity, precision, and negative predictive value by using true positives, false  negatives etc.  However, it is important to note that accuracy is not always reliable which can be due to  imbalanced data. Using cancer data as an example, if our dataset has more healthier people compared to  people with cancer,  we could be in a situation where we can classify people as healthy but perhaps not  accurately classify people with cancer. Here, we will still get a high accuracy value despite this issue due  to the high imbalance of true negatives to true positives (using that formula, we will still get a high value  essentially). Another classification metric is ROC which looks at the true and false positive rate curves  where we are aiming for higher true positive rate and lower false positive rate. In terms of regression  related metrics, we have something known as mean square error where we get the average of the squared distance between predicted and actual values. We also have the root mean square error, which  is the same, but we apply the square root to the average. In mean absolute error, we make use of the absolute difference. We also have the R2 which uses the variance explained by the model and the total variance where higher values indicate a good fit and vice versa for lower values.  Partitioning data is an important aspect of supervised learning. Here, we need to split the data  into a training and test set where we train the model using the training set and evaluating the model  using the test set. Some methods to do this is random sub-sampling, stratified sampling, and cross  validation.  In terms of random sub-sampling, the data is split randomly based on a specified ratio (80% -  20% for example). It is important to note that this is random, and we are not looking at patterns in the  data which can result in imbalanced data (where portions of the data can have more positive results than  negatives). We can use stratified sampling to overcome this where it divides the data based on subgroups  or strata and then does random selection and ensure proportionality is maintained from the strata.  We  also have something known as cross-validation which can help avoid bias from random splitting by  splitting data into k number of subsets, doing the training and testing on different subsets over k  iterations (essentially, test each subset, and have the remaining subsets used for training). Once we get  the performance for each iteration, we then use the average.  Another thing to consider is hyper-parameters  which are used before learning and acts as a  heuristic value that can be tuned which can impact the results. We can use trial and error to find the best  values which can be time consuming. One thing to note is that we have a validation set within the  training set to evaluate the model using different hyper-parameters. We do this to ensure the testing set  is strictly used for comparing/evaluating different models and keep it unbiased from hyperparameter  selection. Techniques that can be used for this is grid search (good for smaller amounts of  hyperparameters), random search (better for higher amounts of hyperparameters) and Bayesian  optimization (also good for large amount of hyperparameters).  Some additional notes on imbalanced data not already mentioned. We can apply some  techniques to help overcome this. Some involve the data itself where we can over-sample from the  minority class or under-sample from the majority class. We can also work with the algorithms and adjust  costs or classification decision thresholds (usually set to 0.5).   