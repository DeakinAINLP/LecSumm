Supervised Learning: In supervised learning we train our models based on the known relationships between input and output. In fact, we know the correct answer, but we need to train a model based on that to predict unseen inputs. So what the model is doing, is creating a function/direct mapping between feature vectors and the output. The model will be trained based on the input and known output (target) to estimate/predict the unseen data. We can test it by defining a set of test data which is selected from the known data from first. Supervised learning can appear in many forms: 1.  Regression problems: inear regression Logistic regression 2.  Classification problems: Support Vector Machines (SVM) for both linear and nonlinear Decision Tree (nonlinear) Random Forest (nonlinear) Neural network: Perception and Multilayer Perception (nonlinear) 3.  Ranking problems So in supervised learning we are dealing with a set of features {x1,x2,…,xn} that have known targets (outputs) as {y1,y2,…,yn}. Hypothesis Space: A hypothesis function is an element of a range of possible true functions. We will select a hypothesis function that we think is similar to the true function behind the data. For example, the space of all linear functions in d-dimensions space of all polynominal functions up to degree p. So how can we measure the quality of the answer? How can we find out how accurate the answer is? Loss Function: The Loss function is really a measure of accuracy. How accurately does your function describe the relationship between features to that target. Empirical Risk: Similar to the loss function, we can use empirical risk and we can select the function that has the lowest empirical risk. We can calculate the empirical risk by averaging the results of the loss function. The concept of model complexity: In high dimensions, we cannot visualize the training data. So we may not know whether the regression problem is linear or non-linear. Similarly, we may not know if the classification problem is linearly separable or nonlinearly separable. So the main question is: What should be the right complexity of the model that we use to fit the given data? If we choose higher complexity than necessary, we would be over-fitting the data. If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalization. Based on the Occam`s Razor, a famous problem-solving principle, all other things being equal, the simplest solution is the best. It means, selecting the theory that introduces the fewest assumptions and has the least complexity. Structural Risk: Based on Occam`s razor and its simplistic principle, we define another risk value as Structural risk. This risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. Classification Metrics: The metrics that we choose to evaluate the machine learning models are very important. Confusion matrix: It`s a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized in confusion matrix. Confusion matrices is a way to understand the types of errors made by a model. One of the reason for using the confusion matrix is that the accuracy is not a reliable metric for the real performance of a classifier. If the data is unbalanced, it will yield misleading results. The diagonal values represents the elements where the predicted classes were equal to the expected classes, and the off-diagonal values represent the elements where the classifier got the value prediction wrong. Based on the confusion we can calculate the accuracy: Recall (or sensitivity or True Positive Rate (TPR)): Recall is the function of true positive samples that have been predicted positive over the total amount of positive samples. False Positive Rate (FPR): FPR is the fraction of false predicted positive samples over the total amount of negative samples. ROC Curve: This curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels. F1 Measure: F1 is another useful metric that could be the combination of Precision and Recall. Regression Metrics: In fact, regression measures how far the expected value is from the actual value. So how to measure regression performance? Mean Square Error (MSE): To measure how close the predictions are to the true target value. From MSE, we can derive Root Mean Square Error (RMSE):As we are talking about the errors, so the lower the MSE of a model, the better its performance is. Another performance measurement definition is Mean Absolute Error (MAE): Explained Variance (R^2): It known as R-square, Explained variance or the coefficient of determination. R-square is measured as the percentage of target variation that is explained by the model. For linear regression, R-square is the square of the correlation between the target values and the predicted target values. So, higher the R-square of the model, the better its performance. Partitioning data for Training and Testing: In considering/selecting train and test dataset, we need to know that if we just select one single training set, it might be affected by some outlier instances. We need a bigger size of training dataset to train the model better with more data. However, we know that with higher number of testing set, we can improve the quality of measuring performance. To solve these issues, we can work with 3 methods for splitting data: Random subsampling Stratified sampling Cross validation Random Sub-sampling: So based on the figure above, instead of just using one training set and one testing set, we create number of random samples and average the accuracies to get an averaged estimate. Stratified Sampling: This is a probability sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata. When using randomly selecting training sets, class proportions may differ between training and test splits. So, the stratified sampling ensures that class proportions are maintained in each random set. Cross-Validation: This is a technique to evaluate models by partitioning the original sample into a training set to train the model and a test set to evaluate it. The main idea is to partition training data into k equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples. The final accuracy will be the average of the k obtained accuracies. Finding the best hyperparameters: In ML, a hyperparameter is a parameter whose value is set before the learning process begins. This means the value of a hyperparameter in a model cannot be estimated from data. They are often used in processes to help estimate model parameters. A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. A validation set needs to be set before evaluation of training set. For example in K-means. K was a hyperparameters that we needed to define before starting modelling. Instead of just selecting one k, we can define a set of ks and the model for each value of k, will run and get the result and we can check the performance of the model for each of them and select the best one. Internal cross-validation: All the techniques that we previously discussed for model assessment are applicable for training/validation set splitting. We still need to assess how a particular hyperparameter is doing on a validation set. At this method, instead of using a single validation set, we can use cross-validation within a training set to select the best set of hyperparameters. It`s the same as the one we check on test/train partitioning; however, here we are partitioning training set into training and validation sets. There are 3 possible ways to navigate the hyperparameter space: Grid-search (not so efficient)- this is what we explained. Random-search (efficient in certain scenarios) Bayesian optimization (efficient in general) Imbalanced Data: This is a very common problem that happens. When the number of instances of one class far exceeds the other, problem arise. So, what is the solution? We can solve this issue in two different ways: 1.  At the data level (Re-sampling): Over-sampling Under-sampling 2.  At the algorithm level: Adjusting the costs Adjusting the decision threshold 