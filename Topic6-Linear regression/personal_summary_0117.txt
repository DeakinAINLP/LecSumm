  Topic 5 - Fundamentals of supervised learning  Supervised Learning : It is the task of estimating a function from a labelled training data.  Types of supervised learning :  Regression problems  Linear Regression  Logistic Regression  Classification problems  Support Vector Machines  Decision Trees  Random Forest  Neural Networks  Ranking problems  Hypothesis space :  We will name a hypothesis function, h, as an element of a range of possible functions  H, usually called the hypothesis space. We'll select a hypothesis function that we think is similar to the true function behind the data.  Finding a function : In supervised learning, given the training data, the learning algorithm seeks a function on h with input space X and output space Y.  Loss function  It is used to compute the error between the actual result of y and our calculated y.  Empirical risk It is calculated by averaging the results of the loss function. Lower the empirical risk based on training  data, closer the function represents the relationship between the pair of values.  Model complexity If we choose higher complexity than necessary, we would be over-fitting the data If we choose lower complexity than necessary, we would be over-fitting the data It is important to get the best possible fit for good generalisation.  Occam's Razor principle When multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity.  Structural risk minimisation It seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones.  Classification Matrix : https://www.evernote.com/client/web?login=true#?an=true&n=a513fda1-edde-bfc1-d440-b77a71998abe&  1/3    Confusion Matrix  It is a summary of prediction results on a classification problem. The number of correct and incorrect  predictions are summarized with count values and divided down by each class. They are a way to  understand the types of errors made by a model. They are also called contingency tables.  True positive rate  Fraction of true positive samples that have been predicted positive over the total amount of samples.  False positive rate  Fraction of false predicted positive samples over the total amount of negative samples.  ROC curve  It is a curve created by plotting the true positive rate against the false positive rate at various threshold  settings. This has to be done to depict relative trade-offs between benefits and costs.  F-1 measure It is a metric that combines both precision and recall in a single number.  Regression Metrics.  Mean Square Error Mean Absolute Error Explained Variance  Partitioning data for training and testing.  Random sub-sampling - Repeatedly partitions the data into random training and test sets in a specified ratio.  Stratified sampling - It is a sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata. Cross-validation - It is a technique to evaluate models by partitioning the original sample into a training set to train the model and a test set to evaluate it.  https://www.evernote.com/client/web?login=true#?an=true&n=a513fda1-edde-bfc1-d440-b77a71998abe&  2/3    https://www.evernote.com/client/web?login=true#?an=true&n=a513fda1-edde-bfc1-d440-b77a71998abe&  3/3   