Machine learning (Pass Task- Topic 5)  Supervised Learning  Supervised learning is a type of machine learning in which an algorithm learns to make predictions or decisions based on labeled examples. In supervised learning, a series of input- output pairs—also known as training examples—are given to the algorithm so it can learn a mapping function between the input and the output.  Data used for Supervised learning is separated in to two parts as training and testing data. Training data will be used to develop the algorithm while the testing data can be used to test your algorithm to make sure it’s giving you usable outputs.  Supervised learning can appear in many forms:  ▪  Regression problems  o  Linear Regression (linear model) o  Logistic Regression (linear model)  ▪  Classification problems  o  Support Vector Machines (both linear and nonlinear) o  Decision Trees (nonlinear) o  Random Forest (nonlinear) o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  Model Complexity  Effects of selecting different models in terms of complexity:  If we choose higher complexity than necessary, Data would be over-fitted o If we choose lower complexity than necessary, Data would be under-fitted. o o  The best possible fit for good generalization. (Prediction of unseen data which is  not a part of training set)  Structural Risk  Structural risk minimization seeks to prevent over-fitting by incorporating a penalty on the model complexity     The metrics that you choose to evaluate your machine learning model are very important  Classification Matrix  1.  Confusion Matrix  A confusion  matrix is a table that summarizes the performance of a classification  algorithm  on a set of test data. It summarizes the predictions  made by the model and compares them to the true labels of the test data. Confusion  matrices are a way to understand the types of errors made by a model. Confusion  matrices are also called contingency tables.  A confusion  matrix is typically  divided into four cells, representing four different scenarios:    True Positive  (TP): The number of samples that were correctly predicted as positive  by  the model.    False Positive  (FP): The number of samples that were predicted as positive  by the model,  but were actually negative.    True Negative (TN): The number of samples that were correctly predicted as negative by  the model.    False Negative (FN): The number of samples that  were predicted as negative by the  model, but were actually  positive.  The accuracy of the model is calculated by:  But accuracy may not be a useful metric for imbalanced class problems. Also, there may be differential  costs of making errors for different classes. For example, an incorrect medical diagnosis  may be more costly than a false positive!  So, we need high confidence predictions only. Therefore, we can define other evaluation  metrics based on a confusion  matrix.    2.  ROC Curve  A ROC (Receiver Operating Characteristic) curve is a graphical plot  used to visualize  the performance of a model. The ROC curve is created by plotting  the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.  The ROC curve illustrates how well the model can separate the positive  and negative classes for various threshold settings.  With a TPR of 1 and an FPR of 0, a perfect classifier would produce a point  at the top-left  corner of the ROC curve. The area under the diagonal  line (AUC-ROC)  of a random classifier would be 0.5 and run from bottom-left  to top-right.     3.  F-1 Measure  This is a combination  of Precision and Recall.  Regression Metrics  Measuring Regression Performance  Mean Square Error  coefficient of determination  This measure is known by many names including:        ▪  R-square ▪  Explained  variance ▪  the coefficient  of determination  Unlike the other introduced  metrics, the higher the R-square of a model, the better its performance.  Sampling for training and testing Data  3 methods for splitting  data:  ▪ ▪ ▪  random subsampling stratified  sampling cross validation.  Random Subsampling  Random sub-sampling repeatedly partitions the data into random training and test sets in a specified ratio.  Stratified Sampling  Divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata.  cross validation.  This is a technique to evaluate models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. The main idea is to partition training data into k equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples.  Hyperparameters  Hyperparameter is a parameter whose value is set before the learning process begins.  A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.    The validation set is used to evaluate a given model and also to fine-tune the model hyperparameters. So, given a choice of hyperparameter values, you use the training set to train the model. validation set can be used to evaluate the performance of the model for different combinations of hyperparameter values  Imbalanced classes  At the data level: (Re-Sampling)  ▪  over-sampling the data from minority class ▪  under-sampling the data from majority class.  Two obvious solutions based on data manipulation which suggests that we can sample more data points from the minority class in order to cover the difference. Or we can under-sample the majority class in order to make them have an equal effect on the algorithm.  At the algorithmic level:  ▪  adjusting the costs ▪  adjusting the decision threshold.  From an algorithmic point of view, we may want to adjust some costs on the points we are observing from the majority class in order to dampen their effect. Also we can manually define some thresholds to cope with the unbalanced data.  Issues of imbalanced classes  Now, let us have a close look on possible issues of imbalanced classes.  ▪  Problem-1: Since the test data contains only few samples from the minority class, even a dumb classifier that always classifies an instance to the majority class will get very high accuracy!  This problem is dealt with by using other evaluation metrics in place of accuracy.  ▪  Problem-2: When doing random subsampling, it is possible that class proportion is not maintained in an individual partition. In fact, we may not sample even one instance from the minority class.  This problem can be solved using Stratified Sampling.    But always remember:  ▪  Any pre-processing over an entire data set (e.g. feature selection, or feature  extraction) must not use the information that you are trying to predict (e.g. labels).  ▪  During the training process, you must not use any information that is not  ▪  available during the training process. If you modify your model again and again by looking at how it performs on a specified test set, then you may be overfitting on the test set.  