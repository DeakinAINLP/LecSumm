In last few topics module, we were focusing on unsupervised learning. We look into KMeans, clustering, Dimensionality reduction et cetera. Now, we will shift over towards supervised learning for topic 5 and 6.  In this topic around, we will focus on:   Differentiate supervised learning from unsupervised learning  Estimate the performance of different supervised learning models Implement model selection and compute relevant evaluation measures  Supervised Learning  Unlike unsupervised, the input data has already been trained or labeled. This means that we will  not be working with raw data and will have relationship and datatype of each input.  There are two types of supervised learning category:   Regression Problems is used to find the correlations between the independence and  dependence variables.  o  Linear regression o  Logistic regression   Classification Problems is used to classified dataset into groups or clusters based on its feature.  o  Support Vector Machines (both linear and nonlinear) o  Decision Trees (nonlinear) o  Random Forest (nonlinear) o  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  The function of supervised learning is h: X -> Y, where h is the hypothesis function, X as input  and Y as output. But the problem is that the hypothesis function has its own accuracy, so to find that we need to use the loss function.  Complexity Just like in KMeans, having high cluster or too low cluster levels will affect the outcome. In  supervised learning, increase the complexity will cause overfitting the model, vice versa. So it is best to choose a simple, least complexity with the fewest assumption according to Occamâ€™s razor. But with this principle, it also has its own risk, Structural Risk. Since the method prevents over-fitting and only uses simple functions, it could cause loss function.  Regression modules are used to measure the difference between the expected and actual value.  We can use Mean Square Error or Explained Variance to evaluate the performance.  Regression  There are many types of classification algorithm that we can use to evaluate a problem, and the  selections of those algorithms will affect the performance as well. In this module there are:  Classification   Confusion matrix: are used to compare the prediction and the actual true value. This can be  used to find the accuracy of the performance by comparing the two values.   ROC curve: are used for signal detection theory over noisy data. We can evaluate the  performance by plotting the true positive and false positive.  F-1 measure: It uses a Precision and Recall ğ¹1 = 2 Ã—  ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™  ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™  Partitioning Data We know that it is hard to learn about a large data at a time, but we can split the data into  intervals to study it. The good thing is that, the more we know about the data, the more we can predict and evaluate the performance. The most common way to split data are:  Sub-sampling: is a method that we split data randomly repeatedly and put it into test and training. Then we can use the average as our performance evaluation. Stratified sampling: instead of randomly splitting, this method classified or put data into category/subgroup, then randomly select the final subjects from the different group. Since the proportion of each strata could have an effect on the performance, we try to maintain the proportion.   Cross validation: This technique instead of splitting the data, it picks out one sub samples, and use the rest for training and testing. The process will perform repetitively according to the number of k. We then use the average value from those runs.  Hyperparameter  These parameters are defined by users to control the learning process of the machine. We then give an unbiased evaluation with those hyperparameter, these evaluations are called the validation set. The sets will help us pick out the best parameters to train the model. We can also use partitioning technique for validation set as well.  Problems  We all know that outliers can heavily affect a model training process. In this case, we can also have imbalanced class, where one class outweigh the other, such as true positive and false positive. To fix this, we can resample the data by overfit the minority and under fit the majority. Another way for algorithmic type is to adjust the cost and the decision threshold.  