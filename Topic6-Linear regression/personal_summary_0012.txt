Forms of supervised learning Supervised learning is a type of machine learning where the algorithm is trained using labelled data, meaning the correct answers are already provided. The goal is to develop a mapping function between the input and output variables. This type of learning is advantageous as it allows for accurate testing of the algorithm. Supervised learning can take diﬀerent forms including regression problems such as linear and logistic regression, classiﬁcation problems such as support vector machines, decision trees, random forest, and neural networks, as well as ranking problems. A supervised learning algorithm Supervised learning involves ﬁnding a function that accurately maps the input to the corresponding output based on labelled training data. The hypothesis space includes a range of possible functions that could be similar to the true function. The quality of the function is measured using a loss function, which calculates the diﬀerence between the predicted output and the actual output. The empirical risk is the average of the loss function for all training points, and the goal is to minimize this risk using an optimization algorithm to ﬁnd the function that best represents the true relationship between the input and output. Model complexity and Occam's razor Occam’s Razor, a famous problem-solving principle, is used as a heuristic guide in the development of theoretical models. This principle otien paraphrased as: All other things being equal, the simplest solution is the best.  It also addresses the problem of Which hypothesis to choose if there are multiple hypothesis with similar ﬁt? In other words, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity. Structural risk minimisation So based on Occam’s razor and its simplistic principle, we deﬁne another risk value which is called Structural Risk. Structural risk minimisation seeks to prevent over-ﬁting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. Classiﬁcation metrics The article discusses various metrics used for evaluating machine learning models, particularly for classiﬁcation problems. It explains the concept of a confusion matrix, which summarizes the predictions of a model and helps identify the types of errors made. The article also discusses metrics such as true positive rate (TPR) and false positive rate (FPR), which can be calculated from a confusion matrix to evaluate a classiﬁer's performance. The receiver operating characteristics (ROC) curve is also explained as a useful tool for depicting the trade-oﬀ between TPR and FPR at diﬀerent threshold setings. The article also mentions the F-1 measure, which combines precision and recall into a single number. Regression Metrics Regression is the measurement of how far the expected value is from the actual value. Measurements include Mean Square Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE) as popular measures for evaluating the closeness of predictions to true target values. Partitioning data for training and testing Random sub-sampling repeatedly partitions the data into random training and test sets in a speciﬁed ratio, while stratiﬁed sampling ensures that class proportions are maintained in each random set. Cross-validation is a technique to evaluate models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. The main idea is to partition training data into equal-sized sub-samples, leaving one sub-sample out for the test set and training the model on the remaining sub-samples. The ﬁnal accuracy is the average of the accuracies obtained from each iteration. Cross-validation makes eﬃcient use of available data for testing. Finding the best hyperparameters To ﬁnd the best hyperparameters, a validation set is used to evaluate a given model and to ﬁne-tune the model hyperparameters. Eﬀect of imbalanced classes Imbalanced classes can lead to issues in the accuracy of the model. Two solutions are proposed, one at the data level (re-sampling) and one at the algorithmic level (adjusting costs or decision threshold). It is important to avoid using information that is not available during the training process and to avoid overﬁting on a test set. 