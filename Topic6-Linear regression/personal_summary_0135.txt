 This topic we learnt concepts regarding the fundamentals of supervised learning. The goal of supervised learning to is to discover a function that closely estimates the true relationship between the input and outputs. The hypothesis space is defined as a range containing all potential functions that fit the modelled data.  The quality of the function is measured using a loss function which calculates the difference between predicted outputs and true outputs (such as square loss and absolution loss for regression and zero-one loss for classification). The empirical risk is a measure of the average error between the predicted outputs and the true outputs for a given hypothesis function and assesses how the output generalises data based on the modelled data.  Model complexity refers to the sophistication of a function used in the supervised learning model. A complex model runs the risk of overfitting data, not being able to recognise the associated patterns in new data. Whereas a simplistic model may cause underfitting and not identifying underlying patterns in the data. Occam’s razor is a principle which suggests the simplest solution is often the best solution and is a principle that should be considered which supervised learning model is best. It is essential to find balance between a model’s complexity and its ability to generalise the data.  Structural risk minimisation aims to minimise overfitting by balancing empirical risk and model complexity. This is achieved by incorporating a penalty on the model complexity in addition to empirical risk, to better understand the trade-off between fitting to training data and generalising to new data.  Classification metrics are used for evaluating the performance of classifications models. These include:    Confusion matrix  The matrix shows the number of correct and incorrect predictions divided into different classifications (i.e. actual class and predicted class).    ROC Curve  Plots the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. This illustrates the trade-offs between true positives (benefits) and false positives (costs)   F-1 measure  Metric which combines precision and recall into a single metric.  Regression metrics are used for evaluating the performance of regression models and determine the difference between predicted values and actual values. These metrics include:    Mean Square Error (MSE)   Root Mean Square Error (RMSE)   Mean Absolute Error (MAE)  Lower values in these metrics indicate better model performance.  Partitioning data is essential for training, testing, and evaluating a models’ performance. Common methods for splitting data include:    Random subsampling – repeatedly partitions data based on a specified ratio.   Stratified sampling – divides data into subgroups based on labels and then randomly selects  data points for each label.    Cross Validation – partitions data into equal sized samples (k-folds), iteratively leaving one  sub-sample out for testing while training on the remaining samples.  