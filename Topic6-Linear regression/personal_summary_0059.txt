Topic 5 Summary Fundamentals of supervised learning Forms of Supervised Learning  Labelled dataset  Known relationship of input/output  Types: Regression problems  linear regression  logistic regression  Classification problems  support vector machines (linear & nonlinear)  decision trees (nonlinear)  random forest (nonlinear) Ranking problems  Supervised learning algorithms  Function: relationship between pair of number apply function to input set = output  Hypothesis space:  h function where X is input space Y output space  h : X − − > Y  how to measure quality of function h? how well does h map X to target Y?  Loss function: measure of accuracy  to describe function h applied training instance x_i  difference between y and y hat  ^yi = h(xi)  L(yi, ^yi)  Empirical risk (similar to loss function)  avg of result loss function. Lower = closer represent true relationship between x and y  5.4 Concept of model complexity  5.5 Model complexity & Occam's razor  When all theories are equal, select the theory that has fewest assumptions & least complexity  5.6 Structural risk minimisation  prevents over-fitting  penalty on more complex models  Rstr(h) = Remp(h) + λC(h)  C(h) = complexity of hypothesis function h & lambda = penalty parameter  Classification metrics  common machine learning application  Confusion Matrix  summary of prediction results on classification problem  Accuracy  accuracy =  T P + T N T P + F P + F N + T N  True Positive Rate  TPR  OR Recall OR Sensitivity  False Positive Rate  FPR   recall =  T P T P + F N  F P R =  F P T N + F P  Receiver Operating Characteristics  ROC  Curve  Used in imbalance class distribution & unequal classification error costs  Regression Metrics  Measures how far expected value from actual value Measuring performance  Mean Square Error  MSE   Root Mean Square Error  RMSE   Mean Absolute Error  MAE   Explained Variance (R^2) (AKA Coefficient of determination)  Expressed as a percentage between  0%   100%  100%   represent model, explains all variation in response variable  RV   0%   model does not explain  Square of correlation between target values & predicted target values  High = better performance  R2 =  V ariance Explained by the model T otal V ariance  Partitioning data for training & testing  Methods for splitting data  Random sub-sampling  Train model with each training set Estimate accuracy against test set avg. accuracies ⟹ avg. est.  Stratified sampling  probability sampling technique, divide entire data into subgroups/strata. THEN randomly  select subjects proportionoally from each strata  Cross validation   Partition original sample into training set   Train model   Evaluate using test set  k-fold cross validation: k-1 sub samples use for training data  1st iteration, leave out s1 for testing, use s2,s3,s4,s5 for training  save & calculate accuracy.  keep doing k= 5 times  final accuracy = avg accuracies  Finding the best hyperparameters  Need to partition training data into separate training & validation sets  Validation set: sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.   