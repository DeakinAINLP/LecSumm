Topic 5: Fundamentals of supervised learning  5.2 Forms of Supervised Learning  Supervised learning includes the knowledge of labels in the training data. The machine learning algorithm is created in relation to the known relationship between the data.  5.4 The concept of model complexity  When developing a model having too high complexity can result in overfitting, which means the model is only finding relationships that are exactly applicable to that current training data, instead of ones that can be extrapolated to other data. If the complexity is too low, we can underfit the data which results in the model being useless since he doesn’t learn enough from the training to be able to do anything. A good middle ground is what is most useful for a model.  5.5 Model complexity and Occam's razor  ‘All other things being equal, the simplest solution is the best.’  ‘In other words, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity.’  5.6 Structural risk minimisation  5.7 Classification metrics  Confusion Matrix is a classification technique in which the number of correct and incorrect predictions are summarised with counts recorded and divided down by each class.  5.8 Regression Metrics  R Square is a measure as a percentage of the variation that the model explains. The higher the value, the better the variation is explained by the model.  It is a percentage value that is always between 0 and 100%.  5.9 Partitioning data for training and testing  5.10 Finding the best hyperparameters.  Hyperparameters are set values that are defined before training that can be tuned and adjusted for certain predictive models.  The number of clusters in kmeans for instance is a hyperparameter.  5.11 Effect of imbalanced classes   In machine learning a problem of extreme imbalance of the classes in which one is significantly more than the other. An issue can be caused by this is the model, if the data is super skewed, may always be dumb and pick the one which is extremely imbalanced every tim. Even when doing this since the data is so skewed, it will still achieve a very high accuracy score.  Q1: Random splitting model.  Q2: Leave one out model.  Since the leave one out model is comparable/less than the random splitting model that means that there isn’t overfitting, and it has generalised well for the most part. This would suggest that predictions on the unseen data can be accurate.            