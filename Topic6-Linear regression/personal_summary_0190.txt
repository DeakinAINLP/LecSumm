This  topic  I  learned  about  fundamentals  of  supervised  learning,  forms  of  supervised  learning, supervised  learning  algorithm,  concept  pf  model  complexity,  model  complexity  and  Occam’s  razor, structural risk minimisation, classiﬁcation of metrics, partitioning data for training and testing, ﬁnding the best hyperparameters, and eﬀect of imbalanced classes.  Let’s talk about above mentioned topics brieﬂy.  In supervised learning, we make the algorithm based on the known relationship between the input and output. The supervised learning has many forms:  Regression problems:  Linear Regression (linear model)  Logistic regression (linear model)  Classiﬁcation problems:  Support vector machines (both linear and non-linear)  Decision trees (non-linear)  Random forest (non-linear)  Neural networks: Perceptron and multi-layer perception (non-linear)  Rankings:  Supervised  learning  algorithm:  If  we  have  two  datasets  i.e.,  input  dataset  and  output  dataset.  The output dataset is obtained by applying the function to the input set. To ﬁnd the relationship between them we need function and this function called hypothesis space. Hypothesis space is the set of all possible functions that can be used to model the relationship between the input variables and the output variables. So, we can say hypothesis function is set of all hypothesis functions and hypothesis function is speciﬁc function selected from hypothesis space to model the relationship between the input and output variables. To measure the accuracy of hypothesis function we have loss function.  To  measure  the  average  loss  incurred  by  model  on  given  dataset  we  have  empirical  risk,  and  it  is calculated by average of the loss function over the training dataset.  The concept of model complexity: Sometimes we cannot visualise the data in high dimensions, and we may not be able to ﬁnd what type of regression problem is i.e., linear or non-linear.  Model complexity and Occam’s razor: It states that, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and  has  least  complexity. So,  based  on  Occam’s  razor  we  deﬁne  another  risk  value  which  is  called structural risk. The structural minimisation seeks to prevent the over-ﬁtting incorporating a penalty on the model complexity that prefer simpler function over complex ones.  Classiﬁcation of metrics: The metrics are used for evaluating the performance and accuracy and can be used for to compare the diﬀerent results. There are several metrics, and these are confusion matrix, ROC curve, F-1 measure, Mean Squared Error (MSE), and Explained Variance.  Partitioning data for training and testing: This is process where we divide the dataset into two or more subsets; one for training a machine learning model and the other for testing the model. We usually work with  three  methods for  splitting the  data  i.e.,  random  subsampling,  stratiﬁed  sampling,  cross validation.  Finding the best hyperparameters: The hyperparameters are parameters whose value is set by user before the learning process begins. The value hyperparameter in a model cannot be estimated from the data but are mostly used in processes to help estimate model parameters.  Eﬀect  of  imbalanced  classes:  we  have  one  problem  with  machine  learning  is  that,  if  one  class  has signiﬁcantly more or fewer samples than the other class and it can lead to imbalanced classes. To solve this, we have two approaches i.e., we will perform some actions on data itself like ‘over-sampling the data from minority class’ and ‘under-sampling the data from majority class. Another approach is we can improve our algorithm i.e., adjusting the costs, and adjusting the decision threshold.  Answer to Python:  Leave-one-out:  LOOCV  is  a  procedure  used  to  estimate  the  performance  of  machine  learning algorithms when we make a prediction on data not used to train the model [1].  When we train and test the dataset with random splitting i.e., 70-30% the MSE (Mean Squared Error ) is 0.12056567388165275 and this is non-zero value and which indicates that the model performanc e has some errors when predicting the target variable on the test set. In random 70-30% random spli tting, the model is trained only on subset of available data and have more errors.  On the other hand, the MSE with leave-one-out is 2.113284409377226e-30 and extremely small and close to zero. This is because the model is trained on all but one data point and tested on the left-ou t point. Smaller the MSE value better performance of the model. So, the leave-one-out is better than random 70-30% split model.  