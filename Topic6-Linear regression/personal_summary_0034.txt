This topic was all about Principal Component Analysis. The following points were covered:  Forms of supervised learning:   In supervised learning, data is labelled with correct output, so a function can be determined that maps input to output.  Regression problems ‚Äì linear and logistic regression. Used when target is a continuous variable (i.e., the price of a house or the price of a stock)  Classification problems ‚Äì determining a categorical label for an input (i.e., what type of vehicle, what type of animal, whose face it is). Example of algorithms that can be used for classification problems include decision trees, random forests and neural networks.  Supervised learning seeks to find a function ‚Ñé, such that ‚Ñé maps the input training data ùëã to the labelled training output ùëå. ‚Ñé: ùëã  ‚Üí ùëå The quality of the function ‚Ñé is determined using a loss function, which  measures the difference between the outputs from the function and the actual outputs (labels).  There are trade-offs to using highly complex models. The higher the  complexity of the model, the more it is ‚Äòover-fit‚Äô to the training data and the less it generalizes well to unseen data. If the model is too simple, the it is ‚Äòunder-fitted‚Äô and the loss will be high.  Structural risk is a risk value that can be used to minimise over-fitting by incorporating a penalty on the model complexity, i.e., it prefers simpler models.  Classification metrics:  Confusion matrix ‚Äì matrix that maps actual labels against predicted labels. The confusion matrix is particularly useful when the data are unbalanced. For example, if a dataset has 90 points belonging to class A and 10 belonging to class B and a model predicts 100% of points belonging to class A, then the model will be 90% accurate despite only ever predicting for the one class. Clearly, in this scenario simply calculating the accuracy score is not a good indicator of how good the model is.  Recall ‚Äì fraction true positive predictions over the actual number of positive samples (i.e., the sum of the true positives + false negatives) Precision ‚Äì fraction of true positives predicted over the total number of positives predicted (i.e., predicted true positives + predicted false positives)  False positive rate ‚Äì fraction of false positives over actual number of  negative samples (i.e., true negatives + false positives)  Receiver Operating Characteristic (ROC) curve ‚Äì depicts trade-off  between true positive (recall) rate and false positive rate. This graph can assist in tuning hyperparameters for a model.  For example, if a model is   used in healthcare to predict disease, then a model that produces a high number of false positives, but is always able to accurately predict a true positive may be favourable, as false positives can always be further investigated, but missing a true positive may have disastrous consequences.  F1-score ‚Äì harmonic mean of precision and recall.  Regression metrics:  Mean Squared Error (MSE) ‚Äì the sum of the square of all predicted  output minus the actual output, divided by the number of samples. The difference is squared so that predictions that are lower than the actual values do not cancel out the error value. This is not an easily- interpretable measure as the error term is in squared units of the measure.  Root Mean Squared Error (RMSE) ‚Äì square root of MSE. More easily  interpretable as error is in same units as the target variable. Mean Absolute Error (MAE) ‚Äì calculated the same as MSE, but the  absolute differences of the predicted value minus the actual value are taken instead of the square.  Explained Variance (R^2) ‚Äì Percentage of target variation explained by the model. Explained variance is a percentage value, and the higher the percentage of explained variance, the better the model.  Partitioning data:  Random sampling ‚Äì repeatedly partitions data into random training and  tests sets based on a specified ratio of train/split.  Stratified sampling ‚Äì entire data is split into subgroups based on a specific characteristic, and then a proportion from each of the subgroups that is representative of the proportion of that subgroup in the whole population, is chosen for training. This ensures that bias is not introduced into the model, such as can be the case when random sampling is used, particularly is there is imbalance in representation of each of the groups in the population.  Cross-validation ‚Äì split the data into k subgroups, then perform training k times, leaving a different subgroup out for testing each time. This way, the model is trained and tested on all of the data. The final accuracy is taken as the average of the accuracies across all training iterations.  Hyperparameters:  A parameter that is set before training commences, and hence cannot  be estimated from the data.  Validation data ‚Äì used to evaluate a model during the training process  and fine-tune the hyperparameters.   Imbalanced classes:   Imbalanced classes cause issues in machine learning models. For example, in a dataset where one class represents the majority of the samples, a machine learning model could output this class 100% of the time and still have a high accuracy.  To address this problem, over- or under-sampling of the minority or  majority classes (respectively) can be performed. At the algorithm level,  there may also be costs on the points in the majority class that can be adjusted to dampen their effect.  How to perform train/test split using scikit learn and how to perform  linear/multivariate regression using scikit learn  