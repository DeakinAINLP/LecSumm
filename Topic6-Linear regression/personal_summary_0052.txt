 Supervised Learning  Is used by most practical machine learning applications.  - -  Data used to train the algorithm is already labelled with the correct predictions. The model outcomes are already provided for the training data, so the model just needs to map the input variables to the output variables. “Supervised Learning is the task of estimating a function from labelled training data.”  - -  Every  input  variable  has  a  corresponding  output  which  is  already  known.  This  gives  supervised  learning an advantage over unsupervised because the desired output of the model is already known.  Supervised learning can come in the forms of  o  Regression Problems  Linear Regression Logistic Regression  o  Classification Problems:  Support Vector Machines (linear) Decision Trees (nonlinear) Random Forest (nonlinear) Neural Networks (nonlinear)  o  Ranking Problems  Regression problems can have linear and or-linear decision boundaries.  Supervised Learning Algorithms  The goal of a supervised learning algorithm is to identify a function as close as possible to the unknown function which maps the relationship between input and output. The output (x) is obtained by applying the function (which defines the relationship) to the input (y), and for every element in the input set exists a corresponding element in the output set.  The  hypothesis  space  (H)  is  defined  as  the  range  of  all  possible  hypothesis  functions  (h).  Hypothesis functions are selected based on their perceived similarity to the true function behind the data.  “In supervised learning, given the training data, the learning algorithm seeks a function on h: X->Y where X is the input space and Y is the output space.”  It is important to accurately measure the quality of function h and to understand how well h can map X to Y.  Loss  Function:  The  loss  function  is  used  to  measure  how  accurate  the  h  function  is  at  describing  the relationship between X and Y. The loss function is used to compute the error between the actual result h(X) and Y.   Empirical Risk  Empirical risk is a factor which quantifies the risk of error for function h. From the hypothesis space, we select the function with the lowest empirical risk. The empirical risk is calculated by averaging the results of the loss function.  Model Complexity  It is important to determine the right complexity for the model that can be used to fit the data. This can be done by examining the effects of selecting different models in terms of their complexity:  - If we choose higher complexity than is required, this is known as over-fitting. -  Choosing a lower complexity than is necessary is called under-fitting the data.  “It is important to get the best possible fit for good generalization.”  Occam’s Razor  “Occam’s Razor, a famous problem-solving principle, is used as a heuristic guide in the development of theoretical models.”  It basically means ‘All other things being equal, the simplest solution is the best.’ When presented with the question of which hypothesis to choose when there are multiple suitable functions available with similar fit, this principle addresses this question. Occam’s Razor recommends that, where possible, we select the theory which introduces the fewest assumptions and is the least complex.  Another risk value, Structural Risk, is based on Occam’s Razor’s principle of simplicity.  