This topics module was an introduction to the fundamentals of supervised learning. I learnt about model complexity, specifically the impacts that overfitting/underfitting have on the performance of a model. We looked at the different types of models that supervised learning covers, specifically we examined linear regression models in depth.  The importance of splitting data into training and test sets was covered and different strategies were mentioned for certain problems. Stratified splitting is useful for classification tasks  (a model can’t be expected to identify a class it hasn’t been trained on!), KFold cross-validation is an evaluation technique that uses sub-sampling of a training dataset to measure the performance of a model.  This module covered optimising hyper-parameters for models using methods such as cross- validation analysis. KFold cross-validation can be used find the best scoring values of hyper- parameters using the metrics MSE and r2 for regression models, and balanced accuracy score and f2 score for classification models.  For the problem solving task I utilised the documentation for the libraries I used, NumPY (https://numpy.org/doc/stable/reference/), pandas (https://pandas.pydata.org/docs/reference/), sklearn (https://scikit-learn.org/stable/modules/classes.html), and matplotlib (https://matplotlib.org/stable/api/index.html) for visualisation.  