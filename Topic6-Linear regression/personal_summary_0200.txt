Evidence of Learning  Module 5: Fundamentals of Supervised Learning  Module learning outcomes –  I certify that I have successfully achieved the below-mentioned learning goals.  1.  differentiate supervised learning from unsupervised learning. 2.  estimate the performance of different supervised learning models. 3.  implement model selection and compute relevant evaluation measures.  Summary and reflection –  The  pages  below  contain  the  handwritten  summary  referring  to  the  given  learning resources. It includes all the important points of the topic 5 module.  In this module, we first learnt the basic forms of supervised learning. Then we got to learn about supervised learning algorithms and the concept of model complexity. Then we learnt about  the  interconnection  between  model  complexity  and  Occam’s  razor.  Then  we  learnt another  important  topic  which  was  structural  risk  minimisation,  its  use  of  it  and  its advantages.  Then  we  had  a  chance  to  learn  about  classification  metrics  and  regression metrics  in  detail.  Then  we  practised  how  to  partition  data  for  training  and  testing. Additionally, we learnt how to find the best hyperparameter. After that, we learnt the effect of  imbalanced  data.  We  used  the  language  Python  to  practically  test  what  we  learnt  in theory.  When we started topic 5, I was not familiar with any of these concepts at all. So it was very interesting and exciting to learn about a new side of machine learning which affects every and all aspects of it. And the practical part done with Python was a little bit confusing at first but  was  able  to  understand  all  the  necessary  commands  and  ways  of  implementation  by referring to both given resources and online resources.  Activity 5.2  Module Activities  Is  it  possible  to  generate  classification  output  from  regression  output?  How?  What  about generated regression value from a classification model?  Yes, it is. However, the conversion process may not be straightforward and requires careful consideration  of  the  specific  context  and  problem  at hand.  We need to define  a  threshold value that separates the classes to generate classification output from regression output.  If the regression output is above the threshold, we can classify it as one class, and if it is below the threshold, we can classify it as the other class. The threshold value can be determined based  on  the  performance  of  the  model  on  a  validation  set  or  using  domain  knowledge. However, it is essential to note that the threshold value may not be optimal for all cases and may need to be adjusted accordingly. On the other hand, to generate regression output from a classification model, you can use the predicted class probabilities as regression targets. For example,  if  the  classification  problem  has  three  classes,  you  can  use  the  predicted probabilities  for  each  class  as  regression  targets.  In  this  case,  the  regression  output  will represent  the  probabilities  of  each  class  instead  of  the  actual  class  labels.  However,  it  is essential to note that the quality of the regression output will depend on the quality of the classification  model's  predictions.  Therefore,  it  is  essential  to  evaluate  the  classification model's performance before using it to generate regression output.  Activity 5.3  Have  you  ever  thought  about  the  true  application  of  0  −  1  loss?  Write  and  share  a  simple explanation of the 0 - 1 loss function and its use?  The  0-1  loss  function  is  a  classification  loss  function  that  measures  the  accuracy  of  a classifier by counting the number of misclassifications. It assigns a value of 1 if the prediction is incorrect and 0 if it is correct. The goal of a classifier is to minimize the 0-1 loss function, i.e.,  to  minimize  the  number  of  misclassifications.  For  example,  let's  say  we  have  a  binary classification problem where we want to predict whether a given email is spam or not. We have a dataset of emails labelled as either spam or spam, and we train a classifier to predict the labels of new emails. If the classifier predicts a spam email as not spam, it incurs a loss of 1,  and  if  it  predicts  a  non-spam  email  as  spam,  it  also  incurs  a  loss  of  1.  If  the  classifier predicts  correctly,  it  incurs  a  loss  of  0.  The  overall  loss  of  the  classifier  is  the  sum  of  the losses over the entire dataset. The 0-1 loss function is easy to understand and interpret since it  directly  measures  the  number  of  misclassifications.  However,  it  is  often  not  used  in practice  due  to  its  discontinuity  and  non-differentiability,  which  makes  it  challenging  to optimize using gradient-based methods. In practice, surrogate loss functions such as cross- entropy or hinge loss are used instead, which approximate the 0-1 loss function while being continuous and differentiable, making them easier to optimize.  Activity 5.6  As  you  know, C(ℎ) is  the complexity  of  hypothesis function ℎ.   How  do  you  think  we  should calculate this value?  The calculation of the complexity of a hypothesis function h, denoted as C(h), depends on the  specific  hypothesis  class  being  considered  and  the  measure  of  complexity  used.  The complexity  of  a  hypothesis  function  represents  the  level  of  complexity  or  flexibility  of  the hypothesis space. Intuitively, a more complex hypothesis function is capable of fitting more complex patterns in the data but may also be more prone to overfitting, while a less complex hypothesis  function  may  be  less  prone  to  overfitting  but  may  not  capture  all  the  relevant patterns  in  the  data.  There  are  different  measures  of  complexity  that  can  be  used  to calculate C(h), depending on the hypothesis class being considered. For example, for linear regression models, the complexity can be measured by the number of parameters (e.g., the number of weights in the linear model), while for decision tree models, the complexity can be measured by the depth or the number of nodes in the tree. In general, the complexity of a hypothesis function is often related to the model's capacity, i.e., its ability to fit the training data, and can be controlled through regularization techniques such as L1 or L2 regularization or by adjusting hyperparameters such as the number of hidden layers in a neural network. Therefore,  to  calculate  the  complexity of  a hypothesis  function  h,  one needs to  determine the  appropriate  measure  of  complexity  for  the  specific  hypothesis  class  being  considered and then apply the appropriate calculation method.  Activity 5.7  How  comprehensive  is  a  confusion  matrix  for  evaluating  a  model?  Can  you  think  of  an example where a confusion matrix is appropriate?  How about a good use of an ROC Curve?  Can  you  offer  some  cases  in  which  a  confusion  matrix  or  ROC  Curve  is  not  enough  for evaluating a model?  A  confusion  matrix  is  a  useful  tool  for  evaluating  a  model's  performance  in  terms  of classification  accuracy.  It  provides  a  summary  of  the  number  of  correct  and  incorrect predictions  made  by  the  model.  It  is  an  effective  method  of  evaluating  a  model's performance,  especially  when  the  classes  are  balanced.  However, it  may  not  be comprehensive enough to evaluate a model's performance in all cases. For example, when the  classes  are  imbalanced,  a  confusion  matrix  may  not  provide  a  clear  picture  of  the model's performance. Additionally, a confusion matrix does not take into account the cost of  incorrect  predictions  or  false  positives/negatives,  which  can  be  important  in  some applications.  An  example  where  a  confusion  matrix  is  appropriate  is  in  evaluating  the performance of  a binary  classification  model  that  predicts whether  a  person  has  a  certain disease or not. The confusion matrix can show how many true positives, false positives, true negatives,  and  false  negatives  the  model  made.  An  ROC  curve  (Receiver  Operating Characteristic  curve)  is  a  plot  of  the  true  positive  rate  against  the  false  positive  rate  at different  classification  thresholds.  It  is  a  useful  tool  for  evaluating  a  model's  performance when the cost of false positives and false negatives is not equal. The curve shows how well the  model  is  able  to  distinguish  between  the  positive  and  negative  classes,  and  the  area under the curve (AUC) provides a summary of the model's performance. A good use of an ROC  curve  is  in  evaluating  the  performance  of  a  diagnostic  test,  where  the  cost  of  a  false positive  or  false  negative  is  different.  For  example,  in  cancer  screening,  a  false  negative result  could  mean  a  delay  in  diagnosis  and  treatment,  while  a  false  positive  could  lead  to unnecessary invasive procedures. However, in some cases, a confusion matrix or ROC curve may not be enough to evaluate a model's performance. For example, in anomaly detection, where  the  majority  of  data  points  are  normal  and  only  a  small  fraction  are  anomalies,  a confusion  matrix  may  not  provide  enough  information  about  the  model's  performance. Other metrics such as precision, recall, and F1-score may be more appropriate in such cases. Additionally, for models that output probabilities rather than binary predictions, calibration curves may be more informative than a confusion matrix or ROC curve.  Activity 5.9  You have learned about three partitioning methods. Which of these you think will help you more for training and evaluation on your models and why?  All three sampling techniques, random subsampling, stratified sampling, and cross-validation are  essential  for  training  and  evaluating  machine  learning  models.  The  choice  of  sampling technique  depends  on  the  specific  task  and  the  type  of  data  being  used.  Random subsampling is a simple and widely used sampling technique that involves randomly dividing the dataset into a training set and a testing set. This technique is useful for large datasets, where  the  testing  set  is  large  enough  to  provide  an  accurate  estimate  of  model performance.  However,  this  technique  can  produce  biased  results  if  the  dataset  is imbalanced,  as  it  may  not  represent  all  classes  equally.  Stratified  sampling  is  a  sampling technique that ensures that each class is represented equally in the training and testing sets. This  technique  is  useful  when  dealing  with  imbalanced  datasets,  where  the  minority  class may be underrepresented. Stratified sampling can improve model performance by ensuring  that  the  model  is  trained  on  representative  data.  Cross-validation  is  a  technique  that involves  dividing  the  dataset  into  k-folds,  where  each  fold  is  used  as  a  testing  set  and  the remaining folds are used as a training set. This technique is useful for small datasets, where there may not be enough data to create a separate testing set. Cross-validation can help to reduce the risk of overfitting and provide a more accurate estimate of model performance.  Activity 5.10  The choice between using grid search or alternative methods such as Bayesian optimization to tune hyperparameters in machine learning models depends on several criteria, including: 1.  Size  of  the  hyperparameter  search  space:  If  the  hyperparameter  search  space  is small,  and the  number of  hyperparameters  to be  tuned  is  low,  grid  search  may  be the  most  efficient  method.  In  contrast,  if  the  search  space  is  large,  and  there  are many hyperparameters to be tuned, Bayesian optimization may be a better choice, as it can explore the search space more efficiently.  2.  Computation  resources:  Grid  search  can  be  computationally  expensive,  especially when  the  search  space  is  large.  Bayesian  optimization,  on  the  other  hand,  can  be more computationally efficient, as it dynamically samples hyperparameters based on their expected utility.  3.  Robustness  to  noise:  In  some  cases,  the  objective  function  used  to  evaluate  the performance  of  the  model  may  be  noisy  or  stochastic.  In  such  cases,  Bayesian optimization can be more robust than grid search, as it can adapt to the noise in the objective function and converge more quickly to the optimal hyperparameters. 4.  Prior  knowledge:  If  there  is  prior  knowledge  about  the  hyperparameters  or  their relationships,  Bayesian  optimization  may  be  a  better  choice.  Bayesian  optimization allows  incorporating  prior  knowledge  into  the  search  process  and  can  converge faster to the optimal hyperparameters.  5.  Exploration  vs.  Exploitation:  Grid  search  explores  the  hyperparameter  space exhaustively,  while  Bayesian  optimization  balances  exploration  and  exploitation, allowing it to find good hyperparameters faster. In some cases, grid search may get stuck  in  local  optima,  while  Bayesian  optimization  is  more  likely  to  explore  the search space and find the global optimum.  In summary, the choice of hyperparameter optimization method depends on the size of the search  space,  available  computation  resources,  robustness  to  noise,  prior  knowledge,  and find  the  optimal the  balance  between  exploration  and  exploitation  needed  to hyperparameters. Grid search can be a good choice for small search spaces, while Bayesian optimization may be a better choice for larger search spaces and when prior knowledge is available.  