Supervised learning: use a dataset with features and label data to build a model predicting the relationship between those ones.  Forms of supervised learning:  Supervised Learning  Regression Problems  Classification Problems  Ranking Problems  - -  Linear Regression Logistic Regression  -  Support Vector Machines( both linear and non-linear) - Decision Trees( non linear) - Random Forest( non - linear) - Neutral network  -  Loss function: measure the accuracy between the predicted outputs and the labels. The goal is to minimize the loss in order to get a good predicting model. Empirical risk: average the results of all loss function. The key is similar to loss function which is to minimize the risk.  Model complexity: complexity should be appropriate, higher complexity could cause over fitting while lower complexity would result in under fitting. Structural risk minimization: prevent over-fitting by implementing a penalty on model complexity.  Classification matrix: -  Confusion matrix: summary of accuracy between predictions and actual labels.  -  ROC curve: plotting the true positive rate against the false positive rate.  -  F-1 score: combination of precision and recall  Regression metrics: -  Mean Square Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Error  (MAE):  Partitioning for training and testing: -  Random subsampling: randomly split dataset into train and test sub data -  Stratified sampling: -  Cross validation:    K-fold: divide the dataset into K sub-groups, use K-1 sub-groups for training, the    last is for testing leave one out: leave one sample for testing and the rest for training, iterate through the whole dataset. Suitable for small dataset.  Imbalanced classes: -  Over sampling the data from minority class: used when the amount of data is not  enough.  -  Under sampling the data from majority class: used when major data is  overrepresented to balance with minority class.            