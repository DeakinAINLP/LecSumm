Task 5.1P – Lesson Review  Summarise the main points that were covered. Main points covered in topic 5 of the unit content:  Introduction to supervised learning.     Overfitting and Underfitting.   Measuring the performance of classification problems and regression problems.   Data partitioning methods.   Hyper parameters.   Python programming for supervised learning.  Reflection My reflection on the knowledge gained this topic from reading the unit contents for this topic with respect to machine learning.  This is a reflection on the knowledge that I gained during topic 5 in regard to machine learning. Above I also listed the main points that were covered in the unit content, that also doubles as knowledge related to machine learning that I gained this topic.  This topic I started learning the topic of supervised learning, the most popular technique used in machine learning applications. In supervised learning, the model is trained with data that already is labelled correctly. From the labelled data the aim is to create a mapping function. This function helps create a machine learning model which can predict the appropriate labels, when given a new set of unknown data without labels. Therefore, the machine learning model is created on the known relationship between input data and its label, which differs to unsupervised learning in topic 4 & 5 where the similarity patterns was the bases for the model.  I was introduced to the term Loss Function, which measures the difference between an input data and the output from the model. This aims to measure the accuracy of the machine learning functions ability to find the relationship between the input and output.  I learnt about the term Hypotheses space, which refers to all the possible functions that can be used. From the hypothesis space we can select the best function from machine learning model using the option that minimises empirical risk. The function with the lowest empirical risk is the best choice, as this option will best describe the relationship between x and y (the data and its label).  When selecting a specific model, I must also take into consideration the models complexity. Underfitting is when the model is not complex enough and doesn’t explain the data very well on either training or test data. This could occur, for example, when there is not enough training data available. Overfitting is when the model is highly complex and represents the data precisely, this may have negative effects once using unknown data within the model. The model fitting the test data too precisely could mean that the model won’t be able to make a good prediction with any unknown data.  Instead, it is better to aim for a complexity range somewhere in-between. This is where is model that explains the relationship more generally and performs well on both training data and unknown  test data. For example, in a linear model, the prediction line travel through the general area of the data points, instead of bending to travel through every point on the graph exactly.  To measure the performance of a model for classification problem in machine learning a ROC curve, Confusion matrix, or F1 Measure can be used. A ROC curve (Receiver Operating Characteristics) plots to a graph the true positive rate against the false positive rate in different thresholds. A confusion matrix can be used to measure the performance of the created machine learning model. Within the matrix, the correct and incorrect prediction results are displayed. This helps to measure and determine the performance of the model, especially in the case where the data was unbalanced. Finally, there is also the F1 measure, which uses the precision (positive results) and recall (all positive results) of a test to measure a tests accuracy.  To measure the performance in relation to regression problems methods such as the MSE (Mean Square Error) and R2 (Explained Variance) can be used.  This topic also discussed data partitioning methods. In supervised learning, the data set provided can be partitioned into sub-sets for training and testing. The three methods for partitioning the data were: random sub sampling, stratified sampling, and cross validation.  I also gained knowledge about hyperparameters. Hyper parameters are the values that are set before the learning process and are used to find the parameters that are the result of the learning process. A validation set is defined from the sample data that is used to tune the potential hyperparameters. A possible range for the hyper parameters is then established. From this range, the performance of each potential hyper parameter is evaluated on the validation data set, so the best option can be selected.  