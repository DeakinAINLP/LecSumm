Forms of Supervise Learning: Regression problems: Output variable = continuous value. The target is to build a model that can predict the value of the output accurately according / base on the input variables  ● Linear Regression (linear model) ● Logistic Regression (linear model)  Classification problems: Output variable = categorical value. The target is to build a model that can predict the categorical of the output accurately according / base on the input variables  ● Support Vector Machines (both linear and nonlinear) ● Decision Trees (nonlinear) ● Random Forest (nonlinear) ● Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  Ranking problems: The target is to build a model that can predict the ranking of the output accurately according / base on the criteria or features inputs  Hypothesis Space (function):  ● A ML algorithm that will search through hypothesis space in order to determine the  best solution based on the problem.  ● Hypothesis function maps input to output data by selecting the hypothesis function  that is similar to the true function behind the data.  ● Hypothesis function = h ● An element of a range of possible functions = H  Empirical Risk  ● Measurement of the performance of the training data. The calculation works by  comparing the predicted output and the actual output of the training data plus the errors between each data. The target is to minimise the error in order to show a more accurate result.  The concept of model complexity  ● Measuring how accurately can the ML predict the unseen data compared to finding  good predictions based on the number / volume of data.  Structural risk minimisation  ● Target is to minimise the true risk ● Looks for a model that is complex enough for the data to fit but at the same time not so complex that the data overfit the training data resulting them to perform poorly on the new and unseen data.  Classification metric  ● Evaluation measurement used in order to assess the performance of classification  models.  ● Types of classification metric  ○ Confusion metric ○ ROC Curve ○ F-1 Measure  Confusion metric / matrix  ● A table / chart that shows the predicting outcomes of classification ● The table / chart will show the list of the prediction that was shown correctly /  incorrectly based on different categories.  ● The table / chart compares the “class” with the “actual” label and displays the information about the false positives, true negatives, false negatives and true positives.  ROC Curve ( Receiver Operating Characteristics )  ● A graph that shows the classification model's performance over all thresholds. ● This curve in the graph plots two parameters: True Positive Rate (TPR) and False  Positive Rate (FPR) over noisy channel  F-1 Score  ● Combination of a model precision and recall scores. It combine them in a single  number  Regression Metrics  ● Measurement of how far the expected value compare to the actual value ● Types of regression metrics ○ Mean square error ○ Explain various (R^2)  Mean square error (MSE)  ● Measurement of the average square difference between the predicted and actual  value  Explain various (R^2) (R-square / Explained variance / coefficient of determination)  ● Measuring the percentage of the dependent variable variance that can be predicted  based on the independent variables  Partitioning data for training and testing ● 3 methods for splitting data ○ Random subsampling ○ Stratified sampling ○ Cross validation  Random subsampling  ● The data is regularly divided into random training and test sets in a ratio that has  already been set using random subsampling.  Stratified sampling  ● Dividing the complete set of data into multiple subgroups / strata before randomly choosing a proportionate number of the final individuals from each stratum. Class proportions can be different between training and test splits when training (or validation) sets are chosen at random.  Cross validation  ● Evaluate models by splitting the original data sample into a training set to train the  model, and a test set to evaluate (validating) the model  Finding the best hyperparameter  ● Hyperparameter: Models that are set before training and cannot be learned from data. ● In order to get the best hyperparameter the data will split into training and validation sets  Internal cross validation  ● Divide data into multiple subsets. Part of the set are used for testing the model and the  remaining are use for training the model  ○ Random subsampling ○ Stratified subsampling ○ Cross-validation    