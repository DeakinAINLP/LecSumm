Practical machine learning applications mostly use supervised learning. This is where the training data is already labelled with correct data, meaning you want the machine to develop its own algorithm on the known relationship between the input and output. This develops a mapping function from the input variable x and the output variable y. The pattern isn’t based on similarity alone, it also learns a direct mapping/function between the vector xi and the output yi such that yi = h(xi). As the data is labelled this means that we know if the algorithm is producing useful outputs. Supervised learning can also appear in regression problems, linear and logistic regression, being linear models. Classification problems, such as support vector machines, decision trees, random forest and Neural Networks: perceptron and multi-layer perceptron. To try and work out the relationship between a pair of numbers, we need a hypothesis function h as an element of a range of possible function H. We select a hypothesis function we think is closest to the true function of the data. The concept of model complexity is about how complex a ML model should be, its costs when a complex model is used and if it is necessary. If we choose higher complexity than necessary, we start to over-fit the data. If lower complexity, then we under-fit the data. It important to get the best possible fit for good generalisation. Generalisation is the prediction on unseen data that it will match. When looking at model complexity we follow Occam’s razor’s heuristic guide of ‘All other things being equal, the simplest solution is the best’. So when choosing a model we choose the one that has the fewest assumptions and least complexity. Based on Occam’s razor and its simplistic principle we define another risk value, structural risk. This seeks to prevent over-fitting by using a penalty on the model complexity that prefers simpler functions over more complex ones. It is supposed to minimise both Structural Risk and Empirical Risk. So a comprehensive definition of risk is made of a loss function over all tr5ianing points in addition to complexity of the proposed model penalty. Classification metrics are very important to evaluate the performance and how it is measured and compared. A confusion matrix is a summary of prediction results on a classification problem. They count the number of correct and incorrect predictions and divide them down by each category. They are a way to understand the types of errors made by a model. A Receiver Operating Characteristics curve is used in signal detection theory to predict the trade-off between the true positive rate and the false positive rate over noisy channels. It is useful for domains with imbalanced class distribution and unequal classification error costs. It is created by plotting the true positive rate against the false positive rate at various threshold settings. Regression measures how far the expected value is from the actual value. We can measure the regression performance with the Mean Square Error. The Explained Variance R^2 which is measured as the percentage of a target variation that is explained by the model. For linear regression with bias term, R-squared is the square of the correlation between the target values and the predicted target values. Partitioning data for training and testing. The limitations on using a single training/testing set means the training set may be affected by some outliers. To get an accurate estimate of the performance a large test set is needed as variance of an estimate is low. The larger the size of the training set the more accurately the model can be learnt. Splitting the training/test data allows us to re-use the same data for both training and evaluation. We can split the data using 3 methods, random sub-sampling, which repeatedly partitions the data into random training and test sets in a ratio to then average out the accuracies to get an estimate. Stratified sampling is a probability sampling technique where we divide the entire data into different subgroups, then randomly select the finals subjects proportionally. Cross validation is a technique to evaluate model by partitioning the original sample into a training set to train the model and a test set to evaluate it. By partitioning the training data into equal sized sub samples and learning out one test set and training on the rest. We can then loop through training each individual test set with different iterations. A hyperparameter is a parameter whose value is set before the learning process begins. They are used to help the estimate model parameters. A validation set is used to evaluate a given model and also fine tune the model hyperparameters in an unbiased way. We can use internal cross validation within a training set to select the best set of hyperparameters.  Imbalanced classes is where ethe total number of one class of data, like positive outcomes, is far less than the total number of another class, like negative outcomes. This is very common in practice and detected in various disciplines. To solve this at the data level we can resample, over sample from the minority class or under sample from the majority class. So they have an equal affect on the algorithm. At the algorithm level we can adjust the costs from the majority class to lessen their effect, or adjust the decision threshold to cope with the unbalanced data. 