 Forms of Supervised Learning  Most real-world machine learning applications make advantage of supervised learning. In supervised learning, the algorithm is trained using data that has already been labelled with the correct responses. You create an algorithm based on the known relationship between the input and output, in other words. A mapping function from the input variable to the output variable is created as a result. We can discover a direct mapping or function between the feature vector and the output (target or label), as opposed to merely looking for patterns based on similarity. Determining a function from tagged training data is thus the goal of supervised learning. Compared to unsupervised learning, there is a big advantage because you know what you're going to obtain. As a result, you may verify that your method is producing useful results by testing it. Many different forms of supervised learning are possible:    Regression Problems  1.  Linear Regression (linear model) 2.  Logistic Regression (linear model)    Classification Problems  1.  Support Vector Machines (both linear and nonlinear) 2.  Decision Trees (nonlinear) 3.  Random Forest (nonlinear) 4.  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)    Ranking Problems  Supervised Learning Algorithm  A supervised learning algorithm with n training data (x, y) is considered. To establish the current relationship between x and y, the goal is to find a function that is as similar to the unknown function as possible. The input and output are two separate sets of data, to put it another way. The function is applied to the input set to produce the output set. Accordingly, there is a corresponding element in the output set for each element in the input set. You're attempting to determine the connections between the pairs of integers. The function is the connection between the two.    Hypothesis space - A hypothetical function, denoted by the letter h, will be one of  several potential functions. The hypothesis space, or H, is a common name. We will  choose a hypothesis function that, in our opinion, closely resembles the actual function underlying the data.    Finding a function - In supervised learning, the learning algorithm looks for a function on h:X->Y where X is the input space and Y is the output space given the training data.    Loss Function - Accuracy is measured by the loss function. It measures how well does  your function explain how X and the target Y relate to one another.   Empirical Risk - We can define a concept known as empirical risk, which is  comparable to the loss function. We choose the hypothesis space function that minimises the empirical risk out of all the possible functions. By averaging the outcomes of the loss function, you may determine the empirical risk. The empirical risk is based on training data, and the smaller the empirical risk, the more accurately the function captures the relationship between the two values. A supervised learning method is thus frequently taught or learned using an optimisation algorithm.  Concept of Model Complexity  It's possible that we won't always be able to see the training data in high dimensions. Therefore, we might not be able to determine if the regression issue is linear or non-linear. In a similar vein, we might not be able to determine if the classification problem is linearly or non-linearly separable.  Overfitting the data would occur if we selected a higher level of complexity than was required. We would be under-fitting the data if we chose a complexity level that was lower than was required. For effective generalisation, it is crucial to achieve the optimum fit. What does generalisation mean? It is prediction based on unobserved data, or data that is not a part of our training set.  A heuristic guidance used in the creation of theoretical models is Occam's Razor, a well- known problem-solving principle. This rule is frequently translated as: The best solution is always the simplest, all else being equal. It also covers the issue of how to decide between competing hypotheses that fit the data similarly. To put it another way, the principle suggests choosing the theory with the fewest assumptions and the least complexity when numerous competing theories are equal in other ways.  Structural Risk Minimisation - We define another risk value, known as Structural Risk, based on Occam's razor and its logically simple principle. By adding a penalty to the model complexity that favours simpler functions over more complicated ones, structural risk minimisation aims to stop over-fitting. Therefore, the overall goal is to reduce both structural risk and empirical risk.  Classification Metrics  The metrics you use to assess your machine learning model are crucial. How performance is assessed and compared depends on the evaluation metrics chosen. Applications of machine learning that include classification issues are the most prevalent. Numerous criteria can be applied to assess predictions for these kinds of issues.    Confusion Matrix  The prediction outcomes for a classification issue are summarised in a confusion matrix. Each class's portion of the total number of correct and incorrect predictions is broken down into count values. Understanding the many kinds of errors, a model makes is possible via confusion matrices. Contingency tables and confusion matrices are two different terms. Accuracy is not a reliable indicator of a classifier's true performance, which is one of the justifications for utilising a confusion matrix. The data set will produce false results if it is imbalanced, or when the quantity of observations in the various classifications differs significantly. A confusion matrix can be used to define additional evaluation metrics.  o  A sample's true positive rate (TPR), also known as recall or sensitivity, is the percentage of true positive (TP) samples that have been correctly anticipated to be positive relative to the total number of positive samples (TP+FN).  o  The fraction of falsely anticipated positive (FP) samples over all falsely negative (TN+FP) samples is known as the false positive rate (FPR).  A machine learning classifier's overall performance can be improved by include all of these variables in its reporting.    ROC Curve  Long used in signal detection theory to show the trade-off between the true positive rate and false positive rate over noisy channels is the Receiver Operating Characteristics (ROC) curve. ROC graphs have been more popular in recent years among machine learning experts. The ROC curve is particularly helpful in fields where the class distribution is unbalanced, and the classification error costs are uneven. The true positive rate (TPR) and false positive rate (FPR) are plotted against one other at different threshold values to produce the ROC curve. This is necessary to illustrate the relative trade-offs between costs (false positives) and benefits (true positives). The Youden Index and the Area Under the Curve (AUC) are two helpful statistics that may be computed using a ROC curve. These explain how accurately a model predicts events as well as the ideal cut point for a particular model (under conditions). The ROC curve can be condensed into a single number using AUC. The classifier performs better when the AUC value is larger! An AUC of 0.5 indicates a random classifier.    F-1 Measure  The intersection of precision and recall may be another helpful metric. F1-measure is a metric that combines Precision and Recall into one figure.  Regression Metrics  How far the expected value differs from the actual value is determined using regression. When you hear the word "regression," you probably don't picture comparing a bunch of data to a line. Ways of measuring regression performance :    Mean square error  Mean Square Error (MSE) is a widely used metric to assess how well the forecasts match the actual target values. A model's performance improves with decreasing MSE.    Explained Variance  R-square, explained variance, and coefficient of determination are only a few names for this measurement. The percentage of target variation that the model successfully explains is known as the R-square. The R-square for a linear regression with a bias term is the square of the correlation between the anticipated and actual target values. Unlike the other metrics, a model's performance improves with increasing R- square. R-squared is always in the range 0%â€“100%. A model with a fit percentage of 0% does not account for any of the response variable's variation around its mean. The regression model and the dependent variable are both predicted by the dependent variable's mean. A model with a percentage of 100% explains all the variation in the response variable around its mean.  Partitioning data for training and testing  The drawbacks of employing a solitary training/testing set:    One training set may be impacted by some outlier instances.   We require a big test set to provide a trustworthy estimation of model performance  (accuracy). Why? since such an estimate has a minimal variation.    However, we are aware that the model can be learned more precisely the higher the  training set size.    We can utilise the same data in many training/test splits for both training and  evaluation.  We often use 3 techniques to divide data:    Random Subsampling  Using random sub-sampling, as opposed to a single split, will yield a more trustworthy assessment of model performance. The data is randomly divided into training and test sets repeatedly and in a predetermined ratio. With each training set, we train the model, and with the corresponding test set, we estimate the accuracy. The accuracy is eventually averaged to produce an estimate.    Stratified Sampling  In a probability sampling procedure known as stratified sampling, the full set of data is divided into various subgroups or strata before the final participants are proportionately drawn at random from each stratum. Class proportions may vary between training and test splits when utilising randomly selected training (or validation) sets. The maintenance of class proportions in each random set is ensured via stratified sampling.    Cross Validation  Cross-validation is a further data partitioning technique that is even more well-liked by academics. In this method, the original sample is divided into training and test sets so that the model can be trained and then evaluated. The primary concept is to divide training data into subsamples of equal size. In the following iteration, train on the remaining sub-samples while omitting one sub-sample from the test set.  Finding the best hyperparameters  A parameter whose value is predetermined before the learning process starts is known as a hyperparameter in machine learning. Therefore, it is impossible to estimate a hyperparameter's value from data. They frequently aid in the estimation of model parameters in processes. Heuristics are typically useful for setting hyperparameters. Frequently, they are adjusted for a specific predictive modelling issue. Partitioning training data into distinct training and validation sets is necessary to look for the optimum hyperparameters.  An evaluation of a model's fit on the training dataset is done while fine-tuning model hyperparameters using a sample of data called a validation set. The validation set is employed to assess a particular model and to adjust the model hyperparameters. So, using the training set, you train the model with a set of possible hyperparameter values. On the other hand, how do you set the hyperparameters' values? The purpose of the validation set is for that. You can use it to compare the performance of your model for various combinations of hyperparameter values (e.g., using a grid search procedure) and keep the model that performed the best during training. By basing your comparisons on data that were not included in any stage of your training or hyperparameter selection process, the test set, however, enables you to evaluate several models in an objective manner.  Effect of imbalanced classes  In datasets where one class of data (in this case, positive outcomes) is significantly underrepresented compared to another class of data (in this case, negative outcomes), there may be an issue with machine learning. The detection of this issue is possible in several areas, including fraud detection, anomaly detection, medical diagnostics, etc. As most machine learning algorithms are aware, they perform best when there are nearly equal numbers of instances of each class. Problems emerge when instances of one class vastly outnumber those of the other.  We can choose between two strategies. Initially, we can make some changes to the data itself. Alternately, we might strengthen our algorithm to make it more capable of coping with such phenomena. Take a look at these two methods:  At the data level:    Over-sampling the data from minority class   Under-sampling the data from majority class  We can sample extra data points from the minority class to account for the disparity, according to two simple techniques based on data manipulation. Alternatively, to ensure  that both classes have an equal impact on the algorithm, we can under sample the majority class.  At the algorithm level:    Adjusting the costs   Adjusting the decision threshold  To lessen the impact of the points we are witnessing from the majority class, from an algorithmic perspective, we may want to change the charges on those points. In order to deal with the uneven data, we may also manually specify specific thresholds.  