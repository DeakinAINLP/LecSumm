 1.  Forms of Supervised learning  a.  Linear  regression  (Linear  model)  –  Making  the  algorithm  based  on  a  prior  knowledge between the input and output.  b.  For  this  model,  it  is  possible  to  form  a  direct  mapping  function  from  the  input  variable to the output variable.  c.  Linear  Regression  is  used  to  handle  regression  problems  whereas  Logistic  regression is used to handle the classification problems.  d.  Logostic regression predicts whether an output is true or false unlike predicting  something continuous. It classifies the output.  e.  Logistic regression has the ability to work with continuous data and discrete data  2.  Classification Metrics  a.  Confusion  Matrix  –  This  matrix  is  a  summary  of  the  prediction  results  on  a  classification problem.  b.  With count values, the number of accurate and inaccurate predictions is tallied and broken down by class. Confusion matrices are a tool for understanding the different kinds of errors a model makes. Contingency tables are another name for confusion matrices.  c.  One of the justifications for utilizing a confusion matrix is the fact that accuracy is not  a  trustworthy  indicator  of  a  classifier's  true  performance.  The  data  set  will produce false results if it is out of balance, or when there are significantly varying amounts of observations in the various classes.  d.  Generally,  Actual  values  are  depicted  as  “True”  and  “False”  and  the  predicted  Image source - https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62  values depicted as “Positive” and “Negative”.  e.  For example,  i.  True  Positive:  A  person  actually  has  cancer  and  the  model  predicted  correctly as well that the person has cancer.  ii.  True  Negative:  A  person  has  cancer,  but  the  model  predicted  that  the  person does not have cancer  iii.  False  Positive:  A  person  doesn’t  have  cancer  in  reality  but  the  system  predicted that he has cancer.  iv.  False  Negative:  A  person  doesn’t  have  cancer  and  the  system  correctly  predicted that the person doesn’t have cancer.  f.  If we have a lot of machine learning models, we can use the confusion matrix to predict  which  ones  are  most  suitable  using  the  recall,  accuracy  and  other performance metrics. g.  Performance metrics:  i.  True positive rate: 𝑇𝑃𝑅 𝑅𝑒𝑐𝑎𝑙𝑙  =  ii.   False positive rate: 𝐹𝑃𝑅  =  𝑇𝑃  𝑇𝑃+𝐹𝑁  𝐹𝑃  𝐹𝑃+𝑇𝑁 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ∗ 𝑅𝑒𝑐𝑎𝑙𝑙  𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙  iii.  F1 Measure: 𝐹1  =  2  ∗ 𝑇𝑃+𝑇𝑁  iv.   𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦  =  𝑇𝑃+𝐹𝑃+𝐹𝑁+𝑇𝑁 h.  Recall is also known as Sensitivity. i.  The true positive rate (TPR) and false positive rate (FPR) for various classification thresholds are plotted on the receiver operating characteristic (ROC) curve, which is a graphic depiction of the performance of a binary classification model.  j.  The ROC curve offers a method for comparing and visualizing the effectiveness of  various categorization methods.  k.  The TPR is the percentage of positive cases that the model correctly classifies as positive, whereas the FPR is the percentage of negative cases that the model wrongly classifies as positive. The TPR and FPR fluctuate as the classification threshold is adjusted, and the ROC curve depicts the resulting trade-off between them. Because it enables us to assess a classification model's performance across a variety of thresholds rather than just one, the ROC curve is helpful. An ROC curve for a model with high predictive power will incline towards the plot's upper left corner, where the TPR is high and the FPR is low.  l.  An ROC curve that is near to the random chance-representing diagonal line  indicates a model with little predictive power, on the other hand.  3.  How about an ROC value of 0.5 or 50% ?  a.  The capacity of the model to distinguish between positive and negative examples is comparable to guessing at random with a ROC value of 0.5, which denotes a random or chance classifier.  b.  In  order  to  forecast  a  binary  outcome,  a  model  with  a  ROC  value  of  0.5  is equivalent to flipping a coin. A diagonal line from (0,0) to (1,1) would represent the  ROC  curve  for  such  a  model  because  the  TPR  and  FPR  are  equal  at  all thresholds. Because it shows that the model has no predictive power and cannot distinguish between positive and negative cases, a ROC value of 0.5 is undesirable for a binary classification model.  4.  Regression Metrics: Regression metrics used to identify the performance of a regression model  whereas  the  performance  metrics  is  used  to  identify  the  performance  of classification models.  a.  Regression metrics include: MSE (Mean square error), RMSE (Root mean square error), MAE (Mean absolute error). These measurements are between predicted and actual values of the target variable.  Reflection on the knowledge gained.  An important concept on classification models is the confusion matrix. It is the summary of the prediction results made by a machine learning model. Actual values are denoted using “True” and “False” while the predicted ones are depicted using “Positive” and “Negative.”  To find the performance of the models with respect to its prediction results, various performance metrics are used. For classification problems, accuracy, precision, recall, F1 score, and confusion matrices. Recall measures the number of positive predictions the model makes from the total positive predictions. F1 measure is the harmonic mean of recall and precision.  However, for regression problems, some common performance metrics are mean absolute error (MAE),  mean  squared  error  (MSE),  and  root  mean  squared  error  (RMSE).  MAE  measures  the average absolute difference between the predicted and actual values, whereas MSE measures the  average  squared  difference  between the predicted  and  actual  values.  RMSE  is the  square root of MSE and measures the average magnitude of the errors in the predicted values.  