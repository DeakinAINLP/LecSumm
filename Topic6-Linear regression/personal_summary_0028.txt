Summary  Forms of Supervised Learning  Supervised Learning is a process that seeks an algorithm to determine an output from input data.  Where unsupervised learning ﬁnds similarities in data, supervised learning determines a function to  map input xi to output yi such that yi = h( xi )  The output for the training data is known, so the model can be evaluated based on its performance.  Supervised learning may take the following forms:  Regression:  Linear regression  Logistic regression  Classiﬁcation:  Support Vector Machines  Decision Trees  Random Forest  Neural Networks  Ranking  Model Complexity  A hypothesis, or model must be suﬃciently complex to describe the relationship between data and  output whilst discounting noise.  A given hypothesis has complexity C(h)  If a model is not complex enough, its performance will be reduced as it is discounting input factors  relevant to the output. This is called 'underﬁtting'  If a model is too complex, it has included noise from the input data when determining the output.  This is called 'overﬁtting'.  The supervised learning process seeks to evaluate performance of diﬀerent levels of model  complexity to optimise the ﬁt.  In order to evaluate model performance, some portion of data is not used during the training  process. Once training of a given model is complete, it is used to test the model performance.  Where multiple models acheive similar performance, Occam's razor suggests that the simplest  should be used.  Some approaches to determining model complexity are:  Number of parameters in function  The range of values for each parameter  The number of training samples (less is more complex as it becomes easier to overﬁt data)  Structural risk minimisation is the process of penalising more complex models during the training  process.  where h is a penalty parameter Rstr = λC(h)  This seeks to bias model selection to simpler models, and is considered alongside empirical risk.  Combined, the risk of a given model is:  R = Rstr (h) + Remp (h)  Classiﬁcation Metrics  Classiﬁcation problems are a common application of machine learning.  A range of metrics exist to assess their performance.  Confusion Matrix. A confusion matrix is a Matrix that summarises the number of ratio of the  predicted result (columns) matching the true result (rows):  Predictions fall into the following categories:  True positive (TP). Correctly predicated true classiﬁcation  True negative (TN). Correctly predicated false classiﬁcation  False positive (FP). Incorrectly predicted true classiﬁcation  False negative (FN). Incorrectly predicted false classiﬁcation  These results for a model can be represented in a 2 by 2 confusions matrix:  Prediction  Positive  Negative  Truth  Positive  TP  Negative  FN  FP  TN  Accuracy is the ratio of correct predictions:  accuracy =  T P + T N T P + T N + F P + F N  True Positive Rate, Recall or Sensitivity is the ratio of correctly predicted positive results over the  number of positive samples:  recall = T P T P + F N  False Positive Rate is the ratio of correctly predicted negative results over the number of negative  samples:  F P R = F P T N + F P  Precision is the ratio of correctly predicted positives over the total number of predicted positives:  precision = T P T P + F P  The relative importance of diﬀerent outcomes should be considered in the context of a models  purpose. For a model diagnosing a fatal disease, false negatives are a terrible outcome for a patient  who may go without important treatment.  ROC Curve. The Receiver Operating Characteristics (ROC) curve is a plot of the TPR against the FPR  for a given model.  It can provide a useful manner to compare the performance of diﬀerent models in the context of the  models purpose by visualizing the trade oﬀ between true and false positives.  F1  measure is a combination of precision and recall:  F1 = 2 × precision × recall precision + recall  Regression Metrics  Linear regression is the process of ﬁnding a linear function to map an explanatory variable (input) to  a dependent variable (output):  y = a + bx  is the dependent variable  y where: x a b y is the is the explanatory variable axis intercept is the gradient of the line  The performance of a linear regression can be measured in a number of ways.  Mean Squared Error. Sum of square loss function for each sample.  The lower MSE, RMSE, and MAE are, the better the performance of the regression.  MSE when compared with MAE applies a higher penalty to errors as they increase. When an error is  doubled, the penalty is quadrupled as the error is squared. The disadvantage is that the units of the  MSE are changed, so it is diﬃcult to compare meaningfully.  RMSE takes the square root of the MSE so that units of the error measurement can be usefully  compared to the original data.  MAE is useful where data included outliers that shouldn't be ampliﬁed in the error measurement.  Model Selection  In order to assess a model, a separate source of data with known results is required.  This is typically acheived by partitioning the available data into separate sets that can be used for  training and assessing the model.  Diﬀerent methods for partitioning the data exist.  Random Sub-sampling. Random sub-sampling will randomly parition data into a series of training  and test sets to meet a speciﬁed ratio.  Each set is then evaluated, and the evaluations averaged to determine overall model performance.  Random sub-sampling may result in class proportion diﬀerences between training and test sets  which is likely to reduce the accuracy of the model.  Stratiﬁed Sampling. Stratiﬁed sampling allocates diﬀerent classes to stratas, and then  proportionally draws from those stratas when forming training and test sets.  This ensures that class proportions are maintained for training and assessment.  Cross Validation. Cross validation divides a data set into  k  partitions. The model is then iteratively  trained on  k − 1  partitions, with the  partition used for validation. Each iteration changes the  kth  partition used for validation until all partitions have been used for validation.  The accuracy of the model is the average of the accuracies achieved for each iteration.  This approach acheives the greatest eﬃciency of training on available data as all data is used for  training.  Hyperparameters  Hyperparameters are paramaters that are set with values determined prior to training.  Hyperparameters are often set using heuristics used to help estimate model parameters.  This provides a starting point for the model, which can then be optimised through the supervised  learning process.  In order to determine the best hyperparameter values, a validation set is created.  A validation set is similar to a test set in that it is separated from the data used to train the model but  is used diﬀerently.  The validation set is used to test the performance of a given model or combination of  hyperparameters.  This ensures that the test set is retained for use to test the overall performance of the model once  training is complete.  Hyper parameter selection via grid search:  1. Set range of possible hyper parameter values  2. Deﬁne search grid in that range (ie. select values to try)  3. Train the model with each value in the grid  4. Evaluate model performance against validation set and select best value  When sets are partioned ﬁrst a training and test set are selected from the available data set.  Then a validation and training set are selected from the training set.  The random sub-sampling, stratiﬁed selection or cross validation methods of partioning may be  used.  The grid search method is exhaustive, and may be ineﬃcient. Other methods to set  hyperparameters:  Random Search  Bayesian Optimisation  Imbalanced Classes  Imbalanced classes occur when there is a large variation between the number of output classes.  For example, in disease testing the positive result class may be very rare, and the negative result  class very common.  This creates issues for machine learning processes which work best with approximately equal  numbers of classes.  Issues arising from imbalanced classes:  A poor classiﬁer may acheive high accuracy by always outputting the more common class.  Models should be evaluated with metrics in addition to accuracy to address this issue.  There is a higher chance that class proportions in random sampling will not reﬂect the overall  data. Other sampling techniques such as stratiﬁed sampling should be used to address this  issue.  To improve class balance:  the data set can be adjusted to oversample minority class or undersample majority class.  The algorithm can be adjusted to increase the cost of the majority class, or use of manual  decision thresholds to address minority cases.  