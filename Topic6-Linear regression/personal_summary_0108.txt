 ● Supervised Learning is a major classification of Machine Learning where the dataset consists of labelled data. This means that we know exactly what each data point represents, it could be a numerical value like share prices or a categorical value like a positive/negative blood test result. Supervised Learning is then divided into two categories viz: Regression and Classification. Regression again consists of two types - Linear and Logistic while Classification consists of four types - Support Vector Machines, Decision Tree, Random Forest and Neural Networks.  ● Since we know the output of all data points and each point/row has a number of  features, our aim in supervised ML is to find the relationship between the features and the output such that a function of all these features is most similar to the output of each row (more or less). This hypothesis function that mostly fits the relationship which comes from an unknown true function lies somewhere in the hypothesis space.  ● But to find the function that fits the most to the true function we need to use a few  concepts in combination. A new function called the loss function and a factor called empirical risk. The loss function tries to minimise the loss for each prediction, meaning for every wrong prediction, it tries to correct the next prediction up until a point where the loss is minimum. Empirical risk works on top of the loss function and tries to minimise the average loss over the data points and the goal is to choose a function that minimises the risk.  ● Once we have selected the function that we think is the best fit we need to validate it by checking the model complexity. Model complexity refers to the amount of generalisation these functions take into account when fitting with the training data. If the complexity is too high, we end up with an overfitted model, on the other hand, if the complexity is too low then we do not get a good generalisation. The goal here is to balance the complexity in such a way that it covers all the possible data points within a reasonable margin. ● If the problem at hand is of classification type and we have chosen a model to use, then  we need to use some metrics that give us a wider picture of how the model has performed in classifying the data. One common way to conclude this is by formulating a confusion matrix. A confusion matrix plots the real data on one axis, the predictions on the other one and each cell value corresponds to the amount of correct prediction in percentage.  ● Otherwise, if the problem is of regression type, then we cannot create a confusion matrix but instead, we calculate how far are the predicted values from the regression line. This is the performance of the model and it is computed in multiple ways. For example -  Means Square Error, Root Mean Square Error, R2 score and such. Each of these measures carries a different meaning and must be chosen as per the requirement.  