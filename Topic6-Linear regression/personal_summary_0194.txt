A type of machine learning called supervised learning uses labelled data to train an algorithm to predict or categorise fresh data. It contains ranking for predicting the order of things based on their features or traits, classification for predicting categorical output variables, and regression for predicting continuous output variables. Using a labelled dataset, supervised learning algorithms forecast output values based on input variables. In order to produce predictions, the method employs a hypothesis function, which is initially initialised to random values and modified during training to reduce the error between expected and actual outputs. The purpose of the method is to minimise this difference throughout the full training dataset. The algorithm finds the optimum function by minimising a cost function that evaluates the difference between predicted and true outputs. The function that minimises the average loss across the entire training dataset is found using the loss function, which calculates the difference between anticipated and actual outputs.  In machine learning, a model's depth is referred to as its complexity. More complex models are able to capture complex patterns despite the danger of overfitting. A model's complexity can be influenced by a number of factors, including the number of parameters, nonlinear transformations, regularisation strategies, and ensemble methods. Regularisation techniques reduce model complexity whereas nonlinear transformations and ensemble methods raise it in order to avoid overfitting.  the Structural Risk Minimisation (SRM) regularisation strategy seeks to strike a balance between model correctness and complexity. It increases the loss function's penalty term, which encourages smaller models and can reduce overfitting and boost generalisation efficiency. Cross-validation is used to determine the regularisation parameter's ideal value.  Classification metrics are used to assess a machine learning model's performance in binary or multi-class classification situations. The confusion matrix, ROC curve, and F-1 measure are the three most widely used metrics. True positives, true negatives, false positives, and false negatives are displayed in the confusion matrix. The ROC curve is a depiction of the true positive rate vs the false positive rate at various categorisation levels. The F-1 measure, which spans from 0 to 1, is a harmonic mean of precision and recall that balances precision and recall.  The performance of a machine learning model that predicts continuous numerical values is evaluated using regression measures. Mean squared error (MSE), explained variance (R2), and mean absolute error (MAE) are three often used regression metrics. MSE is the average squared difference between predicted and actual values, R2 is the proportion of variation in the target variable that the model explains, and MAE is the average absolute difference between predicted and actual values. Lower MSE and MAE values suggest better prediction accuracy, whereas higher R2 values indicate superior model performance.  Partitioning data into training and testing sets is an important step in machine learning for assessing a model's performance on unseen data. Subsampling, stratified sampling, and cross-validation are all methods for partitioning data. Sub-sampling includes randomly selecting a subset of data for training and the remainder for testing, whereas stratified sampling guarantees that the target variable distribution is similar in both groups. Cross-validation involves partitioning data into k-folds and training the model on k-1 folds before testing it on the remaining fold, yielding a more accurate assessment of the model's performance and lowering bias.  When one class has much fewer samples than the other, the model biases towards the majority class. This problem can be addressed using resampling strategies such as over-sampling (raising minority class samples) and under-sampling (decreasing majority class samples). Oversampling can result in overfitting, whereas under sampling might result in information loss from the majority class.  