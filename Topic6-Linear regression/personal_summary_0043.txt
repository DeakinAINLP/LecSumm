 In this topic’s module, the main focus was on Supervised Learning. We learn what it means,  various  types of  supervised  learning,  SL  algorithms, model  complexity,  risk minimization  and  then  continued  with  the  concepts  of  classification  and  regression metrics. This topic covered linear and multi-variate regression and will be followed by recap of these concepts and logistic regression in the next topic.  SUPERVISED LEARNING  When we have a set of labelled data and we want to train an algorithm with it, we call it supervised learning method of Machine Learning. In this situation, we already have the results to what the algorithm should output and we can easily compare the correct and incorrect outputs. The various forms of it are Regression problems, Classification problems, and Ranking problems. To find a function that correctly maps all the inputs to the outputs, we calculate the loss function and the empirical risk and then just try to minimise it leading to more correct outcomes and hence, the best hypothetical function.  MODEL COMPLEXITY  Now,  when  we  apply  these  learning  algorithms,  we  basically  are  coming  up  with  a model that will help us solve our problem. A factor that we need to consider with the models  is called  model complexity.  That  is because  choosing  a  model of  imperfect complexity can lead to under-fitting or over-fitting of data. If the model is very complex, then the data becomes over-fitting, on the othr hand in case of low complexity model, the data remains under-fitted, both scenariosleading to incorrect predictions.  RISK MINIMIZATION  A infamous concept of Occam’s Razor says that if there is a confusion regarding which model to choose, simplicity is the answer, therefore always go for the simplest solution there is. Considering this in mind, there is risk value called Structural risk that has been defined as empirical risk summed up withh the complexity of our hypothesis function with a penalty parameter. This helps in avoiding over-fitting as well.  CLASSIFICATION METRICS  To  measure  the  performance  of  a  classification matrix,  there  are  a  few techniques, including, confusion matrix, ROC curve, AUC, etc. Confusion matrix is a tabular form of values derived from the comparison of predicted and actual results. It gives us the accuracy  for  each  class.  This  can  be  usd  for  accuracy,  precision  or  sensitivity calculations, thus enabling us to use more than one way of comparing performance. ROC – Receiver Operatin Characteristics is a curve plotted between true positives and false positives and gives us the values correspondingly. Then we also introduce AUC, called Area under the curve, which is calculated from the ROC curve and it gives us a number  that  tells  us  the  performcance  of  the  model.  Higher  value  of  this  number means better performance.  REGRESSION METRICS  To calculate regression performance, we have different types of errors that compare the true and predicted values and give us an answer. These include: Mean Square Error  (MSE),  Root  Mean  Square  Error  (RMSE),  Mean  Absolute  Error  (MAE)  and Explained  Variance  (R2). MAE  is  immune  to outliers  because  it  uses  1-norm  of  the error. Explained variance is the ratio of variance explained by the model to the total variance. The more its value, the better is the performance of the regression model.  APPLYING SUPERVISED LEARNING  Now, in supervised learning, we need two sets of data: training and testing. We get that  by  dividing  the  dataset  by  one  of  the  three  techniques:  random  sub-sampling, stratified sampling, and cross-validation, later two being more efficient and widely used than  the  first  one.  There  is  another  set  of  data  that  we  need  to  caculate  the hyperparameters, and we call it the validation dataseet. We derive it from the training dataset using one of the three earlier stated techniques. In the last third of this module, we  used  the  Python  programming  language  to  apply  all  of  these  studied  concepts mainy regression predicting.       