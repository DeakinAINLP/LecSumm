 Main learning points:  Supervised learning:    Rather than finding patterns by similarity only, the model learns a ‘mapping’ that connects  the features to target.    Hence supervised learning is about finding, from the training data, the function that  mechanises this ‘mapping’.   Supervised learning includes:  o  Regressions o  Classification problems  ▪  Support Vector Machines (both linear and nonlinear) ▪  Decision Trees (nonlinear) ▪  Random Forest (nonlinear) ▪  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  Confusion matrix / contingency tables:    Helps illustrate error type:  When it is indeed Cat(b), the model thinks:    it is Cat(a) 6% of times and Cat(c) 6% of times (correct prediction 94% of times).   Key measures:  Accuracy:  True positive rate / recall / sensitivity:  False positive rate:  Receiving operating characteristics (ROC) curve:  F-1 measure:   Measuring regression performance:    Mean Square Error   Mean Absolute Error, which is more robust against outliers.  Partitioning data:    Random subsampling:  o  Multiple partitions and averaging the accuracy.    Stratified Sampling:  o  Characteristic A is found in 0.7 of the total sample. o  Divide the data into train/test where 0.7 of train-group has characteristic A and 0.7 of  test-group has characteristic A.    Cross-validation:  o  Data is partitioned to k equal sized subsamples.  o  Where k = 5    Leave-one-out is when k=sample size.  o  Not computationally demanding when doing multiple linear regressions  