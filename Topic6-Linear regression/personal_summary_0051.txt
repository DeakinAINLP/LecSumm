 Supervised learning I learnt that it is a function that maps the input data to the output data Estimate a function from labeled training data   Forms of supervised learning algorithm 2:05pm:  Regression problem   Linear Regression (linear model) Logistic Regression (linear model)  Classification problem  Support Vector Machines (both linear and nonlinear) Decision Trees (nonlinear) Random Forest (nonlinear) Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  Ranking problem  How Supervised Learning Algorithm Works 2:16pm  Use the function h that is an element of some space of possible functions ùêª,   usually called the hypothesis space. Loss function will measure the quality of function h by comparing the predicted output and the actual output  Some loss functions:  Square loss:  Absolute loss:  0-1 loss:  Square loss is sensitive to outliers in the data Absolute loss is less sensitive to small values Square and absolute loss are useful for regression  0-1 loss is useful for classification  We use empirical risk that measures the average of the loss function we select the function ‚Ñé, which minimizes the empirical risk.  Model complexity  Concept of model complexity 2:27pm    If we choose higher complexity than necessary in a model, we would be overfitting the data. If we choose lower complexity than necessary in a model, we would be under-fitting the data.   It is important to get the right fit for good generalization.   In this example, we see a linear model and a non-linear model, let‚Äôs say that with a non-linear model we get a better result, however high non-linear model is based on the training data and when using a test data it may not be as accurate and getting a low performance thus having an overfit model.  Model complexity and Occam‚Äôs razor 2:34pm   ‚ÄúAll other things being equal, the simplest solution is the best‚Äù, in other words having a simpler model that has an equal performance is usually the best  Structural risk minimization 2:37pm  Structural risk is a combinations of the empirical risk plus the model complexity where we find the minimum structural risk which has a least complexity in the model and the least empirical risk.  Model Evaluation Metrics  Classification Matrics:  Confusion Matrix 2:47pm  Summary of prediction results on a classification problem where the  number of correct and incorrect predictions are summarized with count values and divided down by each class.  Accuracy is not reliable as using imbalance data where the numbers of observations in different classes vary greatly, will cause the accuracy to generate confusing results and therefore we will use other classification metrics such as precision, sensitivity, etc.  ROC Curve 2:52pm  Receiver Operating Characteristics (ROC) curve depicts the trade-off   between the true positive rate and false positive rate. The ideal curve will be having the lowest false positive rate while having the highest true positive rate  Area Under the Curve (AUC) will be used to summarize the ROC curve  using a single number.   F-1 Measure  Regression Metrics 3:01pm:  Mean Square Error (MSE) Root Mean Square Error (RMSE) Mean Absolute Error (MAE) Generally, the lower the MSE is in the model, the better its performance R-Square is between 0 and 100% or 0 to 1. Measures the square of the correlation between the target values and the predicted target values.  Data partitioning schemes 3:11pm  Partitioning data for training and testing  Use the whole dataset and then split it into a training and test set.  There are different methods for splitting data:  Random sub-sampling   It will randomly split the data into training/test sets in a specified ratio Then will find the average estimate using the average accuracies  Since we are randomly splitting the data, we could find that the  training set will contain more positive instances than the negative instances and might create a bad or biased training model.  Stratified sampling  We will split the dataset into the training/test subset and give the proportion of the classes to have a similar pattern to the original dataset.  Cross validation 3:20pm  Use k-fold cross-validation to divide the training data into   sub-samples Then leave one sub-sample as the test set, then replace that test set with another sub-sample and test again. This is the leave-one-out cross validation scheme  Will find the average accuracy over multiple runs/iterations.  Finding the best hyperparameter 3:25pm  Are parameters whose value is set before the learning process begins.  To search for the best hyperparameters we need to partition training data into separate training and validation sets  Validation set is used to evaluate the performance of the model fit on the training dataset  and then fine-tune the hyperparameters This will be used to have the test set unbiased  We will use partitioning methods for splitting the training and validation sets such as  random, stratified subsampling and cross-validation.  Effects of imbalanced class distribution 3:43pm    Imbalance data sets are datasets that contain a ratio of classes that are distributed in a way that was skewed towards some classes. Imbalance datasets are very common such as fraud detection, anomaly detection and medical diagnosis.   To solve this issue we can:  Modify the data itself Somehow improve the algorithm to handle the imbalance data Over-sample the data from the minority class Under-sample the data from the majority class   In the training process, we must not use any information that is not available during the training process  QUIZ   