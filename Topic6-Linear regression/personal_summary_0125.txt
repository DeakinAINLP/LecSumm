 The main difference between supervised and unsupervised learning In supervised learning, we already had the corrected labelled data, and we use this labelled data to develop the functions. While in the unsupervised, we have to predict the output as labelled data and compare the difference between the predicted and the true labelled data.  Supervised learning can appear in many forms: regression problems, classification problems, and ranking problems.  Classification metrics Confusion matrix: a useful tool for evaluating the performance of a model, particularly with an unbalanced data set. ROC Curve: especially useful in unbalanced class distribution, and unequal classification error costs. It plots the true positive rate (TPR) against the false positive rate (FPR) for different classification thresholds to depict relative trade-offs between benefits (TP) and costs (FP). F1 measure: The metric combines Precision and Recall. useful in cases where the number of positive and negative examples in the dataset is unbalanced.  Measuring regression performance Mean Square Error: MSE, RMSE, MAE. The lower these errors, the better of model’s performance. MSE – how close the predictions are to true labelled values MAE – robust to outliers in the test set. Explained Variance – R2: the percentage of the target variation that the model can explain  Partitioning data: using a single train and test set can have limitations so we use some methods to split the data set into different training and testing sets.  Random subsampling: a simple method where the dataset is repeatedly divided into random training and testing sets in a specified ratio. The drawback of random subsampling is that it may not preserve the distribution of classes in the dataset.  Stratified sampling: a method that ensures the same proportion of classes in both the training and testing sets as in the original dataset.  Cross-validation: a method that divides the dataset into k equal-sized folds and then training and tests the model k times, with each fold used as the testing set once and the remaining folds used as the training set.  