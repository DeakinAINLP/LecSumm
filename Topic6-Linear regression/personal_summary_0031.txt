This module’s focus was on supervised learning – model selection, classification and regression problems and evaluation.  SUPERVISED LEARNING  Supervised learning is used by the majority of practical machine learning applications. Superviesd learning maps a feature vector to an output: yi = h(xi). This is referred to as a function. Supervised learning estimates this function instead of simply finding patterns through similarity.  Used in regression and classification problems. Regression has linear and logistic types (both linear) while classification subproblems include Vector Support machines, Decision Trees, Random Forest and Neural Networks, mostly being nonlinear. Classification problems aim to classify a set of data into one or more classes (X or not X) while regression draws a continuous boundary to predict values.  Knowing the outputs of the test data set before training the model gives supervised learning a significant advantage over unsupervised learning as it is easy to test the accuracy of the model.  Datasets are broken into two components, input and output. The function is applied to the input set to generate the output set. The goal of the algorithm is to find this function. All the possible functions for this mapping is referred to as the Hypothesis Space, H. An individual function is a hypothesis function, h. A hypothesis function is chosen based on believed similarity to the true function. Other types of hypothesis space included the space of all linear functions in d dimensions, and the space of all polynomial functions up to degree p.  A loss function is required to determine how accurate h is. An input, Xi is used to find y-hati = h(Xi). We already know the true Yi from the test data, so the results are compared to determine the accuracy using a loss function L(Yi, y-hati).  The average of all results of applying the loss function to the testing set is called the Empirical Risk. Accuracy is determined by this empirical risk. For all possible function in hypothesis space, H, we chose function, h, with the lowest empirical risk (highest accuracy). Optimization algorithms are often used for training the supervised learning algorithm that focuses on minimising risk.  MODEL COMPLEXITY  Models should only be as complex as necessary – reduce complexity where feasible. Solutions are relatively simple to picture in 2d-space but become much harder in higher dimensions. Higher-complexity can lead to over-fitting the data, making the model inaccurate for new data. But too low a complexity can lead to under- fitting, making the model imprecise for all data. So we need good generalization, that is prediction on unseen data. Generalisation (forming an accurate model) requires a good middle-ground.  This is related to Occam’s razor, which states that “all other things being equal, the simplest solution is the best”. In other words, when there are multiple possible models, try the simplest ones first. Model complexity is further defined as Structural Risk. Following the Occam’s Razor heuristic requires minimising structural risk, which is done so by applying a penalty to model complexity, encouraging the algorithm to prioritize simpler functions. This is tied to Empirical Risk, so reducing empirical risk reduces structural risk.  Rstr(h) = Remp(h) + PC(h)  TESTING AND TRAINING DATA  For supervised learning testing requires known result, in fact a considerable amount is required to ensure reliability in determining model performance. But the model also requires labelled data for training purposes, so a known dataset needs to be intelligently split into two groups.  Methods include random sub-sampling, stratified sampling, cross validation.  RANDOM SUB-SAMPLING  Simply separate the dataset into training and testing sets arbitrarily. An 80/20 split for training/testing is a reasonable ratio, though may require some tweaking based on the size of the data set.  Random sub-sampling can lead to unintentional bias, imbalanced data, or create accidental outliers. This is especially risky for binary classification problems. This can be reduced by taking multiple cases of data partitions. Multiple unique splits can be made, in which case the average accuracy of all tested splits should be taken for the model’s accuracy.  STRATIFIED SAMPLING:  Samples by probability. Divides the data set into subgroups (strata). Then randomly separates into training and testing groups by selecting proportionate amounts from each strata (train group will be  X% strata A, testing group will also be X% strata A). Likewise, a validation set is broken off the training set, maintaining strata ratio.  CROSS VALIDATION  Training sets my be impacted by outliers or have unintentional bias.  Cross-validation splits the data into K equal-sized sub-sets or sub-samples. Then k-1 subsets are chosen for training while one subset is used for testing. This repeats iteratively until all subsets have been used as the test subset. The accuracy for each of these cases will be stored, and the average of all these accuracies will be measured. This is known as K-Fold cross-validation.  In the special case where K is equal to the number of data instances, it is known as leave-on-out cross- validation. Each subset is a single data instance and only one is left out per iteration, and there will be count(instances) iterations.  CLASSIFICATION METRICS:  The methods used to evaluate a classification problem model. Standard types include Confusion Matrix, ROC Curve, F-1 Measure. Classification metrics are determined using testing data sets.  CONFUSION MATRIX:  Among the most popular evaluation metrics. For binary classification it compares predictions to true results in a matrix form. It finds the counts of four values: True Positive, True Negative, False Positive (type 1 error), and False Negative (type 2 error). These counts can be used to determine important statistics such as accuracy, specificity, sensitivity, precision.  Accuracy is not always reliable, particularly for imbalanced data. High counts of false positives or false negatives can be disastrous.  Confusion matrices can be extended to multiclass classification problems. In this type of classification problem the TP is the value in the cell P(p), T(p). TN is the sum of all values outside the selected label’s row/column. The FN is the sum of remaining values in the row. The FP is the sum of remaining values in the column.  ROC CURVE  This metric is used for problems with unequal classification error costs.  Generates a curve comparing the True Positive Rate (TPR)(aka sensitivity) vs the False Positive Rate (FPR). This is calculated as a decimal percentage so FPR is 1-TPR. The ROC finds the trade-offs between benefits and costs (TPR vs FPR). The goal is to find ideal ratio of benefits to costs, which depends on how sensitive the real-world problem is to costs. If the benefits are more important then we can ignore high FPR and maximise TPR, but the vice-versa is true if high costs are too detrimental to ignore.  Examples are medical diagnosis vs criminal convictions. For the former false negatives are disastrous while false positives are a minor inconvenience. For the latter false positives are arguably worse as they could lead to innocents being convicted.  There’s essentially three points to choose from depending on the problem. When FN is more impactful aim for the top-right result. When FP is more impactful the bottom-left point is ideal. When FP/FN are equally weighted then the elbow point is the best choice.  The ROC can calculate useful statistics through Area Under the Curve (AUC) and Youden Index. The AUC can summarize the performance of a ROC curve with a decimal percentage. Closer to 1 means more accurate. Random classification scores 0.5.  REGRESSION METRICS:  MEAN SQUARE ERROR  Finds the average of the sum of all the difference of each true value and its respective loss value, squared. Avg(Sum((yi – y-hati)^2)).  The Mean Square-Root Error variation simply takes the square root of the MSE.  Mean Absolute Error instead finds the average of the sum of the magnitudes of the difference between each true and correlated loss values. Avg(sum(|yi – y-hati|).. This method can better handle outliers than the other two.  For each measurement a lower score suggests a more accurate model.  EXPLAINED VARIANCE:  AKA R-square, coefficient of determination.  R^2 = variance in model / total variance  In this method higher values mean better performance (as the difference between predicted variance and true variance decreases the division tends towards 1, and towards 0 as the difference increases). 