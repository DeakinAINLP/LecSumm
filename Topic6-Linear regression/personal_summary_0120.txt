 A. Forms of Supervised Learning:  a) where data used to train the algorithm is labeled. b) c) Regression: Linear Regression (linear model), Logistic Regression (linear  is the task of estimating a function from labeled training data  model)  d) Classification: Support Vector Machines (both linear and nonlinear), Decision  Trees (nonlinear), Random Forest (nonlinear), Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  e) Ranking  B. A Supervised Learning Algorithm:  a) Name a hypothesis function h, as an element of a range of possible functions H, usually called the hypothesis space. Select a hypothesis function that is similar to the true function behind the data. In supervised learning, given the training data, the learning algorithm seeks a function on h: X → Y where X is the input space and Y is the output space. c) Loss function: a measure of accuracy. (ex. square loss, absolute loss, 0 - 1  b)  loss)  d) Empirical risk: Among all functions in hypothesis space, we select the function which minimizes the empirical risk. You can calculate the empirical risk by averaging the results of the loss function.  C. The Concept of Model Complexity:  a) Higher complexity than necessary: over-fitting the data b) Lower complexity than necessary: under-fitting the data c) Occam’s Razor: All other things being equal, the simplest solution is the best.  D. Structural Risk Minimization:  a) Seeks to prevent over-fitting by incorporating a penalty on the model  complexity that prefers simpler functions over more complex ones. So the general idea is to minimize both Structural Risk and Empirical Risk.  E. Classification Metrics (1)  a) A confusion matrix is a summary of prediction results on a classification  problem.  b) The number of correct and incorrect predictions are summarized with count  values and divided down by each class.  c) Also called contingency table. d) The diagonal values represents the elements where the predicted classes  were equal to the expected classes.  e) The off-diagonal values represent the elements where the classifier got the  f)  prediction wrong. The higher the proportion of values on the diagonal of the matrix in relation to values off of the diagonal, the better the classifier is.  g) Confusion matrix for 2 classes:  h) Related formulas:  i. Accuracy: (TP+TN) / (TP+FP+FN+TN) ii. True Positive Rate (TPR) or Recall or Sensitivity: TP / (TP+FN) iii. False positive rate (FPR): FP / (TN+FP)  F. Classification Metrics (2):  a) ROC Curve: Receiver Operating Characteristics (ROC) curve: depicts the  trade-off between the true positive rate and false positive rate over noisy channels. is useful for domains with imbalanced class distribution and unequal classification error costs  b)  c) Plotting the true positive rate (TPR) against the false positive rate (FPR) at  various threshold settings  d) The further the ROC curve is from the diagonal line, the better the model is at  discriminating between positives and negatives in general.  e) Area Under the Curve (AUC) and the Youden Index: shows how well the model predicts and the optimal cut point for any given model. The higher the value of AUC, better performing is the classifier. A random classifier has an AUC of 0.5. F1-measure: a metric that combines both Precision and Recall in a single number: F1 = 2 * (precision*recall) / (precision+recall)  f)  G. Regression Metrics:  a) Mean Square Error (MSE)  b) Root Mean Square Error (RMSE)  c) Mean Absolute Error (MAE)  d) Variance (R^2, R-square, Explained variance, the coefficient of determination):  i. Between 05 and 100% ii. R^2 = (Variance explained by the model) / (Total variance) iii. Errors: lower the better; R-square: higher the better  H. Partitioning Data for Training &Testing:  a) Splitting method:  i. Random sub-sampling: repeatedly partitions the data into random training  and test sets in a specified ratio.  ii.Stratified sampling: a probability sampling technique in which we divide the  entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata.  iii. Cross validation: This is a technique to evaluate models by partitioning the original sample into a training set to train the model, and a test set to evaluate it.  iv. K-fold cross-validation: The main idea is to partition training data into k  equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples.  I. Finding the Best Hyper-parameters:  is a parameter whose value is set before the learning process begins. Its value in a model cannot be estimated from data.  a) b) c) A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training data set while tuning model hyper-parameters. d) We can select the best hyper-parameter set by searching/or optimizing over all  possible values using one of the three methods: i. Grid-search (not so efficient). ii. Random search (efficient in certain scenarios) iii. Bayesian optimization (efficient in general)  J. Effect of Imbalanced Classes  a)  In practice, there are some datasets where the total number of one class of data (i.e. positive outcomes) is far less than the total number of another class of data (i.e. negative) outcomes. (ex. fraud detection, anomaly detection, medical diagnosis)  b) Solutions: At the data level: (Re-Sampling)  i. Over-sampling the data from minority class ii. Under-sampling the data from majority class.  c) Solutions: At the algorithmic level:  i. Adjusting the costs ii. Adjusting the decision threshold.  d) Any pre-processing over an entire data set (e.g. feature selection, or feature  extraction) must not use the information from the labels.  