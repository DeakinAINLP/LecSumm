 Topic 5 for Machine Learning dealt with the fundamentals of Supervised learning, the diﬀerent forms of Supervised learning, model complexity, risk structure minimization, classiﬁcation metrics, regression metrics, hyperparameters and the eﬀect of imbalanced class on Supervised learning.  Supervised Learning  1.  In supervised learning, the data is already labelled with correct answers 2.  We need to create an algorithm based on a known relationship between input and  output.  3.  We need to develop a mapping function between input and output. 4.  Supervised learning is the task of estimating a function from labelled training data.  Forms of Supervised Learning 1.  Regression problems  a.  Linear Regression (linear model) b.  Logistic Regression (linear model)  2.  Classiﬁcation problems  a.  Support Vector Machines (both linear and non-linear model) b.  Decision Trees (non-linear model) c.  Random Forest (non-linear model) d.  Neural Network: Perceptron and Multi-layer Perceptron (non-linear model)  3.  Ranking problems  Supervised learning algorithm  1.  Two sets of data - input and output. The output set is obtained by applying the  function to the input set  2.  Hypothesis space - this is the range of all possible functions. From this set of  functions, we choose a function which we think is similar to the true function behind the data  3.  Finding a function  a.  In order to measure the quality of this function, we deﬁne a loss function ﬁrst b.  Loss function is a measure of accuracy of the function Square Loss (useful for regression) Absolute Loss (useful for regression) 0-1 Loss (useful for classiﬁcation) (equal to 0 if equal and 1 if not equal)  i. ii. iii.  iv.  Other loss functions 1.  Logistic loss 2.  Hinge loss  4.  Empirical risk  a.  We need to select a function from the Hypothesis space which minimizes a  chosen Loss function        5.  Hence a supervised learning algorithm is o[en trained through an optimization  algorithm.  Model Complexity  1.  If we choose higher complexity than necessary, we would over-ﬁt the data 2.  If we choose lower complexity than necessary, we would under-ﬁt the data 3.  We need to ﬁnd the best possible ﬁt for good generalisation 4.  Occam's Razor - all other things being equal, the simplest solution is the best  Structural risk minimisaEon  1.  Seeks to prevent over-ﬁ]ng by incorporating a penalty on the model complexity 2.  General idea is to minimise both Empirical risk and Structural risk  C(h) is the complexity of hypothesis function and ƛ is penalty parameter  ClassiﬁcaEon Metrics  1.  Confusion matrix or contingency tables a.  Used in classiﬁcation problem b.  Values across the diagonal are most important in understanding how good a  classiﬁer is  c.  d.  Recall is same as sensitivity e.  False Positive Rate or FPR = FP/ TN + FP f.  Accuracy is not a good metric for unbalanced class problems 2.  ROC Curve or Receiver Operating Characteristics Curve  a.  ROC curve is useful for domains with imbalanced class distribution and  unequal classiﬁcation error costs.  b.  Created by plo]ng True Positive Rate against False Positive Rate         c.  Useful statistics calculated from ROC Curve - Area under the Curve (AUC) and  Youden Index  d.  Higher the value of AUC, beeer performing is the classiﬁer  3.  F1- measure  a.  Combination of Precision and Recall b.  Deﬁned as 2	x	 !"#$%&%’(	*	+#$,-- !"#$%&%’(.+#$,--  Regression Metrics  1.  Mean Square Error or MSE  a.  Mean Square Error or MSE b.  Root Mean Square Error or RMSE c.  Mean Absolute Error or MAE  2.  Explained Variance or R/ a.  Deﬁned as b.  Lies between 0% to 100%  8’5,-	0,"%,($#  0,"%,($#	#*1-,%(#2	34	56#	7’2#-  ParEEoning data into training and tesEng  1.  Random subsampling  a.  We repeatedly partition the data into random training and test sets in a  speciﬁed ratio  b.  We then estimate the accuracy using the corresponding test set and ﬁnally  average the accuracies to get an averaged estimate  2.  Stratiﬁed sampling  a.  This is a probability sampling technique where we divide the entire data into  diﬀerent subgroups or strata and then randomly select each set proportionally from the diﬀerent subgroups  b.  This ensures that class proportions are maintained in each random set  3.  Cross validation or k-fold cross validation  a.  Divide the data set into k equal sized subsets b.  Leave out one subset as the test set and the rest k-1 sets as training sets c.  Loop through steps 1 and 2 a total of k times d.  Accuracy is calculated by an average estimate for all the k iterations e.  When k is equal to the number of elements in the set, n then we call this as  "leave-out-one cross validation" scheme  Hyperparameters  1.  Hyperparameter is a parameter whose value is set before the learning process begins 2.  Value of a hyperparameter cannot be estimated from the data 3.  A validation set is a sample of data used to provide an unbiased evaluation of a  model ﬁt on the training dataset while tuning model hyperparameters  4.  For example, in K-Means, we select the value of k at the beginning of the process 5.  3 ways to navigate the hyperparameter space -  a.  Grid search (not eﬃcient and computationally intensive) b.  Random search (eﬃcient in certain scenarios) c.  Bayesian optimization (eﬃcient in general)        Eﬀect of imbalanced classes  1.  This occurs when the total number of one class of data is far less than the total  number of another class of data  2.  This can be solved by the following methods -  a.  At the data level  i.  Over-sampling the data from minority class ii.  Under-sampling the data from majority class  b.  At the algorithmic level  i. ii.  Adjusting the costs Adjusting the decision threshold  3.  This can be solved by using Stratiﬁed sampling method  