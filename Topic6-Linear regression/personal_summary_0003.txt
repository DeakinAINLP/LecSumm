Supervised learning estimates a function mapping input features to the known output labels. f(xi) = yi Common forms are  Regression (predicting a real variable eg linear or logistic)  Classification (predicting the category the sample belongs to eg SVM, decision trees, random vector or neural network based)  Ranking (predicting the position based on partial orderings) It requires estimating the mapping h: X → Y that minimises the loss function. The type of loss function affects the degree to which deviations of predictions from the true values are scored, and so affect the shape of the learned function.To minimise the empirical risk, minimise the average of the loss function calculated over the inputs. Depending on the complexity of the model, different shapes of prediction functions can be represented. For example higher order olynomials represent curves while linear functions only represent straight lines. Underfitting occurs when the model is too simple and can not capture the pattern in the data. There will be a high loss on both test and training data.Overfitting occurs when the model matches the points too closely, so that it models the noise in the data closely and does not generalise well with unseen data. The training data will have a very low loss as even the noise has been matched, but the test data will have a higher loss than simpler models. The ideal complexity is in between, where the loss on validation data is minimised, and typically still close to the loss on the training data, but before the validation loss increases due to overfitting. Occam’s razor suggests the simplest model should be preferred where several have similar performance. A simpler model has fewer assumptions, and might result in faster training and inference because it has fewer parameters to learn or evaluate. But it still should be evaluated to confirm it is applicable. To penalise models for complexity, the structural risk is the sum of empirical risk + a scaled complexity of hypothesis function. Measuring Classification Model Performance Accuracy is the proportion of correct answers (see 0-1 loss function above). It has limitations where unbalanced training data results in relative underweighting of classification errors in the minority groups. The confusion matrix is a contingency table separating the data between the true and predicted categories. It shows the proportion of samples correctly predicted, and where the mispredicted entries are placed. Using the proportion corrects for differences in representation of each category in the training data, and the average of the main diagonal measures performance. Sensitivity / Recall / True positive rate – proportion of samples truly belonging to a category which are correctly predicted Recall = TP / (TP+FN) Precision / Positive predictive value – proportion of samples predicted to belong to a category which are actually correct. Precision = TP / (TP + FP) Specificity – proportion of samples not belonging to a category which are correctly predicted Specificity = TN / (TN + FN) False positive rate = 1 – specificity – the proportion of samples incorrectly predicted to be in a category. These values depend on the prevalence (number of samples truly belonging to each category) Receiver Operator Curve graphs the true positive rate against false negative rate for varying thresholds. This can be used to pick a threshold depending on the relative cost of false positives and false negatives eg in screening vs diagnostic tests. The higher the AUC the better the performance. F-1 measure  = 2 / (precision-1 + recall-1) = 2 (precision * recall) / (precision + recall) = 2*TP / (2*TP + FP + FN) This is the harmonic mean of precision and recall and balances performance of precision and recall equally. It is affected by the prevalance of each category. Measurement of Regression Performance MSE represents the average deviation of the prediction from the true values, squared. RMSE is sqrt(MSE) and has the same units as the predicted variable.MAE is the average absolute deviation of the predictions from true values. The noise tends to cancel out, making it more robust to outliers. Explained variance (R2) is the proportion of the change in the output explained by the model. R2 = Explained variance / Total variance. It compares the distance of each estimated values to the mean, with the distance of each actual value to the mean the mean. Partitioning Data A single test-train split is affected by outliers, and reduces the samples available in each group. Random subsampling splits the data by fixed proportion for each epoch. This way outliers in one sample will potentially be in a different set next time, reducing their impact. Some of the data not included in the first sample might be used for training in the others. But the groups may not be representative due to randomly sampling more of one category than another. Stratified sampling splits the data ensuring that the proportion of features included in each group remains the same. K-fold Cross validation separates the data into k partitions, and each time uses a different one for test and the rest for training. All data is thus used for training and testing.When k = n, this is leave out one sampling. When k=1 this is single test-train split. Hyperparameter Estimation To search for hyperparameters, use a test-train-validation split. The training set is divided into actual training and validation partitions, used to test performance of training with varied hyperparameters. Methods for hyperparameter turning are Bayesian optimisation, random search, grid search.Bayesian learning is efficient in general (but sequential), random may be better in some circumstances (eg large number of hyperparameters), and grid search is often computationally expensive. Consider Tree-Prazen estimators (selects each hyperparameter independently, less helpful in NN where they are codependent), Bayesian Optimisation with HyperBand (budget to find good estimates, then optimise, runs in parallel) Unbalanced Samples Avoid measuring performance with accuracy as the classifier is biased towards always selecting the majority. Undersanple the majority (throws away a lot of data) Oversample the minority (outliers in the minority will have a larger effect) Adjust the costs or decision threshold in the algorithm to account for the imbalance. Need to avoid bias in the results - don’t use output variables to preprocess or select samples - repeatedly modifying the model against the same sample can result in overfitting 