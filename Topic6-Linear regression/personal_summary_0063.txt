   Forms of supervised learning  The majority of real-world machine learning applications use supervised learning.  In supervised learning, the data used to train the algorithm is already labeled as correct. That is, you create algorithms based on known relationships between inputs and outputs. From this we create a mapping function from the input variable x to the output variable y  Instead of finding patterns based only on similarity, we can learn direct mappings or functions between feature vectors. xi and the output (target or label) yi such that yi=h(xi).  Supervised learning is therefore the task of estimating functions from labeled training data.  The training data for supervised learning is arranged in the following format: {Xi, (cid:0)i}, [xi,yi] I = 1,,,,,,n  For each data point we have xi output yi. This means that you can test your algorithm to make sure it gives usable output.  Supervised learning can come in many forms.  (cid:0) Regression problems  o linear regression (linear model)  o Logistic regression (linear model)  (cid:0) Classification problems  o Support vector machines (both linear and nonlinear)  o Decision tree (nonlinear)     o Random forest (nonlinear)  o Neural Networks: Perceptrons and multi-layer perceptrons (nonlinear)  (cid:0) Ranking issues   Supervised learning algorithms  n training data {xi, yi} where I=1, 2, 3, ..., n. The goal is to find a function that is as close as possible to the unknown function and determine the existing relationship between them. xi and yi. So there are two data sets, input and output. The output set is obtained by applying a function to the input set. This means that for each element in the input set there is a corresponding element in the output set. I'm trying to understand the relationship between pairs of numbers. The relationship between the two is functional.  hypothetical space Name your hypothesis function. time, h as an element of the range of possible functions H, It is usually called the hypothetical space. Choose a hypothetical function that you think resembles the true function behind your data. An example hypothetical space is: (cid:0) the space of all linear functions in d-dimensional space of all polynomial functions up to degree p.  find the function Let's get back to the main problem. I want to find a function h that can map an input to a corresponding output h:X→Y exactly, taking a value from a set of X and setting a Y. In supervised learning, given training data, the learning algorithm looks for the function h:X->Y where X is the input space and Y is the output space. The issues that arise here are: How can the quality of function be measured? How can it be accurately understood that time can be mapped x to the target? To answer this question, we need to introduce a new function called the loss function.  loss function     A loss function is actually a measure of accuracy. Your h function describes the relationship of X to Y?  The function time is applied to the training instancexi and it gives the output h(xi).  Let's write the function as y^i=h(xi)  However, since we are dealing with a supervised problem, we know that the actual output will be yi.  We fit h training data to measure how it works, so we need to find the difference between yi and y^i.  To measure the difference, we define another equation, the loss function. (yi, y^i).   The concept of model complexity  It is not always possible to visualize training data in higher dimensions. Therefore, it may not be known whether the regression problem is linear or nonlinear. Similarly, you may not know whether your classification problem is linearly or nonlinearly separable. So the big issues are: What is the appropriate complexity of the model to use to fit the given data? First, let's examine the impact of choosing different models in terms of complexity. (cid:0) Selecting more complexity than necessary will result in overfitting the data (we'll look at overfitting later in this course). (cid:0) Selecting a lower complexity than necessary will result in missing data. (cid:0) To generalize well, it is important to get the best possible fit. Generalizations are predictions on unseen data, i.e. her data that are not part of the training set.   Model complexity and Occam's razor    The famous problem-solving principle, Occam's Razor, is used as a heuristic guide in developing theoretical models.  This principle is often restated as:  All other things being equal, the simplest solution is the best.  It also addresses the question of which hypothesis to choose when there are multiple hypotheses with similar fit.  In other words, when multiple competing theories are otherwise equivalent, the principle recommends choosing the theory that introduces the fewest assumptions and the least complexity.   Minimize structural risk  So, based on Occam's razor and its simplified principles, we define another risk value called structural risk.  Structural risk minimization attempts to prevent overfitting by incorporating a penalty in model complexity that favors simple over complex features. So the general idea is to minimize both the previously introduced structural and empirical risks.  where C(ℎ) is the complexity of the hypothesis function ℎ and λ is the penalty parameter. A comprehensive definition of risk is therefore produced by the loss function above. y^y and yi for every training point in addition to the proposed model complexity as a penalty.   Classification criteria  The metric you choose to evaluate your machine learning model is very important. Your choice of metrics affects how performance is measured and compared.     The most common type of machine learning application is a classification problem. There are a myriad of metrics that can be used to assess predictions for this kind of problem.  confusion matrix  A confusion matrix is a summary of prediction results for a classification problem. The number of correct and incorrect predictions are summarized by count value and split into each class. A confusion matrix is a way of understanding the types of errors created by a model. A confusion matrix is also called a contingency table.  One of the reasons for using confusion matrices is that accuracy is not a reliable measure of a classifier's real-world performance. If the data set is imbalanced (that is, the different classes have very different numbers of observations), you will get misleading results.  Now that you understand the basics of confusion matrices, let's explore the concepts in more detail. Consider the following figure as a confusion matrix with only two classes.  We can represent the positive class as class 1 and the negative class as class 0. For the acronyms used in the table (i.e. TP), the second letter (e.g. letter P in TP) indicates what you predicted and the first letter (e.g. letter T in TP) indicates whether it is true or false. indicates whether In this case, we define   precision as:  But as I said before, accuracy may not be a useful metric for problems with imbalanced classes. Also, different classes may have different costs of raising an error. For example, an incorrect medical diagnosis can be more expensive than a false positive. Therefore, only reliable predictions are required. Therefore, you can define other metrics based on the confusion matrix.  ROC curve  Receiver operating characteristic (ROC) curves have long been used in signal detection theory to represent the trade-off between true positive and false positive rates in noisy channels. In recent years, the use of his ROC graph has increased in the machine learning community. ROC curves are particularly useful for domains with imbalanced class distributions and unequal classification error costs.  ROC curves are constructed by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. This should be done to represent the relative trade-off between benefits (true positives) and costs (false positives).  As you can see in the diagram below, different methods work well in different parts of the ROC space. Suppose we have two algorithms like Alg 1 and Alg 2 in the figure. Alg 2 is superior in that it can provide a high true positive rate while keeping the false positive rate low. On the other hand, if Alg 1 allows more false positive rates to occur, Alg 1 can also    provide a higher true positive rate.  Obviously it depends on the specifics of the problem. How much false positive rate can you tolerate? If you can increase the false positive rate, you can also increase the true positive rate.  A model that predicts by chance (random guessing) has her ROC curve like the diagonal dashed line in the figure above. It's not a discriminatory model. In general, the further away the curve is from the diagonal, the better the model can discriminate between positives and negatives.  As another example, consider the following diagram. Let's say you're designing a classifier for medical diagnostics. In this case, you probably don't care about false positives because missing a positive occurrence in disease detection is very costly.    However, you may also care about the false positive rate. A good example of this could be a criminal conviction. I don't want to waste anyone's life with a false positive decision!  There are useful statistics that can be calculated via the ROC curve, such as the area under the curve (AUC) and Youden Index. These show how well the model is predicting and what the best cut point for a particular model is (under certain circumstances). AUC is used to summarize his ROC curve using a single number. The higher the AUC value, the better the classifier performance. AUC for a random classifier is 0.5.  F-1 measures  Another useful metric is the combination of Precision and Recall. f1-measure is a metric that combines both Precision and Recall into one number. The f1-major is defined as:    Regression indicators      Split data for training and testing       We've talked a lot about how to evaluate a trained model. Let's take a closer look at model selection. The first question that might come to mind might be the limitation of using only a single training/test set. (cid:0) A single training set may be affected by several outlier instances (ie, noisy observations). (cid:0) Large test sets are required to get reliable estimates of model performance (accuracy). why? This is because the variance of such estimates is small. (cid:0) However, we know that the larger the training set size, the more accurately the model can be learned. (cid:0) Multiple training/test splits allow reuse of the same data for both training and evaluation on different splits. Three methods are commonly used to divide the data. (cid:0) Random subsampling (cid:0) Stratified sampling (cid:0) Cross-validation. Let's start with random subsampling.  subsampling  Instead of using a single split, more reliable estimates of model performance can be obtained by random subsampling. Random subsampling repeatedly splits the data into random training and test sets by a specified ratio.  As you can see, we train the model on each training set and estimate the accuracy using the corresponding test set. Finally, average the accuracies to get an average estimate.  stratified sampling  Stratified sampling is a probabilistic sampling technique that divides the overall data into various His subgroups or strata and randomly selects    final subjects proportionally from the various strata. When using a randomly selected training (or validation) set, the training and testing splits may have different class proportions.  Stratified sampling maintains class proportions in each random set. The diagram below shows how stratified sampling works. As you can see, we first separate (stratify) the instances by class label, then randomly select instances from each class.  cross-validation  Another method of data partitioning that is more common among researchers is cross-validation. This is a technique for evaluating a model by splitting the original sample into a training set for training the model and a test set for evaluating it.  The main idea is to split the training data into: k equally sized subsamples. Then we iteratively leave out one subsample for the test set and train on the remaining   subsamples. The following diagram illustrates this process.    Find the best  hyperparameters  In machine learning, hyperparameters are parameters whose values are set before the learning process begins.  This means that the model hyperparameter values cannot be estimated from the data. They are commonly used in processes to help estimate model parameters.  (cid:0) What are hyperparameters?  (cid:0) Why do we need hyperparameters?  (cid:0) How to find the best hyperparameters for a particular model?  Hyperparameters can often be set using heuristics. It is often tailored to a specific predictive modeling problem. To find the optimal hyperparameters, we need to split the training data into separate her    training and validation sets.   A validation set is used to evaluate a particular model and fine-tune the model's hyperparameters. Therefore, given a choice of hyperparameter values, we use the training set to train the model. But how do we set the values of the hyperparameters? That's what the validation set is for. It can be used to evaluate model performance for different combinations of hyperparameter values (using, for example, a grid search process) and maintain the best trained model. However, the test set allows us to compare different models in an unbiased way by making comparisons based on data that was not used in any part of the training/hyperparameter selection process. Here's another example: Remember Kmeans from Course 2? The number of clusters ((cid:0)(cid:0)), a hyperparameter. This is because this value is set before training begins. But how can we find the optimal hyperparameters? (cid:0) First, we need to determine the possible ranges of the hyperparameters. For example, a bounded interval [0,1] like (cid:0) Next, define a search grid within the specified range. For example, we may want to choose these values {0,10−3,10−2,10−1,1} as hyperparameters to evaluate the model using them. (cid:0) We then train a model using each hyperparameter value from the search grid and evaluate its performance on the validation set (separated from the training set). (cid:0) Finally, we compute the performance of the validation set for each hyperparameter value and choose the one with the best performance. Once the model is working with the best hyperparameters you have defined, you are ready to test it with different test data.  Internal cross-validation  All previously described techniques for model evaluation are applicable to training/validation set splits.  (cid:0) Random subsampling  (cid:0) Stratified subsampling  (cid:0) Cross-validation  I'm still evaluating how certain hyperparameters are performing on the validation set. Note that this step is internal to the learning process and is distinct from model evaluation on test data.  Let's explore how inner cross-validation works. Instead of using a single validation set, we can use cross-validation within the training set to select the best set of   hyperparameters. So basically it's exactly what you've seen with test/train partitioning. But here we split the data into training/validation sets. The following diagram illustrates this process.   Impact of imbalanced classes  One problem that can arise in machine learning is that the total number of data in one class (i.e. positive outcomes) is much higher than the total number of data in another class (i.e. negative outcomes). It's a small data set.  This problem is actually very common and can be detected in many different areas such as fraud detection, anomaly detection and medical diagnostics.    As we know, most machine learning algorithms work best when the number of instances in each class is approximately equal. Problems arise when the number of instances of one class far exceeds the number of instances of another class.  solution  There are two approaches to follow. First, you can perform some actions on the data itself. Alternatively, the algorithm can be improved to handle such phenomena. Let's take a look at these two approaches  Data level: (resampling)  (cid:0) Oversampling data from minority classes  (cid:0) Undersampling of data from the majority class.  Two obvious solutions based on data manipulation suggesting that more data points can be sampled from the minority class to cover the difference. can do.  Algorithm level:  (cid:0) Cost adjustments  (cid:0) Adjust the decision threshold.  From an algorithmic point of view, we can adjust the cost of the points we are observing from the majority class to dampen their influence. You can also manually define some thresholds to deal with imbalanced data.  imbalanced class problem  Now let's take a closer look at the possible problem of imbalanced classes.  (cid:0) Problem 1: Since the test data contains few samples of the minority class, even a stupid classifier that always classifies instances into the majority class will have very high accuracy.  (cid:0) This issue is addressed by using other metrics instead of accuracy.  (cid:0) Problem 2: When doing random subsampling, class proportions may not be preserved in individual partitions. In fact, he may not be able to sample even one instance from the minority class.  (cid:0) This problem can be solved using stratified sampling.  But always remember:  (cid:0) Any preprocessing (such as feature selection or feature extraction) on the entire data set should not use the information (such as labels) that you are trying to predict.  (cid:0) Do not use information during the training process that is not available during the training process.  (cid:0) Example: I was building a cancer prognostic model to predict whether a patient would survive 1 year from diagnosis? I mistakenly used the Cause of Death field as one of the features. Obviously, this information is not available at forecast time.  (cid:0) If you see how the model performs on a given test set and change the model many times, you may be overfitting on the test set.  