Topic5. Supervised learning  : data used to train the algorithm is already labeled with correct answers  : can test algorithm to make sure it is giving usable outputs  1.  Regression problems  - -  Linear Regression(linear) Logistic regression(linear)  2.  Classification problems  Support Vector Machines, SVM(both linear and nonlinear)  - -  Decision trees(nonlinear) -  Random Forest (nonlinear) -  Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  3.  Ranking problems  Supervised learning algorithm  1.  Hypothesis space  : a hypothesis function(h), as an element of a range of possible functions H : select a hypothesis function which is similar to the true function behind the data  2.  Finding a function 3.  Loss function  : measure of accuracy : used to compute the error between the actual result and what calculated  4.  Empirical risk  : calculate it by averaging the results of the loss function : the lower the empirical risk based on the training data, the closer the function represents the true relationship.  The concept of model complexity  : generalization (prediction on unseen data, which is not part of training set)  : overfitting, underfitting and good generalization  -  Occam`s razor  -  : problem-solving principle, used as a heuristic guide in the development of theoretical models Structural risk minmisation : to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions.  Classification metrics  -  Confusion matrix(contingency table)  : a summary of prediction results on a classification problem  TP  FN  FP  TN       1.  Accuracy = 2.  True Positive Rate, Recall, sensitivity  𝑇𝑃+𝑇𝑁 𝑇𝑃+𝐹𝑃+𝐹𝑁+𝑇𝑁  Recall =  𝑇𝑃 𝑇𝑃+𝐹𝑁 3.  False Positive Rate(FPR) = 4.  RPC curve(Receiver Operating Characteristics)  𝐹𝑃 𝑇𝑁+𝐹𝑃  : is especially useful for domains with imbalanced class distribution and unequal classification error costs.   Created by plotting TPR (benefits)against FPR(costs) at various threshold  settings    Area Under the Curve(AUC) and Youden Index   AUC is used to summarize the ROC curve using a single number. The higher  is better  5.  F-1 Measure(combination of Precision and Recall)    Precision: Positive Predictive Value, measures the proportion of true positive  Precision = TP / (TP + FP) 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑥 𝑟𝑒𝑐𝑎𝑙𝑙 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑟𝑒𝑐𝑎𝑙𝑙    F1 = 2 x  Regression metrics  1.  Measuring  regression performance  -  Mean Square Error: measure how close the predictions are to the true target values-  lower the better  -  Root Mean Square Error(RMSE) – lower the better -  Explained Variance(R-square, the coefficient of determination) : measured as the percentage of target variation that is explained by model : the square of the correlation between the target values and the predicted values : higher is better  𝑅2 =  𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒 𝑒𝑥𝑝𝑙𝑎𝑖𝑛𝑒𝑑 𝑏𝑦 𝑡ℎ𝑒 𝑚𝑜𝑑𝑒𝑙 𝑇𝑜𝑡𝑎𝑙 𝑣𝑎𝑟𝑖𝑎𝑛𝑐𝑒  Partitioning data for training and testing  1.  Random subsampling: repeatedly partitions the data into random training and test sets in a  specified ratio  2.  Stratified sampling: divide the entire data into different subgroups or strata, then randomly  select the final subjects proportionally from the different strata  3.  Cross-validation: partition training data into k equal sized sub-samples, then iteratively leave  on sub-sample out for the test set  Finding the best hyperparameters  1.  Hyperparameters  -  Not learned by the model during training and set by the user before training begins -  Control behavior of the model and affect its performance -  To search for the best hyperparameters, need to partition training data into separate training and validation sets        -  Validation  set: a sample of data used to provide an unbiased evaluation of a model fit  on the training dataset while tuning model hyperparameters  -  Use the validation set to evaluate the performance of model for different combinations  of hyperparameter values    Decide a possible range for hyperparameters   Define a search grid within the specific range   Train a model using each hyperparameter value and assess its performance  on validation set    Compute performance on validation set  2.  Internal cross-validation  Effect of imbalanced classes  1.  Solutions  -  -  at the data level(re-sampling) : oversampling from minority class and under sampling from majority class at the algorithmic level : adjusting costs and adjusting the decision threshold  2.  issues -  always high accuracy, even a dumb classifier: to be dealt with by using other evaluation metrics in place of accuracy  -  when doing random subsampling, it is possible that class proportion is not maintained in  an individual partition: can be solved using stratified sampling  