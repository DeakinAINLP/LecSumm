  Most practical machine learning algorithms use supervised learning   Supervised learning involved first training an algorithm with data where the label is already known and the algorithm known the relationship between the input (x) and output (y).    Unlike unsupervised learning which looks for similarities between features and develops an output supervised learning learns from direct mapping between features and output.    Supervised learning can be in many forms including regression problems (linier or logistic), classification problems (support vector machines, decision trees, random forest, neural networks), or ranking problems.    A loss function is used to determine the accuracy of a supervised learning algorithm.  Common types of loss functions are square loss, absolute loss, logistic loss, and hinge loss.    The empirical risk or error is a measure of error in the sample data being used for training. The true error would be the actual error for the entire population data set but that is unknown in the training stage so empirical error is used instead. Empirical risk is calculated by averaging the results of the loss function.    Occam’s Razor is a problem-solving principle that recommends using the simplest solution with the fewest assumptions and least complexity when multiple solutions are available to choose from.    There are many metrics you can use to measure the performance of classification models.  Common metrics include:  o  Confusion matrix which summarizes how well the model predicted outcomes by counting true positives, false positives, true negatives, and false negatives  o  Receiver Operating Characteristics (ROC) Curve depicts the trade off between the rate of true positives and false positives. It’s useful when you have imbalanced class distribution and unequal classification error costs. AUC and Youden Index can be calculated under the ROC Curve.  o  F1 measure which is a combination of precision and recall in a single number.   Regression measures how far the expected value is from the actual value. Regression  performance is measured with:  o  Mean Square Error which measures how close the predictions are to true target values. The lower the MSE, the better the model’s performance.  o  Explained variance / R square measures the percentage of target variation that can be explained by the model. The higher the R square, the better the model.   It’s necessary to split your data into training and testing sets when developing a model. Some types of test/train splits allow you to re-use the same data for testing and training. There are several methods of splitting data:  o  Random sub-sampling – instead of a single split you can use random sub-sampling to repeatedly partition data into random test and train sets at a specific ratio. You run the model through multiple random splits of the data and then average the results to judge model accuracy.  o  Stratified sampling divides the whole dataset into subgroups or strata then randomly selects data from each strata at a specified ratio. This is done to make sure the class proportions stay the same in each split.  o  Cross-validation it an iterative process that partitions the data into test and train and in each iteration uses a different partition for test so that eventually all data is used to test. Final accuracy is determined by average the results of all partitions.   In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. Hyperparameters are used to control and guide the performance of the model.    A validation set is a sample of the data which is used to provide an unbiased evaluation of model fit and also used to while training a model to tune hyperparameters. Using the validation set you can compare the performance of your model using different hyperparameters in order to determine the ones that provide the best performance.    A validation set can be selected using all the same methods you would use to split your data into train/test (i.e. random sampling, stratified sampling, and cross-validation).    Hyperparameters can be optimized using grid-search, random search or Bayesian   optimization. Imbalanced class distribution is a problem than can occur in machine learning when the total number of one class far outweighs the total number of another class. Machine learning algorithms work better when the class distribution is roughly equal.    There are two possible solutions to imbalanced class distribution. One is to resample the data to oversample the underrepresented class and under sample the overrepresented class to make them more even. Or you can adjust the costs and decision threshold in the majority class.  