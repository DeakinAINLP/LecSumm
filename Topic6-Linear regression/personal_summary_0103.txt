Supervised Learning  Supervised Learning refers to machine learning problems where the model is trained using known, already labelled data. In supervised learning, an algorithm is developed based on the relationship between the data and the correct labels, which can then be used to predict future results for non- labelled data.  Types of Supervised Learning includes Regression problems, Classification problems and Ranking problems.  Model Complexity  When deciding the appropriate supervised learning algorithm to apply to a problem, model complexity must be taken into consideration.  If the algorithm is overly complex in order to cater exactly to the training data, this runs the risk of overfitting the data. This means that the algorithm is well suited to the training data, but that it may be unlikely to apply well to new data.  If the algorithm is too simplistic on the other hand, this can lead to underfitting the data, meaning that the model is not closely enough related to the training data and is therefore unlikely to be useful with new data.  Classification Metrics  Confusion Matrices, also known as Contingency Tables, are a way of visualising the distribution of true and false prediction results for Classification problems. This helps to see not just the true positive results, but also where the model is resulting in incorrect classifications.  Receiver Operating Characteristics (ROC) Curve plots the true positive result against the false positive rate in order to depict the trade off between the two. This allows you to view and select the algorithm that either maximises true positive values or minimises false positive values, depending on the context of the particular problem.  Regression Metrics  Mean Square Error (MSE) calculates how close the predicted values are to the target values. The lower the MSE, the better the performance.  Explained Variance (R2) calculates the percentage of target variation which is explained by the model. The closer to 100% the result, the greater the performance.  Partitioning Techniques  In order to train a model, data must first be divided into training data and testing data. The model is trained using the training data and targets, and then tested for accuracy against the test data. Methods for partitioning data include:  -  Random subsampling – random division of data into training and test sets -  Stratified sampling – data is divided in equal proportions into training and testing sets – for example the same proportion of positive and negative results are present in both the training and testing sets in order to make sure the model is not given biased data  -  Cross validation – divide data into k equally sized subsamples, then run k iterations of  training, each time assigning a different subsample as the test dataset  