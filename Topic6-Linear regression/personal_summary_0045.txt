 OVERVIEW The topic of this topic is supervised learning: the process of creating a functional mapping of input feature vectors to output features/class; using a set of labelled “example” input/output pairs, that can predict the output of novel inputs.  READING I briefly read the course notes to make sure I was familiar with this topics content and I read the sklearn   documentation   to   re-familiarise   myself   with   the   model   selection   methods   i.e. LeaveOneGroupOut()  SUMMARY OF THE MAIN POINTS FOR TOPIC 5 LEARNING The process of supervised learning generally involves selecting a model; a functional form of the presumed input-output relationship (model), and undertaking an iterative process of training and testing to determine which model parameters produce the “best” prediction with novel input (and if multiple models are being tested, which model performs the best).  The process of finding the “best” parameters for a model involves optimising an objective/cost function i.e. minimising some loss function or maximising a reward function, to fit the labelled data.  The predictive performance of a model can be assessed with  various metrics such as accuracy, recall,   precision,   speed   and   many   more.   The   particular   metrics   used   will   be   guided   by   the constraints and objectives of the prediction and generally there will be a combination of several to balance competing interests of the constraints and objectives.  Since we can’t measure how well the model fits novel inputs, we use the performance of our test data as a proxy measure of likely future performance on as yet unseen input data. In order to avoid introducing bias into the model selection process, measuring performance should involve splitting the example data set into at least two groups; training and testing, so that we do not measure performance of the training with data the model has already seen. Ideally we use some form of randomisation of the data points used to split the data into train and test. The overall performance of the model is then taken as the average performance of each iteration that used a different split.  Some models have hyper parameters which are those that cannot be estimated from the data, they are often determined using heuristics. Instead of splitting the dataset into train and test, a 3 rd group should be used which is generally called validation. This allows an iterative process of training and “validating”   the   performance   of   the   model   with   different   hyper-parameters   and   still   having   a separate as yet unseen data set to “test” the performance of the model once the optimal hyper- parameters have been determined, which avoids biasing the model performance evaluation.  Significant differences in the size of classes can be problematic. We can deal with this by re- sampling i.e. over sampling from the “minority” class and under sampling from the “majority” class. Stratified sampling is very useful to ensure we maintain class proportions during training and testing. We can also introduce differences in weighting for different classes e.g. use non-uniform class specific dampening factors to reduce the influence/contribution of “majority” class members. Note that  accuracy is a bad measure for data with a significant imbalance in the number of each class. For example, poor performance predicting a class with few data points does not contribute much to the overall accuracy which can hide the fact that the classifier is a bad predictor for that class.  Example sampling methods: - Random sub sampling: repeatedly perform training and testing using random selections of training and data sub sets in a set proportion i.e. each round of train and test selects a different random selection for the train and test data. - stratified sub sampling: determine sub groups (strata) of the data set and then for each round of train and test, select the train and test sub sets using the proportions that are the same as exist in the strata, but of course randomly select within the strata. - cross validation: i.e. k-fold or leave one out testing. Divide the data into k sub sets, for each train and test iteration, leave out one of the k sub sets and use that as the test set and all the rest are for training. Then average the performance of each iteration, to get the final performance metric.    