Model training on supervised learning when the data is already labelled and comparing performance using different models is discussed. Since the data is already labelled it is easy to map the input variables to output variables and there is no need to find similarity and group data before training model. Depending on the output labels and the type of prediction that is expected supervised learning can be categorized into Classification, Regression,  Ranking  and  others.  Working  of  a  supervised  learning  algorithm  is  discussed  which  aims  at producing a function which is nothing but the relationship between the input and output. During the training process  all  the  possible  solutions  that  the  algorithm  explores  to  produce  the  best  fit  for  given  test  data  is represented by the hypothesis space. Loss function is used as a measure to test the quality of these functions, which helps in understanding how accurately that function describes the relationship between input and output. This  loss  function  is  calculated  by  finding  the difference  between  the true  predicted  supervised  algorithm output from the output function obtained by applying the hypothesis function. The results from loss function can be averaged to find the empirical risk and this is another factor in understanding accuracy. Empirical risk is to be kept minimum to obtain a better relationship. Model complexity is an important factor when determining the fit of the model. More complex model gives rise  to  over-fitting  problem  and  less  complex  models  result  in  under-fitting  problem.  Therefore  for  good generalization, that is how well a model makes prediction about unseen data, which is not included in training data, it is important to get the best fit of model. According to Occam’s razor theory when we have multiple functions  that  produce  similar  or  equal  fits,  then  the  function  with  least  number  of  assumptions  and  least complexity is to be selected. Another risk value called the structural risk this aims at minimising the risk to prevent over-fitting problem by providing better generalisation performance. Classification metrics helps provide meaningful insights about the classification model, that is how well a model  performs  and  these  measures  can  be  used  to  measure  and  compare  outputs  from  different  models. Confusion matrix is used to study the performance of a classification algorithm, where information about true positives, true negatives, false positives, and false negatives is available and these are used to calculate the classification  metrics  which  are  accuracy,  precision,  recall,  true  negative  rate,  F1  score  and  ROC  curve. Precision measures how well the model predicts the true positives among predicted positives and recall helps in identifying positive instances from the total number of positive instances. ROC curve plots the true positive rate against the true negative rate to provide a graphical representation of the classification model. AUC is a statistic obtained from ROC, and the higher the value of AUC the better is the performance of the classification algorithm. F1 score is a combination of precision and recall and is used when there is an imbalance in class that is when equal number of false positives and false negatives are identified carrying equal weights. The higher the score of F1 the better is the performance of model. Regression is another supervised learning approach which measures the changes associated with dependant variable with other explanatory variables. Regression metrics are used to measure regression performance. Mean  square  error,  variance,  root  mean  square  error  (RMSE),  mean  absolute  error  (MAE),  R-squared, explained variance, coefficient of Determination and Mean squared Logarithmic error (MSLE). Mean Square error helps measure how close the predicted values are to the actual values. RMSE, MAE are other metrics derived from this and the lower the values of these metrics the better is the model’s performance. R- square shows how well the dependent variables are successful in fitting the model. In simple words the better the R- squared value the better it explains all the variations in the model. Next step after identifying all the metrics to better understand the model, it is important to partition the data for training and testing. The larger the training data the reliability on the model will be better because the variance of large data set is going to be low. Random subsampling, stratified sampling and cross validation are used for the process of splitting dataset. Random sub-sampling of the dataset is done and average of all the  random  partitions  is  done  to  get  an  averaged  estimate.  In  Stratified  sampling  the  dataset  is  split  into subgroups called strata, and random selection of data from each strata is done to form final set of data. Cross validation is the popular method used by researchers where the training data is split into k equal sub samples and leave one which is used as test set. Once training and test datasets are separated, next step is finding best hyperparameters.  Hyperparameters  are  the  parameters  that  can  be  altered  to  affect  various  aspects  of  the algorithm to optimise the model performance and the value of hyperparameters is usually set by the user before  the learning begins. Validation set is used to test the performance of the model on the unknown dataset for predictions. Validation set is used to evaluate the performance of model for different parameters and select the best  value  for  hyperparameters.  The  previously  discussed  methods  of  random  subsampling,  stratified subsampling and cross-validation are used to split training and validation datasets. Internal cross-validation can be used to select the best hyperparameter. Grid search, Random search and Bayesian optimization are used for hyperparameter tuning process, using which the hyperparameter values that optimize model’s performance are obtained. One common problem that is noticed in the learning process arises when there is an imbalance between  the  classes  because  of  huge  difference  between  positive  outcomes  and  negative  outcomes.  This creates the issue of biased data model outcomes towards the majority class and likelihood that not even one data entry from minority class is used during random subsampling process. One approach to overcome this problem is to oversample the dataset with minimum number of entries or under sampling the dataset with most data entries to eliminate the difference. Another approach is to manually adjust the threshold between the two classes. Anomaly  detection  techniques  can  be  used  to  identify  the  minority  cases  and  separate  them  from majority  class.  Finally,  python  packages  are  explored  to  analyse  the  datasets  using  the  above-mentioned techniques.  