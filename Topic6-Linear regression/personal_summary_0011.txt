This topic focused on introducing the fundamentals of supervised learning. My first impressions when hearing ‘supervised learning’ was that the machine learning was watched over by someone to ensure it was correct. I now understand that it is ‘supervised’ in the sense that there is training and test data, and the supervision can be automated by comparing the predictions on the test data to the known correct outputs. There are many forms of supervised learning but for this topic Linear Regression was the main focus, which is the fitting a line where it has the least squares to the data. This produces a function that can be utilised to predict the value of an output. R-Squared is also used to explain the variance of data. Once a function has been found the quality of it can be assessed through various methods such as loss function and calculating empirical risk. Structural risk is also another method to help prevent over-fitting by introducing a penalty for choosing simpler functions over more complex ones. Combined with empirical risk is helps to improve the fitting of the model by not over-fitting or under-fitting. I also learned about classification metrics to help evaluate models, regression metrics as mentioned, how partition of data for assisting in selecting a model, and finding the best hyper parameters to help set the parameters around the learning of the model.  The pass activity was interesting and challenging. The first part with the linear regression was able to be implemented with similar code in the examples of the topics learning activities. The second part was much more difficult and I learned about sklearns LeaveOneGroupOut and read the documentation. Eventually I was able to implement some working code and it was interesting to see the difference between the performances of both sets of code. The quiz activity presented some interesting questions that prompted further research and consideration of the topics teachings. 