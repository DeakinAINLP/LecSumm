Topic 5 summary  This topic covered the fundamentals of supervised learning, the concept of model complexity, learnt about the complexity of a model, when a complex model should be used, how complex a model should be and what are the costs of using a complex model. Finally, python programming for supervised learning.  The majority of practical ML applications use supervised learning; supervised learning is defined by its use of labeled datasets to train algorithms that classify data or predict outcomes accurately. We can develop a mapping function from the input variable x to the output variable y, yi = h(xi). Thus, supervised learning is the task of estimating a function from labelled training data. Supervised learning can be separated into the following forms.  - regression problems  - linear regression (linear model)  - logistic regression (linear model)  - classification problems  - support vector machines (both linear and nonlinear)  - decision trees (nonlinear)  - neural networks: perception and multi-layer perception (nonlinear)  - Ranking problems  Model complexity is a fundamental problem in machine learning, it’s not always able to visualize the training data in high dimensions. So, the problem is what should be the right complexity of the model that we use to fit the given data.  -  - -  If we choose higher complexity than necessary, we would be over-fitting the data (you will review over-fitting later in this course). If we choose lower complexity than necessary, we would be under-fitting the data. It is important to get the best possible fit for good generalization  Model complexity use a heuristic guide Occam’s razor in the development of theoretical models, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity. And based on Occam’s razor and its simplistic principle, the risk value called structural risk can be defined. Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones.  It is important to choose the right metrics to evaluate the machine learning, the most common type of ml application are classification problems. A confusion matrix is a summary of predication results on that problem, the number of correct and incorrect predication are summarized with count values and divided down by each class. Regression measures how far the expected value is from the actual value.  There are three methods for splitting data for training and testing.   -  Random subsampling - Stratified sampling -  Cross validation  In ML, a hyperparameter is a parameter whose value is set before the learning process begins. They are used in process to help estimate model parameters. We need to partition training data into separate training and validation sets, we can still apply three data splitting shown above.  We also need to consider the problem of imbalanced classes where the total number of one class of data is far less than the total number of another class of data. There are two approaches to solve it, one is at the data level by re-sampling, another is at the algorithmic level by adjusting the costs or the decision threshold.  