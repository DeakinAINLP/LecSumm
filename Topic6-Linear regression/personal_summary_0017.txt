Forms of Supervised Learning Supervised learning is one of the most commonly used types of machine learning in pracAcal applicaAons. The basic idea of supervised learning is to use labeled data, where each data point is associated with a known output or label, to train a machine learning algorithm to predict the output for new, unseen data.  In supervised learning, the goal is to learn a mapping funcAon from input features to output labels. The input features can take diﬀerent forms, such as numerical values, text, images, or other types of data. The output labels can also vary depending on the task, such as conAnuous values for regression problems or discrete classes for classiﬁcaAon problems.  Supervised Learning Overview    Dataset composed of input-output pairs    Problem types: regression, classiﬁcaAon, ranking    Popular algorithms: linear regression, neural networks, logisAc regression, SVMs,  decision trees, random forests   Supervised Learning Algorithm    Goal: learn a mapping funcAon from input to output    Hypothesis space: range of possible funcAons    Empirical risk: factor indicaAng the accuracy of the funcAon on training data    OpAmizaAon algorithm: method to ﬁnd the funcAon that minimizes the empirical risk  Loss funcAon: measure of the accuracy of the hypothesis funcAon  (e.g., gradient descent)  Finding a Func:on Given a set of training data with inputs and corresponding outputs, the goal is to ﬁnd a funcAon that can accurately map inputs to outputs. This funcAon is selected from a range of possible funcAons, called the hypothesis space.  Hypothesis Space A hypothesis funcAon, h(x), is an element of the range of possible funcAons H, which is usually called the hypothesis space. Examples of hypothesis spaces include:    The space of all linear funcAons in d dimensions    The space of all polynomial funcAons up to degree k  Loss Func:on To measure the accuracy of the funcAon, a loss funcAon is used. The loss funcAon is a measure of how well the hypothesis funcAon h(x) describes the relaAonship between input x and output y. A funcAon f(x) is applied to a training instance x and it gives the output ŷ. However, since we are dealing with a supervised problem we know that the true output is y.  Let’s denote the funcAon as f. To measure how well funcAon h ﬁts the training data, we need to ﬁnd the diﬀerence between y and ŷ. To measure the diﬀerence we deﬁne a diﬀerent equaAon, a loss funcAon L:    Examples of loss funcAons include:    Square loss: useful for regression   Absolute loss: useful for regression    0-1 loss: useful for classiﬁcaAon   Hinge loss  LogisAc loss  Empirical Risk Similar to the loss funcAon, we can deﬁne a factor called empirical risk. Among all funcAons in the hypothesis space, H, we select the funcAon h(x), which minimizes the empirical risk. You can calculate the empirical risk by averaging the results of the loss funcAon. The lower the empirical risk based on the training data, the closer the funcAon represents the true relaAonship between the pair of values x and y.  Op:miza:on Algorithm To ﬁnd the funcAon that minimizes the empirical risk, an opAmizaAon algorithm is used. The most common opAmizaAon algorithm used in supervised learning is gradient descent.    Gradient descent is an iteraAve algorithm that updates the weights of the hypothesis  funcAon h(x) in the direcAon of the negaAve gradient of the empirical risk.    Other opAmizaAon algorithms include stochasAc gradient descent, mini-batch  gradient descent, and Newton's method.  Model Complexity When selecAng the complexity of a machine learning model, the main quesAon is what should be the right complexity that we use to ﬁt the given data? Overﬁ‘ng occurs when the model is too complex, and underﬁ‘ng occurs when the model is too simple.  Eﬀects of Model Complexity Choosing a higher complexity than necessary leads to over-ﬁ‘ng the data, while choosing a lower complexity than necessary leads to under-ﬁ‘ng the data.  Generaliza:on The goal of model selecAon is to achieve the best possible ﬁt for good generalizaAon. GeneralizaAon refers to the ability of the model to predict on unseen data, which is not part of the training set.  Occam's Razor Occam's razor is oben paraphrased as "All other things being equal, the simplest soluAon is the best." The principle suggests that simpler explanaAons are generally more reliable than more complex ones.  This useful when trying to determine models – BUT be careful, someAmes the more complex answer is the best soluAon not the apparently simple one!  Model Complexity and Occam's Razor In the context of machine learning, Occam's razor can be used to guide the selecAon of the appropriate level of model complexity. The goal is to choose a model with the right level of complexity that can generalize well on new, unseen data. When mulAple compeAng models have similar performance, the principle of Occam's razor suggests selecAng the simplest model. This is because simpler models are less likely to over- ﬁt the training data and are more likely to generalize well on new data.  Structural Risk Minimiza:on Structural risk minimizaAon is a technique used to prevent over-ﬁ‘ng in machine learning models. It involves incorporaAng a penalty on the model complexity that favours simpler funcAons over more complex ones. The goal is to minimize both structural risk and empirical risk to achieve beger generalizaAon on new, unseen data. The formula for risk includes a loss funcAon for empirical risk, a loss funcAon for structural risk, and a penalty parameter.  Classiﬁca:on Metrics Choosing appropriate evaluaAon metrics is crucial for measuring the performance of machine learning models, especially for classiﬁcaAon problems. A confusion matrix is a common tool for summarizing the predicAon results on a classiﬁcaAon problem. It shows the number of correct and incorrect predicAons for each class and allows understanding the types of errors made by the model.  Confusion Matrix The confusion matrix is parAcularly useful when the dataset is unbalanced, meaning the numbers of observaAons in diﬀerent classes vary greatly. In such cases, accuracy is not a reliable metric for evaluaAng the performance of the classiﬁer. The diagonal values in the confusion matrix represent the elements where the predicted classes were equal to the expected classes, while the oﬀ-diagonal values represent the elements where the classiﬁer made incorrect predicAons. The higher the proporAon of values on the diagonal of the matrix in relaAon to values oﬀ the diagonal, the beger the classiﬁer.  ROC Curve The ROC curve is a graphical representaAon of the trade-oﬀ between the true posiAve rate (TPR) and false posiAve rate (FPR) at various threshold se‘ngs. It helps in understanding the relaAve trade-oﬀs between the beneﬁts (true posiAves) and costs (false posiAves). Diﬀerent methods can work beger in diﬀerent parts of the ROC space, depending on the problem speciﬁcaAon. A model that predicts at chance will have an ROC curve similar to the diagonal dashed line.  Applica:on Scenarios The ROC curve can be useful in diﬀerent scenarios, depending on the cost of false posiAves. For instance, in medical diagnosis, false posiAves may not be as criAcal as missing posiAve occurrences. In contrast, in a criminal convicAon, false posiAves can be costly.  Metrics and Sta:s:cs The ROC curve can also provide useful staAsAcs, such as the Area Under the Curve (AUC) and Youden Index, which indicate the model's predicAve performance and opAmal cut point under speciﬁc circumstances. The AUC summarizes the ROC curve using a single number, with higher values indicaAng beger classiﬁer performance. The F-1 measure is another useful metric that combines both Precision and Recall in a single number.  Regression Metrics    Regression measures how far the expected value is from the actual value.   Mean Square Error (MSE) measures the closeness of predicAons to the true target  values.    Root Mean Square Error (RMSE) is computed from MSE and is another popular  measure.    Mean Absolute Error (MAE) is robust to outliers and measures the closeness of  predicAons to the true target values.    R-square or explained variance is measured as the percentage of target variaAon that  is explained by the model.    R-square is always between 0 and 1.   The higher the R-square of a model, the beger its performance.   R-square is the percentage of the dependent variable variaAon that a linear model  explains.    R-square measures the degree to which the variance in the dependent variable is  predicted by the independent variable.  Par::oning data for training and tes:ng LimitaAons of using a single training/tesAng set:    Aﬀected by noisy observaAons   Need a large test set for reliable esAmate of model performance   Larger training set leads to more accurate model learning  Methods for spli‘ng data:  1.  Random subsampling   Repeatedly parAAon data into random training and test sets in a speciﬁed raAo   Train model with each training set and esAmate accuracy using corresponding test  set    Average accuracies to get an averaged esAmate 2.  StraAﬁed sampling   Probability sampling technique that divides data into subgroups or strata, then  randomly selects ﬁnal subjects proporAonally from each stratum    Ensures class proporAons are maintained in each random set 3.  Cross-validaAon   Technique to evaluate models by parAAoning original sample into a training set to  train the model and a test set to evaluate it    ParAAon training data into k equal-sized sub-samples and iteraAvely leave one sub-  sample out for the test set and train on the rest of the sub-samples    Average the obtained accuracies to get the ﬁnal accuracy   Makes eﬃcient use of available data for tesAng   Special case: leave-one-out cross-validaAon scheme when k equals the number of  instances  Beneﬁts of cross-validaAon:    More reliable esAmate of model performance   Eﬃcient use of available data for tesAng  Finding the best hyperparameters Hyperparameters are parameters that are set before the learning process and cannot be esAmated from data. They are oben used to esAmate model parameters. To ﬁnd the best hyperparameters for a speciﬁc model  Hyperparameter Op:miza:on    Training data parAAoned into separate training and validaAon sets   ValidaAon set used to evaluate hyperparameter combinaAons   Methods: grid-search, random search, Bayesian opAmizaAon   Handling Imbalanced Classes    Issues: high accuracy for dumb classiﬁers, class proporAon not maintained in parAAons    SoluAons: resampling (oversampling or under-sampling), adjusAng costs or decision  thresholds, using other evaluaAon metrics, straAﬁed sampling Reﬂect on the knowledge that you have gained by reading contents of this topic with respect to machine learning. This topic's module on supervised learning has provided me with a more comprehensive and nuanced understanding of the fundamental components of this machine learning approach. The module highlighted for me that our approach to Supervised Learning can have pracAcal impacts in its applicaAons in various ﬁelds, including healthcare, ﬁnance, and markeAng. For instance, healthcare providers can use supervised learning algorithms to predict paAent outcomes and develop personalized treatment plans. But as it relates to human and health, it is crucial we get it right and the wrong outcomes could be disastrous.  In parAcular, I have gained a deeper appreciaAon of the criAcal role of ﬁnding the appropriate level of model complexity to prevent overﬁ‘ng or underﬁ‘ng, which involves considering the hypothesis space, loss funcAon, empirical risk, and opAmizaAon algorithm.  Finally, I’ve gained an appreciaAon of the number of evaluaAon metrics that can be used , including the confusion matrix and ROC curve, and their use in assessing the performance of machine learning models, especially for classiﬁcaAon problems. The module's discussion of hyperparameter opAmizaAon and handling imbalanced classes provided useful insights into techniques for improving model performance.  Lastly, a personal observaAon as we progress in the unit is that, while ML opens up a range of exciAng possibiliAes with what we can get from data, the number of variable in ML’s applicaAon makes it a challenging ﬁeld to tweak all the se‘ngs to “get it right”.  A<empt the quiz given in topicly content and add screenshot of your score (Ñ85% is considered compleCon of the task) in this report.  