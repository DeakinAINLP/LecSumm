Supervised Learning  In supervised learning, the data used to train the algorithm is already labelled with correct answers, where an algorithm is created based on the known relationship between the input and output.  Supervised learning can appear in many forms:    Regression problems  o  Linear Regression o  Logistic Regression    Classification problems  o  Support Vector Machines o  Decision Trees o  Random Forest o  Neural Networks: Perceptron and Multi-layer Perceptron    Ranking problems  Model Complexity  It’s best to have a small amount of data that a model has never seen in order to assess future performance. Training (80%), test (10%) and validation (10%) data is split into multiple tiers.  The validation set is used to evaluate a given model without bias and also to fine-tune the model hyper-parameters (discussed later).  Over-fitting  More complex models better fit training data; however they may predict poorly when testing new data that has never been seen before (errors creep in). Selecting a higher complexity model than is necessary, could result in over-fitting the data. Over-fitting is a modelling error that introduces bias to the model because it is too closely related to the data set.  Under-fitting  Alternatively, if a model of lower complexity is selected, there is a risk of under-fitting the data.  Under-fitting is a scenario where a data model is unable to capture the relationship between the input and output variables accurately.  Occam’s Razor  No more assumptions should be made than are absolutely necessary. When multiple competing theories are equal in other respects, this principle recommends selecting a theory that introduces the fewest assumptions and has the least complexity – all things being equal, the simplest solution is often the best.  Structural Risk  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones.    Classification Metrics  The choice of model evaluation metrics can influence how performance is measured and compared.  Confusion Matrix  A confusion matrix is a summary of the performance of prediction results of a classification model or algorithm for a machine learning processes. Confusion matrices help with predictive analysis and can be an effective tool for evaluating what functions a machine learning system performs correctly and incorrectly.  As accuracy is not a reliable metric for the real performance of a classifier, confusion matrices are a better way to understand the types of errors made by a model. One of the reasons for using a confusion matrix is that, if the data set is unbalanced (i.e. when observations numbers in different classes vary greatly), it will yield misleading results.  ROC curve  An ROC curve depicts relative trade-offs between benefits (true positives) and costs (false positives). It essentially separates the ‘signal’ from the ‘noise’.  F1 Measure  The F1 measure is the harmonic mean between Precision & Recall into one number: F1. This score is very useful when you are dealing with imbalanced classes problems (problems when one class can dominate the dataset).  Regression  Regression measures how far the expected value is from the actual value.  Mean Square Error  Difference between estimated value and actual value. MSE measures the average squared difference between the estimated values and the actual value: the lower the MSE, the more closely a model is able to predict the actual observations. If you want to train a model which focuses on reducing large outlier errors then MSE is a better choice. MSE essentially gives us an idea of how well a model will perform on data it hasn’t previously seen.  Explained Variance  R-squared is a measure that can range from 0% to 100% where: 0% indicates that the response variable cannot be explained by the predictor variable at all. On the other hand, 100% indicates that the response variable can be perfectly explained without error by the predictor variables. R-squared can also be negative because the model can actually be worse.    Data Partitioning – Training versus Testing  Sub Sampling  A more reliable estimate of model performance can be obtained by random sub-sampling, where data is partitioned into random training and test sets in a specified ratio.  Stratified Sampling  Is a technique where the entire data into different subgroups (strata), then randomly select the final subjects proportionally from each different subgroup. This will ensure that class proportions are maintained in each random set.  Cross Validation  Is a technique used to evaluate models by partitioning the original sample set into a training and test set. The training set is used to train the model and a test set is used to evaluate it.  Hyper-parameters  These hyper-parameter values are set before the learning process begins and are used to control the learning process. They have a significant effect on the performance of machine learning models. They are tuneable and can directly affect how well a model trains.  Most machine learning algorithms come with default hyper-parameter values. These default values do not always perform well on different types of learning projects. They must be optimised first in order to get the right combination that will provide the best performance. Optimisation can take place via Grid searching, Random searching and Baysian optimisation.  A validation data set can be used to evaluate the performance of a model for different combinations of hyper-parameter values in an unbiased way, as they weren’t part of the training/hyper-parameter process.  Internal cross Validation  Instead of using a single validation set, we can use cross-validation within a training set to select the best set of hyper-parameters.  Imbalanced Class Distribution  Algorithms work best when the number of instances of each classes are roughly equal. When the number of instances of one class greatly exceeds the other (over sampling from minority class/under sampling from majority class), problems can arise.  Two approaches can be used to help overcome this issue: manipulation of the data itself or improve the algorithm.  Data can be pre-processed to reduce over and under sampling so that they have an equal effect on the algorithm. Alternatively, algorithmic cost can be adjusted from the majority class or thresholds can be manually defined or adjusted to dampen effects.       