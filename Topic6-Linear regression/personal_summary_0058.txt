 This topic we learnt about supervised learning algorithms. The one that was focused on was linear regression. With supervised learning algorithms, there is a risk of overfitting models. This means that the model has high complexity and tries to hit every data point within the training data. This can result in large errors when using test data as the model has no flexibility in being closer to those test data points. This means that going for the most complex model is not necessarily the best decision.  We then learnt about different methods of partitioning data for testing and training. The two that were used in the pass activity were random selection and a cross validation technique called leave-one-out. In random selection, a portion of data is randomly selected to be the training data and a smaller portion is randomly selected to be the test data. This can be run multiple times to get a large variety of different data sets to use as training and test data to ensure the model is as accurate as possible.  The other method is leave-on-out. In this method, the data is broken up into chunks. One chunk is used as the test data and the rest are used as training data. This process is run as many times as there are chunks so that each one has been used as test data. The average of the performance scores then gives a more realistic accuracy for the model selected.  Reflection  This topic was really interesting. I had never done any partitioning of test data before in python and I hadnâ€™t done linear regression for a long time. I can definitely see how using these methods of partitioning and evaluating a model's performance can make analysing large data sets a lot easier. These programmatic ways of selecting training and testing data sets mean that testing data sets do not need to be made up, eliminating any accidental bias if the made up test data set is skewed in any way. This is especially helpful when analysing large data sets where there is not necessarily a lot of oversight over the specific values of each cell.   