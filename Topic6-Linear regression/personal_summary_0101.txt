Learning Summary Forms  of  Supervised  Learning:  Majority  of  machine  learning  applications  take  use  of supervised  learning  approach.  Where  the  trained  data  is  labeled  with  correct  answers.  This algorithm is  where  one  can  make an  algorithm  on  the  basis of  the  relation  between  input  and output. By which one develops a mapping function from input variable x to output variable y. It is a task of estimating a function on the basis labeled training data.  Supervised learning  can be in many forms:    Linear Regression Model   Logistic Regression Model  Classification Problem:   Random Forest   Support Vector Machine   Neural Networks   Decision Trees  Ranking Problems  Understanding the  Supervised  Learning  Algorithm:  There  are  two  sets  of  data,  input  and output. Where the output is obtained by the application of a function to the input set, meaning that for each element within the input there is a corresponding element within the output set. Where we are figuring out the relation among the pair of numbers, and the relation among the two is the function.  Hypothesis Space: A function named h, is an element of a range of possible functions as H. This is known as hypothesis space. A selection of a hypothesis function will take where we think it has a similar to true function behind its data. An instance of this is: space of all linear functions in  d- dimension space of all polynomial functions up to the degree of  p.  Loss  Function:  This  function  measures  accuracy,  where  how  accurately  has  the  h  function described the relation among X and target Y. So a function h is applied to an instance for training  xi that gives the output = h(xi) Since this is a supervised learning problem we are aware that the true output for this is yi. So for figuring out how well h fits into the training data the difference  between yi and variable of  .  And for measuring this a different  equation is defined  which is  a  loss function L(yi,  ).  Empirical Risk: The  calculation of empirical  risk takes place  by averaging out  the result of  the loss function. The lower it is on the basis of training data, the closer it represent the relation among the pairs of value xi and yi.  Occam’s razor: Is a famous problem-solving principle where  it is used as a heuristic guide  for the  development  of  theoretical  models.  Where  in  other  words,  it  can  be  said  that  multiple competing  theories  equal  in  other  respects,  it  recommends  the  selection  of  a  theory  which introduces the least number of assumptions comprising of the least complexity. Structural Risk Minimization: A risk value that prevents, over-fitting by incorporating a penalty on the  complexity model  preferring  simpler function  over more  complex  ones, where  the  main idea is minimizing both Structural and Empirical risk.  Classification Metrics: There are several metrics that are used for comparing performance, they are as follows:   Confusion Matrix: It’s a summary for prediction results on a particular classification problem. The number of correct and incorrect predictions are summarized with values of count that are divided in on the basis of each particular class. These matrices are a way of understanding the types of errors that are made by a model, this matrix is also known as contingency tables.   ROC Curve:  This  is  a  curve  that  has  been  used  mostly  in  signal  detection  theory  for  the depiction of trade-off among true positive and false positive over noisy channels. This curve turns out to be useful for domains that comprise of imbalanced class distribution and unequal classification error costs. The curve can be created by plotting the True Positive Rate against False Positive Rate with various threshold settings, which helps in depicting relative trade-offs among benefits and costs.    F-1  Measure:  This  is  a  metric  which  combines  both  Recall  and  Precision  within  a  single  number. This is defined as:  Regression Metrics: There are various ways by which we can measure regression performance, some of them are as follows:    Mean Square Error: This tells that how close the predictions were true to its target values.  This is defined as has better performance.  the low the MSE is for a particular model, it    Explained Variance (R2): This is a measure that has a lot of names like Explained Variance, R-Square, the coefficient of determination where R-Square gets measured by percentage of target  variation  which is  explained  by  the model.  For  Linear Regression  Model  with  bias term, R-Square is the square of correlation among target values and prediction target values. The higher the R-Square value is, it signifies better performance. R-Square is measured on follows: This the  percentage. computed basis as of is Methods for Splitting data: There are 3 methods that we work with for splitting data and they are as follows:    Stratified Sampling: This is a probability sampling technique where the entire data is divided into various subgroups or strata, and  where it randomly selects final subjects  proportionally from different strata. This method ensures that the proportion of class is maintained in each random set.     Random  Subsampling:  A  method  where  it  repeatedly  partitions  data  into  random  training and test sets into a specific ratio. Let’s test_size is set to 0.3, so in that case train set will be 70% and test set will 30%.    Cross  Validation:  A  method  where  we  evaluate  models  on  the  basis  of  partitioning  the original  sample  into  a  training  set  for  training  the  model  and  a  testing  set  for  evaluation purposes.  Hyper parameter: A parameter where its value is set before the learning process starts, meaning that  the  value  of  a  hyper  parameter  within  a  model  cannot  be  estimated  from  its  data.  This parameter is often used within processes for helping estimate the model parameters.  