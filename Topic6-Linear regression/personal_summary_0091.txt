 This topic’s content focusses on the fundamentals of supervised learning. Supervised learning is a machine learning technique in which labelled data is used to train a model to predict outcomes and learn patterns. This data consists of input and corresponding target values, from which the model can learn a mapping function to take new, unseen data and predict an output label.  Supervised learning can be divided into different categories. In regression problems, the output is a continuous value rather than one that can be categorised. In contrast to this, the goal of classification problems is to assign input data to predefined values or categories. Ranking problems seek to order items depending on their suitability or importance, and are often found in recommendation systems or search engines. A loss function determines how well a model is performing based on the difference between the output of the training data and the model’s predictions.  Simpler machine learning models that rely on a smaller number of features to make predictions are less prone to overfitting and are more simple to understand. Their predictions may be restricted though due to their limited input. In comparison, a more complex model with a greater number of features and more flexibility may initially appear to have a better fit. However, this can lead to overfitting, in which the model learns to fit too closely to the training data and does not perform well to new data. It is important to strike a balance between simple and complex models through thorough testing and consideration of computational efficiency.  Occam’s razor is a philosophical principle that can help guide this balance. It states that when models have a similar rate of accuracy in their predictions, the simpler option with fewer parameters should be prioritised. This concept can be extended through structural risk minimisation, in which the risk of overfitting is minimised by preferencing simpler models while retaining predictive accuracy.  Classification and regression metrics exist to evaluate the two types of machine learning models. To quantify the accuracy of classification tasks, the ROC curve, F-1 measure, or the true or false positive rate can be used. For regression problems, the mean squared error or R-squared metrics can be used to assess the accuracy of the model’s predictions and its fit to the data.  Partitioning methods divide data into subsets for training and testing purposes. Cross-validation divides data into subsets, each of which is used in both the training and testing process.  Stratified sampling is best suited for imbalanced data sets, as it ensures an even distribution of categories when partitioning the dataset. Subsampling also works well with imbalanced datasets. It selects a random subset of the majority class to match the number of samples in the minority class so that they can be compared fairly. However this could result in a loss of potentially important information.  