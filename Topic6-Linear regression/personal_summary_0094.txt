Topic 5 Notes – Fundamentals of supervised learning    Supervised learning is the main practical machine learning method. An algorithm is made  based on the known relationship between inputs and outputs. The main forms of supervised learning come in Regression problems (Linear and Logistic for linear models), Classification problems and ranking problems.    Model complexity is quite important when used to fit given data as this will change the  outcome quite a bit.    Occam’s Razor suggests that when multiple theories are equal in other respects, it is best to choose the theory that is most simple or in other words has the fewest assumptions and less complexity.    Structural risk minimization is a way of preventing over-fitting by introducing a penalty on    the complexity of a model which chooses simple functions over complex ones. Classification metrics are introduced in 5.7 which includes the confusion matrix which shows the summarized results of predictions based on a classification problem. This tells us the types of errors made by a model. The ROC curve and F-1 measure are other types of classification metrics that do different things.    5.8 introduces regression metrics, which are simply ways to measure how close the  prediction values are to the true target values. The mean square error (MSE) is a popular way of measuring this. Explained variance is another way of doing this and is also quite popular, it is also known as R-square and the coefficient of determination.    5.9 teaches us that when partitioning data for training and testing, we should use large test sets and multiple of them so we can re-use the data for training and evaluation. There are 3 main methods to work with when splitting data: random subsampling, stratified sampling, and cross validation. Random subsampling simply partitions data many times into random training and test sets in a specific ratio. Stratified sampling is a technique that divides our data into subgroups or strata based on characteristics shared. Cross-validation partitions the data into a training set to train the model and a test set to evaluate which is a very popular method amongst researchers.    A hyperparameter is simply a parameter that’s value is set before the learning process starts,    it cannot be estimated but rather we use heuristics to set them. We can find the best hyperparameter set by either using Grid-search, random search or Bayesian optimization. Imbalanced classes is a very common issue that can occur in machine learning where one class of data can have a much lower or higher value than another class of data. The solution to this problem is to either over-sample or under-sample depending on the issue, or we can adjust the values and the decision threshold at an algorithmic level. This issue should always be rectified as it can cause issues and bad results.    Our main packages for Python in this module are NumPy, pandas, matplotlib and seaborn   