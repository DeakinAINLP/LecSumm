 Introduction to Topic 5  ● differentiate supervised learning from unsupervised learning  ● estimate the performance of different supervised learning models ● implement model selection and compute relevant evaluation measures  5.2 Forms of Supervised Learning  Forms of Supervised Learning  Supervised learning can appear in many forms:  ■ Regression problems  ○ Linear Regression (linear model) ○ Logistic Regression (linear model)  ■ Classification problems  ○ Support Vector Machines (both linear and nonlinear) ○ Decision Trees (nonlinear) ○ Random Forest (nonlinear) ○ Neural Networks: Perceptron and Multi-layer Perceptron (nonlinear)  ■ Ranking problems  Let’s see some examples.  Example 1  The following figure illustrates a regression problem about the sale of Yogurt with seasonal temperature. Let’s estimating the relationships among the feature variables (e.g. the sale of frozen yogurt and its temperature).  Figure. Yogurt sale with season temperature.  Example 2  The following is another example of regression, where we look to find relationships among feature variables. The figure illustrates sample data for GoPro stock price against date. Imagine the amount of money you could earn by intelligently predicting prices in the stock market!  Figure. goPro stock price with time.  Example 3  The following familiar figure illustrates the classic classification problem for classifying two types of data. As you can see, sometimes we can successfully find a linear boundary and sometimes we have to search for a more complex boundary.  Figure. Linear boundary vs nonlinear boundary for classification.  5.3 A supervised learning algorithm  5.4 The concept of model complexity  The concept of model complexity  How complex should a machine learning model be? What are the costs when a complex model is used? When is it necessary to use a complex model?  Consider the following figure as an example of a classification problem. Which boundary line you think is more appropriate for this problem?  Figure. Two different boundary lines for a classification problem.  We may not always be able to visualise the training data in high dimensions. So, we may not know whether the regression problem is linear or non-linear. Similarly, we may not know if the classification problem is linearly separable or non-linearly separable. So, the big question is:  what should be the right complexity of the model that we use to fit the given data?  Let’s first examine the effects of selecting different models in terms of complexity:  ■ If we choose higher complexity than necessary, we would be over-fitting the data (you  will review over-fitting later in this course).  ■ If we choose lower complexity than necessary, we would be under-fitting the data. ■ It is important to get the best possible fit for good generalisation.  What is generalisation? It is prediction on unseen data, that is, the data, which is not part of our training set.  We’ll discuss these concepts in detail later in this course.  5.5 Model complexity and Occam's razor  Model complexity and Occam's razor Occam’s Razor, a famous problem-solving principle, is used as a heuristic guide in the development of theoretical models.  This principle often paraphrased as:  All other things being equal, the simplest solution is the best.  It also addresses the problem of Which hypothesis to choose if there are multiple hypothesis with similar fit?  In other words, when multiple competing theories are equal in other respects, the principle recommends selecting the theory that introduces the fewest assumptions and has the least complexity.  5.6 Structural risk minimisation  Structural risk minimisation So based on Occam’s razor and its simplistic principle, we define another risk value which is called Structural Risk.  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity that prefers simpler functions over more complex ones. So the general idea is to minimise both Structural Risk and Empirical Risk which we introduced before:  5.7 Classification metrics  Classification metrics The metrics that you choose to evaluate your machine learning model are very important. The choice of evaluation metrics influences how performance is measured and compared. The most common type of machine learning applications are classification problems. There are myriad metrics that can be used to evaluate predictions for these types of problems. Confusion Matrix A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and divided down by each class. Confusion matrices are a way to understand the types of errors made by a model. Confusion matrices are also called contingency tables. One of the reasons for using a confusion matrix is that, accuracy is not a reliable metric for the real performance of a classifier. If the data set is unbalanced (i.e. when the numbers of observations in different classes vary greatly), it will yield misleading results. For example, if there were 90 apples and only 10 oranges in the data set, a particular classifier might classify all the observations as apples. But is this wise? The following figure illustrates a sample confusion matrix for a classification problem with 11 classes. The diagonal values represents the elements where the predicted classes were equal to the expected classes, and the off-diagonal values represent the elements where the classifier got the prediction wrong. The higher the proportion of values on the diagonal of the matrix in relation to values off of the diagonal, the better the classifier is (why?).  Figure. Confusion matrix for a classification problem. Now that we understand the fundamentals around confusion matrices, let’s explore more detailed concepts. Consider the following figure as a confusion matrix for only two classes.  Figure. Confusion matrix for two classes. You could represent the positive class as class 1 and the negative class as class 0. For the acronyms used in the table (i.e. TP), the second letter (e.g. letter P in TP) says what we predicted and the first letter (e.g. letter T in TP) says whether it was true or false. In this case we define the accuracy as:  ROC Curve  Receiver Operating Characteristics (ROC) curve has long been used in signal detection theory to depict the trade-off between the true positive rate and false positive rate over noisy channels. Recent years have seen an increase in the use of ROC graphs in the machine learning community. ROC curve is especially useful for domains with imbalanced class distribution and unequal classification error costs.  The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. This has to be done to depict relative trade-offs between benefits (true positives) and costs (false positives).  As you can see in the figure below, different methods can work better in different parts of ROC space. Lets say there are two algorithms like Alg 1and Alg 2 in the figure. The Alg 2 is good in the sense that it can give you high true positive rate while keeping the false positive rate low. whereas in Alg 1, if it is allowed to incur more false positive rate, then Alg 1 can give us better higher true positive rate too.  Figure. Sample ROC curve  Obviously it depends on the specification of the problem. how much can we afford false positive rate. if we can afford higher false positive rate, we can have higher true positive rate too.  A model that predicts at chance (random guessing) will have an ROC curve that looks like the diagonal dashed line in the figure above. That is not a discriminating model. The further the curve is from the diagonal line, the better the model is at discriminating between positives and negatives in general.  Consider the following figure as another example. Lets say we are designing a classifier for a medical diagnosis. In this case we probably do not mind false positives since missing positive occurrence in detection of diseases are extremely costly.  Figure. Illustration of different scenarios in ROC curve.  But, there can be situations where we do mind the false positive rate. A good example of that could be in conviction for a crime. You do not want to waste someones life with a false positive decision!  There are useful statistics that can be calculated via ROC curve, like the Area Under the Curve (AUC) and the Youden Index. These tell you how well the model predicts and the optimal cut point for any given model (under specific circumstances). AUC is used to summarize the ROC curve using a single number. The higher the value of AUC, better performing is the classifier! A random classifier has an AUC of 0.5.  Better resource: Captures Precision  https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1- score-ade299cf63cd  5.8 Regression Metrics  Regression is a strange word for a simple concept.  Regression measures how far the expected value is from the actual value? The word, ‘regression’ doesn’t really bring to mind measuring a set of values against a line. However the term was applied to the concept by a 19th century scientist, Sir Francis Galton. He was working on how tall children were in relation to their parents and how closely their heights ‘regressed’ towards the average height at the time. The term stuck. Read the story if you’re interested.  Now for the mathematical aspect.  Measuring regression performance What are the ways of measuring regression performance?  Mean Square Error To measure how close the predictions are to the true target values, Mean Square Error (MSE) is a popular measure. MSE is defined as:  Explained Variance (r^2) This measure is known by many names including:  ■ R-square ■ Explained variance  ■  the coefficient of determination  R-square is measured as the percentage of target variation that is explained by the model. For linear regression with bias term, R-square is the square of the correlation between the target values and the predicted target values. Unlike the other introduced metrics, the higher the R-square of a model, the better its performance. As we said, R-squared is the percentage of the dependent variable variation that a linear model explains:  5.9 Partitioning data for training and testing  Partitioning data for training and testing You have covered a lot of ground on how to assess a trained model. Let’s now investigate the details on model selection. The first question that might come to mind might be the limitations of using only a single training/testing set.  ■ A single training set may be affected by some outlier instances (i.e. noisy observations). To get a reliable estimate of model performance (accuracy), we need a large test set. Why? Because variance of such an estimate is low.  ■  ■ However, we know that the larger the size of the training set, the more accurately the  model can be learnt.  ■ Multiple training/test splits allow us to re-use same data for both training and evaluation  in different splits.  We usually work with 3 methods for splitting data:  ■  ■  ■  random subsampling stratified sampling cross validation.  Lets start with Random sub-sampling.  Sub-sampling Instead of using a single split, a more reliable estimate of model performance can be obtained by random sub-sampling. Random sub-sampling repeatedly partitions the data into random training and test sets in a specified ratio.  Figure. Sub-sampling.  As you can see in the figure, We train the model with each training set and estimate an accuracy using the corresponding test set. We finally average the accuracies to get an averaged estimate.  Stratified Sampling  Stratified sampling is a probability sampling technique in which we divide the entire data into different subgroups or strata, then randomly select the final subjects proportionally from the different strata. When using randomly selecting training (or validation) sets, class proportions may differ between training and test splits.  Stratified sampling ensures that class proportions are maintained in each random set. The figure below shows how Stratified Sampling works. As you can see it first separates (stratifies) instances by class label, then randomly selects instances from each class.  Figure. Stratified Sampling by 50/50 stratified split.  The figure below illustrates another example of Class-wise random selection in a specified split ratio. In the left image we can see 2 samples are selected from the red class and 1 from blue class. Same as the right image which shows a ratio of 1:2:1  for blue, red, and green respectively.  Figure. Class-wise random selection in specified split ratio.  Cross-validation Another method for partitioning data which is even more popular among researchers is Cross-validation. This is a technique to evaluate models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. The main idea is to partition training data into k equal sized sub-samples. Then iteratively leave one sub-sample out for the test set, train on the rest of the sub-samples. The following figure, illustrates this process.  Figure. Cross validation, partitioning data into k=5 equal sized subsamples.  So, to conclude:  When using subsamples, we call it k-fold cross-validation. In special cases, when k is equal to the number of instances , > we call it as leave-one-out cross validation scheme. > Cross-validation makes efficient use of the available data for testing.  5.10 Finding the best hyperparameters  Finding the best hyperparameters In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. This means the value of a hyperparameter in a model cannot be estimated from data. They are often used in processes to help estimate model parameters. In this lesson we are addressing the following questions:  ■ What is hyperparameter? ■ Why do we need to have hyperparameters? ■ How to find the best hyperparameter for a specific model?  Hyperparameters can often be set using heuristics Often they are tuned for a given predictive modelling problem. To search for the best hyperparameters, we need to partition training data into separate trainingand validation sets.  Figure. Training set, validation set and test set. We already know about training and test data. But what is a validation set? A validation set is a sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The validation set is used to evaluate a given model and also to fine-tune the model hyperparameters. So, given a choice of hyperparameter values, you use the training set to train the model. But, how do you set the values for the hyperparameters? That’s what the validation set is for. You can use it to evaluate the performance of your model for different combinations of hyperparameter values (e.g. by means of a grid search process) and keep the best trained model. However the test set allows you to compare different models in an unbiased way, by basing your comparisons in data that were not use in any part of your training/hyperparameter selection process. Here is another example. Remember Kmeans in course 2? The number of clusters (k), is a hyperparameter. Because this value is set before the learning begins. But, how can we find the best hyperparameter?  Internal cross-validation All the techniques that we previously discussed for model assessment are applicable for training/validation set splitting: ■ Random subsampling ■ Stratified subsampling ■ Cross-validation  We are still assessing how a particular hyperparameter is doing on the validation set. Remember, this step is internal to the learning process and different from model assessment on the test data. Let us examine how an internal cross-validation works. Instead of using a single validation set, we can use cross-validation within a training set to select the best set of hyperparameters. So basically it is exactly the same as the one we saw for test/train partitioning. However, in here we partition the data into training/validation sets. The following figure illustrates this process.  Figure. Internal cross-validation within a training set. Lets work on another example of that.  ■ Say, we want to do 10 -fold Cross-validation to estimate the model performance of  Elastic Net model.  ■  ■ We can divide the data into 10 equal subsamples and then train the model using 9 subsamples and test the model using the 10th subsample. We repeat this 10 -times using each subsample for the test purpose and all other subsamples for the training. In the above train the model step, best hyperparameter can be selected using an internal cross-validation. If we want to use 5-fold cross-validation for this. Then for each possible hyperparameter set, we compute 5-fold cross validation (CV) accuracy and select the best hyperparameter set.  ■  ■ So in this example, we have an external 10-fold cross-validation for partitioning  training/testing. Also we ran a 5-fold cross-validation for partitioning training/validation inside the training set for finding the best hyperparameters.  Remember that we can select the best hyperparameter set by searching/or optimizing over all possible values. Let us show you 3 possible ways to navigate the hyperparameter space:  ■ Grid-search (not so efficient). This is what we are using and explaining! ■ Random search (efficient in certain scenarios) [Bergstra et al. (JMLR 2012) ■ Bayesian optimization (efficient in general) [Snoek et al. (2012)]  For better understanding, you can read this article on hyperparameter tuning.  5.11 Effect of imbalanced classes  Effect of imbalanced classes One problem that can occur in machine learning is datasets where the total number of one class of data (i.e. positive outcomes) is far less than the total number of another class of data (i.e. negative) outcomes. This problem is very common in practice and can be detected in various disciplines including fraud detection, anomaly detection, medical diagnosis, etc. As you know most machine learning algorithms work best when the number of instances of each classes are roughly equal. When the number of instances of one class far exceeds the other, problems arise. Example: When developing a breast cancer diagnosis model, imbalanced class problem is encountered because the risk of a female being diagnosed with breast cancer (by their 85th birthday) is 1 in 8. This means that a representative training set will have 7 times more instances in negative class than the positive class (see figure below).  Figure. Sample imbalance class distribution. Almost 900 negative samples vs. almost 100 positive samples.  So what are the possible solutions to overcome this problem?  Solutions  We have two approaches to follow. First, we can perform some actions on the data itself. Alternatively, we can improve our algorithm to be able to handle such phenomenon. Let’s look at these two approaches  At the data level: (Re-Sampling)  ■  ■  over-sampling the data from minority class under-sampling the data from majority class.  Two obvious solutions based on data manipulation which suggests that we can sample more data points from the minority class in order to cover the difference. Or we can under-sample the majority class in order to make them have an equal effect on the algorithm.  At the algorithmic level:  ■  ■  adjusting the costs adjusting the decision threshold.  From an algorithmic point of view, we may want to adjust some costs on the points we are observing from the majority class in order to dampen their effect. Also we can manually define some thresholds to cope with the unbalanced data. Issues of imbalanced classes  Now, let us have a close look on possible issues of imbalanced classes.  ■ Problem-1: Since the test data contains only few samples from the minority class, even a dumb classifier that always classifies an instance to the majority class will get very high accuracy!  ■  This problem is dealt with by using other evaluation metrics in place of accuracy.  ■ Problem-2: When doing random subsampling, it is possible that class proportion is not  maintained in an individual partition. In fact, we may not sample even one instance from the minority class.  This problem can be solved using Stratified Sampling.  ■  ■  But always remember:  ■ Any pre-processing over an entire data set (e.g. feature selection, or feature extraction)  must not use the information that you are trying to predict (e.g. labels).  ■ During the training process, you must not use any information that is not available during  the training process.  