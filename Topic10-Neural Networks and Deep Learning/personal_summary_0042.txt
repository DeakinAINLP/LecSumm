In this topics unit learning outcomes we dived a little more deep into the python coding the finding the accuracy of the datasets. In this topics highlights we learned about Ml models, we learned about Random forest classification and also learned about boost models which are Gradient Boosting model and Ada Boost model . We also leaned about Hyper parameter tuning and how it effects the accuracy and polish the results of the models.  Now here are some define understanding of mine of topics we learned this topic, A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results. Ada-Boost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier. In contrast to using just one classifier, bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision. Based on the bagging decision tree idea, we can define a new method called a random forest. The random forest classifier creates a set of decision trees from randomly selected subsets of the training data set. It then aggregates the votes from different decision trees to decide the final class of the test objects.  Further more I gained the knowledge about logistic regression. logistic regression is also known as legit model. logistic regression is often used for classification and predictive analytic. Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given data set of independent variables.    