Evidence of learning Ensemble learning The idea of ensemble learning is to train multiple models to achieve the same goal without learning the exact same thing, then strategically combine them together.  A popular ensemble method is Random Forests  To reduce the variance of unstable learning methods such as decision trees, we can train multiple decision trees and combine their decisions together by averaging their decisions (for regression trees) or using voting (for classification trees). This is the ensemble method.  Bootstrap sampling Bootstrap sampling is the process of creating a new dataset from the original dataset called a “bootstrap sample”.  In many cases a bootstrap can result in less variance and more accurate results  To make a bootstrap:  1.  Take a dataset with N instances 2.  Randomly take a sample from the dataset and put it back before taking the next one N times  (drawing with replacement)  3.  The new dataset is the bootstrap sample.  AdaBoost Adaptive boosting is a machine learning algorithm for classification problems that combines weak classifiers together to create a strong classifier.  Steps:  Initialize the weights of the training examples uniformly.  1. 2.  For each iteration:  a.  Train a weak learner (e.g., a decision tree) on the training data, weighted by the  current example weights.  b.  Calculate the weighted error of the weak learner on the training data. c.  Compute the weight of the weak learner's vote in the final prediction, based on its  error rate.  d.  Update the example weights, giving higher weights to the misclassified examples.  3.  Repeat the above steps for a predefined number of iterations or until a stopping criterion is  met.  4.  Combine the individual weak learners into a strong classifier by assigning weights to each  weak learner based on their performance.  5.  Make predictions using the ensemble of weak learners, where each weak learner's  prediction is weighted by its assigned weight.  The final model is a weighted combination of the weak learners, with higher weights given to the more accurate ones. AdaBoost focuses on repeatedly adjusting the example weights to prioritize the misclassified examples and improve the model's performance iteratively.  AdaBoost primarily reduces bias in the model rather than variance.  Bagging Bootstrap aggregation or bagging uses multiple classifiers trained on different under-sampled subsets then makes them vote on the classification to come to a final decision.  It is a general-purpose procedure for reducing the variance of statistical learning methods.  The effectiveness of bagging may be less significant if the samples in dataset are highly correlated or dependent to each other.  Bagging primarily reduces variance in the model rather than bias.  Random forest Random forest is an ensemble method that works by creating multiple decision trees. Random forest classifiers works by training multiple classification trees using bagging, with each trained from random subsets of the training dataset, then aggregating the votes of each tree to determine the classification of a new sample.  Random forests are known for their ability to handle high-dimensional data, reduce overfitting, and provide robust predictions.  Training a random forest  1.  For each tree that needs to be trained where T is the number of trees to train:  a.  Select a new bootstrap sample from the training set b.  Build an un-pruned tree from this bootstrap sample c.  At every node in the tree, the assigned bootstrap sample is split into two based on some threshold value which uses m features from the total features in the dataset, chosen at random (where m is a hyperparameter)  Generally, a good number of features to use for each node of the trees is:  𝑚𝑡𝑟𝑦 = √𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠   Testing a random forest The output of a random forest can be the mean or majority vote of all the trees.  The error rate of a random forest depends on:  -  Correlation between trees (lower is better) - Strength of single trees (higher is better) - Increasing the number of features used for each split (m):  o o  Increases correlation Increases the strength of single trees  Out of bag error Each tree in the random forest is assigned a bootstrapped sample of the original dataset. On average each tree makes use of 2/3rds of the training instances. The remaining 1/3rd is called the out-of-bag (OOB) instances.  Instances are sampled into a training set and any unused points are used as the testing set.  Advantages/disadvantages of random forest  -  Building a random forest is fast and it is even faster to predict -  decision tree complexity is O(d * n * log n). A random forest with T trees would have a  complexity of O(T * d * n * log n) where d is the number of features (dimensions) and n is the number of instances. Parallelizable, since you can train and use the trees in parallel  - -  Random trees do not require the data to be rescaled, transformed, or modified and is  resistant to outliers.  -  Automatic handling of missing values (a property of decision trees) -  Produces less interpretable results compared to a single decision tree.  Feature importance in Random Forest The significance of each feature in the dataset can be assessed based on how much each feature contributes to reducing the impurity in decision trees.  To increase the model’s performance, less important features can be removed from the dataset.  This simplifies the model improving its interpretability, and potentially enhancing its performance by reducing the noise or redundancy caused by less important features.  