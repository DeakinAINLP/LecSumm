Some models only work on specific occasions. Ensemble learning combines these models into a generalized solution. Importantly, each model needs to learn something a little different. Using multiple learned models reduces variance, using averaging for regression and voting for classification.  RANDOM FOREST  Some models only work on specific occasions. Ensemble learning combines these models into a generalized solution. Importantly, each model needs to learn something a little different. Using multiple learned models reduces variance, using averaging for regression and voting for classification.  Bootstrapping is a resampling method that takes large numbers of same-size small samples from a larger sample. This is done randomly and with replacement, meaning that the data instances used in a sub-sample remain in the original sample pool and can be drawn again.  The bootstrap samples can be used to get error bars (confidence intervals) on estimates, and be used for other statistical analysis and model-building. The total number of data instances in a bootstrap sample is the same as the total of the original sample space â€“ N data instances generates B bootstrap samples of N datapoints. This means that each bootstrap sample will contain duplicate data.  Ultimately a portion of the dataset will not be included in any of the bootstrap samples. This portion, the out of bag sample, can be used for testing. Bootstrapping reduces variance and improves model reliability.  Bagging is a technique where multiple classifiers are trained with different sub-samples from a sample space, and the final solution is derived from a vote from all classifiers. This is done to reduce variance in learning, however the reduction is less powerful on dependent estimates.  Random Forest is an algorithm built upon the concept of bagging. This model focuses on reducing variance, though as a side effect increases bias slightly. It creates decision tree models by using bootstrapped samples of the training data and randomly selected feature subsets. The trees are not pruned. Ideally the trees are totally independent from one another. The final model prediction is the mode or mean of all the trees. Ideally the trees will all be highly independent from one another and the result thus quite accurate.  The model has two parameters, the number of trees and the size of feature sub-sample.   Increasing the number of features for each split increases tree correlation but also improves the performance of individual trees, so a trade-off needs to be made. More features and more trees can lead to overfitting.  BOOSTING  AdaBoost generates a forest of single-split decision trees. These are called weak classifiers, or stumps. Each data instance is assigned equal starting weight, and a stump will be used to update the weights based on if a datapoint is labelled correctly or not. Incorrect labels are given higher weights, so the next stump will give higher importance to those instances. Through the usage of multiple models the results become more accurate over time, combining the weak classifiers into a strong classifier.  Gradient boosting works on a similar principle, combining decision tree modelled weak learners into a strong learner. These trees allow for more splits than AdaBoost. Gradient Boost starts with a basic model and finds the residuals using a loss function depending on the type of problem (classification or regression). It then minimizes these residuals by building a decision tree, using the residuals as the target. The result, tempered by a learning rate hyperparameter, is used to update the base tree. Then the residuals are recalculated and this process repeats.  