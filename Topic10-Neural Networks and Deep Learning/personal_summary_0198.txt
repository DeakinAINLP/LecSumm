Evidence of Learning  Module 9: Nonlinear models (Boosting and random forest)  Module Learning Outcomes –  I certify that I have successfully learned and understood the following topics:  1.  Ensemble learning 2.  Bootstrap estimation 3.  AdaBoost 4.  Bagging 5.  Random forest algorithm 6.  Out-of-bag error and feature importance 7.  Advanced topics (feature importance of using random forest)  Summary and reflection –  The  pages  below  contain  the  handwritten  summary  referring  to  the  given  learning resources. It includes all the important points of the topic 9 module.  In this module, we learned about ensemble learning and bootstrap estimation. The provided video explained the concept really well. Then we were introduced to AdaBoost aka. Adaptive boosting in steps. Then we learnt bagging, which uses classifiers to vote on a final decision. Then we learnt about a very important concept which is the random forest algorithm. In the module’s  next  part,  we  were  introduced  to  out-of-bag  errors  and  feature  importance  and their advantages and disadvantages. Then we were introduced to the feature importance of using random forests.  When we started topic 9, I was not familiar with any of these concepts at all. So, it was very interesting and exciting to learn about a new side of machine learning which affects every and all aspects of it. And the practical part done with Python was a little bit confusing at first but  was  able  to  understand  all  the  necessary  commands  and  ways  of  implementation  by referring to both given resources and online resources.  Topic 9 Activity Sheet  Activity 9.2  Can you explain why ensemble models have a lower variance compared to other models?  Ensemble models, which combine predictions from multiple individual models, tend to have lower variance compared to single models. This is due to the principle of "wisdom of crowds" or the concept of averaging out individual errors.  Here's an explanation of why ensemble models have lower variance:  1.  Diverse  perspectives:  Ensemble  models  typically  involve  combining  predictions  from multiple individual models, which may have different biases, assumptions, or approaches to learning  from  the  data.  By  incorporating  diverse  perspectives,  the  ensemble  model  can capture a wider range of patterns and make more robust predictions. Different models may focus  on  different  aspects  of  the  data  and  bring  complementary  information  to  the ensemble.  2.  Error  cancellation:  Individual  models  in  an  ensemble  may  make  different  errors  or  have different strengths and weaknesses. When their predictions are combined, the errors tend to cancel out, leading to an overall reduction in variance. This effect is particularly noticeable when  the  individual  models  are  not  highly  correlated  with  each  other.  By  averaging  their predictions, the ensemble model can achieve a more accurate and stable estimate.  3.  Robustness  to  overfitting:  Overfitting  occurs  when  a  model  learns  the  noise  or idiosyncrasies  of  the  training  data,  leading  to  poor  generalization  on  new,  unseen  data. Ensemble  models  can  mitigate  overfitting  because  individual  models  may  overfit  different parts of the data or capture different subsets of patterns. By combining their predictions, the ensemble can generalize better and be less susceptible to the overfitting of any single model.  4. Increased stability: Ensemble models are generally more stable and less sensitive to small perturbations in the training data compared to individual models. This stability is attributed to  the  aggregation  of  multiple  predictions,  which  smooths  out  random  variations  and reduces the impact of outliers or noisy samples. As a result, ensemble models tend to have a lower variance and are less prone to overemphasizing individual data points or noise.  Overall,  by  leveraging  the  diversity  of  multiple  models  and  aggregating  their  predictions, ensemble  models  can  achieve  a  lower  variance  and  provide  more  reliable  and  robust predictions.  They  often  outperform  individual  models,  especially  in  scenarios  where  the underlying data is complex, noisy, or uncertain.  