Main points summary  -  Ensemble learning. Ensemble learning is the process by which multiple models, are generated and combined to solve a particular problem. Combining multiple models can reduce variance.  -  Bootstrapping - A bootstrap sample is a smaller sample that is generated  (bootstrapped) from a larger sample, and bootstrapping involves repeatedly drawing samples from our source data with replacement.  -  AdaBoost. AdaBoost stands for Adaptive Boosting and is a machine-learning  algorithm for classification problems. It works by combining weak classifiers to create a strong classifier.  -  Bootstrap aggregation or bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision, and in doing so reduces the variance.  -  Random forest is based on the bagging idea. The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  -  Feature importance of using Random forest. Random Forest can be used to  determine the significance of each feature in the input dataset.  