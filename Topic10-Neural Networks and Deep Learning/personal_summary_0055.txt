Topic 9 Summary Nonlinear models (Boosting & random forest)  Ensemble learning: process by which multiple models are generated & combined to solve  computational intelligence problem.  eg. Random Forest  Bootstrap estimation (bootstrap sampling)  AdaBoost  adaptive boosting: ML algorithm for classification combines weak classifiers to create strong classfier  Inputs: X: dataset of features  y: vector of corresponding labels (+1 or -1) T: number of iterations (number of weak classifiers to train)  Outputs: List of weak classifiers  Steps:  1. Initialise weights  2. Train weak classifier  3. Evaluate classifier  4. Calculate classifier weight  5. Update weights  Bagging  uses multiple classifiers trained on different under-sampled subsets. Subsets classify to vote on final decision  reduces variance  Random forest algorithm  all trees fully grown, no pruning  number to trees T too many = overfitting  number of features m_try  Training:  For each T iterations  1. Select new bootstrap sample from training set  2. build un-pruned tree on sample  3. at each iteration, randomly select m_try features & determine best split  Testing: error rate depends on  1. Correlation between trees (lower = better)  2. Strength of single trees (high = better)  3. increase number of features for each split. Increases correlation & strength of single tree  Adv:  fast to build & predict  fully parallelizable  can handle data without pre-processing  data doesn't need to be transformed/modified (resistant to outliers)  automatic handling of missing values (property of DT)  DisAdv  Difficult to interpret result than single DT  Out of bag error & feature importance  Feature importance of using RF  Significance can be determined based on the impact of reducing impurity in DT. Higher contribution,  more important feature.   