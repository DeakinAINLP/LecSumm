In this topic we learnt about Ensemble learning, Bootstrap Estimation, Bagging, and RF algorithm. Ensemble  learning  is  a  machine  learning  technique  that  combines  multiple  models  to  improve overall  accuracy  or  decision  making.  Topic  and  inaccurate  models  (called  base  models)  are combined to make an accurate models. Predictions are made on same dataset. A single decision tree model has the risk of over fitting and increased variance. We can use ensemble learning in this case where we train multiple trees with slightly different subset of data. Take their combined results to make predictions. We can use majority voting in case of classification and averaging in case of regression problems.  Random Forest is an example of ensemble of decision trees. Bootstrap estimation can be used to generate multiple subsets of dataset from the original dataset for ensemble learning. Bootstrap is a smaller sample that is generated from a larger sample. Bootstrap results in less variance and more accuracy. Bagging is a general purpose procedure for reducing the variance of a statistical learning method. Uses multiple classifiers trained on different under-sampled subsets and then allow these classifiers  to  vote  on  a  final  decision.  In  Random  Forest  classifier  multiple  decision  trees  are created  from  a  subset  of  training  set.  It  makes  predictions  based  on  the  aggregation  of  all  the outputs. Two main parameters used in Random Forest algorithm are number of trees and number of features. All trees need to be fully grown without pruning. Increasing the number of features results in more correlation and increased strength of single trees. Moreover correlation and strength of  single  trees  parameters  are  used  to  determine  the  error  rates.  Lower  correlation  and  higher strength  of  single  trees  is  preferred  for  greater  performance.  Out  of  Bag  Error  estimates  the goodness  of  fit  of  a  bagging  model.  Advantages  of  Random  forest  are  fast  to  build  and  faster prediction,  ability  handle  data  without  preprocessing.  One  of  the  disadvantage  is  it  is  less interpretable results than a single decision tree.  Another  technique  used  in  Ensemble  Learning  is  Boosting.  In  Boosting  individual  models  are trained sequentially in a way that each model tries to improve the performance on previous model. Boosting improves the accuracy of model and can handle complex dataset with high variance and noise even though it is computationally expensive. Different types of ensemble learning techniques based on Boosting are AdaBoost, Gradient Boost, XGboost etcetera. Random forest can also be used to determine the feature importance.  Another type of Ensemble Learning Technique is Voting Classifier. It combines the predictions of various types of classifiers such as decision trees, k-nearest neighbors, SVMs etcetera to provide a  final  prediction.  Stack  Classifier  is  also  an  Ensemble  Learning  technique  that  aggregates  the predictions of various separate  classifiers. Stack Classifier can  increase the prediction accuracy and generalization by learning more complicated decision boundary and minimizing the chance of over fitting.    