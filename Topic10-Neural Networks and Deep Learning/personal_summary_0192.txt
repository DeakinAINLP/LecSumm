 In order to produce more accurate predictions or classifications, the machine learning technique known as ensemble learning combines numerous individual models, often known as base learners or weak learners. The Random Forest algorithm is an ensemble learning method that combines the concepts of bagging and random feature selection to build a robust and accurate predictive model.  A statistical resampling method known as bootstrapping is used to estimate the sampling distribution of a statistic or draw conclusions about a population parameter. It enables us to approximate the statistical uncertainty without having to make strict assumptions about the distribution of the data's underlying structure.  AdaBoost is an ensemble learning algorithm that combines a few weak learners to produce a strong learner. It involves training a string of inexperienced learners, each of whom is focused on fixing the errors committed by the ones before them. Weak learners are models, such as shallow decision trees or straightforward rules, that only slightly outperform random guessing.  Bagging is a method of ensemble learning that combines several base learners to produce a more reliable and precise predictor.  From the initial training data, several bootstrap samples are produced. Instances from the original dataset are randomly chosen with replacement to create a bootstrap sample. Although certain occurrences may be repeated and others may be excluded, each bootstrap sample is the same size as the original dataset.  During the training phase, the out-of-bag (OOB) error is calculated as a performance indicator for the model. It offers a prediction of the Random Forest's ability to generalise to new data. The OOB error is calculated by comparing each ensemble tree's predictions to instances that were excluded from the bootstrap sample used to train that tree.  