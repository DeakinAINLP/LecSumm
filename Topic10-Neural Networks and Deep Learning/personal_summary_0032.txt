This topic was all about the nonlinear machine learning models: Boosting and Random Forest. The following points were covered:  Ensemble learning:  The process by which multiple models, such as classifiers, are  strategically generated and combined to solve a particular problem. In order to reduce the variance of learning methods such as decision trees, multiple decision trees can be trained on slightly different subsets of data.  Bootstrap sampling:  ‘Bootstrapping’  o Repeated, random sampling with replacement. Used in machine  learning to create simulated samples from which summary statistics for the population such as mean and standard deviation can be calculated.  Bootstrap Aggregating:  ‘Bagging’  o Builds on Bootstrapping. Multiple models of the same machine learning  algorithm are trained on different bootstrapped samples of the population, and then the final results from each are aggregated, either by averaging or voting (depending on whether it is a classification or regression task) to get a final result.  This training on different models is done in parallel, and the results from  one model do not affect the training of another.  Boosting:  Unlike Bagging, where models are trained in parallel, Boosting trains  models sequentially, and so data points that are falsely classified in the previous iteration are given greater weight in the next iteration.  Adaboost:  Stands for Adaptive Boosting. Used in classification problems. Combines  weak classifiers to create a strong classifier.  AdaBoost is an ensemble method that often uses Decision Trees as the individual weak classifiers. Commonly, the decision trees will be made up of just a single stump (or decision) per tree.  Random Forest:  Creates as set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class.  Each tree is built using a bootstrap sample of the data. At each node, only a subset of the features (chosen at random) is selected for consideration when determining how to split the data. In this way, the trees are kept as uncorrelated as possible to each other.  The result from all the individual trees is then averaged (regression) or  used in a vote (classification) to determine the final output.  Out-of-bag error and feature importance:  Equivalent to validation or test data. Each tree in a random forest is trained on a bootstrapped sample. Each bagged tree makes use of approximately two-thirds of the training instances, with the remaining instances being referred to as the ‘out-of- bag’ instances.  Like cross-validation, OOB instances are not used for learning. The importance of each feature can be determined when using a  random forest.  