Ensemble learning  Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Consider this scenario:    We know that a single decision tree might not perform well.   But, it is super fast.   What if we learn multiple trees?   We just have to make sure that they do not all learn the same thing.  To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data. Then when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification). This is called the ensemble method.  Bootstrap estimation  A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results.  AdaBoost:  AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier. The algorithm has the following steps:  Inputs:    X: dataset of features   y: vector of corresponding labels (+1 or -1)   T: number of iterations (i.e., number of weak classifiers to train)  Outputs:    List of weak classifiers, each with an associated weight  Step 1: Initialize weights       Step 2: Train weak classifier  Step 3: Evaluate classifier  Step 4: Calculate classifier weight  Step 5: Update weights  where Zt is a normalization constant that ensures the weights sum to 1:  Final output:  where:  H(x) is the final strong learner that classifies the input vector x  T is the number of iterations (weak learners) in AdaBoost  Ht(x) is the weak learner at iteration t  at is the weight assigned to the weak learner at iteration t  sign(z) is the sign function that returns +1 if z>=0 and -1 otherwise.          Bagging  In contrast to using just one classifier, bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision.  Bootstrap aggregation or bagging (B+agg), is a general-purpose procedure for reducing the variance of a statistical learning methods.  Given a set of n independent estimates Z1, Z2, Z3…Zn each with a variance of  , the variance of  their mean  is  (n-times lower).  when the estimates are not independent, reduction in variance is lower.  Consider the following figure as an example of bagging. As you can see, different independent classifiers voted for different boundaries (light green), but if you take the aggregation of these boundaries (dark green) it will clean up and improve the final decision boundary.  Also as you can see in the figure below, one could either choose to classify with 1 tree (the red original tree in top left) or 11 trees based on the dataset. Usually it’s more powerful to use bagging decision trees so that you can utilize as much information as possible.       Random forest algorithm  Based on the bagging decision tree idea, we can define a new method called a random forest.  The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  Random forest builds on the idea of bagging. Each tree is built from a bootstrap sample of data. Node splits are calculated from random feature subsets to make sure each of the trees is as independent as possible. Then we randomly pull out a subset and try work with the subset. Whenever it needs to split to from the tree, based on the best feature, we choose the best feature       from the subset. Ultimately you have to do these steps T times, where T is the number of the trees.  If you are wondering whether this model increases the bias, you are right. It does! It uses subsets of features in different independent trees so it is likely to slightly increase the model bias.  A useful rule of thumb states that the number of features is:  In random forest:    all trees are fully grown with no pruning   we are dealing with two parameters:  o  number of trees (T); Remember if you raise this value too much and make too  many trees, you are likely get trapped in the overfitting problem!  o  number of features (mtry)  Out of bag error and feature importance  It is possible to estimate the goodness of a bagged model in the same way as every model in machine learning. Out of Bag is equivalent to validation or test data.  Each tree in a random forest is trained on a bootstrapped sample. It can be shown that on average, each bagged tree makes use of 2/3 of the training instances. The remaining 1/3 of the instances are referred to as the out-of-bag (OOB) instances. We can predict the response for the i-th observation using each of the trees where that observation was OOB. This will yield around B/3 predictions for the i-th observation, which we then average.  Feature importance of using Random forest (RF)  The significance of each feature in the input dataset can also be determined using Random Forest. Based on how much it helps to reduce impurity in the decision trees, the significance of each characteristic is assessed. The higher the contribution, the more important the feature is. To increase the model's performance, feature importance utilising Random Forest can be utilised to find the most pertinent features for classification and feature selection.          Voting Classifier:  An ensemble learning technique called a voting classifier combines the predictions of various separate classifiers to provide a final prediction. Several types of classifiers, such as Decision Trees, K-Nearest Neighbors, or Support Vector Machines, can be used individually. Each classifier is given one vote, and the final forecast is determined by a majority vote. Voting Classifier can increase prediction accuracy and robustness because it incorporates the benefits of various models while minimising the effects of their particular flaws.  Stack Classifier: Another ensemble learning technique that aggregates the predictions of various separate classifiers is the Stack Classifier, which is more complex than the Vote Classifier. The first layer of a stack classifier comprises multiple separate classifiers that create predictions based on the input data. The second layer then integrates the previous layer's predictions to arrive at a final prediction. Several algorithms might be used at the second layer, including Decision Trees and Logistic Regression. Stack Classifier can increase the prediction's accuracy and generalizability by learning a more complicated decision boundary and minimising the chance of overfitting.  