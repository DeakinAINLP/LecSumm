 A. Ensemble learning:  i.  The process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem  ii. Random Forest: To reduce variance, train multiple decision trees, each with  slightly different subsets of data, use classification/regression to take combined decisions (via averaging for regression or voting for classification)  B. Bootstrap Estimation  i. A bootstrap sample is a smaller sample that is generated (bootstrapped) from  a larger sample. It resamples many times with replacement.  ii. iii. Used to get error bars (confidence intervals) on estimates iv.  In many cases bootstrap can result in less variance and more accurate results.  C. AdaBoost:  i. Adaptive Boosting -- a machine-learning algorithm for classification problems.  ii.  It works by combining weak classifiers to create a strong classifier. Inputs: X: dataset of features; y: vector of corresponding labels (+1 or -1); T: number of iterations (i.e., number of weak classifiers to train) iii. Outputs: List of weak classifiers, each with an associated weight iv. Step 1: Initialize weights; Step 2: Train weak classifier; Step 3: Evaluate  classifier; Step 4: Calculate classifier weight; Step 4: Calculate classifier weight, and produces the final output  D. Bagging:  i. Bagging uses multiple classifiers trained on different under-sampled subsets  and then allows these classifiers to vote on a final decision.  ii. Bootstrap aggregation or bagging (B+agg), is a general-purpose procedure for  reducing the variance of a statistical learning methods. If the estimates are not independent, reduction in variance is lower  iii.  E. Random forest algorithm  i.  The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  ii. The difference is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly, to make sure each of the trees is as independent as possible. It uses subsets of features in different independent trees so it is likely to slightly increase the model bias.  iii.  iv. Need to determine number of trees and number of features v. Error rate depends on: Correlation between trees (lower is better) and  vi.  Strength of single trees (higher is better) Increasing number of features for each split: Increases correlation and Increases strength of single trees F. Out of bag error and feature importance  i. Out of Bag = validation or test data (out-of-bag (OOB) instances) ii. Advantages / Disadvantages  1. Fast to build and even faster to predict 2. More complex  3. Fully parallelizable since you can run trees in parallel to go even faster! 4. Can handle data without pre-processing (normalization) 5. Data does not need to be rescaled, transformed, or modified! (Resistant to  outliers)  6. Automatic handling of missing values (a property of decision trees) 7.  less interpretable results than a single decision tree  G. Feature importance of using Random forest (RF)  i. Depending on how much it helps to reduce impurity in the decision trees  H. Ensemble Learning Techniques  i.  Voting Classifier: combines the predictions of separate classifiers to provide a final prediction. (Decision Trees, K-Nearest Neighbors, or Support Vector Machines), and increases accuracy.  ii. Stack Classifier: aggregates the predictions of various separate classifiers  1. The first layer of a stack classifier comprises multiple separate classifiers  that create predictions based on the input data.  2. The second layer then integrates the previous layer's predictions to arrive  at a final prediction. (Decision Trees and Logistic Regression) Increases accuracy and generalization  