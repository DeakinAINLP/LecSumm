Overview In topic 9, we explored supervised learning further, experimenting with nonlinear supervised learning models like random forest.  Ensemble Learning The process that strategically generates and combines multiple models, such as classiﬁers or experts, to solve a particular computational intelligence problem.  Example:  To reduce the variance of unstable learning methods such as decision trees, we can train multiple decision trees each with diﬀerent subsets of data. Then when doing classiﬁcation/regression you take their combined decisions (averaging for regression or voting for classiﬁcation. This is ensemble method.  Bootstrap Estimation It’s a resampling method to reduce the variance.  The steps:  -  Tale a dataset with N data instances -  Create re-sampled version of dataset by randomly drawing N times with replacement. This provides a bootstrap sample.  AdaBoost Adaptive Boosting is a machine learning algorithm for classiﬁcation problems. It works by combining weak classiﬁers to create a strong classiﬁer.  The inputs:  -  X: dataset of features -  Y: vector of corresponding labels (+1 or -1) -  T: number of iterations (i.e. number of weak classiﬁers  to train)  The outputs:  -  List of weak classiﬁers, each with an associated weight.  Bagging Bagging uses multiple classiﬁers trained on diﬀerent under- sampled subsets and the allows these classiﬁers to vote on a ﬁnal decision, as opposed to using just one classiﬁer. It’s a general purpose procedure for reducing the variance of a statistical learning methods.  Random Forest Algorithm This method is based on the bagging decision tree idea. The random forest classiﬁer:  -  creates a set of decision trees from randomly selected  subsets of the training dataset.  -  Then aggregates the votes from diﬀerent decision trees  to decide the ﬁnal class of the test objects.  The diﬀerence between the random forest classiﬁer and the decision tree algorithm is that in the former one, the  processes of ﬁnding the root node and spliting the feature nodes will run randomly.  In random forest:  -  All trees are fully grown with no pruning -  We deal with two parameters: o  Number of trees (T) o  Number of features Mtry  Advantages and Disadvantages of Random Forest  -  Fast to build and faster to predict -  Ability to handle data without pre-processing.  Normalization is required all the time.  -  Data does not need to be rescaled, transformed, or  modiﬁed.  -  Automatic handling of missing values -  Less interpretable results than a single decision tree.  Feature Importance of Using RF The signiﬁcance of each feature in the input dataset can also be determined using RF. Each characteristic is assessed based on how much it helps to reduce impurity in the decision trees  