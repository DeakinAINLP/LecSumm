Ensemble learning is when several diﬀerent models are combined together to solve a speciﬁc computa;onal  problem.  A>er  training  mul;ple  decision  trees  each  comprising  of  diﬀerent sets of data, a combined decision is then used for the classiﬁca;on/regression. This yields a lower  variance  of  unstable  learning  methods.  Ensemble  methods  have  a  lower  error,  less overﬁDng. For example, the biases for each machine learning models can be depicted in table 1.  Model Linear Regression  Logis;c Regression  Decision Tree  Support vector Machines  Bias Assumes variables Assumes linear decision boundary between classes Creates axis-aligned splits, diagonal decision boundaries. Assumes  clear  margin  (shortest  distance between  observa;on threshold) between  classes.  (Picking  threshold  less sensi;ve  to  training  data  and  allowed misclassiﬁca;ons  resul;ng  in  higher  bias lower  variance;  distance resul;ng between  threshold  and  observa;ons is called so> margin. Using cross valida;on to determine  which  points  are  allowed  within the the  best classiﬁca;on). Assumes  similar  instances  having  similar outputs Create  axis-aligned  splits,  limit  ability  to capture complex decision boundaries Can overﬁt if not regularized  When each learner is trained of diﬀerent sets of data it is called bootstrap aggrega;ng also known as bagging. Random forest builds on the idea of bagging. A>er crea;ng a number of subsets of the original data. This data is collected randomly. For example for the ﬁrst subset for n number of data, n prime being the number of instances allocated to each “bag”, with replacement and also collected randomly.   Together m being the number of groups of bags are created consis;ng of n prime diﬀerent data instances chosen at random with replacement. Typically n prime is lesser than n , typically 60% lesser of training instances than the instances in the bag (60% for train and 40% for test).  M is the number of bags N is number of training instances from dataset. N prime is the number of instances in each bag.  Each collec;on of the data is used to train the respec;ve learning models. In an ensemble of diﬀerent learning models, each model is queried with the same X and the output are collected. The Y output of each model is averaged and the Y for the ensemble is retrieved. The bias that is collected will be compara;vely low as each model is compe;ng against one another.  AdaBoost is Adap;ve Boost. It consists of a forest of stumps which are not great at making accurate classiﬁca;ons. AdaBoost has one node and two leaves called stumps. Stumps only use one variable to make a decision. Larger stumps contribute more to classiﬁca;ons than smaller  stumps.  In  a  forest  of  stumps  made  with  AdaBoost,  order  is  important.  The  errors made  by  ﬁrst  stumps  inﬂuences  how  the  second  stump  and  so  on  are  made.  AdaBoost combines a lot of weak learners comprising of stumps.  Out of bad is equivalent to the valida;on of the test data. As each tree uses 2/4 of the training instances, the remaining 1/3 is referred to as the out-of -bag (OOB) instances.  