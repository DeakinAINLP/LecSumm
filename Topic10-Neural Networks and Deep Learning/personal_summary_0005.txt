Ensemble learning is the process of how multiple models are strategically generated and combined to solve a particular computational intelligence problem. The critical point is to try and design an ensemble model in a way you can train different models with slightly different subsets of data. Boosting Bootstrapping is a type of resampling method. It can be used for error bars or confidence intervals. We take a dataset with N data instances and the create a re-sampled version by randomly drawing N times with replacements giving bootstrap sample, We can then estimate any quantity repeatedly. This can result in less variance and more accurate results. AdaBoost, or Adaptative boosting, is a ML algorithm for classification problems. It combines weak classifiers to create a strong classifier. It does this by taking the dataset features, vector of corresponding labels, and number of iterations. You first initialise weights, train weak classifier, evaluate classifier, calculate classifier weight and then update weights to get the list of weak classifiers, each with an associated weight. Bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on the final decision. Bootstrap aggregation (Bagging) is a general-purpose procedure for reducing the variance of statistical learning methods. Random Forest Based on the bagging decision tree we can get a new method, random forest. The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It aggregates the votes from different decision trees to decide the final class of the test objects. The random forest compared to decision tree is that it processes finding the root node and splitting the feature nodes will run randomly. Each tree is built from a bootstrap sample of data. Out of bag error is the equivalent to validation or test data. Each random forest is trained on a bootstrapped sample and can be shown that on average each bagged tree makes use of 2/3 of training instances and 1/3 are referred to as out of the bag. Random forest is fast to build and even faster to predict, it is fully parallelizable since you can run trees in parallel to go faster, it can handle data without pre-processing, data does not need to be rescaled or modified in anyway but results are less interpretable. The significance of each feature in the input dataset can be determined using random forest, based on how much it helps to reduce impurity in the decision trees, the significance of each characteristic is assessed. The more it contributes the more important the feature is. Utilising feature importance can help to find the most pertinent features for classification and feature selection. 