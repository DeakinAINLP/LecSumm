Non-Linear Models  Ensemble Learning  Ensemble learning involves taking multiple ‘weak’ or ‘inaccurate’ models and combining them to create a larger model which is much more accurate. This works as each of the weaker models will have a perspective through which it attempts to predict based on data. When you combine these models and have them provide ‘votes’ rather than a final prediction you can combine these votes to find the majority and take predictions from this majority vote taking average when using regression models or voting for classification. It has been shown that the variance of these ensemble models is much lower.  Boosting involves adjusting the weights of a parameter mis-classified datapoints to allow the next classifier in the ensemble to focus more on these to get these more accurate. This boosts the performance of the overall model from the interactions from each of the weaker models, allowing the overall model to be more accurate. A boosting algorithm we use for classification problems is AdaBoost.  We can use the bootstrapping method, which involves taking a subset of data from the main dataset for the purpose of resampling in order to effectively train these models. These subsets will change with each sample, providing a broad mix of training data. Bootstrapping is often used to get error bars or confidence intervals on estimates.  Bagging, which is short for bootstrap-aggregation, expands on this by using multiple classifiers trained on different under-sampled data subsets. This is a general purpose method to reduce the variance of the learning methods.  One model we use is the Random Forest Classifier, which is based on the bagging decision tree. This bagging model is expanded upon, where each tree in the ‘random forest’ is built from a bootstrap sample of data. We calculate Node Splits, based on the random feature subsets to make these trees as independent as possible. A random subset of data isthen pulled and tried with the model, with the trees splitting based on the best feature from the subset. We then evaluate the model from unseen testing data, as with other machine learning models to determine it’s efficiency.            