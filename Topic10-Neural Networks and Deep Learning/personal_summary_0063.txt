  Ensemble learning, process in which multiple models  such as classifiers or experts, are strategically  generated and combined to solve a particular computational intelligence problem    Can reduce the variance of unstable learning methods such as decision trees   Variance of ensemble models are lower   Bootstrap estimation is a resampling technique used to estimate parameters by repeatedly drawing  samples from source data with replacement    Adaboost ( adaptive boosting), is a machine learning algorithm for classification problems   Bagging uses multiple classifiers trained on different under sampled subsets   Used to reduce variance of a statistical learning method   Random forest classifier creates a set of decision trees from randomly selected subsets of training  dataset. Aggregates votes and decides on final class    Difference from decision tree algorithm is that root node and splitting the feature nodes will run  randomly    All trees are grown with no pruning, two parameters of number of trees and number of features    Out of bag is equivalent to validation or testing data     Advantages of random forest, fast to build faster to predict, fully parallelizable, does not require pre processing, does not need rescaled, transformed or modified, automatic handling of missing values, less interpretable results than a single decision tree        