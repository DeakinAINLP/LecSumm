TOPIC 9: Nonlinear models (Boosting and random forest)  9.2 Ensemble learning  To reduce some of the variance in some unstable learning methods, ensemble learning can be used in which you can potentially train multiple of a model and then by using classification or regression you acquire the combined results.  9.3 Bootstrap estimation  9.4 AdaBoost  Adaptive boosting is used for classification problems. It has inputs of x: dataset of features, y: vector of labels and T: number of iterations.  9.5 Bagging  Bagging is short for bootstrap aggregation. Its aim is to reduce the variance of statistical learning methods.  An example of how it works is taking the input of the boundaries from individual classifiers and taking the aggregation of them. As for example in:  9.6 Random Forest algorithm  Random forest is a method in which multiple decision trees are modelled to create the forest.  9.7 Out of bag error and feature importance  This is essentially random forests equivalent of validation or test data.  9.8 Advance topics  The significance of each feature can be assessed. The more one features contribution, the more important the feature is deemed to be. Using Random Forest, model performance can be increased by finding the most notable features to be used in classification and feature selection.  