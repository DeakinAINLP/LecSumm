The key points covered in topic nine are:  1.  The concept of Bootstrap and how to generate random subsamples. 2.  AdaBoost which uses bootstraps and assigns weights to training sample that are passed on  to subsequent estimators.  The weights are based on the training errors.  3.  The concept of Bagging or Bootstrap Aggregation and how it has the effect of reducing  variance in the individual models.  4.  Random Forest and how it differs from Decision Tree.  Unlike an ensemble meta-estimator such as BaggingClassifier or BaggingRegressor which can use any model as base estimator, random forest uses a decision tree as a base estimator.  5.  Considerations for building a better random forest model such as tuning hyperparameters  maximum features and number of trees.  6.  Out of bag evaluation as an alternate to cross-validation on the training dataset. 7.  Feature importance based on how much it helps to reduce impurity in the decision trees. 8.  Python libraries to create ensemble classifiers such as Random Forest and AdaBoost  