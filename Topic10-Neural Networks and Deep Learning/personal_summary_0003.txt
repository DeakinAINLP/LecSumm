This module covered nonlinear models with a specific focus on boosting and random forest. Boosting or AdaBoost (adaptive boosting) is a ml algorithm used for classification problems which combines strong and weak classifiers. This is achieved by first training on the dataset with a decision tree, then going back over the weak predictors and adjusting their weights to improve their predictions. This is done iteratively. Random forest on the other hand is a collection of decision trees (forest), where each tree is trained on a subset of the data. During the prediction process, the best tree has the most votes and the final outcome is determined by the majority. Random forest of often referred to as somewhat of a black box in terms of how it works. Other topics covered were bagging (a technique that involves training  multiple  models  on  different  subsets  of  the  data  and  then  aggregating  their  predictions), bootstrap estimation (a statistical method that involves repeatedly sampling data with replacement to estimate uncertainty), and the ensemble method (a technique used for combining multiple models to make more accurate predictions or classifications). 