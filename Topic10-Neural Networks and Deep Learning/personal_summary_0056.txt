An overview of the topics explored this topic are as follows:   Nonlinear supervised learning models  Random Forest  To develop decision trees and random forest methods and address challenges in the  real world, you will use Python packages.   Also look at how selecting parameters affects how well-developed models work.  SUMMARY  Ensemble Learning  Your intended classifier may occasionally perform poorly and inaccurately on a dataset. Even though you may have created numerous classifiers, some of them might be erroneous while others might work better in certain situations.  Ensemble learning is a method for solving specific computational intelligence problems by carefully generating and combining several models, such as classifiers or experts. Take the following example:  It is, however, quick.   We are aware that one decision tree could not function adequately.   What happens if we learn several trees?  All we need to do is watch that they don't all learn the same thing.  We can train numerous decision trees, each with a slightly different sample of data, to lower the variance of unstable (high variance) learning techniques like decision trees. Next, while performing classification or regression, you consider both of their judgements (either by average for regression or voting for classification). The ensemble method is what is used in this.  The Random Forest is a popular ensemble technique (Breiman, 2001). It has been demonstrated that these ensemble models have lower variance. The key is to attempt to create an ensemble model that allows you to train various independent models using slightly different subsets of data. It can be difficult to feed the data into the models.  Bootstrap Estimation  Bootstrapping is the process of creating a smaller sample from a bigger sample. It employs a statistical resampling technique. Bootstrap can frequently produce less volatility and more accurate outcomes.  AdaBoost  A machine-learning technique for categorisation problems is called AdaBoost, which stands for Adaptive Boosting. In order to generate a strong classifier, it combines weak classifiers. Following are the steps of the algorithm:  Inputs:   Features dataset X  y: a vector of labels with the same values (+1 or -1)  T: the number of iterations, or the quantity of novice classifiers that must be trained.  Outputs:   List of weak classifiers, each with a corresponding weight.  Step 1: Initialize weights.  Step 2: Train weak classifier.  Step 3: Evaluate classifier.  Step 4: Calculate classifier weight.  Step 5: Update weights.  Bagging  Instead of employing a single classifier, bagging employs a few classifiers that have been trained on several under-sampled subsets. These classifiers are then given the opportunity to vote on the final classification.  A general-purpose strategy for lowering the variance of statistical learning methods is called bootstrap aggregation or bagging (B+agg). The reduction in variance is lower when the estimates are not independent, so take note of this and keep it in mind. To use as much information as possible, it is more effective to employ bagging decision trees.  Random Forest Algorithm  We may design a new technique known as a random forest based on the concept of the bagging decision tree. A collection of decision trees is produced by the random forest classifier using a set of randomly chosen subsets from the training dataset. The final class of the test items is then determined by averaging the votes from various decision trees.  The random forest algorithm differs from the decision tree algorithm in that the operations of locating the root node and dividing the feature nodes are carried out at random. Building on the concept of bagging is random forest. A bootstrap sample of data is used to build each tree. To ensure that each of the trees is as independent as feasible, node splits are computed using random feature subsets. Then we attempt to deal with the subset that was chosen at random. We select the best feature from the subset if it is necessary to split the tree based on the best feature. You must repeat these processes until you have reached the quantity of trees.  You are correct to wonder if the bias is increased by this model. The answer is yes! The model bias is likely to be slightly increased because it uses subsets of features in various independent trees.  In random forest:   Since there has been no pruning, we are dealing with two parameters:  number (T) of trees; Keep in mind that if you increase this amount excessively and create an excessive number of trees, you risk becoming locked in the overfitting dilemma.   Number of features  Out of Bag Error  Like how every model in machine learning can be estimated, so can the goodness of a bagged model. Out of Bag is the same as test or validation data. A bootstrapped sample is used to train each tree in a random forest. Each bagged tree uses, on average, 2/3 of the training cases, as can be shown. The out-of-bag (OOB) instances make up the remaining one-third of the total number of instances. With the help of each tree where the i-th observation was OOB, we may forecast the reaction for that observation. For the i-th observation, this will produce predictions around B/3, which we then average.  Data points are sampled to create a training set, while unused data points are used to create a test set. Using this training data, we then construct the random forest. The unobserved test data will be used later to evaluate our model. Performance estimation using out-of-bag samples, like cross-validation, is calculated using data that were not utilised for learning. The estimate will likely be inaccurate if the data were processed in a way that transferred information between samples.  Advantages/Disadvantages of Random Forest      It takes little time to create and anticipate a random forest. totally parallelizable, as many trees can be run simultaneously to move more quickly! to manage data without pre-processing. Normalising your dataset before using this procedure is not always necessary.   No scaling, transformation, or editing of the data is required! (Impervious to outliers)  Decision trees have the ability to handle missing values automatically.  findings that are less comprehensible than a single decision tree  Feature importance of using random forest  Using Random Forest, it is also possible to establish the importance of each feature in the input dataset. Each characteristic's importance is determined by how much it contributes to lowering impurity in the decision trees. The importance of a characteristic is inversely correlated with its contribution. Finding the most relevant features for classification and   feature selection can be done using Random Forest's feature importance method to improve the performance of the model.  REFLECTION  Examining ensemble ML algorithms was the focus of this topic. We have gained a theoretical grasp of these methods in addition to actual Python application skills. To test these ideas practically, we have also created several Python programmes. We also had the chance to experiment with Python programming as well to learn to implement all the above summarised topics.  Screenshot of Quiz Score     