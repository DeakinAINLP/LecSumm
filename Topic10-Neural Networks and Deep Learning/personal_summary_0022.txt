In the previous topic, we looked into 2 non-linear model supervised learning algorithm, KNN  and Decision Tree. We have gone through how these two algorithms are able to split and divides data, next on using it for prediction dataset. Now we will continue on the supervised algorithm, we will study about another nonlinear models’ algorithm.  In this topic, we will be learning about ensemble learning to optimize prediction performance of  a tree, boosting to create a stronger classifier and a supervised learning algorithm Random Forest. The objective for this topic module will be:  -  Analyze the performance of ensemble classifier with respect to a single model -  Construct multi-layer neural network to demonstrate data representation, classification and  evaluation skills  Before we get into Random Forest, we need to learn about some technique and algorithm that can  help optimizing the dataset performance such as Boost, Ensemble Learning, Bootstrap and Bagging.  Adaboost  Adaboost is an algorithm for classification problems. So what does it do? When we are splitting  the dataset, there will be datapoints that are weak for prediction cases. But with Boosting, it will combine those weak classifiers to become an impactful and strong classifier. The algorithm will identify the weak classifier and start training it, then change the weight of the classifier.  Ensemble Learning  Before we get into the Random forest, we need to know what is Ensemble learning. We know  that a decision tree can partition dataset fast, but depending on the tree depth or features, it might not have a great performance like other algorithms. To do optimize the algorithm performance, we use ensemble learning. Ensemble learning is a way to learn train multiple trees, then average all the prediction to optimize the prediction performance. One way you can use all these multiple trees is using bootstrap. Bootstrap replicate the original data a few times by replacing the old variable instances, then when getting the mean from all those replications, we will be able to reduce the variance of the dataset and increase the accuracy. One common ensemble learning algorithm that we used is called Random Forest, and we are going to investigate that in this topic modules.  Bagging  When bootstrapping the original data for replication and trying to pick what is best, this is called  Bagging. Bagging is a technique used to reduce the variance of the dataset, and allow the classifier to vote on a final decision. One of the bagging techniques is random forest as it ensemble and group up the classifier to have better result.  Random Forest As I stated earlier, Random Forest is different from Decision Tree, it uses all the technique mentioned above such as bootstrapping, ensemble learning and bagging classifier and combine them to achieve the best result by picking the best features. However, unlike decision tree, random forest does not need to prune which makes the algorithm way slower than other algorithm, however, it will reduce variance and increases accuracy.  This algorithm has two parameters, the amount of tree we want to build, and the features we are picking. While training, it selects new bootstrap by replicating instances, build the unpruned tree, randomly selects a feature, iterates it, and pick the best one. To test it, we use the majority or the means of the classifier.  Advantage  Disadvantage  Resistances to outlier, so does not require scaling. We don’t have to normalize the dataset as well Reduces overfitting and variance  Both for classification and regression Automates missing values  Slower than Decision Tree, as it multiplies with the amount of Tree we are bootstrapping  