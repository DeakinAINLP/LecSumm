Improving model performance has always been the priority and the models that were discussed so far had room  for  improvement.  Ensemble  learning  is  the  technique  where  multiple  techniques  are  combined  to produce improved model performance. When multiple modelâ€™s outputs are combined to make predictions the accuracy of resultant model is improved, and better generalization can be observed. Ensemble methods helps reduce  high  variance  that  is  observed  in  models  like  decision  trees.  Random  Forest  is  one  of  the  popular ensemble methods where outputs of multiple decision trees are combined to make predictions. It is important to make sure that the input dataset is different every time in these models which are combined to find ensemble outputs. Bootstrap estimation is a method where a random sample of data with replacement is used, with a few data points been used multiple times in a sample set. Smaller data sample is extracted in this way from a large set and the resultant output is has better accuracy and less variance. AdaBoost ensemble method where decision tree is the most common that  is used with only one split. This helps improve accuracy  of binary classifications by combining weak classifiers to form a strong classifier. The steps involved in the algorithm formation  are  discussed  which  includes  initializing  weights,  training  weak  classifiers,  evaluating  them, calculating classifier weight, and updating accordingly to come up with final output classification. Bagging or bootstrap aggregation combines multiple models which are trained using bootstrap method to combine to give a model that results in variance reduction and improved model performance by allowing the classifiers to vote for the best model. Bagging decision trees can be used to utilize maximum information during model training process. Random forest classifier selects multiple subsets using bootstrap method of sample selection from the training data and form decision trees for these datasets. Then votes are aggregated from these decision trees to decide the final classification outcome. This method is likely to increase the bias of the final model. During the training process a bootstrap sample is selected from the training dataset and a random feature is selected from the internal nodes and best split is done using these selected features. Finally mean from each trained tree is calculated to find the output. Correlation, strength of each tree and increasing split affects the error rate in random forest. Random forests are fast to build and predict, requires little to no pre-processing, no need for data transformation or any kind of modifications, and can handle any missing values easily without external interferences. The samples that are not used during the bootstrap sampling process are used as test dataset.  Performance  is  calculated  on  this  dataset. This  refers  to  Out-of-Bag  error  in  random  forests. The amount of feature reduction that each decision tree offers helps in selecting the important features in dataset. The higher the contribution that is received from a decision tree the higher is its importance. This way features are selected to obtain the best model performance. Voting Classifier is an ensemble technique used to improve prediction  accuracy  by  using  voting  method  when  using  multiple  models.  Stack  classifier  on  other  hand aggregates the predictions in layers from various models to improve the performance accuracy. Finally, python codes are studied for these ensemble methods.   