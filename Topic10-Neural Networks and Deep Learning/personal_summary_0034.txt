Ensemble Learning Sometimes your designed classifier on a dataset is weak and inaccurate. You may have designed many classifiers but some of them could be inaccurate and some could perform better on specific occasions. Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data. Then when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification)  Bootstrap estimation A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results.  AdaBoost (adaptive boosting) machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier  Inputs:   X: dataset of features  y: vector of corresponding labels (+1 or -1)  T: number of iterations (i.e., number of weak classifiers to train)  Outputs:   List of weak classifiers, each with an associated weight  Bagging In contrast to using just one classifier, bagging uses multiple classifiers trained on different under- sampled subsets and then allows these classifiers to vote on a final decision. It involves creating multiple subsets of the original dataset by randomly sampling with replacement, and then training individual models on each subset.  Random forest algorithm Based on the bagging decision tree idea, we can define a new method called a random forest.  The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  Random forest builds on the idea of bagging. Each tree is built from a bootstrap sample of data. Node splits are calculated from random feature subsets to make sure each of the trees is as independent as possible. Then we randomly pull out a subset and try work with the subset.   Whenever it needs to split to from the tree, based on the best feature, we choose the best feature from the subset  Advantages/Disadvantages of Random Forest  random forest is fast to build and even faster to predict   ability to handle data without pre-processing. You are not always required to normalize your  dataset before running this method   data does not need to be rescaled, transformed, or modified! (Resistant to outliers)  automatic handling of missing values (a property of decision trees)  less interpretable results than a single decision tree      