Topic 9  Key Learning Points Ensemble Learning  Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Consider this scenario:    We know that a single decision tree might not perform well.   But, it is super fast.   What if we learn multiple trees?   We just have to make sure that they do not all learn the same thing.  Bootstrap Sampling       AdaBoost:  AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier.  Bagging In contrast to using just one classifier, bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision. Bootstrap aggregation or bagging (B+agg), is a general-purpose procedure for reducing the variance of a statistical learning methods.  Random forest algorithm Based on the bagging decision tree idea, we can define a new method called a random forest.      The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  Feature importance of using Random forest (RF) The significance of each feature in the input dataset can also be determined using Random Forest. Based on how much it helps to reduce impurity in the decision trees, the significance of each characteristic is assessed. The higher the contribution, the more important the feature is. To increase the model's performance, feature importance utilising Random Forest can be utilised to find the most pertinent features for classification and feature selection.  Voting Classifier:  An ensemble learning technique called a voting classifier combines the predictions of various separate classifiers to provide a final prediction. Several types of classifiers, such as Decision Trees, K-Nearest Neighbors, or Support Vector Machines, can be used individually. Each classifier is given one vote, and the final forecast is determined by a majority vote. Voting Classifier can increase prediction accuracy and robustness because it incorporates the benefits of various models while minimising the effects of their particular flaws. Stack Classifier: Another ensemble learning technique that aggregates the predictions of various separate classifiers is the Stack Classifier, which is more complex than the Vote Classifier. The first layer of a stack classifier comprises multiple separate classifiers that create predictions based on the input data. The second layer then integrates the previous layer's predictions to arrive at a final prediction. Several algorithms might be used at the second layer, including Decision Trees and Logistic Regression. Stack Classifier can increase the prediction's accuracy and generalizability by learning a more complicated decision boundary and minimising the chance of overfitting.  External resources and other learning materials    document (psu.edu)   https://youtu.be/D_2LkhMJcfY   https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-  scratch-in-python/    Random Forest Hyperparameter Tuning in Python - GeeksforGeeks   machine learning - GridSearchCV with Random Forest Classifier - Data Science Stack Exchange   python - Random Forest tuning with RandomizedSearchCV - Stack Overflow   Reflect on knowledge gained from the unit This topic some more advanced methods for the classification problems were discussed. This helped me to expand my knowledge on the modelling approach and I got familiar with some very popular and practical Machine Learnings to solve Classification problems.  The Hype Parameter Tuning methods are also very practical topics that can really impact the success of a machine learning project and are critical steps in the model development cycle.        