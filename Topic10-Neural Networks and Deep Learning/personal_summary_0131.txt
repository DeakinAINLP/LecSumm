 Ensemble Learning  A method where multiple models are trained to the solve the same problem based on slightly different subsets of data. For example, multiple decision tree models can be trained, then an average of the combined decisions can be taken to help reduce the overall variance of the model.  Bootstrap Sampling  A statistical method used for estimating the sampling distribution of an estimator by resampling from the original dataset. The method works by drawing multiple new samples from the original dataset. This technique involves drawing with replacement, meaning that a data point can be selected more than once for a given sample.  Adaptive Boosting (“AdaBoost”)  A machine learning algorithm used for classification problems which works by combining weak classifiers to create strong classifiers. It works by setting weights to classifiers and data points, forcing the classifiers to focus on the observations that are difficult to classify correctly.  Bagging  Also known as bootstrap aggregating, bagging is a method which involves generating multiple subsets of the original data, training a model on each subset, and then combining their predictions. This process creates greater diversity in the model and helps reduce variance while mininising the movement in bias.  Random Forests  Extending from the bagging principle, the random forest algorithm generates multiple decision trees and the average of the output from these decision trees is used as the final prediction. A key difference between random forests and decision trees is that the process of finding the root node and the splitting the feature nodes runs randomly in the random forest algorithm.  Out of Bag Error (“OOB”)  OOB is used to the measure the prediction error of “bagging” based methods, such as Random Forest. When the subsets of the data are created to train the model, generally one-third of the data is not used and are referred to as “out-of-bag” instances. These instances can then be used to test the accuracy of the algorithm without needing a separate validation set.  Voting and Stack Classifiers  Voting classifier is technique used to combine the prediction of various separate classifiers to provide a final prediction, where the final forecast is determined by a majority vote.  Stack classifier is a technique that aggregates the predictions from various separate classifiers. This technique involves a base level model which is trained on the complete training dataset, and then the next level model (or stack) is fitted on the outputs from the based model. The second level model is then used for the final predictions.  