Linear and non-linear classifiers     Random forest  o  Two Parameter; number of T, number of Features o  Training/testing  ▪  Testing (best result); Lower correlation b/w trees, higher strength  independently.  ▪  * Increasing features is good  o  Feature importance/ Hyper-parameters for classifiers (Improve prediction accuracy)  ▪  Voting classifier; Decision Trees, K nearest neighbour    Can increase prediction accuracy  ▪  Stack Classifier; Logistic regression    Minimises chance of over fitting    Bootstrap sample through bootstrap estimation  o  Uses smaller sample generated from a larger sample o  Less Variance & more accurate results  ▪  Uses confidence intervals through error bars  o  Generating N data instances and randomly re-drawing N times with replacement    AdaBoost  o  Steps   Initialise weights   Train weak classifier   Evaluate classifier   Calculate classifier weight   Update weight  o  Stumps = weak learners o  Can be used in multi-class data  ▪  SAMME  o  Bagging/Bootstrap Aggregation o  Reduces Variation o  Determines an improved decision boundary by cleaning up different independent  classifiers from different boundaries ▪  Uses multiple classifiers  