 Topic 9: Nonlinear models (Boosting and random forest)  Learning objectives:  Analyze performance of ensemble classifiers with respect to a single model Construct a multi-layer neural network using a backpropagation training algorithm to  demonstrate data representation, classification and evaluation skills  Learning summary: Ensemble learning:  -  Ensemble learning is the process by which multiple models, such as classifiers or  experts, are strategically generated and combined to solve a particular computational intelligence problem.  To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data. Then when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification). This is called the ensemble method.  A popular ensemble method is the Random Forest  Bootstrap estimation:  A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger  sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results.  Resampling method from statistics Often used to get error bars on estimates Bootstrap samples are generated as follows: Take a dataset with N data instances. Create a re-sample version of the dataset by randomly drawing N times with  replacement. This provides a bootstrap sample.  AdaBoost:  AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier. The algorithm has the following steps: Inputs:  o  X: dataset of features y: vector of corresponding labels (+1 or -1) T: number of iterations (i.e., number of weak classifiers to train)  Outputs:   List of weak classifiers, each with an associated weight  Step 1: Initialize weights Step 2: Train weak classifier Step 3: Evaluate classifier   Step 4: Calculate classifier weight Step 5: Update weights  Bagging:   In contrast to using just one classifier, bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision.  Bootstrap aggregation or bagging (B+agg) is a general-purpose procedure for reducing  the variance of a statistical learning methods.  Random forest algorithm:  The random forest classifier creates a set of decision trees from randomly selected  subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  Random forest builds on the idea of bagging. Each tree is built from a bootstrap sample of data. Node splits are calculated from random feature subsets to make sure each of the trees is as independent as possible. Then we randomly pull out a subset and try work with the subset. Whenever it needs to split to from the tree, based on the best feature, we choose the best feature from the subset In random forest:  o  All trees are fully grown with no pruning We are dealing with two parameters:  Number of trees (T); if the value is raised too much and make too many  trees, it would likely get trapped in the overfitting problem.  Out of bag error and feature importance  Number of features mtry  o  It is possible to estimate the goodness of a bagged model in the same way as every model in machine learning. Out of Bag is equivalent to validation or test data.  Each tree in a random forest is trained on a bootstrapped sample. It can be shown that  on average, each bagged tree makes use of 2/3 of the training instances. The remaining 1/3 of the instances are referred to as the out-of-bag (OOB) instances.  Advantages/Disadvantages of Random Forest:  Random forest is fast to build and even faster to predict Fully parallelizable since you can run trees in parallel to go even faster! Ability to handle data without pre-processing. You are not always required to  Normalize your dataset before running this method  Data does not need to be rescaled, transformed, or modified! (Resistant to  outliers)  Automatic handling of missing values (a property of decision trees)  Less interpretable results than a single decision tree  Advance topics  Feature importance of using Random forest (RF):  The significance of each feature in the input dataset can also be determined using  Random Forest. Based on how much it helps to reduce impurity in the decision trees, the significance of each characteristic is assessed. The higher the contribution, the more important the feature is. To increase the model's performance, feature importance utilising Random Forest can be utilised to find the most pertinent features for classification and feature selection.        