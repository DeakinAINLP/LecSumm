 Ensemble Learning  - Probable outcomes of developing machine learning models  - Sometimes weak and inaccurate - Some perform better on specific occasions.  - Ensemble learning  - Generating and combining multiple models (classifiers or experts) to solve a  particular computational intelligence problem.  - Consider this scenario:  - We know that a single decision tree might not perform well  - But, it is super fast - What if we create multiple trees?  - We just have to make sure that they do not all learn the same thing.  - Problem with single decision tree  - Risk of overfitting or increased variance  -  To reduce the variance of unstable learning methods (such as Decision Tree) use ensemble method  - -  Train multiple decision trees, each with slightly different subsets of data Take their combined decision  - Classification - Voting  - Regression  - Averaging  - Random Forest  - Ensemble of Decision Tree  Bootstrap Estimation - 2:16pm  - A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger  sample. It uses a resampling method found in statistics.  - - Bootstrap in many cases can result in less variance and more accurate results.  -  Bagging  - Bootstrap aggregation or bagging  - General-purpose procedure for reducing the variance of a statistical learning  method.  - Given a set of ğ‘› independent estimates  each with variance ğœ2, the variance of their mean is ğ‘›âˆ’times lower (ğœ2/n). - When the estimates are not independent, reduction in variance is  lower.  - Uses multiple classifiers trained on different under-sampled subsets and then allows  these classifiers to vote on a final decision.  -  Random forest algorithm - 2:30pm - Random forest classifier  - Creates a set of decision trees from randomly selected subset of training set.  Form the tree based on the best feature from the subset  - Each tree is built from a bootstrap sample of data - - Repeat these steps ğ‘‡ times, where ğ‘‡ is the number of the trees -  Impact on bias  -  Increases. Why?  - Aggregates the votes from different decision trees to decide the final class of the  - Uses subsets of features in different independent trees  test object.  -  In random forest:  -  all trees are fully grown and no pruning  - we are dealing with two parameters: number of trees (ğ‘‡);  -  - number of features (ğ‘šğ‘¡ğ‘Ÿğ‘¦)  Increasing the number can result in overfitting problem.  - - - Random forest algorithm Training - 2:42pm  - For each of ğ‘‡ iterations (ğ‘‡ is the number of trees you may like to build):  - Select a new bootstrap sample from the training set - Build an un-pruned tree on this bootstrap sample - At each internal node of the tree, randomly select ğ‘šğ‘¡ğ‘Ÿğ‘¦ features and determine  the best split using only these features.  -  Increasing number of features (ğ‘šğ‘¡ğ‘Ÿğ‘¦) for each split:  - -  Increases correlation Increases strength of single trees  -  Random forest algorithm Testing - 2:47pm  - Output overall prediction as a mean (or majority vote) from all individually trained trees. -  In random forest, the error rate depends on:  - Correlation between trees (lower is better) - Strength of single trees (higher is better)  Out of bag error - 2:53pm  - Estimate the goodness of fit of a bagged model  - Out of Bag has been introduced which is equivalent to validation or test data.  - Each tree in a random forest is trained on a bootstrapped sample.  - On average, each bagged tree makes use of 2/3 of the training instances.  -  The remaining 1/3 of the instances are referred to as the out-of-bag (OOB) instances.  - We can predict the response for the ğ‘–ğ‘¡â„ observation using each of the trees in which that  observation was OOB.  - Average them to find the out of bag error  -  Feature importance - 3:00pm  - Each node in the tree (single feature based split)  - How much each feature decreases the weighted impurity (Gini or Entropy) of the  tree This provides rank of all features used in a tree  -  -  In Random Forest  - Multiple trees  - Multiple rank values of single feature  - Average impurity decreasing scores across all trees for getting overall score and  rank of the feature  Advantages/Disadvantages of Random Forest - 3:03pm  -  Fast to build and even faster to predict!  Fully parallelizable since you can parallelly run the trees to go even faster!  - - Decision Tree complexity is ğ‘‚(ğ‘‘Ã—ğ‘›Ã—ğ‘™ğ‘œğ‘”ğ‘›). - A random forest with ğ‘‡ trees would have ğ‘‚(ğ‘‡Ã—ğ‘‘Ã—ğ‘›Ã—ğ‘™ğ‘œğ‘”ğ‘›), where ğ‘‘ is the number  of features and n is the number of data points  - Ability to handle data without pre-processing  - Not always required to normalize your dataset before running this method data does not need to be rescaled, transformed, or modified! (Resistant to - outliers) automatic handling of missing values (a property of decision trees)  -  -  Less interpretable results than a single decision tree.  Ensemble Learning - 3:15pm  - Boosting  -  Training a bunch of individual models in a sequential way. Each individual model learns from mistakes made by the previous model.  - - AdaBoost - GradientBoost - XGBoost (extreme Gradient Boosting)  AdaBoost - 3:21pm  - AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier. The algorithm has the following steps:  -  Inputs:  - X: dataset of features - -  y: vector of corresponding labels (+1 or -1) T: number of iterations (i.e., number of weak classifiers to train)  - Outputs:  -  List of weak classifiers, each with an associated weight  -  -  Feature importance of using Random forest (RF) - 3:25pm  -  The significance of each feature in the input dataset can also be determined using Random Forest.  - Based on how much it helps to reduce impurity in the decision trees  Voting Classifier - 3:28pm  - An ensemble learning technique called a voting classifier combines the predictions of  various separate classifiers to provide a final prediction.  - Several types of classifiers, such as Decision Trees, K-Nearest Neighbors, or Support  Vector Machines, can be used individually.  - Each classifier is given one vote, and the final forecast is determined by a majority vote. - Voting Classifier incorporates the benefits of various models while minimising the effects  of their particular flaws.  Stack Classifier - 3:29pm  - Another ensemble learning technique that aggregates the predictions of various  -  -  separate classifiers is the Stack Classifier The first layer of a stack classifier comprises multiple separate classifiers that create predictions based on the input data. The second layer then integrates the previous layer's predictions to arrive at a final prediction.  - Several algorithms might be used at the second layer, including Decision Trees and  Logistic Regression.  - Stack Classifier can increase the prediction's accuracy and generalizability by learning a  more complicated decision boundary and minimising the chance of overfitting.   