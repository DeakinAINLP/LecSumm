Ensemble learning: combining multiple classifiers to solve particular computational intelligence program. Bootstrap example: is a smaller sample taken from a larger sample through resampling method. Adaptive Boosting: Also known as adaboost â€“ a machine algorithm that combines weak classifiers to create a strong classifier. Used for classification problems. Algorithm steps:  1.  Initialize weights. 2.  Train weak classifiers. 3.  Evaluate classifiers. 4.  Update weights.  Bagging: uses multiple classifiers trained on different under-sampled subsets. Reduces variance of a statistical learning methods. Random forest  -  A set of decision trees made up of randomly selected subsets of the training dataset. -  Each tree is a bootstrap sample of data. Node splitting is calculated from random  feature subsets to ensure each tree is independent.  -  The process of finding the root node and splitting the feature node is random. -  Will result in a slightly higher bias. -  Training: First select a new bootstrap sample from the training set. Then build n unpruned tree using the sample. For each internal node, randomly select features to determine the best split using those features. -  Testing: Output overall prediction as a mean from all individually trained trees. -  Error rate: dependent on three factors:  1.  Correlation between trees with lower correlation desired 2.  High strength in single trees 3.  Increasing number of features per split.  -  Advantages  o  Data does not need to be rescaled/transformed o  Data does not need to be normalized o  Fast to build and gain a prediction  -  Feature importance: the importance of each feature is an important factor. The  contribution of each feature towards mitigating impurity will determine its important. The higher the contribution, the higher the importance.  Out-of-bag error: a tool that measures the prediction error of ensemble models that use bagging.  