1) Machine learning experts utilise a multitude of models in an ensemble approach to predict outcomes for each data point. Different models' forecasts are treated as distinct votes. Therefore, the forecast produced by the majority of models is considered to be the final prediction. 2) Bootstrap aggregating, often known as bagging, is a machine learning ensemble approach that makes use of bootstrap sampling. As a result, overfitting is reduced, and machine learning algorithms are more stable. Bagging is the process of extracting replacement subsets of a dataset in a predetermined number and size. 3) The Boosting approach known as AdaBoost algorithm, sometimes known as Adaptive Boosting, is used as an Ensemble Method in machine learning. As the weights are reassigned to each instance, bigger weights are given to examples that were mistakenly categorised, thus the name "adaptive boosting." 4) An ensemble learning approach called bagging, often referred to as Bootstrap aggregating, aids in enhancing the efficiency and precision of machine learning algorithms. It lowers the variance of a prediction model and is used to handle bias-variance trade-offs. 5) An example of a is the Boosting algorithm. In theory, the Boosting method may be used with any classification or regression procedure, however it turns out that tree models are particularly well suited. 6) A supervised machine learning algorithm is random forest. Using a few randomly chosen subsets of the training data, this technique builds a collection of decision trees and selects predictions from each tree. The random forest algorithm then chooses the ideal response through voting. 