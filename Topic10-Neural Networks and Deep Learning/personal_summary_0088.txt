Ensemble Learning  ● A ML technique that combine several individual models / weak learner to create a  stronger and more accurate predictable model  ● Ensemble method: The same training data are used to train multiple kinds of base  models, which are then combined to produce a final prediction  Bootstrap estimation  ● a statistical method that resamples the initial data to determine the variability and  ●  uncertainty of a sample statistic Involves creating several bootstrap samples by repeatedly taking samples with replacement from the initial data set. The size of each bootstrap sample equals that of the initial dataset  AdaBoost (Adaptive boosting)  ● ensemble learning method that combines multiple weak learners to create a strong  predictive model  ● AdaBoost's basic concept is to train weak learners in a sequential manner using  various weighted versions of the training data  How does AdaBoost works ?  1. Assign equal weight to all training instances 2. Use the weighted training data to train a weak learner 3. Calculate the weak learner weighted error rate 4. Set the training instance weights 5. Decrease the weights of correctly classified instances. 6. Repeat steps until condition is met 7. Combine every weak learner's predictions using a weighted voting system where the  weights are based performance of each weak learner during training  8. Final prediction is created by combining the weak learners predictions based on the  weights  Bagging  ● Bagging combines many classifiers trained on different under-sampled subsets  compared to just one, allowing these classifiers to provide votes for the final decision  ● Goals to increase machine learning models' accuracy and stability by combining  different independently trained models.  Random Forest Algorithm  ● Forest classifier creates a collection of decision trees starting from a set of random  chosen subsets of the training dataset  ● To find out the final class of the test objects it will then combines the votes from  multiple different decision trees  ● Random forest algorithm uses randomisation to determine the root node and split the  feature nodes  ● The ability of decision trees is combined with the logic of bagging in the ensemble  learning method in order to produce a stronger and more accurate predictive model  Advantage  Disadvantage  Faster / quicker in making predictions compared to other algorithms  More difficult to read the individual trees to understand the importance of the basic features.  No pre-processing require  The ability to handle missing values in the dataset which make it more convenient to tackle with incomplete data   