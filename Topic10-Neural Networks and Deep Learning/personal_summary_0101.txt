 This topic in the Machine Learning unit we learnt:  -  Ensemble learning is the concept of combining multiple trained models to solve a regression  or a classification problem, with the idea being by strategically training those models, better  performance  could emerge  than from a single more complex model.  This could be seen in  Decision trees where variance could be reduced by combining multiple trees together.  -  AdaBoost  and  Bagging  are  examples  of  methods  that  use  Ensemble  learning  as  both  use  multiple weaker models and combine their output to produce the final output.  -  Random  forest  algorithm  is  another  Ensemble  method.  There,  a  set  of  decision  trees  are  trained  using  subsets  of  the  total  training  data.  Then,  each  tree  votes  and  the  votes  are  aggregated  to  generate  the  final  classification.  The  subsets  are  randomly  generated  to  strengthen  the  independence  between  the  trees  and  the  trees  are  not  pruned  after  their  creation.  -  Each tree in random forest is trained using a subset of the total data. Out-of-Bag data entries  are data entries that werenâ€™t used in the training process. These data are used as the test set  for trees in the random forest model.  -  Random forest is fast and parallelizable as the trees within the model predict independently.  Random forest can handle data without any needed pre-processing such as scaling or outliers  handling.  -  Voting  classifier  is  another  Ensemble  learning  classifier.  It  uses  a  combination  of  different  classifiers such as Decision trees and SVMs. Each classifier gives a vote regarding the output  and the final output is determined by the majority. By combining different types of classifiers,  Voting classifier take advantage of those classifiers while avoid their challenges.  -  Stack  classifier  is  also  an  example  of  the  Ensemble  learning  concept.  The  Stack  classifier  consists of different classifiers within two layers. Classifiers in the first layer takes the data as  their inputs while the second layer classifiers take the outputs of the first layer classifiers as  input.  