 Topic -9 gave the details of the basics of  non-linear models including Boosting and random  forest. Some of the important points are listed below:  1.  Ensemble learning is a machine learning paradigm where multiple learners (also known  as base or individual learners) are trained to solve the same problem. Instead of making  a  prediction  separately,  these  learners  are  combined  in  some  way  to  make  a  final  prediction.  The goal  of  ensemble methods is  to  improve the predictive performance  compared to a single learner [1].  2.  Bootstrap  estimation  is  a  powerful  statistical  method  for  estimating  the  sampling  distribution  of  an  estimator.  It  involves  repeatedly  resampling  observations,  with  replacement, from the original dataset and recalculating the estimate of interest [4].  3.  AdaBoost, short for "Adaptive Boosting", is a powerful ensemble learning technique  that combines multiple "weak learners" to create a "strong learner". A weak learner is  any machine learning algorithm that performs slightly better than random guessing [5].  4.  Bagging,  short for bootstrap aggregating, is  an ensemble learning technique used to  improve  the  stability  and  accuracy  of  machine  learning  algorithms.  It  also  helps  to  reduce overfitting and variance.  5.  Random Forest is an ensemble machine learning algorithm that follows the bagging  technique. It is based on the concept of decision trees, where multiple decision trees  are created, and the final prediction is the most common prediction from all the trees  (majority voting) [6].   