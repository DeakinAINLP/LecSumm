Summary:  9.2:  Ensemble learning involves using multiple models which are combined to solve a problem. One popular method for this technique is the random forest algorithm.  9.3:  A bootstrap sample is bootstrapped from a larger sample. This is intended to reduce the variance. The resampling is done by randomly drawing a number of times equal to the data instances with replacement.  9.4:  Adaptive boosting (AdaBoost) is a ML algorithm applied to classification problems. It combines weak classifiers to create stronger ones.  9.5:  Bagging utilises the multiple classifiers trained on different under-sampled sets. The classifiers then vote on a final decision. Bootstrap aggregation of these results enables reducing variance of learning methods.  9.6:  On the basis of the bagging results, a random forest can be defined. This is a classifier which creates a set of decision trees from random subsets in the training set. The votes from these decision trees are aggregated themselves to classify the instance. This process increases bias.  9.7:  The ability of a bag model can be estimated like other models in ML. Out of bag is equivalent to test data in this case.  Random forest can be built and predict quickly, trees can be run in parallel, requires no pre-processing of data, rescaling, normalisation and automatically handles missing values. However, its results are harder to interpret compared to single decision trees.          9.8:  Feature significance can be determined with random forest, this is done by measuring how it helps to reduce impurity in the decision trees. This process can increase performance by helping select pertinent features for use in the model.  