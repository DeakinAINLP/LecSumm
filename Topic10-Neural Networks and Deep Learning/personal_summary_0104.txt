 ● If a single Machine Learning model is not enough to make a decision, we can combine multiple models together to form something known as an Ensemble model. Random Forest is an example of an ensemble ML model where it randomly generates multiple decision trees and combines their results together to form the ultimate model. The other example of ensemble models is AdaBoost, Bagging, Extra Trees, Gradient Boosting, Voting, Stacking, etc.  ● In the process of training multiple models at once, we need to make sure that all of these models learn something different from different subsets of data. Creating a subset from a large set of data is known as bootstrapping and this bootstrapped data is then used to train a number of models in the ensemble model.  ● AdaBoost stands for Adaptive Boosting. This ML model fits multiple copies of the same classifier on a dataset in such a way that poor-performing classified models are used as corrections for the next model that fits the same dataset. In this manner, it “boosts” the performance of the same model over and over again until it reaches a stable point. ● Bootstrap aggregation, also known as bagging, is another ensemble ML model which, in  contrast to AdaBoost, makes use of multiple different classifiers to train different bootstrapped data and then the final outcome is the vote of these models combined together. Here, each model is trained on a sample and by itself it may not carry an importance but since it is trained on a subset, it can predict the outcome for that subset of data really well.  ● Random Forest, as stated before, generates a forest of decision trees grown on subsamples of data and then a vote is taken for the final outcome of the required dataset. It is very similar to Bagging. Each DT in Random Forest undergoes no pruning. This makes generating the trees very fast.  ● It is also possible to combine the outputs of different types of classifiers such as Decision Tree, K-Nearest Neighbors or Support Vector Machines and take a vote from all of them to conclude the final outcome. This is called Voting Classifier  ● Another method is to create a layer of classifiers that have some output and then  another layer of classifiers learn from the previous layer and so on and ultimately stack the layers. This is known as Stacking  ● Voting and Stacking are advanced-level applications of classifiers  