Ensemble Learning  Ensemble learning is the process of training a number of classifiers from the available data, then  using their combined output to make a prediction.  The output is typically combined by majority vote for classificaiton problems, or by averaging for  regression problems.  The advantage of this approach is that variance is reduced when compared with a single decision  tree prediction.  The challenge of ensemble learning is selecting data to train different classifiers so that they are, in  fact, different. This is acheived by using slightly different sets of data.  Bootstrap Estimation  Bootstrap estimation is a technique used to resample data.  It is often used to determine confidence intervals for a given sample of data.  Bootstrapping is performed as follows:  . For a dataset of size N, draw N random samples. A given sample may be drawn more than  once.  . Calculate the result form the new sample (eg. class for classification or mean for regression).  . Repeat as many times as necesary.  This process results in a distribution of results that can be used to assess the confidence of the  results from any particular sample.  It can also be used to reduce model variance by selecting the modal or mean result form the  increased samples.  Adaptive Boosting (AdaBoost)  AdaBoost is a classification algorithm combines predictions from many weak learners to predict an  overall result.  The contribution of each weak learner is weighted.  Predictions are made by taking a weighted majority across all of the weak learner.  Each learner is made with consideration of the previous learners mistakes. They are generated as  follows:  . Allocate each sample equal weighting  . Generate a series of weak classifiers (typically trees with one level - stumps)  . Evaluate classifier performance.  . Calculate the weighting of the tree based on its total error:  . Update sample weights. The weight of samples incorrectly classified by the previous learner is  increased. The weight of samples correctly classified is decreased.  For an incorrectly predicted sample:  new sample weight = current sample weight × elearner weight  For a correctly predicted sample:  new sample weight = current sample weight × e−learner weight  . Normalise new sample weights  . Randomly resample data set weighted by new sample weights  . Repeat.  Bootstrap Aggregating (Bagging)  Bagging trains several different classifiers on bootstrap datasets of the original dataset, then predicts  the majority result.  It produces a resulting model with a variance equal to the mean of the variance of each of the input  models.  The number of data points in each of the bootstrap samples is typically a subset of the data points  available.  The samples not used are called the Out-Of-Bag dataset. This can be used for validation of the  classifiers built not using a given sample.  The proportion of out of bag samples classified correctly is the accuracy of the model.  Random Forest  A random forest is a number of decision trees trained on bootstrap datasets that increases prediction  accuracy over using a standard decision tree. It is trained by:  . Select bootstrap dataset  . Generate unpruned decision tree on the bootstrap sample.  . For each node in the tree, only select  mtry  features to determine the split.  The prediction is the mean or mode of the trees that have been trained.  A random forest is evaluated by its error rate. The error rate is determined by:  Tree correlation. Increased correlation between trees will increase error rate.  Individual tree performance. Poorer individual tree performance will reduce overall  performance.  This drives a trade off as the number of features for each split is set. A higher number of features  increases tree correlation, but also increases the strength of a particular tree.  As for bagging, out of bag samples are used to evaluate a random forest model.  Each tree uses 2/3 of the available data points, leaving 1/3 for validation.  For each point in the data set, each of the trees where that point was out of bag makes a prediction.  The proporation of these predictions that is correct is the model accuracy.  Advantages:  Low complexity:  observations and  O(d × n × log n) T  where d is the number of trees.  is the number of features, n  is the number of  Trees are independent, thus simple parallelizable.  Resistant to outliers, thus pre-processing is reduced.  Missing values are automatically handled.  The major disadvantage over a single tree is that results are less interpretable.  Random Forest for Feature Selection  There are two objectives to feature selection which seek to meet different purposes.  The first objective is to find the minimal optimal set of features that allow accurate predictions  to be made. This is useful to determine efficient solutions to machine learning problems by  determining the features which are most useful for accurate predictions.  The second objective is to determine all relevant features for an outcome. This is useful for  cases where the analysis is to identify correlations between some set of features and a given  outcome. An example of this is finding all relevant genes for a particular biological feature.  The all relevant features problem is challenging, and requires all feature combinations to be analysed  to solve comprehensively.  A random forest approach can be used to hueristically solve the all random feature problem.  The varying features used in each of the classifiers generated for a given forest allow the important of  different features to the outcome to be determined with greater confidence as a feature that weakly  contributes is likely to become apparant in combinations without features that strongly contribute.  The risk of this approach is that random correlations between a feature and outcome are detected.  This risk can be addressed by generating a random data set for the features under consideration and  determining whether the same correlation is detected.  