The topic of this topic is ensemble learning methods. An ensemble model is just the aggregate prediction of many models (predictors). We can combine many different models, or many version of the   same   model.   We   can   also   train   each   model   on   different   data   sets,   overlapping   and   non overlapping sub sets. We then make a final prediction using the combined predictions of those many models, either through voting i.e. we  choose  the majority prediction, or through averaging i.e. compute the mean of all the predictions. Ensemble methods can produce strong learners from many weak learners if there is sufficient diversity (minimal correlation) among the weak learners i.e. each weak learner learns something different. Ensemble methods can create highly non-linear models from simpler; even linear, sub models. Ensemble methods tend to introduce some bias from the models trained on subsets of features or subsets of data, but that is often offset by the diversity in each of the models, and overall achieve lower variance than non ensemble methods.  READING To revise, I read the relevant sections of Hands-on Machine Learning with Scikit-Learn, Keras & relevant   section   of   the   course   notes. TensorFlow   by   Aurelien   Geron   and   the  The main ensemble methods are Bagging, Boosting, Stacking & Voting.  Boosting sequentially trains multiple versions of the same model, but only 1 model is trained at a time, and at each subsequent step, the new model attempts to correct errors the previous model made. The way errors are corrected for depends on the particular boosting method. AdaBoost for instance corrects each models errors by re-weighting the data so that the samples the previous model got wrong are weighted more heavily than other samples and therefore any errors on those samples will have a stronger influence on the weights of the current model being trained. Each model is given a vote that’s weighted by how accurate it is on it’s training predictions. The final prediction is whichever prediction has the highest sum of weighted votes.  Bagging trains many models that all use the same version of an algorithm, but each model is given a random selection of samples chosen from a larger data set. Each model sees a subset of the data that are randomly chosen with or without replacement i.e. with replacement means the combination of samples any given sub model sees is likely to be different to any other model, but any given data sample may be seen by many different models. Using random subsets of features is actually called a random subspace method. And if we use random subsets of both features and data it’s called a random patch method. A Random Forest is an ensemble of Decision Trees trained with the random patches method.  Stacking  combines multiple layers of multiple models. The first layer consists of several models that all use different algorithms but are all trained on the same data. Then a subsequent layer is trained to combine the results from all the models in the first layer. It is essentially like a DNN. As many layers can be combined as desired but deeper models are prone to overfitting.  Voting is just a single layer stack classifier.   