 This topic we covered Ensemble learning, which is where multiple models are combined to solve a particular problem. We covered bootstrap estimation, which is a technique for estimating quantities by averaging estimates from multiple smaller data samples. AdaBoost, Adaptive Boosting, is a machine learning algorithm for classification problems. Bagging uses multiple classifiers uses multiple classifiers trained on different under-samples subsets, then “votes” on a final decision.  We also covered Random Forest algorithm, which is a classifier that creates a set of decision trees from randomly selected subsets of the training data. We also went into detail on using Random Forest algorithm in Python.  