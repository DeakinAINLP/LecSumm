  Ensemble Learning  The classifier you designed for a dataset may be weak and inaccurate. You may have designed many classifiers, but some of them may be inaccurate, and some may perform better in certain situations.  You may have faced this phenomenon in your own machine learning practice.  Ensemble learning is a process in which multiple models, such as classifiers and experts, are strategically generated and combined to solve a speciﬁc computa?onal intelligence problem. Consider the following scenario:  But it's super fast.  §  We know that a single decision tree may not perform well. § §  What if you learn multiple trees? §  We just need to make sure that not everyone learns the same thing.  To reduce the variance of unstable (highly dispersive)  learning methods, such as decision trees, you can train multiple decision trees, each containing a slightly different subset of data. Then, when you classify/regress, you make a decision to combine them (through averaging regressions or voting on classifications). This is called the ensemble method.  A popular ensemble technique is the random forest (Breiman 2001).  The variance of these ensemble models has been shown to be small. The key is to try to design an ensemble model that can train a variety of independent models using slightly different subsets of data. Entering data into a model can be challenging.   Adaboost  AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier. The algorithm has the following steps:  Inputs:    § § §  X: dataset of features y: vector of corresponding labels (+1 or -1) T: number of iterations (i.e., number of weak classifiers to train)  Outputs:  §  List of weak classifiers, each with an associated weight    Bagging  Also as you can see in the figure below, one could either choose to classify with 1 tree (the red original tree in top left) or 11  trees based on the dataset. Usually it’s more powerful to use bagging decision trees so that you can   utilize as much information as possible.   Random Forest Algorithm    Based on the idea of a bagging decision  tree, you can define a new method called a  random forest.    The random forest classifier creates a set of decision trees from a randomly selected subset of the training dataset. It then aggregates votes from the various decision trees to determine the final class of the test object.    The difference between the Random Forest algorithm and the Decision Trees  algorithm is that the Random Forest algorithm randomly performs the process of finding the root node and splitting the feature node.    Random Forest is built on the idea of bagging. Each tree is built from a data  bootstrap sample. Node splitting is calculated from a random subset of features to    ensure that each tree is as independent as possible  . Next, you'll randomly extract a subset and try to manipulate that subset. Whenever you need to split from the tree based on the best features, select the best features from the subset. Ultimately you need to perform  the following steps times, where is the number of trees. If you're wondering if this model increases bias, you're right. I will! Because it uses a subset of features in a different, independent tree, the bias of the model may increase slightly.      According to a useful rule of thumb, the number of features is:    In the random forest:  § §  All trees are fully grown without pruning. It handles two parameters.  o  Number of trees (T);  Note that if you raise this value too much and create too many trees, you can run into overﬁMng  problems.  o  Number of features m mtry  Let T be the number of trees to build. Training  For each  T iteration (T is the number of trees to build):  1. 2. 3.  Select a new bootstrap sample from the training set this bootstrap sample builds an untrimmed tree At each internal node of the tree, randomly select meter mtry to determine the opOmal division using only these features.  Test  You can easily output the overall prediction as an average (or majority vote)  from all individually trained trees.  Now let's look at the error rate.  In a random forest, the error rate is determined by the following factors:  o o o  CorrelaOon between trees (lower is be3er) The strength of a single tree (the higher the be3er) Feature increase for each split: Increase correlaOon Increasing the strength of a single tree  § §  As you can see, as with most concepts in machine learning, there are trade-offs here. By using more features to create trees, the strength of a single tree is increased and the correlation between trees increases.     Out-of-bag errors and the  importance of func?ons    In the same way as all models in machine learning, we can estimate the goodness of a bagged model. An out-of-bag is equivalent to validation or test data.    Each tree in the random forest is trained on a bootstrapped sample. Each tree  bagged is, on average,  2/3 of the training instance. Of the remaining 1/3  instances, they are called  out-of-bag (OOB) instances. You can predict the reaction to  i—The third observation using each tree (that observation was OOB). This gives an approximate result of averaging   the i-th observa?on predicted for B/3.    As you can see from the image above, data points are sampled into the training set, and unused data points make up the test set. Next, you'll use this training data to build a random forest. Later, you  can evaluate the model using invisible test data. Similar to cross-validation, performance estimates using out-of-bag samples are calculated using data that was not used for training. If the data is processed in a way that communicates information between samples, the estimates  will (probably) be biased.  Merri+ Demeli+o of Random Forest  Let's summarize the important facts about random forests.  §  Random forests are faster to build and predict even faster.  The complexity of a decision tree is ○(d×n×log n). Random Forest T  Trees will do so ○(T×d×n  ×log n ）。  n is the number of features, and is the number of data points  § §  § § §  Trees can be run in parallel to run even faster, making them fully parallelizable. The ability to process data without preprocessing. It is not always necessary to normalize the dataset before execuOng this method. You don't need to rescale, transform, or modify the data. (Resistant to outliers) AutomaOc handling of missing values (decision tree properOes) Consequences that are harder to interpret than a single decision tree       Advanced topics  The importance of the ability to use random forest (RF)  The importance of each feature in the input dataset can also be determined using a random forest. The importance of each characteristic is evaluated based on how much it helps reduce impurities in the decision tree. The higher the contribution, the more important the feature. To improve model performance, you  can use feature importance using random forests to find the most appropriate features for classification and feature selection.  Vote classiﬁer: An ensemble learning technique called a vote classiﬁer combines predic7ons from diﬀerent individual classiﬁers to provide a ﬁnal predic7on.   Several types of classiﬁers can be used independently, including decision trees, k-nearest  neighbor methods, and support vector machines. Each classiﬁer is given  one  vote, and the ﬁnal predic7on is determined by majority vote. Vo7ng classiﬁers incorporate the beneﬁts of diﬀerent models and can improve the accuracy and robustness of predic7ons because they minimize the impact of that par7cular ﬂaw. Stack classiﬁer: Another ensemble learning technique that aggregates predic7ons from diﬀerent individual classiﬁers is the stack classiﬁer. This is more complex than a vote classiﬁer. The ﬁrst layer of stack classiﬁers consists of mul7ple separate classiﬁers that make predic7ons based on input data. The second  Cer then integrates the predic7ons from the previous 7er to arrive at the ﬁnal predic7on.  The second layer may use several algorithms, such as decision trees and logis7c regression. Stack Classiﬁer can improve predic7on accuracy and generalizability by learning more complex decision boundaries and minimizing the chance of overﬁIng.  