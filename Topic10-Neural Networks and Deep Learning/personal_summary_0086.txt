Summary: Through my study of Topic 9 I learnt by reading the content, watching the videos, taking notes and working through the activities. Evidence can be seen below.  Activity 9.2:  1.  Ensemble learning 2.  A popular ensemble method is the Random Forest  Activity 9.3:  1.  Bootstrap estimation  Activity 9.4:  1.  AdaBoost, Adaptive Boosting for classification problems.  Activity 9.5: 1.  Bagging.  Activity 9.6:  1.  Random forest algorithm.  a.  Training b.  Testing  i.  Correlation between trees (lower is better) ii.  Strength of single trees (higher is better) iii.  Increasing number of features for each split:  1.  Increases correlation 2.  Increases strength of single trees  Activity 9.7:  1.  Out of bag error and feature importance 2.  Advantages/Disadvantages of Random Forest  a.  random forest is fast to build and even faster to predict b.  fully parallelizable since you can run trees in parallel to go even faster! c.  ability to handle data without pre-processing. You are not always required to  normalize your dataset before running this method  d.  data does not need to be rescaled, transformed, or modified! (Resistant to  outliers)  e.  automatic handling of missing values (a property of decision trees) f.  less interpretable results than a single decision tree  Activity 9.8: (SIT720) Feature importance of using Random forest (RF) Voting Classifier Stack Classifier  Activity 9.9: Random Forest in Python.  Activity 9.10: Random Forest in Python  Activity 9.11: Boosting with Python AdaBoost   Evidence: Active Classes  