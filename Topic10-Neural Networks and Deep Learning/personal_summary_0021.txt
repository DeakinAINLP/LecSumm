Summary (main points)  Ensemble learning  Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem.  To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data. Then when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification). This is called the ensemble method. A popular ensemble method is the Random Forest. It has been shown that the variance of these ensemble models are lower. The critical point is to try to design an ensemble model in which you can train different independent models with slightly different subsets of data. The way the data is fed into the models can be challenging.  The "forest" in Random Forest refers to a collection of decision trees. Each decision tree is built independently using a subset of the training data and a random selection of features. This randomness in building the trees helps to reduce overfitting, which is a common problem in decision tree algorithms.  The Random Forest algorithm works as follows:    Data Preparation: The first step is to prepare the training data. The data is divided into random subsets, known as bootstrap samples, through a process called bootstrapping. Each bootstrap sample is used to train a decision tree.    Building Decision Trees: For each bootstrap sample, a decision tree is constructed. The decision tree is built by recursively splitting the data based on different features and their values. The splits are chosen to maximize the information gain or minimize the impurity in the resulting subsets.    Random Feature Selection: At each node of the decision tree, only a random subset of features is considered for splitting. This randomness helps to create diversity among the trees in the forest and prevents overfitting.    Combining Predictions: Once all the decision trees are built, they make predictions  independently. For classification tasks, each tree votes for the class label, and the class with the majority of votes becomes the final prediction. For regression tasks, the predictions of all trees are averaged to obtain the final output.  Random Forest offers several advantages:    Robustness: Random Forest is less prone to overfitting compared to individual decision trees. By combining predictions from multiple trees, it reduces the impact of outliers and noisy data.    Feature Importance: The algorithm provides a measure of feature importance, which indicates the contribution of each feature in making predictions. This information can be helpful in feature selection and understanding the data.  Versatility: Random Forest can handle both categorical and numerical features without requiring  extensive data preprocessing. It can also handle missing values in the data.    Scalability: The algorithm can efficiently handle large datasets with a high number of features.  Random Forest has a wide range of applications, including but not limited to, image classification, sentiment analysis, fraud detection, and predicting stock prices. Its ability to handle complex tasks, provide robust predictions, and handle various data types makes it a popular choice among machine learning practitioners.  Bootstrap estimation  A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It can be used to estimate summary statistics such as the mean or standard deviation. It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data.    The bootstrap method involves iteratively resampling a dataset with replacement.   When using the bootstrap you must choose the size of the sample and the number of repeats.   The scikit-learn provides a function that you can use to resample a dataset for the bootstrap  method.  AdaBoost, which stands for Adaptive Boosting  AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique used as an Ensemble Method in Machine Learning. It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances. Boosting is used to reduce bias as well as variance for supervised learning. It works on the principle of learners growing sequentially. Except for the first, each subsequent learner is grown from previously grown learners. In simple words, weak learners are converted into strong ones.  The algorithm has the following steps:  Inputs:    X: dataset of features   y: vector of corresponding labels (+1 or -1)   T: number of iterations (i.e., number of weak classifiers to train)  Outputs:   List of weak classifiers, each with an associated weight  Bagging  In contrast to using just one classifier, bagging uses multiple classifiers trained on different under- sampled subsets and then allows these classifiers to vote on a final decision. Bootstrap aggregation or bagging (B+agg), is a general-purpose procedure for reducing the variance of a statistical learning methods. Note and remember that when the estimates are not independent, reduction in variance is lower.  Consider the following figure as an example of bagging. As you can see, different independent classifiers voted for different boundaries (light green), but if you take the aggregation of these boundaries (dark green) it will clean up and improve the final decision boundary.  Also as you can see in the figure below, one could either choose to classify with 1 tree (the red original tree in top left) or 11 trees based on the dataset. Usually it’s more powerful to use bagging decision trees so that you can utilize as much information as possible.  Random forest algorithm  The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  Random forest builds on the idea of bagging. Each tree is built from a bootstrap sample of data. Node splits are calculated from random feature subsets to make sure each of the trees is as independent as possible. Then we randomly pull out a subset and try work with the subset. Whenever it needs to split to from the tree, based on the best feature, we choose the best feature from the subset. Ultimately you have to do these steps T times, where T is the number of the trees.  If you are wondering whether this model increases the bias, you are right. It does! It uses subsets of features in different independent trees so it is likely to slightly increase the model bias.  In random forest:    all trees are fully grown with no pruning   we are dealing with two parameters:  o  Number of Trees (n_estimators): This parameter specifies the number of decision trees to include in the forest. Increasing the number of trees can improve the performance of the Random Forest up to a certain point. However, adding more trees beyond that point may not provide significant benefits and can increase computational cost. o  Number of Features Considered at Each Split (max_features): This parameter  determines the number of features randomly selected at each node for considering the best split. It controls the level of randomness in feature selection. The value of max_features can be set as a fixed number, a fraction of the total features, or a predefined value such as "sqrt" or "log2". A smaller value can reduce overfitting, while a larger value can provide more diversity among the trees.  These two parameters, along with other hyperparameters like maximum depth of the trees or minimum samples required for splitting, can be tuned using techniques like cross-validation to find the optimal values that yield the best performance for a given problem.  Out of bag error and feature importance  It is possible to estimate the goodness of a bagged model in the same way as every model in machine learning. Out of Bag is equivalent to validation or test data.  As you can see in the above figure, data points are sampled into a training set and unused data points make up a test set. Then we build the random forest by using this training data. Later we can evaluate our model using the unseen test data. Like cross-validation, performance estimation using out-of-bag samples is computed using data that were not used for learning. If the data have been processed in a way that transfers information across samples, the estimate will (probably) be biased.  RANDOM FOREST ADVANTAGES    Versatile uses    Easy-to-understand hyperparameters   Classifier doesn't overfit with enough trees  RANDOM FOREST DISADVANTAGES  Increased accuracy requires more trees     More trees slow down model   Can’t describe relationships within data  The significance of each feature in the input dataset can also be determined using Random Forest. Based on how much it helps to reduce impurity in the decision trees, the significance of each characteristic is assessed. The higher the contribution, the more important the feature is. To increase the model's performance, feature importance utilising Random Forest can be utilised to find the most pertinent features for classification and feature selection.  