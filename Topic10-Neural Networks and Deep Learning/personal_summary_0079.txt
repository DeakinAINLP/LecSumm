Topic 9 Nonlinear models Ensemble learning  ● Ensemble learning combines multiple models to improve accuracy and reduce  overﬁtting.  ● Techniques for combining models include bagging, boosting, and stacking. ● Bagging trains models on different subsets of the training data and  aggregates their predictions.  ● Boosting iteratively trains models that focus on previously misclassiﬁed  examples.  ● Stacking trains a meta-model that combines predictions from multiple base  models. AdaBoost  ● AdaBoost is a machine learning algorithm used for classiﬁcation and  regression analysis.  ● It combines multiple weak classiﬁers into a stronger one. ● The algorithm trains weak classiﬁers iteratively on the same dataset, focusing  on the misclassiﬁed examples of the previous classiﬁer.  ● During each iteration, the algorithm assigns weights to the training examples,  emphasizing the importance of misclassiﬁed examples. ● The ﬁnal output is the weighted sum of all weak classiﬁers.  Bagging  ● Bagging (short for Bootstrap Aggregating) is a machine learning algorithm  used for ensemble learning.  ● It involves training multiple models independently on random subsets of the  training data.  ● The algorithm uses bootstrap sampling to create multiple subsets of the  training data, each with replacement.  ● Each model is trained on a different subset of the data and produces a  prediction.  Random forest algorithm  ● Random Forest is an extension of Bagging and ensemble learning algorithm  used for classiﬁcation and regression analysis.  ● It involves creating multiple decision trees on randomly selected subsets of  the training data.  ● Each decision tree is trained on a different subset of the data, and at each  node, the algorithm selects a random subset of features to split on. ● The ﬁnal output for a new input is the majority vote (for classiﬁcation) or average (for regression) of the predictions made by all decision trees.  ● Random Forest helps reduce overﬁtting and improve generalization  performance.  Out-of-bag error and feature importance  ● Out-of-bag (OOB) error is a technique used in Random Forest to estimate the generalization error of the model without the need for a separate validation set.  ● OOB error is calculated by measuring the classiﬁcation error or mean squared error of the predictions made by each decision tree on the training examples that were not included in the bootstrap sample used to build that tree. ● OOB error is a useful metric for evaluating the performance of a Random  Forest model and for tuning its hyperparameters.  