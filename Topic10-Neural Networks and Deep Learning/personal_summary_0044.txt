The topic 9 content starts by giving a brief recap on what has been covered in the previous topics and then introduces the topics which are to be covered in this topic 9 and the next topic 10 contents. This includes the testing and developing of nonlinear supervised learning models, with random forests covered this topic and neural networks and deep learning covered next topic. Using Python, we will learn the importance of parameter selection and the effect this has on a model’s performance.  The content then introduces the topic of Ensemble learning, which is the concept of training multiple decision trees on slightly different data subsets and averaging (for regression) or voting (for classification) to combine the multiple decision trees into one. The ensemble method process reduces variation in machine learning models.  Since data sets can have outliners and can be biased by particular data sets, training multiple models on various different subsets of data and then averaging the results is a good way to remove variance as it provides a more accurate and balanced prediction on future unseen data, as the average is a better performance predictor than any single model testing on one subset of data. This is the overall idea behind the Ensemble learning approach and why models using this approach typically have a lower variance.  Then the content introduces the concept of Bootstrap Elimination/ Sampling, which is the process of selecting a smaller (bootstrapped) sub-sample from the larger sample set, via the process of resampling. Typically, this process produces a better-performing model which is more accurate and has less variance. This topic is then explored in further detail in a short video.  The content then moves to the topic of AdaBoost, which is short for Adaptive Boosting. AdaBoost is a machine learning algorithm that is used for classification problems, this approach utilises a concept of weights and combines weak classifiers to produce strong classifiers. The content then covers the steps and the mathematics behind the method in detail, followed by a short video that further elaborates on the method.  Then the content shifts to the topic of Bagging, which is a technique of using multiple classifiers to train various under-sampled data subsets to facilitate the classifiers to vote on a final decision. The term Bagging gets its name from the combination of the words ‘Bootstrapping’ and ‘Aggregation’, which is shortened to ‘B-Agg(ing)’. This method aggregates the multiple bootstrapping sample subsets’ boundary lines into one, averaging the results of the multiple bootstrapping samples to produce the best results for reducing the variance of the model.  The content then explores the topic of the Random Forest Algorithm, this approach is based on the Bagging decision tree concept, the key word here being Forrest, which implies multiples (Decision) Trees. A random forest classifier generates a series of decision trees from subsets selected at random from the training data. Then the votes from the various decision trees are aggregated into one to produce the final class of test objects. A key difference between the random forest algorithm and the decision tree algorithm is that the method used to identify the root node and the splitting of feature nodes is performed completely arbitrarily. The implementation and mathematical aspects are covered in detail, then the concepts are expanded on in more detail in a short video.  Then the content shifts to the topic of Out of Bag Error (OOB) and Feature Importance in machine learning, OOB is used as the test data for validation. Since the average bagged tree utilises on average two-thirds of the training instances, the remaining third portion of instances is known as the out-of-bag (OOB) instances and can be used to produce the average of each prediction. This process acts as a cross-validation to estimate the performance of a model.  The content then moves to the area of Feature importance using Random Forests, Voting Classifiers, and Stack Classifiers. Feature importance can be used in Random Forest models to identify the most important or optimal features to use for the model to reduce impurity in the decision trees and improve performance in both prediction accuracy and reducing processing time. The concepts of Voting classifiers and Stack Classifiers are covered in detail, including their applications and benefits.  The content wraps up by demonstrating how these concepts can be implemented in Python code, with examples of how to apply these ideas in practical applications. This includes how to clean and prepare data for machine learning with Random Forest Classification and AdaBoost in Python, and how to train and test the models accordingly.  