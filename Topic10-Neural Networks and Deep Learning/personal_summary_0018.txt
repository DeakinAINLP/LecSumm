Tristen Clifton 221211319 SIT307 (Topic 9)  Summarize the main points that is covered in this topic.  Ensemble learning is the process of combining multiple models to solve a given problem, E.g., regression/classification.  To reduce the variance (changes between models) of certain ML methods ensemble methods are used.  In high variance models like decision trees, an algorithm might make use of a forest of trees to find the average decision to result in a much more accurate model.  Bootstrap sample is a sample that is generated from another sample using resampling with replacement (same value can be resampled). This allows us to create N number of samples.  Adaptive boosting is the process of combining weak classifiers to create a strong one. This is done by repeatedly updating the weights on the final classifier.  Bagging is the process of using multiple classifiers to vote on a final decision and then calculating the mean decision to get a much more reliable decision.  Random forest is an algorithm that makes use of bagging to combine the collective decision power of a forest of decision trees.  Out of bag error is the fraction of incorrectly classified values out of the bagged samples.  Provide summary of your reading list – external resources, websites, book chapters, code libraries, etc.  https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html  Reflect on the knowledge that you have gained by reading contents of this topic with respect to machine learning.  Most of this topic’s content was entirely new to me, however I think the most important thing I learned was in practice of how importance adjusting hyperparameters are in the process of improving the reliability of a given model.  Attempt the quiz given in topicly content (9.12) and add screenshot of your score (>85% is considered completion of the task) in this report.     