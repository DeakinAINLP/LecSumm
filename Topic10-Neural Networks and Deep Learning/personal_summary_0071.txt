The topic of this topic is Ensemble Learning and the idea behind this is to generate and combine  the results of multiple models to solve a problem (such as getting a classification) which can improve  accuracy and robustness.  This is applied to Decision Trees which we call Random Forest and here, we can train multiple  trees on different subsets of data to reduce the variance (decision trees by themselves, have a high risk of  overfitting) and improve the overall prediction. When we get multiple responses since, we have multiple  trees, we can use majority voting for classification problems, and we can use the average of the results  for regression problems. We also have the concept of bootstrapping which is used in ensemble learning  to create the subsets of the dataset containing some records and some features (random sampling with a  chance of replacement), ultimately leading to more diversity in the training data, reducing the variance  and more accurate results. We also have the concept of bootstrap aggregation or bagging which is a  procedure for reducing variance by using multiple models created from the subsets of original training  data which we can get a final decision based on the methods mentioned already above. Something  additional to note with random forests is that we are forming trees based on the best features from the  subset and as such, our trees can end up with different subsets of features which can increase the bias  (due to limited access to the data). However, this wonâ€™t be an issue since we are combining multiple trees  here. Another thing to note is that the trees is fully grown (expanded completely ) with no pruning. Two  parameters to consider as well is the number of trees where if we have a high value, it will increase  computation cost and complexity which can lead to overfitting while simultaneously, if not enough trees,  can lead to underfitting and less accurate models. The other parameter is number of features which will  help with diversity among trees and reduce their correlation. High and low values of this will lead to the  same issues of overfitting and underfitting. When it comes to testing, the error rate will depend on the  correlation between trees where lower values are better and strength of a single tree where higher is  better. We also have the concept of Out of Bag error which is similar to the validation set used for testing  hyperparameters and here, we have 1/3 of the original training instances (remaining samples not picked  for training) as out-of-bag instances. One of the key things to consider here is feature importance. In a  tree, we have the capabilities to rank the features in a single tree. Now, because we have a forest of  trees, we will have multiple rank values and here, we can take the average impurity to get the overall  rank of the features.  In terms of the advantage of a random forest, it is fast to build and predict which is good for  large datasets and has good resistance to overfitting and can handle high dimension data.  It may not  require normalization and is resistant to outliers and can also handle missing values. The main downside  of using this is that it is less interpretable and may not be good for smaller datasets.     Another Ensemble Learning technique is something called Boosting where we combine weak  models to form a stronger model. The basic idea is we focus on misclassified sample (through the use of  weights on the data) in a sequential flow where we improve on the previous model. While computational  expensive, this is also a good technique that is interpretable.  In AdaBoost, we create a weak model  through trees based on each feature with one level of depth and each record will get a weight. We can  then apply Entropy or Gini index to see which model has the smallest value, to which we use that model  with the data set to see which are misclassified. Weights are updated based on formulas that incorporate  the total error rate based on the misclassified samples and the old weight (the initial weight before doing  the DT will be based on number of samples). We also need to normalize the weight to ensure the total is  1. In the next iteration, we get a new dataset based on the weight where misclassified samples will be  passed in and so on.  