Topic9. Nonlinear models(boosting and random forest)  Ensemble learning  : the proves by which multiple models (classifiers or experts) are strategically generated and combined to solve a particular computation intelligence problem.  - - -  Problem with single decision tree: risk of overfitting or increased variance To reduce variance of unstable learning methos(ie DT) use ensemble method Take their combined decision   Classification – voting   Regression – averaging  -  Random forest  : ensemble of DT  Bootstrap estimation  -  Bootstrap same is a smaller sample that is generated from a larger sample -  Uses a resampling method from statistics -  Result in less variance and more accurate  AdaBoost(Adaptive Boosting)  -  Machine learning algorithm for classification problems -  Works by combining weak classifiers to create a strong one. -  Weak learners are almost always stumps - - -  Some stumps get more say in the classification than others Each stump in made by taking the previous stump`s mistakes into account X: dataset of features, y: vector of corresponding labels, T: number of iterations 1. Initialize weights 2.  Train weak classifier 3.  Evaluate classifier 4.  Calculate classifier weight 5.  Update weights  Bagging(aggregation)  -  Uses multiple classifiers trained on different under_sampled subsetes and then allows  theses classifiers to vote on a final decision For reducing variance of a statistical learning method  - -  Hen the estimates are not independent, reduction in variance is lower  Random Forest algorithm  -  Creates a set of decision trees from randomly selected subsets of the training dataset. Then  aggregates the votes from different decision trees to decide the final class of the test objects.  -  Difference from DT is the process of finding the root node and splitting the feature nodes  will run randomly  Form the tree based on the best feature from the subset  -  Builds on the idea of bagging and each tree is built from a bootstrap sample of data - -  Repeat these steps - Increases bias  -  -  -  in random forest   All trees are fully grown with no pruning   Dealing with two parameters: number of trees(risk of overfitting), number of features Training For each of T iterations(T is the number of trees) 1.  Select a new bootstrap sample from the training set 2.  Build an un-pruned tree on this bootstrap sample 3.  At each internal node of tree, randomly select features and determine the best split  using only these features  Testing Output overall predictions as a mean( majority vote) from all individually trained trees Error rate depends on 1.  Correlation between tress(lower is better) 2.  Strength of single trees(higher is better) 3.   Increasing number of features for each split: Increases correlation and strength of single trees  Out of bag error and feature importance  -  Out of bag error(OOB)    Estimate the goodness of fit of a bagged model   OOB has been introduced which is equivalent to validation or test data   On average, each bagged tree makes use of 2/3 of the training and raining 1/3 are  referred to as OOB instances    Predict response for the observation using each of trees in which that was OOB and get  average  -  Feature importance   Each node in the tree(single feature based split)    a.  How much each feature decreases the weighted impurity(Gini or entropy) b.  This provides rank of all features used in a tree In Random Forest a.  Multiple trees(multiple rank values of single feature) b.  Average impurity decreasing scores  -  Advantages/disadvantages of Random Forest   Fast to build and even faster to predict  a.  Fully parallelizable since you can parallelly run the trees to go even faster b.  Random forest complexity = Tx DT complexity    Ability to handle data without pre—processing  a.  Not always required to normalize b.  Data does not need to be rescaled, transformed, or modified(resistant to outliers) c.  Automatic handling of missing values(a property of decision trees) Less interpretable results than a single DT    -  Voting classifier    Combines the predictions of various separate classifiers to provide a final prediction   Each classifier is given one vote, and the final forecast is determined by a majority vote   Can increase prediction accuracy and robustness because it incorporates the benefits of  various models while minimizing the effects of their particular flaws  -  Stack classifier    Aggregates the predictions of various separate classifiers   More complex than the vote classifier   First layer: comprises multiple separate classifiers that create predictions based on the  input data    Second layer: integrates the previous layer`s predictions to arrive at a final prediction   Several algorithms might be used at the second layer   Can increase the predictions’ accuracy and generalizability by learning a more complicated decision boundary and minimizing the chance of overfitting  Extra reading  Adaboost  