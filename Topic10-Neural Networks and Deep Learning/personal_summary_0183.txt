 Summarise the main points that were covered. Main points covered in topic 9 of the unit content:    Overview of ensemble learning algorithms   Bootstrap technique   Bagging   Random Forest algorithm   Adaptive Boosting  Reflection My reflection on the knowledge gained this topic from reading the unit contents for this topic with respect to machine learning.  This is a reflection on the knowledge that I gained during topic 9 in regard to machine learning.  This topic I learnt about bagging, an ensemble learning method which is used for both classification and regression and assists in reducing variance. It utilises the bootstrapping technique to create random sample data sets from the training data (which can overlap in values). For each of these sample data sets an independent regression or classification model is created. These models are brought together when finding a solution to a problem. It uses the multiple solutions from the many models to create the final result. For example, the final solution would be the average or majority of the many solutions found by the independent models.  Also discussed this topic was the Random Forrest Method which is another ensemble learning method that can be used for both classification and regression. The Random Forest method creates multiple decision trees and brings them together to find the solution to a problem. Similar to Bagging, each of the trees is created from a sample of data that was formed using the bootstrap method (similar to what was used in the bagging method). The trees created are not pruned. The result is a forest of decision trees, which results in a more accurate prediction. To create a prediction, all decision trees in the forest provide a solution, and the final result is the majority vote of those provided solutions.  This topic I gained knowledge on Adaptive Boosting (AdaBoost). It is an algorithm based on the ensemble method called Boosting. Adaptive Boosting is used for classification problems. In summary, the basic concept is a group of weaker classifiers are better than one. The method works by iteratively building models and testing them, then using the outcome of the model/modelâ€™s to repeat the process. A model is created and trained using the test data, however, some data points might not be classified very well with this model. The method then uses this error in classification to assign weights to the data. The data points that were not as well classified previously, are more likely to be chosen to be in the group of data that trains the next model. When each new model is created and trained, it is tested alongside all the previous models that were created.  The process of training the multiple models during Adaptive Boosting differs from Random Forest and bagging. In Random Forest and bagging, the models can be trained independently or in parallel. Whereas Adaptive Boosting creates multiple models using the previously created models, meaning it is a more iterative process.  