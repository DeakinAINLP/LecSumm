Ensemble Learning  Ensemble learning is the process by which mul@ple models, such as classiﬁers or experts, are strategically generated and combined to solve a par@cular computa@onal intelligence problem.  A popular ensemble method is the Random Forest  Bootstrap es*ma*on  A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample.  AdaBoost AdaBoost, which stands for Adap@ve Boos@ng, is a machine-learning algorithm for classiﬁca@on problems. It works by combining weak classiﬁers to create a strong classiﬁer.  Bagging In contrast to using just one classiﬁer, bagging uses mul@ple classiﬁers trained on diﬀerent under-sampled subsets and then allows these classiﬁers to vote on a ﬁnal decision.  Random forest algorithm The random forest classiﬁer creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from diﬀerent decision trees to decide the ﬁnal class of the test objects.  Out of bag error and feature importance  Important facts about the random forest:    Random forest is fast to build and even faster to predict!    ability to handle data without pre-processing. You are not always required to  fully parallelizable since you can run trees in parallel to go even faster!  normalize your dataset before running this method    data does not need to be rescaled, transformed, or modiﬁed! (Resistant to outliers)   automa@c handling of missing values (a property of decision trees)  less interpretable results than a single decision tree  Feature importance of using Random forest (RF) The signiﬁcance of each feature in the input dataset can also be determined using Random Forest.  Other Readings Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boos9ng)- Step by Step Explained Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boos@ng)- Step by Step Explained | by Lilly Chen | Towards Data Science (medium.com)  A Gentle Introduc@on to Ensemble Learning - MachineLearningMastery.com  Reﬂect on the knowledge that you have gained by reading contents of this topic with respect to machine learning. One of the beneﬁts of ensemble learning is that it can lead to be]er performance compared to a single model. The ensemble can capture the strengths of each individual model while minimizing their weaknesses. Ensemble learning can also improve the model's ability to generalize and perform well on new data.  However, this module has shown, ensemble learning also has some drawbacks. One poten@al issue is the increased complexity of the model, which can make it harder to interpret and explain the model's predic@ons. Ensemble learning can also be computa@onally expensive and @me-consuming, requiring signiﬁcant computa@onal resources to train and test mul@ple models.  Overall, I am interested to try applying ensemble learning to see the types of results and performance it oﬀers machine learning models.  