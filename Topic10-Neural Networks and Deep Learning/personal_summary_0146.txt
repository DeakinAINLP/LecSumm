This topic, we explored supervised learning further. We experimented with nonlinear supervised learning models, such as random forest in topic nine, and neural networks and deep learning in topic ten. We utilized Python packages to create decision trees and random forest algorithms to solve real-world problems. Additionally, we examined how the choice of different parameters affected the performance of the developed models.  We delved into ensemble learning, which is a powerful technique that combines multiple machine learning models to improve predictive performance. We explored bootstrap estimation, which involves creating multiple training sets through random sampling with replacement. We then studied AdaBoost, a popular boosting algorithm that iteratively trains weak learners and combines their predictions. We also learned about bagging, which employs bootstrap estimation to train multiple models in parallel and aggregate their predictions.  One of the highlights was the random forest algorithm, which is an ensemble of decision trees that leverages bagging and features randomness to enhance performance. We also discussed out-of-bag error, a metric used in random forests to estimate the model's accuracy without the need for an additional validation set. Lastly, we explored feature importance, a valuable output of random forests that ranks the importance of each feature in the prediction process. These topics provided us with a comprehensive understanding of ensemble learning and its applications in various machine learning scenarios.  