 Main learning points:  Ensemble modelling:    Ensemble learning is when multiple models are strategically combined   Bootstrap estimation, AdaBoost, Bagging and Gradient boosting are examples.   Classifiers may be ‘voting’ or ‘stack’  Bootstrapping:  1.  To create a bootstrapped dataset, that is the same size as original, randomly sample rows  from the original WITH REPLACEMENT.  2.  Randomly select two columns.    Replace 'two' with the square-root of sample size. Then through cross-validation, try  values above and below it.  3.  Start to create a decision tree with the bootstrapped dataset, using only the 2 selected columns. The column chosen to be the root node is the one that provides the best split (lowest Gini Impurity).  4.  Out of the remaining candidates, randomly choose two columns again. Then split again down the existing tree, using the variable out of the chosen two that provides lowest the Gini Impurity.  5.  Continue building the tree while only considering a random subset (of 2) variables at each  step.  6.  Make a new bootstrap dataset and repeat the steps. This should make many decision trees  - random forest.    About a third of the original dataset does not end up in a given bootstrapped dataset.   This is the out-of-bag dataset, which consists rows of data NOT in bootstrapped dataset  7.  Run a row of the out-of-bag sample through all other trees that were built without it. 8.  Use the votes of the trees to decide what it is assigned to  o  Boostrapping the data and using the aggregate to make a decision is called bagging  Ultimately, we can measure how accurate a random forest is by the proportion of out-of-bag samples that were correctly classified by the random forest.    Proportion of out-of-bag rows that are incorrectly classified is the out-of-bag error  AdaBoosting (decision forest):    AdaBoost uses a forest of stumps.  o  A stump is a single node with two leaves o  Stumps are 'weak learners'   o  Stumps of the forest are not made independently  ▪  Errors made in earlier stumps deterine how later stumps are made  o  The votes of the stumps do not have equal say  1.  Each row is assigned a 'sample weight'  o  At the start, all samples get the same weight (1/n_samples)  2.  Find the column that classifies the target with the lowest GI. Make that column the first  stump in the forest.  3.  Determine how much say this stump will have in the final classfication.  o  The total error for a stump is the sum of weights associated with the incorrectly  classified samples  o  Amount of Say = 0.5( log( 1-total error / total error) )  ▪  When a stump is not better at classification than random, total error = 0.5,  ▪  amount of say = 0. If a stump is consistently worse than random selection it would have a negative amount of say  4.  For a row that is incorrectly classified, its accuracy needs to be emphasised for the  subsequent classifications. This is done by:  o  Increase weight of incorrectly classified samples:  ▪  New sample weight = (sample weight) * e^(amount of say)  o  Decrease weight for correctly classified samples:  ▪  New sample weight = (sample weight) * e^(-amount of say)  5.  Scale the column of sample weights to add up to 1 6.  Repeat the process so that stumps are formed, while taking into account the new weight. 7.  When deciding the classification of a test row, the test row is run through all stumps.  o  The amount of say of stumps is the currency of votes  