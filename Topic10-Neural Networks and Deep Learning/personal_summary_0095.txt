 Ensemble learning: To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data. Then when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification). This is called the ensemble method. A popular ensemble method is the Random Forest  Bootstrap estimation: A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results.  AdaBoost: AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a  strong classifier. The algorithm has the following steps:  Bagging: In contrast to using just one classifier, bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision. Bootstrap aggregation or bagging (B+agg), is a general-purpose procedure for reducing the variance of a statistical learning methods.  Random forest algorithm: This algorithm is based on the bagging decision tree idea. The algorithm classifier generates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test object. The only difference between the random forest algorithm and the decision tree algorithm is that processes of finding the root node and splitting the feature nodes will run randomly.  Advantages and Disadvantages of Random Forest:  Feature importance of using Random Forest: Feature importance is basically a technique to determine the relative importance of each feature in the model. In the Random Forest specifically, this technique is identified by computing the mean decrease impurity (MDI) of each feature. The MDI measures the reduction in impurity that results from splitting on a particular feature. There are some ways that feature importance of RF can be applied for, such as feature selection, feature engineering, model interpretation, prioritizing future data collection.  