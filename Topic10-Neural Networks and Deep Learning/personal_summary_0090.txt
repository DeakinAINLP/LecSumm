Topic 9 Summary Notes – Nonlinear models (Boosting and random forest)    Ensemble learning is a process that is used to face the problem of having a weak and  inaccurately designed classifier on a dataset. Its process involves generating and combining multiple models to produce a better solution, one of these methods is called Random Forest.    Bootstrap samples are smaller samples that are created from larger ones and are done with  the same resampling methods found in statistics.    Adaptive Boosting, also known as AdaBoost, is a machine learning technique that is used to solve classification problems and simply works by combining weak classifiers to make stronger ones.    Bagging which is also called bootstrap aggregation is a machine learning ensemble method  that combines the classifiers of multiple models which can make predictions more accurately. This is mainly used to reduce variance and stability of predictions. These multiple models are trained under different subsets of data to reduce overfitting and improve generalization.    The Random Forest algorithm is introduced in 9.6 and is an algorithm that creates a set of  decision trees at random from the subsets of a training dataset and then it will use this data from all the different decision trees to make predictions. The main difference between this and the decision tree algorithm is the randomized process.    The out of bag error is an estimate on the performance of a Random Forest model on data that is remaining out of the training instances (out of bag samples). It is basically a way to find the average predictions and model performance from these unseen data samples. This gives us a good idea of the model’s ability to generalize.    Advantages of random forest include faster to build and predict, fully parallelizable, can handle data without pre-processing and sometimes don’t need to normalize, resistant to outliers, handles missing values automatically (decision tree property), results are however less interpretable when compared to single decision trees.    We can find the significance of each feature in the input dataset using Random Forest. This  significance is based on how much it will help with the Mean Decrease in Impurity. The more influential the feature is, the more important it will be.                    Quiz Evidence  Reading List: https://www.analyticsvidhya.com/blog/2020/12/out-of-bag-oob-score-in-the-random- forest-algorithm/ https://towardsdatascience.com/understanding-feature-importance-and-how-to- implement-it-in-python- ff0287b20285#:~:text=Feature%20Importance%20refers%20to%20techniques,to%20predic t%20a%20certain%20variable.         