 AdaBoost â€“ a ML algorithm for classification problem A method that combines multiple weak classifiers to create a strong classifier. It is a popular and powerful boosting algorithm that iteratively adjusts the weights of training instances to focus on the ones that are difficult to classify correctly. The algorithm contains 5 steps: Initialization weights, train weak classifier, evaluate classifier, calculate classifier weight, and update weights. AdaBoost can improve accuracy, handle imbalanced data. However, it is sensitive to noise and outliers and can be computationally expensive.  Bagging The technique where we train a set of individual models that are independent of each other. Each individual model differs from each other because they were trained with different, random subsets of the training dataset. Combines multiple machine learning models to improve performance and reduce variance. Bagging can be applied to various algorithms, such as decision trees (Random Forest), neural networks (Bootstrap Neural Networks).  Random Forest The algorithm combines multiple decision trees to create a robust and accurate predictive model. Random forest aims to reduce the variance. It randomly selects subsets of the data and features, trains decision trees on these subsets. For prediction, with classification tasks, the class with the majority vote from the decision trees is selected as the final prediction. With regression problems, the average or median of the predictions from the decision trees is taken.   The algorithm is robust against overfitting, provides feature importance, handles missing data, and is computationally efficient. However, it is less interpretable than a single decision tree and requires tuning of hyperparameters.  Gradient Boosting The algorithm combines weak models (usually decision trees) to create a strong predictive model. It iteratively fits new models to the mistakes of the previous models, gradually improving accuracy. The algorithm can improve accuracy, feature importance, and handle missing data. However, it requires hyperparameter tuning and can be computationally intensive.  