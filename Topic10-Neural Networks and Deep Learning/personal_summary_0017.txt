The content within this module intends for us both analyse performance of ensemble classifiers and to construct multi-layer neural networks through backpropagation. The module first explains what ensemble learning is, through the strategical generation of multiple models, and how they are combined to solve a computational intelligence problem. The module provides an example of ensemble learning through Random Forest and provides a deeper explanation of what this algorithm is, and how it works through decision trees, its advantages and its disadvantages. The module also explains upon the concepts of Bootstrap estimation and how it works. The module also explains Adaptive Boosting, how it works and its inputs and outputs, with steps provided. Furthermore, the module explains upon the concept of bagging, and how it reduces variance with the assistance of Bootstrap aggregation or bagging (B+agg). Finally, the module explains the Out of bag (OOB) error and feature importance, and how it is equivalent to validation or test data.  