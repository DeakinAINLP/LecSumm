For topic 9 of SIT307, we learnt about non-linear models, as well as boosting and random forests. The following topic was split into these specific topics so that we could further our understanding:  -  Ensemble Learning, which is the process in which multiple models (classifiers or experts) are created and combined to solve a particular CI problem. Methods such as Random Forests are known to be popular when utilizing ensemble learning.  -  Bootstrap estimation, which is a smaller sample that is derived from larger samples of data. It also uses resampling methods that are found in statistics. A lot of cases of using bootstrap estimation will end up having less variance and more accuracy in results.  -  Adaboost, which is adaptive boosting. Essentially, it is an ML algorithm that is used on  classification problems. It works by combining weak classifiers to create stronger classifiers. -  Bagging, where instead of using just one classifier, it uses multiple classifiers that are trained on different under-sampled subsets, which then allows the classifiers to make a vote on the final decision.  -  Random forest algorithm, which is a classifier that creates a set of decision trees from randomly selected subsets of the provided training dataset. From this, it then aggregates those votes from different decision trees to decide on the final class of the test objects.  -  Out of bag error and feature importance (OOB), which is a method that is used to validate  -  random forests, by measuring prediction errors made by random forests, boosted decision trees or other MLL that are using bootstrapping. Feature importance of using Random Forest (RF), which explains how the significance of each feature in the implementation of the dataset can be determined using Random Forest. Depending on how much it helps to reduce impurity of the decision trees, the significance of each characteristic is then assessed.   