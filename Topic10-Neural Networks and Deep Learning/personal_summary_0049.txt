 Ensemble Learning  “Sometimes your designed classifier on a dataset is weak and inaccurate. You may have designed many classifiers but some of them could be inaccurate and some could perform better on specific occasions.”  The process of generating multiple models and combining them to solve a computational intelligence problem, is called ensemble learning. In a scenario where we know a single decision tree may not perform well but it is highly efficient, we can use multiple trees that are all learning different things. To stabilize methods such as decision trees we can train multiple trees each with slightly different subsets of the training dataset. The ensemble method involves combining their results to form an average or voting outcome (average for regression problems, voting for classification). The decision affects the final model performance. One Model, One decision can lead to;  Inaccurate results  - -  Biased towards certain data domain (eg. Training data), overfitting.  In order to improve model performance we can analyse performance of ensemble classifiers with respect to a single model. We can also construct a multi-layer neural network using a backpropogation training algorithm to demonstrate data representation, classification and evaluation skills.  Random Forest  Random forest is an example of ensemble learning, which involves creating multiple decision trees. The variance of these models are demonstrably lower than their singular counterparts. In random forest, all trees are fully grown with no pruning. Therefore model complexity depends on two things; the number of trees, and the number of features.  In random forest, the error rate depends on  1.  Correlation between trees (lower is better) 2.  Strength of individual trees (stronger is better) 3.  Increasing number of features for each split. (increasing correlation increases strength in trees)  There must be a balance between using more features to create strong trees which also creates more correlation between the trees. “As you can see, like most concepts in machine learning there is a trade-off here. By using more features in creating the trees you are increasing the strength of single trees and increasing the correlation among the trees!”  Random forest can also be used to determine feature importance. The significance of a feature is determined based on how much it helps to reduce impurity. High feature importance means the feature contributes a lot to the purity of the resulting predictions. Random Forest’s feature importance assessment can be used to identify the most pertinent features for classification and feature selection.   Summary of Random Forest:  -  Random Forest is fast to build and fast to generate predictions. -  Decision tree complexity is  where the number of features (d) and number of data points  (n).  -  Random forest is fully parallelizable which can further optimize efficiency. -  Random forest can handle data without any preprocessing required. There is no need to normalize  data with this method.  -  Random forest is resistant to outliers. It does not need data to be rescaled, transformed or modified. - -  It automatically handles missing values (as do decision trees in general) It’s results are less interpretable than the results of a single decision tree.  AdaBoost  AdaBoost is an ensemble learning method suitable for classification. AdaBoost stands for Adaptive Boosting and it combines a bunch of weak classifiers to form a strong classification model. Each learner learns from the previous learners (in a sequential manner). The steps, which occur iteratively, are as follows:  1. Initialize weights 2.  Train a weak classifier 3.  Evaluate the classifier 4.  Calculate the classifier weight 5.  Update the weights.  Bagging  Bagging, also known as the bootstrap aggregation method, uses multiple classifiers training on different under-sampled subsets and then allows each classifier to vote on the final decision. The reduction in variance is lower in this method because each estimate is not independent.  -  Creates multiple base models, each trained on a different bootstrapped datasets. - -  Majority vote or average (unweighted combination)  Select a random subset of features at each split point (i.e decision node) in the tree.  “The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.” Example pictured below.  Bootstrap  Bootstrap is a resampling method:  -  Repeatedly randomly take samples from original dataset with replacement (i.e. bootstrapped samples  or bootstrapped dataset)   -  Each model uses a slightly different bootstrapped dataset: lower the variance  Important training step:  -  The total error of the weak learner: sum of the weights of the misclassified instances. -  Update the weights of the instances (probability determined by weights)  o  Increase the weights of misclassified instances, making them more important for the next weak learner.  o  Decrease the weights of correctly classified instances, making them less important for the  next weak learner.  -  Weighted majority vote or weighted sum to form the strong classifier (weighted combination)  Out-of-Bag (OOB)  Out of bag error and feature importance is a method of validation to estimate the goodness of the model. The instances that remain outside of the training instances are known as out of bag samples, and these can be used for testing.  Encoding: IS the order of the values important? If yes-> use ordinal encoder. If no-> use label encoder or OneHotEncoder  