In this topic, I have taken the experiment with nonlinear supervised learning models, such as random forest in topic nine, and neural networks and deep learning in topic ten. I have learnt Python packages to create decision trees and random forest algorithms to solve real world problems. I have also examined how the choice of different parameters effects the performance of developed models.  Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Consider this scenario:    We know that a single decision tree might not perform well.   But, it is super-fast.   What if we learn multiple trees?   We just must make sure that they do not all learn the same thing.  To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data. Then when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification). This is called the ensemble method.  A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results.  In contrast to using just one classifier, bagging uses multiple classifiers trained on different under- sampled subsets and then allows these classifiers to vote on a final decision.  Based on the bagging decision tree idea, we can define a new method called a random forest.  The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  It is possible to estimate the goodness of a bagged model in the same way as every model in machine learning. Out of Bag is equivalent to validation or test data.  It is possible to estimate the goodness of a bagged model in the same way as every model in machine learning. Out of Bag is equivalent to validation or test data.     