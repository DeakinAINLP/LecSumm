Ensemble learning: collection of different models to cooperatively solve intelligent computational problems. Example of ensemble model: random forest.  Bootstrap estimation: statistical resampling method to create multiple samples from an original dataset by allowing random and duplicated instances.  Adaboost: machine learning algorithm for classification. The goal is to combine multiple weaker classifiers to form a stronger one.  - Initialize weight -  Train weak classifiers -  Evaluate classifiers -  Calculate classifier weight -  Update weights  Bagging: ensemble machine learning method which create multiple subsets from the original one, train them using multiple classifiers. Each classifier would contribute to the final classification.  Random Forest: ensemble machine learning method which use decision tree and bagging.  -  Training: for each iteration:  o  Randomly create bootstrap samples o  Create un-pruned decision tree o  Select features and decide split node  -  Testing: error rate depends on those following criteria:  o  Correlation between trees o  Strength of individual trees o  Number of features  Voting classifier: combine several classifiers such as Decision trees, KNN or SVM to determine the final classifier. Stack classifier: a more complex combined classifiers when later classifiers layers would depend on former classifier layers.               