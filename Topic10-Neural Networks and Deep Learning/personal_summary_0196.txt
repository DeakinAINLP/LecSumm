Lesson Review: Boosting and Random Forest Module Learning Objectives  1.  Analyse performance of ensemble classifiers with respect to a single model 2.  Construct a multi-layer neural network using a backpropagation training algorithm to  demonstrate data representation, classification and evaluation skills.  Summarising the content: Ensemble  Learning  –  is  the  process  by  which  multiple  models  are  strategically  generated  and combined  to  solve  a  particular  computational  intelligence  problem.  To  reduce  the  variance  of unstable learning methods such as decision trees – we can train multiple decision trees, each with slightly different subsets of data. The in the classification or regression their combined decision are used via averaging for regression or voting for classification, as an ensemble method.  Bootstrapping – uses random sampling to estimate the properties of a larger data set by measuring those properties by sampling smaller portions of the dataset. Bootstrapping can result in less variance and more accurate results.  Adaptive Boosting (AdaBoost) – is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier.  Bagging – is a method that uses multiple classifiers trained on different ‘under-sampled’ subsets and then  allows  the  classifiers  to  vote  on  a  final  decision.  Bootstrap  aggregation  is  a  general-purpose procedure for reducing the variance of a statistical learning methods.  Random Forest – based on the bagging decision tree, the random forest classifier creates a set of decision  trees  from  randomly  selected  subsets  of  the  training  data.  It  aggregates  the  votes  from different decision trees to decide the final class of the test objects. In this case each tree is built from a bootstrap sample of data. The advantages and disadvantages of a random forest:  ▪  Fast to build and even faster to predict  ▪  Can run trees in parallel to go even faster. ▪  ability to handle data without pre-processing (don’t need to normalize data) ▪  Resistant to outliers, data doesn’t need to be rescaled or transformed. ▪  Automatic handling of missing values ▪  Less interpretable results than a single decision tree.  Out of Bag error – is the equivalent to validation or test data. Each bagged tree uses a portion of the training instances, where the remaining portion is referred to as the out-of-bag (OOB) instances. This allows the model to be tested and use cross-validation across the data set.  Feature Importance – The importance of each feature in a dataset can be determined using a random forest  based  on  how  much  it  helps  to  reduce  impurity  in  the  decision  trees.  The  higher  the contribution the more important the feature is.   