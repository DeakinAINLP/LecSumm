 The Main points that I’ve covered in the eighth topic are as follows:  ➢  I have understood the concept of Decision Trees (DT) ➢  The implementation of DT on classification data. ➢  I got the knowledge of Nonparametric method for regression and classification. ➢  I have understood the implementation of KNN and Decision Trees using Python.  My Learning summary Based on 9th Topics Lecture was gaining Knowledge of Boosting techniques and implementation for difficult issues where the connection between the input data and the goal variable is not linear, nonlinear models are potent machine learning tools. We can examine two well-liked nonlinear models Random Forest and Boosting. These models are a component of ensemble learning strategies, which integrate a number of ineffective models to produce a powerful prediction model. Boosting is a technique that sequentially joins various weak models to create a strong model. Each weak model is trained on a different version of the dataset that gives the samples from the preceding model's incorrect classifications more weight. An ensemble of these weak models makes up the final model. AdaBoost, Gradient Boosting, and XGBoost are examples of boosting algorithms that have been very successful in different domains.  Knowledge gained with Lecture slides.  In the Overall topic 9 I have explored different Ensembling techniques and feature importance, out of bag error, ensemble learning, bootstrap estimation, ad boost, bagging techniques, random forest algorithms, and bootstrap estimation.  Nonlinear models, including Boosting and Random Forest, have excelled at handling challenging prediction problems. Boosting methods gradually combine several weak models to form a strong model, whereas Random Forest builds a collection of decision trees. Both methods take advantage of ensemble learning's potential to boost prediction accuracy. Convenient tools are available for building these models, figuring out feature importance, and optimising hyperparameters etc.  