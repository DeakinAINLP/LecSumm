In the next two topics, we are going to explore supervised learning further. You will experiment with nonlinear supervised learning models, such as random forest in topic nine, and neural networks and deep learning in topic ten. You will use Python packages to create decision trees and random forest algorithms to solve real-world problems. We will also examine how the choice of different parameters affects the performance of developed models.  Learning Goals By the end of topic 10 you will be able to:  ● analyse the performance of ensemble classifiers with respect to a single model ● construct a multi-layer neural network using a backpropagation training algorithm to  demonstrate data representation, classification and evaluation skills  ● Key learning resources ● Visit the Resources tab for a list of all activities and topics in this program so far.  9.2 Ensemble learning  Ensemble learning Sometimes your designed classifier on a dataset is weak and inaccurate. You may have designed many classifiers but some of them could be inaccurate and some could perform better on specific occasions.  You may have faced this phenomenon in your own machine-learning practices.  Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Consider this scenario:  We know that a single decision tree might not perform well. But, it is super fast. What if we learn multiple trees? We just have to make sure that they do not all learn the same thing. To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data. Then when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification). This is called the ensemble method.  A popular ensemble method is the Random Forest (Breiman 2001)  It has been shown that the variance of these ensemble models are lower. The critical point is to try to design an ensemble model in which you can train different independent models with slightly different subsets of data. The way the data is fed into the models can be challenging.  Activity Can you explain why ensemble models have a lower variance compared to other models? Share your thoughts.  Ensemble models have lower variance due to the combination of diverse models, reduced overfitting, error cancellation, improved stability, and better handling of outliers and noise. By aggregating predictions from multiple models, each trained independently or with different subsets of data, ensembles capture a broader range of patterns and reduce individual model  biases and errors. Regularisation techniques and randomization further help prevent overfitting. The independent errors of diverse models can cancel each other out, leading to more accurate estimates. Ensembles are also more stable and robust to changes in data, with the ability to handle outliers and noisy data points better. Although ensembles may not always have the lowest bias, they strike a balance between bias and variance for improved performance.  Breiman, L 2001, ‘Random Forests’, Machine Learning, vol. 45, no. 1, pp. 5 -32, October 2001, Springer.  9.3 Bootstrap estimation  Bootstrap estimation The video above explores how bootstrap sampling works.  A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results.  Side PanelExpand side panel Table of Contents  Learning Resources  TOPIC 9: Nonlinear models (Boosting and random forest) 9.4 AdaBoost 9.4 AdaBoost Side PanelExpand side panel Table of Contents  Learning Resources  TOPIC 9: Nonlinear models (Boosting and random forest)  9.4 AdaBoost  AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier. The algorithm has the following steps:  9.5 Bagging  Note and remember that when the estimates are not independent, reduction in variance is lower.  Consider the following figure as an example of bagging. As you can see, different independent classifiers voted for different boundaries (light green), but if you take the aggregation of these boundaries (dark green) it will clean up and improve the final decision boundary.  Figure. Bootstrap aggregation or bagging  Also as you can see in the figure below, one could either choose to classify with 11 tree (the red original tree in top left) or 11  trees based on the dataset. Usually it’s more powerful to use bagging decision trees so that you can utilize as much information as possible.  Figure. Bagging decision trees.  9.6 Random forest algorithm  Random forest algorithm Based on the bagging decision tree idea, we can define a new method called a random forest.  The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects.  The difference between the random forest algorithm and the decision tree algorithm is that in the random forest algorithm, the processes of finding the root node and splitting the feature nodes will run randomly.  Random forest builds on the idea of bagging. Each tree is built from a bootstrap sample of data. Node splits are calculated from random feature subsets to make sure each of the trees is as independent as possible. Then we randomly pull out a subset and try work with the subset. Whenever it needs to split to from the tree, based on the best feature, we choose the best feature from the subset. Ultimately you have to do these steps T times, where T is the number of the trees.  If you are wondering whether this model increases the bias, you are right. It does! It uses subsets of features in different independent trees so it is likely to slightly increase the model bias.  A useful rule of thumb states that the number of features is:  9.7 Out of bag error and feature importance  9.8 Advance topics  Feature importance of using Random forest (RF) The significance of each feature in the input dataset can also be determined using Random Forest. Based on how much it helps to reduce impurity in the decision trees, the significance of each characteristic is assessed. The higher the contribution, the more important the feature is.  To increase the model's performance, feature importance utilising Random Forest can be utilised to find the most pertinent features for classification and feature selection.  