Learning Summary Ensemble Learning: This type of learning is a process where multiple models like experts/classifiers get generated strategically and combined for solving out a specific computing intelligence problem. One of the most popular ensemble method is said to be the Random Forest.  Bootstrap Estimation: A bootstrap sample is said to be a smaller sample that is bootstrapped (generated) from a large sample size. The usage of a resampling method takes place that found mainly in statistics, in majority of the cases bootstrap potentially results in less variance leading to more accurate results.  Ada  Boost:  Short  form  of  Adaptive  Boosting,  this  is  a  machine  learning  algorithm  that  is  used  for classification problems, where weak classifiers are combined for the creation of a stronger classifier.  Bagging: This is a method where multiple classifiers are trained on several under-sampled subsets, further allowing those classifiers to vote for a final decision. It is a general procedure for the reduction of variance for a statistical learning method. Although, one needs to keep in mind that when estimates are not independent the reducing of variance lowers down.  Random Forest Algorithm: A set of decision trees are created by the random forest classifier from random subsets that are selected at random, aggregating the votes from various decision trees for getting the final decision for the test object. The main difference among the random forest and decision tree algorithm is, within random forest; the finding of the root node and the feature node to be split takes place at a random basis.  The algorithm is built on the ideology of the concept of bagging, where the buildi ng of the tree takes place from a bootstrap of a sample data, and the splitting of the nodes gets calculated from random feature subsets for ensuring that each tree is independent as possible. After that a random subset gets pulled out and is worked with, and when it needs to be split up from the tree, on the basis of the best feature it’s chosen from the subset.  Ultimately, one needs to follow this process T times, where T refers to the number of trees. With the use of the model the bias is tend to increase as the usage of subsets of feature within various independent trees take place leading to a minor increase in model bias. The useful rule of thumb states that the number  of features are:  Within Random Forest, the trees are grown fully without any pruning; and deals with two parameters: number of trees, where if there are too many trees this could potentially lead to a trap of over fitting the problem, and the number of features.  Training of Random Forest: For every T iterations (T being the number of tree one likes to build), a fresh bootstrap sample  needs to  be selected  from the  training set;  after that  an un -pruned  tree for  that bootstrap sample needs to build; and finally every internal node of the tree selects features at random determining the best split where only features are being used.  Testing of Random  Forest: When  it comes to  the error  rates of Random  Forest it  depends over  the strength of the tree; there correlation among the trees and increasing number of features for every split.   Out of Bag Error: It is referred as the equivalent of test data or validation. Every tree present within the random forest get trained on the basis of a bootstrapped sample and on average every bagged tree makes up to 2/3 of the instances of training and the remaining of the instance is referred as out-of-bag instances. Such sample get computed with the use of data that have not been used for learning.  Pros and Cons of Random Forest: Their pros and cons are as follows:  1.  They can built very quickly as well as they are faster to predict also. 2.  They are fully parallelizable as trees can run in parallel making them operate much quicker. 3.  Data handling can take place without pre-processing, hence the requirement of normalizing  dataset is not mandatory before the dataset is run.  4.  They have less interpretable results compared to single decision trees.  Feature Importance of Random Forest: The importance of every feature within the input dataset can be determined with the usage of random forest, this is done on the basis of how much impurity is reduced within the decision tree. The higher the contribution is, the more important the feature turns out to be. This can help in using the most pertinent feature for the purpose of classification and selection of features.  Voting Classifier: It is a technique where the predictions of various separate classifiers are combined for providing a final prediction. This technique leads to increase the accuracy of prediction and its robustness as it incorporates advantages of different models while the effects of their particular flaws are minimized.  Stack Classifier: This technique turns out to be more complex than voting classifier, where the first layer has multiple separate classifiers which creates predictions on the basis of the input data, the second layer then integrates with the previous layer’s prediction leading to arrive at a final prediction. This technique potentially can lead to increase prediction accuracy and generalize by learning a more complex decision boundary and minimize the potentiality of over fitting.  