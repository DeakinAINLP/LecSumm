This topic we learnt about ensemble machine learning algorithms. The idea behind ensemble machine learning is to have multiple weaker models be used with different inputs or parameters and take the average/majority voted outputs as the classification. For example, The random forest algorithm creates multiple decision trees with subsets of the training data. The output classifications of those trees are then tallied up and the test data is classified as the majority of those multiple trees.  To make the models more accurate, hyperparameter tuning may be performed so the best performance can be achieved from the ensemble machine learning model. Using Grid Search, a list of parameters can be trialled with the model and the best performing model can be selected based on a set of performance metrics such as accuracy, precision, recall and f1 score.  We then learnt how to implement an ensemble machine learning algorithm in python, with the one I chose being a random forest algorithm  Reflection: I had previously never considered using multiple machine learning models on a dataset. It seems like a good idea to use multiple so that the classification or regression is more stable as the information is being passed through multiple models for analysis. I can definitely see this being used in big datasets where accurate classification is important.   