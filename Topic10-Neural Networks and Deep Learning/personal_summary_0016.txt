Non-linear Model  Lesson Overview – Non-linear Models (Boosting and random Forest)  Ensemble  method  in  machine  learning  refers  to  the  combination  of  using  multiple  models,  with different  strengths and  weaknesses,  and  sometimes even weak  and  inaccurate, to  achieve a more accurate and robust result.  Using ensemble methods, variance and risks of overfitting can be reduced. To  obtain  an  overall  prediction  through  ensemble  learning,  combined  decision  from  voting (classification  problems)  or  averaging  (regression  problems)  can  be  performed.    Voting  classifiers combines the predictions made by different classifiers, such as Decision trees, k-nearest neighbours, or support vector machines, and give each classifier a vote to determine the majority vote which will become the final forecast.  Flaws of each classifier can often be minimised when voting classifier is used.  To  use  ensemble  learning,  sample  subsets  of  the  original  dataset  will  be  required.    Bootstrap  is  a sampling  method  which  generates  smaller  subsets  of  random  samples,  with  replacement,  from original dataset to estimate a population parameter.  In many cases, the use of Bootstrap method can result in less variance and more accurate results by the introducing diversity in the training data, and hence capturing more features and characteristics of the data.  Bootstrap Aggregation (Bagging) is a machine  learning ensemble  method  to improve the accuracy  and  robustness of  a  model.   Bagging works by creating bootstrap samples, with replacement,  from a training dataset, each subset of data is subsequently trained using a different model, and the final prediction is made using a combination of all models. Adaptive Boosting (AdaBoost) is a machine learning ensemble method which works by combining multiple weak classifiers to create a strong classifier to achieve more accurate predictions, AdaBoost can be used for binary classification as well as regression tasks.  AdaBoost works by tweaking weak  classifiers  iteratively,  where  each  model  tries  to  classify  the  data  correctly.    Through  each iteration,  the  classifiers  would  focus  on  previously  misclassified  data  and  try  to  reclassify  them correctly.  More weights are given to models which had performed better, and the final prediction is made using predictions of weak models to form a strong model.  Random Forest is a machine learning algorithm that build a set of decision trees and combines them to make accurate predictions.  The random forest is an extension of the bagging technique where each of the decision trees is trained on a randomly subset of the training data, training is repeated N times according to the number of trees, and the final predictions is obtained by aggregating the votes from different decision trees.  Two parameters are essential when Random Forest algorithm are employed, number of trees (T) and number of features (mtry), additionally, all trees in the random forest are not pruned  until  they  are  fully  grown.    The  error  rate  of  a  random  forest  depends  on  the  correlation between  trees  in  the  forest  and  the  strength of  single  trees.    If  correlation  between  trees  is  high, meaning the trees are very similar to each other, and making similar predictions and errors, it can mean  that  the  trees  have  not  picked  up  all  characteristics  and  features  of  the  original  data. Additionally, if the trees have not performed well, have high bias and variance, meaning that the trees individually have not captured the features and characteristic of the original data set.  Strength of each tree is required to be high for the model to perform well.   Random Forest is fast to build and fast to predict as each tree can be grown in parallel, resistant to overfitting and it can handle high dimensional data without pre-processing.  However, results achieved is less interpretable and may not be effective for small datasets.  Additionally, tuning of hyperparameters is required to achieve a good performance random  forest.    The  use  of  random  forest  can  also  help  with  identifying  the  features  which  are important in a dataset based on the impurity result.  Out of bag method is used to estimate the fitting performance of a bagged model without the need of  a  validation/test  dataset.    Using  Bootstrap,  commonly  2/3  of  the  original  dataset  is  selected randomly as a subset and the remaining data from the original dataset can be used for validation, this remaining data is referred to as the out-of-bag (OOB) instances.  Stack classifiers is another ensemble learning technique that combine multiple base classifiers to make a final prediction.  Stack classifiers involves using a few different classifiers for the same dataset and then using the outputs from these classifiers as input for a higher-level classifier, a “super” model. This can often result in improved performance comparing to using a single model, as the combined model can learn the strengths from each individual model. 