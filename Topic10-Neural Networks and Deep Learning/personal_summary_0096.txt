 Ensemble learning  -  Sometimes classifiers on dataset are weak and inaccurate, ensemble learning is a process by  which multiple models such as classifiers or experts are strategically generated and combined to solve a particular computational intelligence problem  -  To reduce the variance of unstable learning methods, e.g., decision trees, we can train multiple  decision trees each with slightly different subsets of data. This when doing classification/regression you take their combined decisions (via averaging for regression or voting for classification) this is called the ensemble method  -  Critical point is to try and design an ensemble method in which you can train different  independent model with slightly different subsets of data  Bootstrap estimation  -  A bootstrap sample is a smaller sample that is generated from a larger sample -  Resampling method from statistics -  Often used to get error bars (confidence intervals) on estimates -  Generated as follows:  o  Take a dataset with N data instances o  Create a re-sampled version of the dataset by randomly drawing N times wth replacement  -  We can estimate any quantity repeatedly from many such bootstrapped samples  -  Adaboost  -  Adaptive boosting   -  Machine learning algorithm for classification problems, works by combining weak classifiers to  create a strong classifier, has the following steps: Inputs:  -  o  X = dataset of features o  Y = vector of corresponding labels(+1 or –1) o  T = number of iterations  -  Outputs  o  List of weak classifiers, each with an associated weight   -  Bagging uses multiple classifiers on different under-sampled subsets and then allows these  classifiers to vote on a final decision  -  Bootstrap aggregation or bagging (B+agg), reduces the variance of a statistical learning method. -  Remember when the estimates are not independent, reduction in variance is lower  -  Random forest algorithm  -  Random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset, it then aggregates the votes from different decision trees to decide the final class of the test objects The difference between the random forest algorithm and the decision tree algorithm is that:  -  o  In the random forest algorihtm, the processes of finding the root node and splitting the feature nodes will run randomly  -  Based on the idea of bagging; each tree is built from a boostrap sample of data, node splits rae calculated from random feature subsets to make sure each of the trees is as independent as possible, we then randomly pull out a subset and work with it. Whenever it needs to split from the tree, based on the best feature, we choose the best feature from the subset.  o  Have ot do these steps T times, where T is number of trees  o  -  In random forest:  o  All trees are fully grown with no pruning o  We are dealing with 2 parameters:  ▪  Number of trees T (careful as too many trees means likely to have overfitting)  ▪  Number of features   Training   For each T iterations (T is the number of trees you may like to build):  1.  Select a new bootstrap sample from the training set 2.  Build an un=pruned tree on this bootstrap sample 3.  At each internal node of the tree, randomly select mtry features and determine the best split  using only these features  Testing  You can easily output overall prediction as a mean (or majority vote) from all individually trained trees  In random forest, the error rate depends on:  1.  Correlation between trees (lower is better) 2.  Strength of single trees(higher is better) 3.  Increasing number of features for each split:  a. b.  Increases correlation Increases strength of single trees  -  By using more features in creating the trees you are increasing the strength of single trees and  increasing the correlation among the trees  Out of bag error and feature importance  -  Out of bag Is equivalent to validation or test data -  Each tree in a random forest is trained on a bootstrapped sample, it can be shown on average each bagged tree makes use of 2/3 of the training instances, the remaining 1/3 of the instances are referred to as the out-of-bag (OOB) instances  - -  Data points are sampled into a training set and unused data points make up a test set -  We then build the random forest by using this training data -  We can evaluate our model on using the unseen test data -  Like cross-validation, performance estimation using out-of-bag samples is computed using data that were not used for learning If the data have been processed in a way that transfers information across samples, the estimate will be biased probably  -   Advantages/Disadvantages of Random Forest  -  Random forest is fast to build and even faster to predict - -  Ability to handle data without un-processing, you are not always required to normalize your  Fully parallelizable since you run trees in parallel to go even faster  dataset before running this method  -  Data does not need to be rescaled, transformed or modified (resistance to outliers) -  Automatic handling of missing values (a property of decision trees) -  Less interpretable results than a single decision tree  Feature importance of using Random Forest (RF)  - Significance of each feature in the input dataset can also be determined using Random forest -  Based on how much it helps to reduce impurity in the decision trees, the significance of each  - -  characteristic is asses The higher the contribution, the more important the feature is To increase the model’s performance, feature importance utilizing random forest can be utilized to find the most pertinent features for classification and feature selection  Random Forest in Python   Boosting with Python    -  Boosting methods are built sequentially, and one tries to reduce the bias of the combined  estimator, the motivation is to combine several weak models to produce a powerful; ensemble  Adaboost  