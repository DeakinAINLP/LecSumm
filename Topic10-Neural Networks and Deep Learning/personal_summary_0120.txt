Topic -9: Non-linear models    Boosting   Random forest  Ensemble learning:  On a dataset, your built classifier may occasionally be inaccurate and topic. Although you may have created many classifiers, some of them might be inaccurate while others might work better under certain conditions.  To solve a specific computational intelligence problem, many models, such as classifiers or experts, are strategically developed and merged in an ensemble learning process.  We can train numerous decision trees, each using slightly different subsets of data, to lower the variance of unstable (high variance) learning techniques like decision trees. Afterwards, you take their cumulative judgements into account when performing classification or regression (via averaging for regression or voting for classification). The ensemble approach is used in this situation.    A popular ensemble method is the Random Forest.  Random forest:  Random Forest is an ensemble learning technique that brings together various decision trees to produce a reliable and precise predictive model. By training each decision tree on a random subset of the training data and features, this bagging-based ensemble technique makes use of the wisdom of crowds.  Bootstrap estimation:  Bootstrap estimation, sometimes referred to as bootstrap sampling or resampling, is a statistical method for estimating the sampling distribution of a statistic by resampling from the original data. It is a non-parametric approach that can be used to calculate a statistic's variability, bias, and confidence intervals even in the absence of data on the distribution's underlying properties or when certain presumptions are broken.  Steps involved in bootstrap estimation are:    Data resampling   Statistic calculation     Repetition of steps 1 and 2   Statistical analysis  AdaBoost:  AdaBoost, which stands for "Adaptive Boosting," is an ensemble learning technique that combines several weak or base learners to produce a powerful predictive model. It is an ensemble technique built on boosting that gives weights to different training samples based on how well they perform and, in later iterations, concentrates more on misclassified examples.  Bagging: Bootstrap Aggregating is known as "bagging." On several subsets of the training data, sampled with replacement, it includes training multiple base models. Each basis model is trained independently, and the combined forecasts from all base models are averaged or decided upon via voting. A bagging ensemble algorithm that combines decision trees is one such as Random Forest.  Random Forest algorithm:    We may design a new technique known as a random forest based on the  bagging decision tree concept.    From a set of randomly chosen subsets of the training dataset, the  random forest classifier generates a collection of decision trees. The final test object class is then determined by averaging the votes from various decision trees.    In contrast to the decision tree technique, the random forest algorithm uses randomisation to determine the root node and divide the feature nodes.    The concept of bagging is expanded upon in random forest. Each tree is  constructed using a bootstrap sample of the data. Node splits are calculated from random feature subsets to make sure each of the trees is as independent as possible.    Then, we choose a subset at random and try to work with it. We select  the best feature from the subset if the tree needs to be divided according to the best feature.  Out-of-bag error:  A Random Forest model's performance can be estimated using the Out-of-Bag (OOB) error approach without the usage of a separate validation set. It is a  valuable metric for assessing the model's accuracy and can shed light on its generalizability.  Each decision tree in the Random Forest technique is trained on a bootstrap sample, which means that some of the original data points are excluded or "out-of-bag" in each tree. The OOB error is then determined by comparing the out-of-bag samples' predictions to their respective decision trees.  Advantages of Random Forest:    Because it combines several decision trees and can handle highly  dimensional data, Random Forest is extremely accurate and robust.   It can manage missing values and outliers, gives feature importance measurements, resists overfitting, and is appropriate for a variety of machine learning tasks.  Disadvantages of Random Forest:    The computational cost of Random Forest can be considerable, especially when dealing with large datasets and many trees.    Due to the ensemble structure of Random Forest and the intricate  interactions between the decision trees, it may be difficult to interpret the model.  We have learnt the feature importance of using random forest.  We have also learnt how to use the random forest and boosting in python.        