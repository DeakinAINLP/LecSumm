This topic provided an in-depth coverage of ensemble learning methods. We started by exploring how bootstrap sampling works.  We looked at adaptive boosting and how it combines weak classifiers to solve classification problems.  We then explored bagging and the use of multiple classifiers along with voting to generate the final solution.  We also explored the random forest algorithm, its difference from decision trees and how nodes and trees are selected from subsets of the data. We also dive deeper into how training works and how the results can be tested.  We covered out of bag error and feature importance that allow us to validate the random forest model and explored the advantages and disadvantages of the algorithm.  The second part of the topic focused on the use of the Python programming language and how it can be used in order to perform classification using the random forest algorithm and how to implement the adaBoost algorithm using the SKLearn library.  