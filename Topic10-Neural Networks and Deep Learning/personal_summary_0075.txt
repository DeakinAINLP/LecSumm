 This topic, we have learned about some non-linear methods for classification  models. In this topic, we first talked about Random Forests. In fact, it is one of the most popular Ensemble Learning Methods, where multiple smaller models are combined strategically to solve a particular problem. One advantage of this is that it reduces the variance of smaller methods. With ensemble methods, you can train different independent models with slightly different subsets of data.  We also learned about bootstrap sample, which is a smaller sample that is generated from a larger sample. It uses resampling method found in statistics. In many cases bootstrap can result in less variance hence more accurate results. Another method of ensemble methods is ‘AdaBoost’, Which is short for adaptive boosting. It works by combining weak classifiers to create a strong classifier. It initialises weights for each classifier, then trains a weak classifier, then evaluates the classifier and then calculate the weight of the classifier, an updates weight correspondingly. We also talk about Bagging, which in contrast to using a single classifier, uses multiple classifiers drain on different under sample subsets and then allowed to vote on a final decision. It is also called ‘Bootstrap Aggregation’, which is a general-purpose procedure for reducing the variance of statistical learning methods. Based on the Bagging concept, we can define a new method call Random forests. It is a collection of decision trees from randomly selected subsets of the training data set, which aggregates the votes from different decision trees to determine the final answer or the final class of the test objects. Random forests are fast to build and even faster to predict. They are fully parallelizable since you can run trees in parallel to go even faster. They can handle data without any need of pre-processing (even normalising for the matter). It can automatically handle missing values. However, one major drawback is that such a model provides less interpretable results then a single decision tree.  In this topic, we also learned about the Out-of-Bag error, which is an equivalent to  validation or test data in a bagging model. Each smaller model in an ensemble model, is trained on a bootstrap sample, which can be shown that on average each sub-model uses some training instances and the remaining instances as test instances. From this we can then predict the response of our model. We also learned about feature importance while using random forests, where we assess the significance of each feature in the input data set. Based on how much it helps to reduce impurity in the decision trees, the significance of each characteristic is assessed. The higher the contribution, the more the importance of the feature. We also learned about Two types of ensemble methods: Voting Classifiers and Stack Classifiers. A voting classifier combines the predictions of various separate classifiers to provide a final prediction, whereas a star classifier aggregates the predictions of those classifiers call mom which is in fact more complex in technicality than the former.  