Summary Topic 9   Ensemble learning is a process through which multiple models are strategically generated  and brought together to solve a particular computational intelligence problem.   To mitigate variance of unstable learning methods for instance decision trees, we can train  multiple decision trees   A bootstrap sample is a smaller sample that is derived from a bigger sample   AdaBoost which signifies Adaptive Boosting refers to a machine learning algorithm used  for classification problems. The algorithm is calculated as follows:     Bagging utilises several classifiers trained on non-similar under-sampled subsets and then  allows these classifiers to vote on a final decision   The random forest classifier generates a set of decision trees from a  randomly selected  subsets of the training dataset. After this, it combines the votes from different decision trees  to decide the final class of test objects       To summarise the advantages of random forest, they are fast to build and faster to predict,  fully parallelizable and  had  the  ability to  handle  data without  pre-processing amongst  others.   However random forest is incapable of automatic handling of missing values an produces  less interpretable results than a singular decision tree          