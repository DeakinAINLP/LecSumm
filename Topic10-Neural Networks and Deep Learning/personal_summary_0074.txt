Summary and Reflection of Topic 9:  Random forests – able to be used for both regression and classification tasks. In general, the more tress in the forest the more accurate the predictions.    A form of non-linear supervised learning model.   An ensemble method - is the process by which multiple models, such as classifiers or experts are strategically generated and combined to solve a particular computational intelligence problem.   Used to reduce the variance of unstable (high variance) learning methods such as decision trees  = train multiple decision tress each with slightly different subsets of data.    Classification  /  regression  =  you  take  the  combined  decision  (via  averaging  for  regression  or  voting classification).    Creates  a  set  of  decision  trees  from  randomly  selected  subsets  of  the  training  dataset.  It  then aggregates the votes from different decision tress to decide the final class of the test objects.  Difference between random forest versus decision tree – in the random forest algorithm = the process of finding the root node and splitting the feature nodes will run randomly.  Advantages:    Both classification and regression tasks   Handles the missing values and maintains accuracy for missing data   Will not overfit the model   Handles large data set with higher dimensionality  Disadvantages:    Good job at classification but not as good as for regression   Very little control on what the model does  Bagging – machine learning ensemble meta – algorithm designed to improve the stability, reduce variance and accuracy. It uses multiple classifiers trained on different under-samples subsets and then allows these classifiers to vote on a final decision.  Boosting – machine learning ensemble meta – algorithm for primarily reducing bias, and also variance in supervised learning.  Ad boost unlike random forest = only has one node and two leaves (called stumps). Stands for Adaptive Boosting = a machine-learning algorithm for classification problems. Works by combining weak classifiers to create a strong classifier.  Testing (error rate), depends on:  1.  Correlation between tress (lower is better) 2.  Strength of single tress (higher is better) 3.  Increasing number of features for each split – increase correlation / increase strength of single tress (trade-off)  Tradeoff – by using more features in creating the tress you are increasing the strength of single trees and increasing the correlation amongst the trees.  Out of the bag error and feature importance  It is possible to estimate the goodness of a bagged model in the same way as every model in machine learning. Out of Bag is equivalent to validation or test data.   Feature importance of using Random Forest  Based on how much is helps to reduce impurity in the decision tress, the significant of each characteristic is assessed.    The higher the contribution – the more important the feature is.   Feature importance utilizing Random Forest can be utilized to find the most pertinent features for  classification / feature selection.  Voting Classifier:  An ensemble learning technique:    Combines the predictions of various separate classifiers to provide a final prediction.   Several  types  of  classifiers,  such  as  Decision  Trees,  K-Nearest  Neighbors,  or  Support  Vector Machines,  can  be  used  individually.  Each  classifier  is  given  one  vote,  and  the  final  forecast  is determined by a majority vote.    Can increase prediction accuracy and robustness because it incorporates the benefits of various  models while minimising the effects of their flaws.  Stack Classifier: Another ensemble learning technique: A    Aggregates the predictions of various separate classifiers is the Stack Classifier, which is more  complex than the Vote Classifier.    The first layer of a stack classifier comprises multiple separate classifiers that create predictions based on the input data. The second layer then integrates the previous layer's predictions to arrive at a final prediction.    Several  algorithms  might  be  used  at  the  second  layer,  including  Decision  Trees  and  Logistic  Regression.    Stack  Classifier  can  increase  the  prediction's  accuracy  and  generalizability  by  learning  a  more  complicated decision boundary and minimising the chance of overfitting.  The above summary has been created using the full course material for Topic 9 (including the relevant YouTube clips)     