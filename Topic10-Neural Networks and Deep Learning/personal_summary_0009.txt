This topic further explored non-linear models for Machine Learning and focused on the random forest and boosting supervised learning algorithms. Both of these models can be utilised for classification and regression problems and are a type of ensemble learning algorithm where multiple models are created and then combined to produce meaningful output. The Random Forest algorithm is an expansion upon the decision trees algorithms that we had learned in the previous topic and creates multiple decision trees in parallel with random subsets of the data being chosen for each tree, this is known as bootstrapping. Each tree then will produce an output based on the input data average result is the output. So for a regression problem it is the average of the values, and for a classification problem it is the most voted for class which is known as bagging. This seems like it is very similar to the KNN algorithm but the random forest considers all of the decision trees to make up a vote whereas KNN only considers the datapoints within itsâ€™ decision boundaries. Boosting similarly can utilise decision trees to form a model, but does so in a series and combines weak classifiers to form a stronger classifier. Boosting will assign initial equal weighting to the instances of the classifiers. It will iteratively train and evaluate a classifier and assign a weighting to that classifier and re-evaluate and update the weightings of the previous classifiers. Similarly, to random forest it will then aggregate the votes or predictions of each instance to produce and output. The pass activity was a great exercise. Having the option to choose what kind of ensemble classifier was a good way to challenge my thinking and I had to look into various methods and how to perform them. I ultimately chose the random forest algorithm. Also having the question on modifying the hyperparameters was good to see the effect that they can have on the output of the algorithms and to further consider how the functions are performed. The 