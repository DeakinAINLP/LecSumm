This topic we learned about non linear models   In supervised learning, a powerful technique is used that couples predictions from various models in order to provide a ﬁnal prediction output, technique is called ensemble learning. By being able to gather information/knowledge from the diverse models that uses its able to improve the overall performance.   One re-sampling technique used in machine. Learning speciﬁcally for statistics is bootstrap estimation and is used to estimate the very ability of a statistic or the performance of a model. Wisdom through the creation of multiple samples, through random selections of observation, and where the replacement from the original dataset.   Another ensemble learning algorithm is called adaptive boosting, or Adaboost for short. The adaboost works is that it takes the misclassiﬁed instances and adjust its weight in order to iteratively improve the overall performance of the model. It gets multiple predictions, combining them from the topic learners in order to make a ﬁnal production.   Through techniques, such as majority voting or averaging multiple individual models that are used for prediction are aggregated through these techniques. They are then used by bagging or bootstrap aggregating. Bootstrap, aggregating or bagging is another ensemble learning technique where multiple models are trained on samples of boot strap from the original dataset.   Now going into bagging, we have another extension, called the random forest algorithm and works by combining various decision trees in order to make predictions. Each split happens because of the randomness that is introduced into the training of each tree on a bot strap sample, which only considers a random subset of features.   The out of bag error is able to provide a form of an estimate on the models performance on the unseen data, which is used in the ensemble methods, such as a bagging and random Forest. By evaluating each instance, using only the models that were not trained on that speciﬁc point does it calculate the OOB.   And another important thing in non-linear models is the feature importance. In ensemble methods like random forests, feature importance can be determined by analysing the impact of individual features had on the reduction in false predictions across the trees. Ensemble learning is a form of powerful machine learning technique which is able to combine prediction from diﬀerent models which helps to improve overall performance.  