SIT720 â€“ Machine Learning Topic 9 report  Summarise the main points that is covered in this topic  This topic we learnt about Ensemble learning, Bootstrap es7ma7on, AdaBoost, Bagging and Random Forest.  Ensemble Learning  1.  Employs a divide-and-conquer approach 2.  This is the process by which multiple models such as classifiers or experts are  strategically generated and combined to solve a particular problem  3.  A single Decision Tree may not perform well, but we can train multiple Decision  Trees with different training data sets and then take a decision on it  4.  For classification - maximum votes 5.  For regression - average the outcome 6.  Random Forest is another ensemble method 7.  Ensemble learning reduces the variance of the models  Bootstrap estimation  1.  A bootstrap sampling is a smaller sample which is generated from a larger sample by  using resampling method  2.  Bootstrap sampling usually results in less variance and high accuracy results 3.  Smaller samples are drawn repeatedly with replacement 4.  This is often used to get error bars or confidence intervals on estimates 5.  They are generated as follows -  a.  Take a dataset with N instances b.  Create a re-sampled version by repeatedly drawing with replacement from  the N instances  AdaBoost  1.  Stands for Adaptive Boosting and is used to solve classification problems 2.  Works by combining weak classifiers to create a strong classifier 3.  Steps -  a.  Initialize weights b.  Train weak classifier c.  Evaluate classifier d.  Calculate classifier weight e.  Update weights  Bagging  1.  Uses multiple classifiers trained on different under-sampled subsets and then these  classifiers vote on a final decision 2.  Stands for Bootstrap Aggregation 3.  It reduces the variance and bias 4.  If the individual estimates are not independent, then reduction in variance is lower 5.  Designed to improve the stability of the model 6.  Helps to avoid overfitting        Boosting  1.  A machine learning Meta-algorithm 2.  Used to primarily reduce bias and also variance in supervised learning  Random Forest  1.  Creates a set of decision trees from randomly selected subsets of the training data 2.  It then aggregates the votes from the different decision trees to decide on the final  class of the test data  3.  Increases bias of the model 4.  All trees are fully grown without any pruning 5.  Two parameters to tune -  a.  Number of Trees (T) - too high value of T will result in overfitting b.  Number of Features - rule of thumb is this value should be square root of the  Number of features  6.  Error rate depends on -  a.  Correlation between trees b.  Strength of single trees c.  Increasing number of features for each split -  i. ii.  Increases correlation Increases strength of single trees  Advantages and disadvantages of Random Forest  1.  Quite fast to build and predict 2.  Fully parallelizable 3.  Can handle data without pre-processing 4.  Data need not be normalised, scaled or transformed 5.  Resistant to outliers 6.  Automatically handles missing values 7.  Less interpretable results than a single Decision Tree  Voting Classifier  1.  Combines the prediction of various separate classifiers to provide a final prediction 2.  Different classifiers such as Decision Tree, KNN, SVM can be used individually 3.  Each classifier gets one vote and final forecast is determined by the majority vote  from the different classifiers  4.  This increases prediction accuracy and robustness  Stack Classifier  1.  The first layer of the stack classifier comprises of multiple separate classifiers that  create predictions based on the input data  2.  The second layer then integrates the previous layer's prediction to arrive at a final  prediction  3.  Stack classifier can increase the prediction's accuracy and generalizability by learning a more complicated decision boundary and minimising the chance of overfitting      