 Ensemble  learning  was  covered  in  this  topic,  On  a  dataset,  your constructed classifier may occasionally be unreliable and erroneous. Although you may have created a large number of classifiers, some of them  might  be  erroneous  while  others  might  work  better  under certain conditions.  This phenomenon might have affected your own machine  learning  techniques.  To  address  a  specific  computational intelligence problem, many models, such as classifiers or experts, are strategically developed and merged in an ensemble learning process. We  can  train  numerous  decision  trees,  each  with  slightly  different subsets  of  data,  to  lower  the  variance  of  unstable  (high  variance) learning techniques like decision trees. Then, you use their aggregated for judgements classification)  when  performing  classification  or  regression.  The ensemble  approach  is  used  for  this.  A  bootstrap  sample  that  is produced  (bootstrapped)  from  a  bigger  sample  is  known  as  a bootstrap sample. It makes advantage of a resampling technique from statistics. The findings of a bootstrap can frequently be more accurate and have less volatility. AdaBoost, which stands for Adaptive Boosting, is a  machine-learning algorithm  for classification  problems. It  works by combining weak classifiers to create a strong classifier. regression  or  voting (through  average for Bagging  employs  many  classifiers  trained  on  various  under-sampled subsets as opposed to just one, allowing these classifiers to cast votes for the ultimate judgement.  Random forest algorithm We  may  design  a  brand-new  technique  known  as  a  random  forest based on the bagging decision tree concept.  From  a  set  of  randomly  chosen  subsets  of  the  training  dataset,  the random forest classifier constructs a collection  of decision trees.  To determine the final class of the test items, it then combines the votes from various decision trees.    The random forest algorithm and the decision tree algorithm vary in that  the  random  forest  algorithm  uses  randomisation  to  determine the root node and split the feature nodes. Building off of the bagging concept is random forest. Every tree is constructed using a bootstrap sample  of  the  data.  To  ensure  that  each  tree  is  as  independent  as feasible,  node  splits  are  calculated  using  random  feature  subsets. Then, we choose a subset at random and try to work with it. We select the  best  feature  from  the  subset  if  the  tree  needs  to  be  divided according  to  the  best  feature.  In  the  end,  you  must  repeat  these processes T times, where T is the total number of trees.  You are correct to worry whether this model heightens the prejudice. It  does.  Because  it  incorporates  portions  of  features  in  numerous unrelated trees, the model bias is probably significantly increased.  Out of bag error and feature importance, A bagged model can have its goodness estimated in the same  manner as other machine  learning models. Validation or test data are equivalent to Out of Bag.  Feature importance of using Random forest (RF) , The significance of each  feature  in  the  input  dataset  can  also  be  determined  using Random Forest. The degree to which a property lowers impurity in the decision  trees  determines  how  important  it  is.  With  contribution,  a trait becomes more significant. The performance of the model can be enhanced  by  selecting  the  most  pertinent  characteristics  for classification and feature selection using Random Forest.  Voting Classifier: A voting classifier is an ensemble learning technique that  combines  the  results  of  multiple  independent  classifiers  to provide a single final prediction. You can employ a variety of classifiers independently,  including  Decision  Trees,  K-Nearest  Neighbours,  and Support  Vector  Machines.  Each  classifier  has  one  vote,  and  the majority of votes cast determines the final forecast. Voting Classifier utilises  the  advantages  of  different  models  while  minimising  the       consequences  of  each  model's  specific  defects,  which  can  boost prediction accuracy and robustness.  The  Stack  Classifier  is  a  more  sophisticated  ensemble  learning technique  than  the  Vote  Classifier  that  combines  the  predictions  of multiple independent classifiers. Multiple distinct classifiers that make predictions based on the input data make up the first layer of a stack classifier.  The  final  prediction  is  then  reached  by  integrating  the predictions from the previous layer in the second layer. At the second layer,  a  number  of  techniques,  such  as  Decision  Trees  and  Logistic Regression,  might  be  applied.  By  learning  a  more  complex  decision boundary  and  reducing  the  likelihood  of  overfitting,  Stack  Classifier can  improve  the  prediction's  accuracy  and  generalizability.  We  had hands on experience on all these topics in python.  