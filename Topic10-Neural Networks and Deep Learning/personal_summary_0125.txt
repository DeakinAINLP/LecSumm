In topic 9,we continued to look at nonlinear models, but specifically more about random forest and boosting. We covered the topic of ensemble learning and how to overcome the issue of our classifier on a dataset being either too weak or inaccurate. Ensemble learning is the process by which multiple models are generated and combined to solve a computational intelligence problem. Random forest is one of these ensemble methods, and allows for the reduction of high variance that is typical of decision trees, by training multiple trees. We explored the technique of how to use them, and under what circumstances you want to use them for. Also discussed was AdaBoost, short for Adaptive Boosting, which is a machine learning algorithm used for classification problems. It is used to overcome the weaknesses of weaker classifiers by combining them to create a stronger classifier.   