Main Point Summary  Ensemble Learning  Since classifiers can sometimes be weaker than others or only perform well in specific situations we sometimes use ensemble learning which is a learning process which uses multiple models combined to solve a particular problem. For example, the variance of learning methods such as decision trees can be reduced by training multiple decision trees and using their combined decisions when performing classification or regression.  Bootstrap Estimation  A bootstrap sample is a resampling method where a smaller sample is taken from a larger sample. A dataset with N instances can create a re-sampled version of the data by randomly drawing N times with replacement providing a bootstrap sample. This allows us to estimate any quantity repeatedly from multiple bootstrap samples and can result in less variance and more accurate results.  Adaboost  Adaptive Boosting (Adaboost) is a machine learning algorithm designed for classification problems that involves the combination of weak classifiers to create a strong classifier. It takes in the data (X), label (y) and number of iterations/classifiers to train (T) and it returns a list of weak classifiers and their associated weights.  Bagging  Bagging (Bootstrap Aggregation) involves the use of multiple classifiers trained on different sample subsets and then has these classifiers vote on a final decision, it allows for the reducing of the variance of statistical learning methods. If the estimates are not independent then the reduction in variance is lower.  Random Forest Algorithm  The random forest classifier creates a set of decision trees based on randomly subsets of the training data. The votes from the trees in this set are aggregated and used to decide the final class of test objects. The difference between this and the decision tree algorithm is that the processes of finding the root node and splitting the features nodes runs randomly. This builds upon bagging as each tree is built from a bootstrap sample of that data while the node splits are calculated from random feature subsets to make sure each tree is as independent as possible. After which a random subset is pulled out and whenever it needs to split from the tree according to the best feature we are able to choose the best feature from the subset. This does mean that this model increases bias since the subsets are features in different independent trees.  Out of bag error and feature importance  The quality of a bagged model can be evaluated using Out of Bag as it is equivalent to validation or test data. If each tree in a random forest is trained on a bootstrapped sample and it makes use of 2/3 training instances then the left over 1/3 instances are what we call the out-of-bag (OOB) instances. Then other observations can be predicted using all the trees where that observation was OOB.  Advance Topics  Feature importance using Random Forest   The significance of each feature in the input database can be determined with Random Forest. The significance of each characteristic can be assessed based on how much it reduces impurity in the decision trees, higher contributed to reduces impurity means the more important the feature is. This can be used to increase the models performance as feature importance is determined so that most the most useful features are selected and used for classification.  Reflection  Mostly everything I learnt in this topic was new to me and was helpful in better understanding the methods and reasoning behind regression and classification techniques. For example, while I didn’t know about ensemble learning the concept had crossed my mind but how this could be utilised with bootstrap estimation, adaboost, bagging and the random forest algorithm was new to me as I didn’t know about these techniques prior to this module.  