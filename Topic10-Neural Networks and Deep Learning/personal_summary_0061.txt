 To address a specific computational intelligence problem, many models, such as classifiers or experts, are strategically developed and merged in an ensemble learning process.  We can train numerous decision trees, each with slightly different subsets of data, to lower the variance of unstable (high variance) learning techniques like decision trees.  Then, you utilize their combined judgements when performing classification and regression. The ensemble approach is used for this.  A smaller sample that is produced (bootstrapped) from a bigger sample is known as a bootstrap sample.  It makes advantage of a resampling technique from statistics. The findings of a bootstrap can frequently be more accurate and have less volatility.  An technique for machine learning used to solve categorization issues is called AdaBoost, which stands for Adaptive Boosting. It functions by fusing weak classifiers to produce a powerful classifier.  From a set of randomly chosen subsets of the training dataset, the random forest classifier constructs a collection of decision trees. To determine the final class of the test items, it then combines the votes from various decision trees.  The random forest algorithm and the decision tree algorithm vary in that the random forest algorithm uses randomization to choose the root node and split the feature nodes.  A bagged model can have its goodness estimated in the same manner as other machine learning models. Validation or test data are equivalent to Out of Bag.  Random Forest can also be used to determine the importance of each feature in the input dataset. The importance of each trait is determined by how much it aids in lowering impurity in the decision trees.  One attempts to lessen the bias of the combined estimator by building boosting algorithms in a progressive manner.  