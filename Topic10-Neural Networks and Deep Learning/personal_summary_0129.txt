 Ans: Boosting & Random forest are non-linear models which are used for non- parametric nonlinear regression.  ➢  Ensemble learning is a machine learning technique that combines  several base models in order to produce one optimal predictive model.  ➢  To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train multiple decision trees, each with slightly different subsets of data.  ➢  Bootstrap estimation is a statistical procedure that resamples a single dataset to create many simulated samples. This process allows for the calculation of standard errors, confidence intervals, and hypothesis testing.  ➢  AdaBoost , short for Adaptive Boosting, is a statistical classification  meta-algorithm  s a technique in Machine Learning used as an Ensemble Method. The most common estimator used with AdaBoost is decision trees with one level which means Decision trees with only 1 split. These trees are also called Decision Stumps.  ➢  Bootstrap aggregation or bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision.  ➢  Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.  ➢  Out of Bag is equivalent to validation or test data. ➢  A Voting Classifier is a machine learning model that trains on an  ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.  ➢  Stacking is an ensemble learning technique to combine multiple  classification models via a meta-classifier.  ➢  Implementation of Random forest & boosting via python.  