Topic 9 taught us about the principles of Ensemble learning algorithm and decision trees.  Ensemble Learning  Ensemble Learning is the process of assembling a “team” of machine learning models to collaborate to reach a decision, by spreading out this load and having different “focus” points the collaborative nature allows a “voting” or “average” method to determine which models are correct. This dilutes the effect of individual model bias and reduces variance. An example is a “Random Forest” which as a collaboration of several decision trees. Another example is Adaboost which does a similar process with classifiers, i.e amalgamating the classifications from weaker classification models to generate a strong classification model.  Types of Ensemble Methods    AdaBoost – Classifiers (Weak) but on same or similar datasets   Bagging (Bootstrap) – Under-sampled datasets (Can be used with decision trees as well)  Bagging concept would have multiple decision trees with “part” of the puzzle for different regions of a conventional tree and a voting system to determine the final classification for a datapoint.  Bagging and Bootstrap principals lead to methods such as the Random Foreset, which creates a series of decision trees which are trained with random subsets of the training dataset.  hyperparameters are relevant for a Random Forest, Number of Tree and Number of Features. Too many trees are likely to cause overfitting.  Advantages of Random Forest  1.  Fast to Build and faster to predict 2.  Can split the resource constraint from individual trees across multiple computing resources (as  they are running as individual decision tree models)  3.  As it runs off the decision tree methodology, it does not encoding, or scaling as it is not measuring  distances.  4.  Automatic handling of missing values 5.  Feature importance can be derived  Disadvantages  1.  Data handling (follow the bouncing ball) of this method is more obfuscated (but still devisable) in  comparison to a standard decision tree.  2.  Other  disadvantages  of  decision  trees  (some  are  mitigated  though  by  the  random  forest amalgamation  principal  but  notionally;  high  computing  cost,  sensitive  to  small  changes,  large computing requirements).  Voting and Stack Classifiers  Ensemble  methods  aren’t limited to  “Common  Algorithm”  methodologies of amalgamation/voting. It is also possible to combine both KNN and decision trees models to generate a encompassing model that incorporated both elements (or from an even wider set). Each model could be given an equal vote or have it be “weighted”. This is a voting classifier.  Another type is an iterative (pyramid style) method of classifiers by having a “first round of classifiers” and then a second (or more rounds) of classifiers use the outputs from the first round as inputs for the second  round.  This  can  increase  the  predictions  accuracy  and  generalisability  by  achieving  a  more complex decision boundary.  