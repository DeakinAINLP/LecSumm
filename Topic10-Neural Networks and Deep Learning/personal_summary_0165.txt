In topic 9, we learnt about non-linear machine learning (ML) models with focus on random forest and ensemble methods. We started with an introduction to the concept of “Wisdom of Crowds” which states that the opinions and inputs of a larger group of people is more insightful than the knowledge of a single individual.  Ensemble learning tries to mimic the above concept by bringing together a combination of multiple ML models to provide better results than a single ML algorithm. For example, a single decision tree may have high speed but tends to overfit the data. Instead, if we create multiple trees via the random forest algorithm we can produce much better results and it reduces variance and enhances accuracy. In short, Random forest is the ensemble of decision tree.  Bootstrap Estimation: A ‘resampling’ method that extracts a smaller sample from the larger sample dataset. It may result in lower variance and higher accuracy results.  Bootstrap aggregation/bagging: General-purpose procedure for reducing the variance of a statistical learning method.  Random Forest Algorithm  The random forest algorithm is an ensemble method that is built on the concept of bagging and decision trees. In this case, it randomly creates decision trees by selecting subsets from the training dataset (bagging). It will then gather votes from the selected decision trees and choose the best one.  Characteristics:  If the number of trees are too high, it might lead to overfitting.    There is no pruning. The trees are ‘fully grown’.    The number of features is another important factor.   Lower correlation between trees leads to lower error rate. Increasing the number  of features also increases the correlation.    The higher the strength of each tree, the lower is the error rate.  We then learnt ways to measure the goodness of a bagged model, about the ‘out of bag’ error and feature importance. These would revolve around the test or validation dataset.  AdaBoost Algorithm      AdaBoost  or  adaptive  boosting  algorithm  is  primarily  used  in  classification  problems where several weak classifiers are combined to develop one strong classifier.  Feature Importance  The  importance  of  a  feature  can  be  gauged  by  the  random  forest  algorithm,  thereby helping in the feature selection process. This is done by measuring how much the feature helps  in  reducing  the  impurity  of  the  decision  tree,  and  therefore  the  higher  the contribution, the more important is the feature.  We were also  introduced to some advanced topics such as voting classifier and stack classifier. Overall, topic 9 was a very enriching topic in terms of knowledge and practical application.       