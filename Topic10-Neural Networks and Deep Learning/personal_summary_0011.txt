This topic’s content takes a further dive into supervised learning which is going to make up the last two topics of the trimester. This topic specifically focuses on decision trees and random forests. Ensemble Learning Ensemble learning is a ML process that works to combine a group of multiple models that have been strategically generated in order to solve a specific problem. The point of ensemble learning is to find the most accurate result based on the outputs of the different models. Each model will make slightly different errors in their predictions due to biases and how they have been configured but by combining the outputs, the most correct answer can be found. Bootstrap Estimation Bootstrap estimation, or bootstrap resampling, is a statistical technique that is used to estimate the variability of a sample statistic by sampling the original dataset with replacement over and over. For each estimation or resample, a sample set of the same size as the original is created. The values in the new dataset are selected from the original dataset but with replacement. This means that a single value from the original dataset can be selected multiple times and thus repeated in the new dataset. This is done over and over and the calculations are performed on the new datasets to allow for the possible variability to be seen which can be used to estimate the sampling error of the original sample statistic. AdaBoost Adaptive Boosting (AdaBoost) is an ML algorithm used for classification problems. AdaBoost is an example of ensemble learning as it works to combine multiple, weaker ML models into a single, strong model. The way AdaBoost works is that it calculates the weight of each sub model based on its performance. Each model starts with an equal weight to all the others and this is adjusted following an iterative process in which the model is trained, evaluated and updated several times before a final weight is determined. AdaBoost further works to focus each subsequent model on the misclassified examples from the previous model. This works to build stronger subsequent models and a stronger model overall. Bagging Bootstrap Aggregation (Bagging) is another ensemble learning technique that makes use of bootstrapping to create multiple subsets of a dataset that are then used to train multiple models. These models are often the same but since they are trained on different datasets, they will have slightly different biases which may lead to slightly different results. The models are then aggregated or combined by way of a majority vote for classification problems and an averaging of the models’ predictions for regression problems.  An example of bagging being applied is the random forest algorithm. Random Forest Algorithm The Random Forest Algorithm is an ensemble learning algorithm that applies the concepts discussed in bagging. It is a collection of decision trees that are generated from randomly selected subsets of the training dataset. Each tree is completely built out and is not pruned in any way. Further, at each node of each tree the features that will be used for splitting are randomly selected, this further increases the independence of the trees. When a prediction is needed, each tree makes a prediction and then these results are aggregated via either a vote for classification problems or by the averaging of the results for regression problems. Random Forests are very useful as they are able to function with missing data whilst maintaining accuracy and they won’t overfit the training data. They are also very useful for handling large, high-dimensional datasets.    Out of Bag Error As with any model, bagging methods and the Random Forest specifically need an evaluation method. This is where the Out of Bag (OOB) Error comes in. In bagging models, when each bootstrap sample set is created, some observations are excluded due to the sampling with replacement nature of bootstrapping. These excluded observations are known as Out of Bag observations. These OOB observations are then used as a validation dataset to test each model’s performance. OOB error provides an estimate of each model’s accuracy given that it is unseen data, thus OOB can be useful for comparing models with different algorithms or hyperparameter settings. Feature Importance When using a Random Forest, the significance of each feature within the dataset can be determined based on how much it helps to reduce the impurity within the decision trees. The goal of finding feature importance is to help increase the performance of the model as identifying the most important features can help provide insight into the underlying relationships within the dataset. This can then be used to increase the performance of each individual model which ultimately increases the performance of the entire ensemble model.Reflection This topic covered another set of extremely important content as decision trees are widely used within the ML space for things like email spam classification, movie recommendation, fraud detection and anomaly detections in networks or with equipment. 