  Ensemble learning is when multiple models are combined to perform analysis  Random forest is an ensemble method  Random forest combines multiple decision trees instead of one  Bootstrapping is obtaining a smaller sample from a larger sample  Adaboost combines weak classifiers to create a strong classifier  Bagging uses multiple classifiers which then vote at the end  Random forest is based on bagging  Random forest is fully parallelizable and can handle data with no preprocessing  Reflection: I learned about ensemble methods and random forest. I learned about how ensemble methods can perform better. In the python module I worked on creating a random forest for a dataset and then ran hyper parameter tuning to find the best parameters.  Quiz:   