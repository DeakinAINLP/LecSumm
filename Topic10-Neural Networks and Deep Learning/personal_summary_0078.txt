This topic's task required us to use an ensemble machine learning model to predict a target variable in a given dataset. I chose to implement this task using the Random Forest algorithm which I’ll detail below. The target class within the dataset was heavily imbalanced and as such made it difficult to tune the hyperparameters for the model. After many iterations and a 2.4-hour long Grid Search Cross Validation calculation, I found a range of parameters that did optimize the model better than my “educated” guesses.  The key takeaway for me this topic was not only the importance of the hyperparameters of the model but looking beyond the scoring metrics into the actual counts of the models true positive and true negative rates. I was able to achieve similar scores, but the underlying models were in fact quite different.  Ensemble Machine Learning  Ensemble machine learning essentially involves aggregating numerous models, or base learners which are often considered weak learners, into a single strong learner with the goal of improving the accuracy of predictions. The outcome of this approach enhances the model's performance by decreasing bias, minimizing variance, and improving predictions.  The critical mechanism that makes an ensemble method successful is the method used to select subsets of the data that are used to generate the base learners. Each subset needs to be slightly different to enable each base learner to contribute some different knowledge to the aggregated outcome. Ensemble methods can be applied to both classification and regression problems.  Bootstrap Estimation & Bagging  Bootstrap estimation is a resampling method used in statistics to approximate the variability of an estimate by creating many resamples with replacement from the dataset. It is often used in ensemble machine learning algorithms such as bagging.  Bagging also known as bootstrap aggregating, works by creating multiple subsets of the data, each of which is then used to train a model before aggregating the output of all the models as the final output. This process helps to reduce overfitting and improve the robustness of the model.  Like anything, bagging isn’t a magic bullet and can be computationally expensive with large datasets and it assumes that the data is a good representation of the population without bias.  AdaBoost  Adaptive Boosting (AdaBoost) is a machine learning algorithm designed for classification problems but can be adapted for regression problems. AdaBoost works by fitting a sequence of small decision trees (stumps) on repeatedly modified versions of the dataset which are then used to produce the final prediction by combining the sum of each of the stumps weighted results.  Through each iteration, the data is modified by giving more weight to the classifications that were incorrect helping them to have a better chance of a correct prediction in the next iteration. This process repeats until the defined number of iterations is reached. This process helps to create a more accurate and robust model but can overfit the model if the number of iterations is set too high.  Random Forest  The Random Forest is a machine learning ensemble algorithm that is the result of combining bagging and a variation on the decision tree based AdaBoost discussed above. The key difference is that Random Forest allows the decision trees to grow without any pruning to their full size. Individually, predictions made by decision trees may not be accurate, but combined, they produce a more robust and accurate model.  The Random Forest algorithm starts by bootstrapping the data using the bagging technique. Each of the subsets is then used to build a decision tree by selecting a random set of features at each node, then choosing the feature that provides the best split based on an objective function, such as Gini Impurity. Once all the decision trees have been built, each tree in the forest, which has its own prediction, are summed and the class with the majority of votes becomes the model's prediction. For a regression problem, the average prediction becomes the final prediction.  