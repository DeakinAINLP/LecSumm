TOPIC 9 NONLINEAR MODELS  Summary Ensemble learning Ensemble  learning  is  the  process  by  which  multiple  models,  such  as  classifiers  or  experts,  are strategically generated and combined to solve a particular computational intelligence problem.  To reduce the variance of unstable (high variance) learning methods such as decision trees, we can train  multiple  decision  trees,  each  with  slightly  different  subsets  of  data.  Then  when  doing classification/regression you take their combined decisions (via averaging for regression or voting for classification). This is called the ensemble method.  A popular ensemble method is the Random Forest  It has been shown that the variance of these ensemble models are lower. The critical point is to try to  design  an  ensemble  model  in  which  you  can  train  different  independent  models  with  slightly different subsets of data. The way the data is fed into the models can be challenging.  Bootstrap The  bootstrap  method  is  a  resampling  technique  used  to  estimate  statistics  on  a  population  by sampling a dataset with replacement during the sampling process.  It can be used to estimate summary statistics such as the mean or standard deviation. It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data.  A desirable property of the results from estimating machine learning model skill is that the estimated skill can be presented with confidence intervals, a feature not readily available with other methods such as cross-validation.  Replacement and Sampling In order to better understand bootstrapping, it is helpful to understand what’s meant by replacement and the impact that replacement has on probability. Replacement means that every time an item is drawn from the pool, that same item remains a part of the sample pool that will be drawn from in the next instance. This rule continues to apply for all subsequent samples. If you were to have completely removed the first sample from the sampling pool without placing it back in and then drew the second sample, the items drawn in that sample would not be as likely to occur as the items in the first sample because the overall population would now be smaller.  As you draw samples, you will make statistical calculations based on each one and then find the mean of that statistic across all samples. Once you have all of the statistics for each bootstrapped sample, you  can  plot  them  to  understand  the  shape  of  your  data  and  calculate  bias,  variance,  hypothesis testing and confidence intervals. Because each bootstrapped sample represents a randomly chosen subset  of  the  population,  we  can  make  inferences  about  the  entire  population  (Masters  in  Data Science, 2023).  We can summarize this procedure as follows:  1.  Choose a number of bootstrap samples to perform 2.  Choose a sample size 3.  For each bootstrap sample  1.  Draw a sample with replacement with the chosen size 2.  Calculate the statistic on the sample 4.  Calculate the mean of the calculated sample statistics.  The procedure can also be used to estimate the skill of a machine learning model.  The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. (Brownlee, 2018)  AdaBoost AdaBoost,  also  called  Adaptive  Boosting,  is  a  technique  in  Machine  Learning  used  as  an  Ensemble Method.  Less interpretable results than a single decision tree  Feature importance of using Random Forest (RF) The significance of each feature in the input dataset can also be determined using Random Forest. Based  on  how  much  it  helps  to  reduce  impurity  in  the  decision  trees,  the  significance  of  each characteristic is assessed. The higher the contribution, the more important the feature is. To increase the model's performance, feature importance utilising Random Forest can be utilised to find the most pertinent features for classification and feature selection.  Voting Classifier: An ensemble learning technique called a voting classifier combines the predictions of various separate classifiers to provide a final prediction. Several types of classifiers, such as Decision Trees, K-Nearest Neighbours, or Support Vector Machines, can be used individually. Each classifier is given one vote, and  the  final  forecast  is  determined  by  a  majority  vote.  Voting  Classifier  can  increase  prediction accuracy and robustness because it incorporates the benefits of various models while minimising the effects of their particular flaws.  Stack Classifier: Another ensemble learning technique that aggregates the predictions of various separate classifiers is the Stack Classifier, which is more complex than the Vote Classifier.  The first layer of a stack classifier comprises multiple separate classifiers that create predictions based on the input data. The second layer then integrates the previous layer's predictions to arrive at a final prediction. Several algorithms might be used at the second layer, including Decision Trees and Logistic Regression. Stack Classifier can increase the prediction's accuracy and generalizability by learning a more complicated decision boundary and minimising the chance of overfitting.  Reading List Brownlee, J., 2018. A Gentle Introduction to the Bootstrap Method. [Online] Available at: https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/ [Accessed 22 May 2023].  Geeks for Geeks, 2023. Bagging vs Boosting in Machine Learning. [Online] Available at: https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/ [Accessed 20 May 2023].  Masters in Data Science, 2023. What Is Bootstrapping?. [Online] Available at: https://www.mastersindatascience.org/learning/machine-learning- algorithms/bootstrapping/ [Accessed 20 May 2023].  Reflection Boosting is a process that has the following advantages.  Improved Accuracy – Boosting can improve the accuracy of the model by combining several weak models’ accuracies and averaging them for regression or voting over them for classification to increase the accuracy of the final model.  Robustness to Overfitting – Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly.  Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data points that are misclassified.  Better Interpretability – Boosting can increase the interpretability of the model by breaking the model decision process into multiple processes.  Boosting vs Bagging  Boosting  Bagging  In Boosting we combine predictions that belong to different types  Bagging is a method of combining the same type of prediction  The main aim of boosting is to decrease bias, not variance  The main aim of bagging is to decrease variance not bias  At every successive layer Models are weighted according to their performance.  All the models have the same weightage  New models are influenced by the performance of previously built models.  Each model is built independently.  Every new subset contains the elements that were misclassified by previous models.  Different training data subsets are selected using row sampling with replacement and random sampling methods from the entire training dataset.  Boosting tries to reduce bias.  Bagging tries to solve the over-fitting problem.  If the classifier is stable and simple (high bias) the apply boosting.  If the classifier is unstable (high variance), then apply bagging.  In this base classifiers are trained sequentially.  In this base classifiers are trained parallelly.  Example: The AdaBoost uses Boosting techniques  Example: The Random forest model uses Bagging.  Disadvantages of Boosting Algorithms  Boosting algorithms also have some disadvantages these are:    Boosting Algorithms are vulnerable to the outliers    It is difficult to use boosting algorithms for Real-Time applications. It is computationally expensive for large datasets     