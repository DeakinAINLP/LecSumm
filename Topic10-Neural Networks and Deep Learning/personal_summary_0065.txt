In topic 9, we learnt about Nonlinear models (Boosting and random forest)  Ensemble Learning: Ensemble learning is a machine learning technique that combines multiple weak learners/models to form a strong predictive model. Each model makes its own prediction, and the final prediction is made based on the majority vote (for classification) or the average (for regression) of the individual predictions. Ensemble learning methods can reduce both bias and variance in machine learning models, and they are less likely to overfit.  Bootstrap Estimation: Bootstrap estimation is a statistical method for estimating the sampling distribution of an estimator by sampling with replacement from the original sample. The bootstrap method is often used to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients.  AdaBoost: Adaptive Boosting (AdaBoost) is a boosting ensemble method that adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. AdaBoost also gives more weight to the more difficult to classify instances. The models are built sequentially, with each new model being trained to correct the errors made by the previous ones.  Bagging: Bootstrap Aggregating, or Bagging, is an ensemble method where the dataset is divided into sub- datasets using a process called bootstrapping. The sub-datasets are used to train a set of models, which independently make their predictions. The final prediction is made by averaging the predictions (in regression) or taking the majority vote (in classification). Bagging can decrease variance without increasing bias.        Random Forest Algorithm: Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It introduces randomness into the model construction process, which in turn provides a better model by reducing overfitting.  Out-of-Bag Error and Feature Importance: Out-of-bag (OOB) error is a method of measuring the prediction error of random forests, bagging, and other machine learning models utilizing bootstrap aggregating. In random forests, each tree is trained on a unique subset of the total training set. The OOB error is the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample.  Feature Importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable. Random forests indicate the importance of your features after training. This is achieved by aggregating the decrease in node impurity, averaged over all trees in the ensemble.  Feature Importance Using Random Forest (RF): The RF algorithm has an in-built method to rank the importance of features based on the number of times a feature is used to split data in all trees, and the resulting improvement in the prediction. Highly important features are those that lead to significant improvements in prediction or reduction in impurity.  Voting Classifier: A Voting Classifier is a model that takes an ensemble of different classifiers, fits them to the data, and predicts the class label as the class receiving the majority of votes. In case of a tie, it selects the class based on the highest averaging predicted probability. The two types of voting are: 'hard' voting where the predicted labels are used for majority rule voting, and 'soft' voting where the predicted probabilities are averaged and the class with the highest probability is predicted.  Stacking Classifier: Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is fitted based on the outputs, or the "meta- features", of the base level models. The idea is that the meta-model can learn to correct the predictions of the base learners, improving the overall performance        