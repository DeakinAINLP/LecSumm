 Topic 9 - Nonlinear models (Boosting and random forest) - Evernote  Topic 9 - Nonlinear models (Boosting and random forest)  Ensemble learning : It is the process by which multiple models, such as classifiers or experts, are  strategically generated and combined to solve a particular computational intelligence problem. For  example, to reduce the variance of unstable learning methods such as decision trees, we can train  multiple decision trees, each with a slightly different subsets of data. Then when doing  classification/regression you take their combined decisions.  Bootstrap sample :  A bootstrap sample is a smaller sample that is generated from a larger sample. It uses a resampling  method found in statistics.  AdaBoost :  Adaptive boosting is a machine-learning algorithm for classification problems. It works by combining  weak classifiers to create a strong classifier.  Bagging : It uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to  vote on a final decision. It is a general-purpose procedure for reducing the variance of statistical learning methods.  Random forest algorithm : It creates a set of decision trees from randomly selected subsets of the training dataset. It then  aggregates the votes from different decision trees to decide the final class of the test objects. In this method, the processes of finding the root node and splitting the feature nodes will run randomly.  In this method all trees are fully grown with no pruning We have two parameters, number of trees and number of features. If we increase the number of trees too  much it might cause overfitting.  In this method the error rate depends on the following : correlation between trees ( lower the better ) strength of single trees ( higher the better )  increasing number of features for each split :  increase correlation increases strength of single trees  Advantages and Disadvantages :  it is fast to build and predict trees can be run in parallel making it even faster ability to handle data without pre-processing  https://www.evernote.com/client/web?login=true#?an=true&fs=true&n=aec28b28-89b5-de3d-b144-602d46f03442&  1/3   Topic 9 - Nonlinear models (Boosting and random forest) - Evernote  data does not need to be rescaled, transformed or modified.  less interpretable results than a single decision tree.  Feature importance of using Random Forest (RF)  Based on how much a feature helps to reduce impurity in the decision trees, the significance of each  characteristic is assessed. The higher the contribution, the more important the feature is.  Voting Classifier : It combines the predictions of various separate classifiers to provide a final prediction.  It can increase prediction accuracy and robustness because it incorporates the benefits of various models  while minimising the effects of their particular flaws.  Stack classifier : Its first layer comprises multiple separate classifiers that create predictions based on the  input data. The second layer then integrates the previous layer's predictions to arrive at a final  prediction. It can increase the prediction accuracy and generalizability by learning a more complicated  decision boundary and minimising the chance of overfitting.  https://www.evernote.com/client/web?login=true#?an=true&fs=true&n=aec28b28-89b5-de3d-b144-602d46f03442&  2/3   Topic 9 - Nonlinear models (Boosting and random forest) - Evernote  https://www.evernote.com/client/web?login=true#?an=true&fs=true&n=aec28b28-89b5-de3d-b144-602d46f03442&  3/3   