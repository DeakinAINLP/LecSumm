   Ensemble learning is the process by which multiple models, such as classifiers or experts, are  strategically generated and combined to solve a particular computational intelligence problem.    Ensemble method trains multiple trees, each with slightly different subsets of data and then  combine decisions when doing classification/regression (via averaging for regression or voting for classification) to reduce the variance of unstable (high variance) learning methods.    A bootstrap sample is a smaller sample that is generated (bootstrapped) from a larger sample. It uses a resampling method found in statistics. In many cases bootstrap can result in less variance and more accurate results.    AdaBoost, which stands for Adaptive Boosting, is a machine-learning algorithm for classification problems. It works by combining weak classifiers to create a strong classifier. AdaBoost steps:  o  Step 1: Initialize weights o  Step 2: Train weak classifier o  Step 3: Evaluate classifier o  Step 4: Calculate classifier weight  Step 5: Update weights    Bagging uses multiple classifiers trained on different under-sampled subsets and then allows these classifiers to vote on a final decision. Bootstrap aggregation or bagging (B+agg), is a general-purpose procedure for reducing the variance of a statistical learning methods.    The random forest classifier creates a set of decision trees from randomly selected subsets of the training dataset. It then aggregates the votes from different decision trees to decide the final class of the test objects. The processes of finding the root node and splitting the feature nodes will run randomly.    Random forest builds on the idea of bagging. Each tree is built from a bootstrap sample of data. Node splits are calculated from random feature subsets to make sure each of the trees is as  independent as possible    Advantages/Disadvantages of Random Forest  1.  random forest is fast to build and to predict 2.  fully parallelizable since you can run trees in parallel 3.  ability to handle data without pre-processing and does not always require to dataset  normalisation  4.  data does not need to be rescaled, transformed, or modified (Resistant to outliers) 5.  automatic handling of missing values (a property of decision trees) 6.  less interpretable results than a single decision tree    Voting Classifier:  An ensemble learning technique called a voting classifier combines the  predictions of various separate classifiers to provide a final prediction. Several types of classifiers, such as Decision Trees, K-Nearest Neighbors, or Support Vector Machines, can be used individually. Each classifier is given one vote, and the final forecast is determined by a majority vote. Voting Classifier can increase prediction accuracy and robustness because it incorporates the benefits of various models while minimising the effects of their particular flaws.    Stack Classifier: Another ensemble learning technique that aggregates the predictions of various separate classifiers is the Stack Classifier, which is more complex than the Vote Classifier. The first layer of a stack classifier comprises multiple separate classifiers that create predictions based on the input data. The second layer then integrates the previous layer's predictions to arrive at a final prediction. Several algorithms might be used at the second layer, including Decision Trees and Logistic Regression. Stack Classifier can increase the prediction's accuracy and generalizability by learning a more complicated decision boundary and minimising the chance of overfitting.      