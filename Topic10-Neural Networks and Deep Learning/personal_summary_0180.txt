In topic 9 of the course we covered ensemble learning, largely in the form of examining the random forest algorithm. We also covered some of the techniques used in ensemble learning, such as bootstrapping, adaptive boosting and bootstrap aggregation.  Ensemble learning  Ensemble learning is a technique in which multiple models1 are created and combined to form a more accurate predictive model. Having a collection of models allows us to train each model on a different subset of the data. The technique of combining multiple models and aggregating their output is known as the ensemble method (‘Ensemble Methods: Elegant Techniques to Produce Improved Machine Learning Results | Toptal®’ n.d.).  If we do have a collection of models, we need to have a way of aggregating the predictions of the models. There are several options here:   Voting: the final output is obtained by aggregating the models results through a voting  scheme.   Bagging  : the models are trained on different subsets of the training data, and the predictions  are either averaged or voted for.  1  Sometimes termed the ‘base’ models or ‘weak’ learners.    Boosting: the models are trained and run sequentially, with each model focusing on  correcting the mistakes of the prior model in the sequence. Typically the output is obtained by weighted voting, with each models vote being weighted according to its performance.    Stacking: a new ‘meta’ model is introduced to combined the predictions of the base models and produce the final prediction.  Ensemble models will have a lower variance than other models because:  1. Diverse models are aggregated. This means that multiple predictions will be aggregated into one prediction, thus reducing the impact of any errors in the individual models that are used to build the result.  2. The risk of over-fitting is reduced. As each model in the ensemble is trained on a subset of the data, this reduces the chances of the combined output over fitting the entire dataset.  3. Adaptive learning and error correction. If the ensemble is iteratively trained on as part of a sequence, each subsequent model in the sequence will correct the errors of the previous model. This adaptive learning will reduce the overall variance and improve the prediction accuracy.  4. The impact of outliers or noise in the data is reduced. If the ensemble is making predictions based on a voting or consensus scheme, the impact of outliers or noise will be reduced, as the variations in the individual predictions will be filtered out.  Ensemble methods are still actively researched, and so new variations are being tried and developed.  Bootstrap estimation  “Bootstrap” is a statistical technique that describes iteratively sampling from a population with replacement. When used in machine learning it has the advantage that the estimate can be presented with confidence metrics.  The samples are randomly drawn from the data source sequentially. Once drawn they are returned to the data source. Thus a given data point can appear in several samples. Data points that don’t appear in a given sample are termed the “out of bag” samples.  The bootstrap process proceeds as follows:  1. Choose the number of bootstrap samples that will be taken.  2. Chose the sample size that will be used.  3. Randomly draw a sample with replacement; then:  a) Fit a model on the sample.  b) Estimate the skill of the model using the out of bag samples.  4. Calculate the mean of the sample using the skill estimate.  Steps 3 and 4 are repeated until the selected number of samples has been taken.  Adaptive Boosting (AdaBoost)  Boosting is a technique whereby we build and a model on the training data: then we build another model to rectify the errors in the output of the first model. We do this repeatedly until the errors are minimised and the predictions are acceptable. We are effectively combining the output of multiple models to produce an acceptable final output.  There are several different boosting algorithms in use: however, the course has focused on Adaptive Boosting (AdaBoost).  When using AdaBoost the steps are as follows:  1. Build a model and use it to make predictions.  2. Assign higher weights to the data points that our current model miss-classified.  3. Build a new model.  4. Repeat steps 2 & 3 until the predictions are acceptable.  5. Make the final model by using the weighted average of all of the individual models.  There is a very good illustration of this process from Analytics Vidhya (Saxena 2021):  The data consists of blue ‘+’ and red ‘-’.  Model 1 miss-classifies three blue ‘+’. So they are given a higher weight and model 2 is built.  Model 2 now classifies the three blue ‘+’ correctly, but miss-classifies three red ‘-’. So they are given a higher weight and model 3 is built.  As can be seen, between them the 3 models do not classify the points correctly. However, the final ensemble model that aggregates the three using the weighted mean of the individual models correctly classifies all of the data.  Bootstrap aggregation (Bagging)  Bagging is a form of bootstrap estimation in which the final prediction is obtained by aggregating the predictions of the base models. The aggregation done can take different forms dependent on the prediction being done. If a classification is being performed, then majority voting is commonly used. If a regression is being performed, then averaging the predictions to get the final prediction is a common practice.  Random forest  A random forest model is one in which the output of multiple uncorrelated decision trees is combined to reach the final result.  The random forest model has built in feature importance that can be computed in two ways:  1. Gini importance: (the decrease in impurity), which is found in the random forest structure. Each decision tree in the forest is a set of nodes and leaves. In each node, the selected feature is used to to make a decision on how to divide the dataset into two separate sets. It is possible to measure how each feature decreases the impurity of the split. This is the default method used in the scikit-learn (Pedregosa et al. 2011) implementation of the random forest. The advantage of using this method is that all the needed values are computed during training, making it fast.  2. Mean decrease accuracy: a way of computing the importance on out-of-bag samples.  Out-of-bag error and feature importance  If we are using a bagged model, we can estimate the goodness of the output. Each new tree is fitted with a bootstrap sample of the training observations. The out-of-bag error is the average error for each wrong prediction made by trees for sample data not in their bootstrap samples. This is a convenient way to estimate the performance of the ensemble model without using a separate validation data set. Note that if the data is processed in such a way that information is transferred across the bootstrap samples then any estimate made will probably be biased.  Classifiers  SIT 720 students were required to examine:   Voting classifiers    Stack classifiers  Voting classifier  Voting classifiers are ensemble methods that combine the predictions of multiple models and predict an output based on the the highest probability of their chosen output. In essence they simply   aggregate the findings of each model whose output is passed into the voting classifier and select the output based on a voting scheme. There are several different voting schemes commonly used:   Hard voting: the class label that receives the majority of the votes is chosen. If there is a tie then the class label can either be selected according to a preset rule, or randomly selected.    Soft voting: the models provide both a prediction and a confidence indicator. The indicators are then averaged, and the class label with the highest confidence is chosen.   Weighted voting: similar to soft voting, but weights are associated with the predictions of  each model. The vote given to each model is then based on the weights.  The process of creating a voting classifier is typically is:  1. Select the base classifiers to be used as the base models in the ensemble.  2. Train each of the base classifiers independently on the training data.  3. Run the classifiers on unseen data and gather the predictions they make.  4. Aggregate the predictions using the selected voting strategy to determine the final  prediction.  Stack classifier  Stack generalisation is an ensemble method in which the the predictions of multiple models are combined through another model, which is trained to make predictions based on the input from the base models. This is clearly a more complex structure than voting classifiers.  