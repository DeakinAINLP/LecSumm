 Main Points  Ensemble Learning : probable outcomes of developing machine learning models  -sometimes weak and inaccurate  -some performs better on specific occasions  Generating and combining multiple models to solve a particular computational  intelligence problem  Bootstrap Estimation  A bootstrap sample is a smaller sample that is generated from a larger sample.  It uses a resampling method found in statistics.  Bootstrap in many cases can result in less variance and more accurate results.  Bagging  Bootstrap aggregation or bagging is the procedure for reducing the variance of the  statistical learning method. It uses multiple classifiers trained on different under-  sampled subsets.  Random Forest Classifier: create a set of decision trees from randomly selected  subset of training set.  Each tree is built from a bootstrap sample of data  Form the tree based on the best feature from the subset  Repeat these steps times, where is the number of the trees  Impact on bias  Random Forest Algorithm:  All trees are fully grown and no pruning  Dealing with two parameters: number of trees; number of features  Select a new bootstrap sample from the training set  Build an un-pruned tree on the bootstrap sample  At each internal node of the tree, randomly select features and determine the best  split using only these features.  Random Forest Algorithm Testing:  Output overall prediction as a mean from all individually trained trees  The error rate depends on: correlation between trees, strength of single trees  Summary of Reading  