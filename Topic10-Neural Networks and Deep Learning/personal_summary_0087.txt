 Ensemble methods involve using different machine learning models together to improve accuracy and performance. They can be models of the same or different types, but it is important to ensure that they are learning different things. The predictions can then be combined using averaging or voting. Bootstrap sampling is a technique from statistics in which random samples are drawn with replacement from a dataset to create new datasets. It can be useful to reduce variance or improve accuracy.  AdaBoost is a type of ensemble method that combines weak classifiers to form a strong one. By assigning weights to each instance in the training set, AdaBoost iteratively adjusts weights to help instances that were previously misclassified.  Bootstrap aggregating or Bagging is a technique in which the original dataset is bootstrapped and each subset is used to train a separate base model. The predictions from each model are then combined to form a final prediction.  The Random Forest algorithm uses a large number of decision trees all trained on different subsets of the training data. The independent predictions from each of the trees are then averaged or voted to determine the final prediction. Random Forests are accurate, scalable, robust and can easily handle missing data. A potential negative is that their output and decision process are often difficult to interpret when compare to single decision trees.          