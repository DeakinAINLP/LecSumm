topic 9 covered bootstrap estimation and bagging bootstrap estimation a statistical model that estimates the sampling distribution of a parameter by creating multiple resamples. bagging an ensemble learning method that uses bootstrap estimation to create multiple training sets. each training set is used to train an individual base model the final prediction is obtained by averaging in regression problems voting in classification problems adaboost another ensemble learning method that iteratively trains a series of weak learners ie decision trees and combines them into a final strong learner. for each iteration the weak learner focuses on the instances misclassified by the previous learners assigning higher weights to these instances the final prediction is a weighted combination of the predictions made by each weak learner random forest algorithm yet another ensemble learning method that constructs many decision trees and combines their predictions. improves on the bagging method by adding an additional layer of randomness when building each tree only a random subset of features is considered for splitting each node this helps reducing both the variance and overfitting improving to better overall performance out of bag error occurs in bagging and random forests is an unbiased estimate of the ensemble model's performance it is calculated by using the instances not included in the bootstrap sample to test the model works due to the fact that each bootstrap omits approximately a third of the original dataset as such these instances can be used to assess the model's performance without the need for a separate validation set feature importance is a measure of how useful a feature is for making accurate predictions in the context of ensemble methods feature importance is typically calculated based on the decrease in impurity when splitting on a feature the more a feature contributes to the decrease in impurity, the more important it is considered for making predictions. 