 Ensemble Learning  Ensemble learning is the process by which multiple models (classifiers) are generated then combined to solve a problem – it helps to overcome a dataset which is weak or inaccurate.  Ensemble learning involves the use of multiple decision trees, each with slightly different subsets of data. Decision trees can be trained in order to reduce the variance of learning methods.  For regression, a training average is used during ensemble learning to help reduce variance, whereas classification makes a combined voting decision to reduce variance.  Bootstrap  A bootstrap sample is a smaller sample that is generated from a larger sample, which can result in less variance and more accurate results.  Adaboost  Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.  Bagging  Bootstrap aggregation or (bagging) reduces the variance of statistical learning methods. It uses multiple classifiers (decision trees) trained on different under-sampled subsets, which then permits a vote on a final decision.  Random Forest  A popular ensemble method is the Random Forest, which is a classifier that creates a set of decision trees from randomly selected subsets of the training dataset.  Votes are aggregated from different decision trees to decide the final class of the test objects.  Random Forest “feature importance” is used to find the most significant features for classification and selection. The higher the contribution the more important a feature is.  Classifiers  Vote Classifier  Instead of creating separate dedicated models and finding the accuracy for each them, a single “voting classifier” model is created which is trained by these models. An output is predicted based on the highest probability of the combined majority vote for each output class.  Voting Classifier supports two vote types:    Hard vote: the predicted output class is a class with the highest majority of votes, or the class which had the highest probability of being predicted by each of the classifiers.    Soft vote: the output class is the prediction based on the average of probability given to that  class.  Stacking Classifier  Stacking is an ensemble learning algorithm that learns how to best combine the predictions from multiple well-performing machine learning models  The benefit of stacking is that it can maximise the capabilities of a range of well-performing models on a classification or regression task and make better performing predictions.      