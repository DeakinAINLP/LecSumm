 What is unsupervised learning? Unsupervised  learning  is  a  type  of  machine  learning  in  which  models  are  trained  using unlabeled dataset and are allowed to act on that data without any supervision.  Clustering Clustering is a data mining technique which groups unlabeled data based on their similarities or  differences.  Clustering  algorithms  are  used  to  process  raw,  unclassified  data  objects  into groups represented by structures or patterns in the information. Clustering algorithms can be categorized  into  a  few  types,  specifically  exclusive,  overlapping,  hierarchical,  and probabilistic.  Dimensionality reduction Dimensionality reduction is a technique used when the number of features, or dimensions, in a given dataset is too high. It reduces the number of data inputs to a manageable size while also preserving  the  integrity  of  the  dataset  as  much  as  possible.  It  is  commonly  used  in  the preprocessing data stage.  Types of distance measurements: 9 Distance Measures in Data Science  1.  Euclidean Distance:  Euclidean distance works great when you have low-dimensional data and the magnitude of the vectors is important to be measured.  2.  Cosine Similarity:  We  use  cosine  similarity  often  when  we  have  high-dimensional  data  and  when  the magnitude of the vectors is not of importance.  3.  Hamming Distance:  Hamming distance is the number of values that are different between two vectors. It is typically used to compare two binary strings of equal length. It can also be used for strings  to  compare  how  similar  they  are  to  each  other  by  calculating  the  number  of characters that are different from each other.  4.  Manhattan Distance:  When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes.   5.  Chebyshev Distance:  Chebyshev distance can be used to extract the minimum number of moves needed to go from one square to another.  6.  Minkowshi:  Common values of p are:    p=1 — Manha5an distance   p=2 — Euclidean distance   p=∞ — Chebyshev distance  This measure has three requirements:    Zero Vector — The zero vector has a length of zero whereas every other vector has a positive length. For example, if we travel from one place to another, then that distance is always positive. However, if we travel from one place to itself, then that distance is zero.    Scalar Factor — When you multiple the vector with a positive number its length is changed whilst keeping its direction. For example, if we go a certain distance in one direction and add the same distance, the direction does not change.   Triangle Inequality — The shortest distance between two points is a straight  line.  7.  Jaccard Index:  The Jaccard index is often used in applications where binary or binarized data are used. When you have a deep learning model predicting segments of an image.  8.  Haversine:  Haversine distance is the distance between two points on a sphere given their longitudes and latitudes. It is very similar to Euclidean distance in that it calculates the shortest line between two points. The main difference is that no straight line is possible since the assumption here is that the two points are on a sphere.         Task3_1  9.  Sørensen-Dice Index:  The  Sørensen-Dice  index  is  very  similar  to  Jaccard  index  in  that  it  measures  the similarity  and  diversity  of  sample  sets.  Although  they  are  calculated  similarly  the Sørensen-Dice index is a bit more intuitive because it can be seen as the percentage of overlap between two sets, which is a value between 0 and 1.  K-Means Clustering A K-means clustering algorithm tries to group similar items in the form of clusters. The number of groups is represented by K.  k-means  clustering  tries  to  group  similar  kinds  of  items  in  form  of  clusters.  It  finds  the similarity between the items and groups them into the clusters. K-means clustering algorithm works in three steps. Let’s see what these three steps are.  1.  Select the k values. 2.  Initialize the centroids. 3.  Select the group and find the average.   There are two types of evaluation metrics for clustering,    Extrinsic  Measures:  These  measures  require  ground  truth  labels,  which  may  not  be    available in practice. Intrinsic Measures: These measures do not require ground truth labels (applicable to all unsupervised learning results)  There are two types of evaluation metrics for clustering, Extrinsic Measures: These measures require ground truth labels, which may not be available in practice Intrinsic Measures: These measures do not require ground truth labels (applicable to all unsupervised learning results)1/  1.  Rand Index:  Rand  Index  (RI,  ARI)  measures  the  similarity  between  the  cluster  assignments  by making pair-wise comparisons. A higher score signifies higher similarity.  2.  Mutual Information:  Mutual  Information  (MI,  NMI,  AMI)  measures  the  agreement  between  the  cluster assignments. A higher score signifies higher similarity.  3.  V-measure:  V-measure  measures  the  correctness  of  the  cluster  assignments  using  conditional entropy analysis. A higher score signifies higher similarity.  4.  Fowlkes-Mallows Scores:  Fowlkes-Mallows  Scores  measure  the  correctness  of  the  cluster  assignments  using pairwise precision and recall. A higher score signifies higher similarity.  5.  Silhouette Coefficient:  Silhouette  Coefficient  measures  the  between-cluster  distance  against  within-cluster distance. A higher score signifies better-defined clusters.  6.  Calinski-Harabasz Index:  Calinski-Harabasz  Index  measures  the  between-cluster  dispersion  against  within- cluster dispersion. A higher score signifies better-defined clusters.  7.  Davies-Bouldin Index:  Davies-Bouldin  Index  measures  the  size  of  clusters  against  the  average  distance between clusters. A lower score signifies better-defined clusters.   A list of 10 of the more popular algorithms is as follows:    Affinity Propagation   Agglomerative Clustering   BIRCH   DBSCAN   K-Means   Mini-Batch K-Means   Mean Shift   OPTICS   Spectral Clustering   Mixture of Gaussians  1.  Affinity Propagation  Affinity Propagation involves finding a set of exemplars that best summarize the data.  2.  Agglomerative Clustering  Agglomerative  clustering  involves  merging  examples  until  the  desired  number  of clusters is achieved.  3.  BIRCH  BIRCH  Clustering  (BIRCH  is  short  for  Balanced  Iterative  Reducing  and  Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.  4.  DBSCAN  DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications  with  Noise)  involves  finding  high-density  areas  in  the  domain  and expanding those areas of the feature space around them as clusters.  5.  K-Means  K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.  6.  Mini-Batch K-Means  Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.  7.  Mean Shift  Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.  8.  OPTICS  OPTICS  clustering  (where  OPTICS  is  short  for  Ordering  Points  To  Identify  the Clustering Structure) is a modified version of DBSCAN described above.  9.  Spectral Clustering  Spectral Clustering is a general class of clustering methods, drawn from linear algebra.  10. Gaussian Mixture Model  A Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.  