The topic for this topic centered on Clustering, which is a data analysis technique employed in unsupervised learning. Its purpose is to group similar data points together using a similarity metric, thereby uncovering patterns and facilitating exploratory analysis. By clustering data points with similar properties, clustering algorithms enable the discovery of data properties. There are three steps that all clustering algorithms adhere to, define a method to calculate the distance between data points, define an objective function, and optimize the objective function. When define clusters we aim to minimize the intra-distance between data points of the same cluster and maximize inter-distance between data points of different clusters.  Distance metrics are the primary method of identifying similarity and clustering data points. For any distance metrics the function that calculates distance must, return zero when measuring the distance / similarity with itself, return a non-negative number when calculating the similarity between any two data points, the metric between any two data points must be symmetrical (i.e. provide the same measure when measuring from point a to point b or point b to point a) and adhere to triangular equality (for any three data points the sum of the distance between data points a & b and b & c must be greater than the distance between a & c).  Distance metrics are used in both supervised and unsupervised learning, and can group / cluster data points together based on distance between the data points. There are multiple distance measurement calculations. In this topic’s content we touched on, nearest neighbor, Euclidean, Cosine, Mahalanobis, Manhattan, Minkowski, Jaccard and Chebyshev.    Nearest neighbor adds a data point to a cluster based on the group the nearest neighbor.   Euclidean distance calculates the straight-line distance between two points.   Cosine distance calculates the angular distance between two points.   Manhattan distance calculates the distance between two points, while traversing the in straight  lines at 90 degree angles.  K-means is a commonly used cluster algorithm for its combination of accuracy / flexibility and simplicity. In the K-means algorithm data points are grouped in K number of clusters, however, there are multiple methods that can be used to choose the initial centroids. The most basic method for choosing a centroid is selecting the points at random. This approach while simple can lead to poor results depending on the placement of the centroids. Another method is to choose a random first centroid then for the second centroid place it at the location of the data point that is furthest from the initial centroid, and repeat this process for the remaining centroids. This approach is known as K-means++. As new data points are added to the cluster the centroid’s location is update to be at the centre of all of the data points that are part of the cluster. This is then repeated until the centroid barely moves.  There are several known limitations of K-means clustering. It gives different clusters each time it is run on the same data, have to provide the number of clusters (K value), can only provide uniform clusters and cannot accurately detect noisy data points. To address some of these issues the Elbow method is used to find the best value of K for the algorithm by running the K-means algorithm multiple times with different K values and calculation the square error (SSE) each time then plot / analysis to identify where adding additional clusters does not reduce the error rate much.  To assess the effectiveness of clustering, two methods are used: external assessment and internal assessment. External assessment measures how accurately the data was clustered based on actual grouping of data in the dataset. The Rand index, purity, and mutual information are commonly used in external assessment. Internal assessment calculates how well the data set is clustered without any prior knowledge about the data set. The Silhouette coefficient is commonly used in internal assessment to measure how similar an object is to its own cluster compared to other clusters.  This topic briefly touches on but not go into any detail about other clustering algorithms, such as, hierarchical, density, shape based, grid based and model based. Additionally, the unit site also includes examples of K-means clustering and evaluation using Python.  