 Topic 3 for Machine Learning was all about the diﬀerent types of distance measurements and the diﬀerent types of clustering algorithms.  First, we learnt about what is a Distance Metric and then we learnt about the diﬀerent types of Distance measurement.  Distance Metrics  1.  Measuring similarity or distances between diﬀerent data points.  a.  Unsupervised learning like K-means b.  Supervised learning like K-Nearest  2.  Distance measures are funcHons that deﬁne a distance between two data instances  and how similar the instances are  3.  Distance measures saHsﬁes 3 criteria to become a Distance Metric  a.  Distance with itself is 0 b.  The distance is non-negaHve and symmetric for any instance pair c.  Distance measure follows triangular inequality  Distance measurement types  1.  Cityblock/ManhaOan distance  a.  Absolute sum of distance along any coordinate system  2.  Euclidean distance  a.  Straight line distance between two points in Euclidean space  3.  Chebyshev distance  a.  Greatest of distance along any coordinate system  4.  Minkowski distance  a.  Deﬁnes a distance between 2 points in a normed vector space b.  1st normed distance is Cityblock/ManhaOan distance c.  2nd normed distance is Euclidean distance  5.  Cosine distance  a.  Represented by a d-dimensional feature vector b.  Used in cases when the magnitude of the vector does not maOer and we are  concerned about the angle between the two vectors  6.  Mahalanobis distance  a.  MD can be thought if scaling each data dimension by its variance and  adjusHng for their relaHonships  b.  When data is independent, then MD becomes same as Euclidean distance c.  Used when we have conHnuous numerical variables and we want to reﬂect  absolute distance but we want to remove redundancies. If we have repeated variables, their repeaHng eﬀect will disappear  7.  Jaccard distance  a.  Is a distance used to measure the diversity of any two sets b.  Is deﬁned as 1 - (the logical and operaHon between elements of two sets  divided by the logical or operaHon between elements of the same two sets)  Next, we learnt about what the goals are for clustering algorithm and the steps in it.  Goal of clustering algorithms are  1.  Group objects of similar properHes together 2.  Discover clusters and groups in the data 3.  Find valid organizaHon of data 4.  Two main goals  a.  Minimise intra-distance between points in same cluster b.  Maximise inter-distance between diﬀerent clusters  Clustering method steps  1.  Deﬁne a distance metric 2.  Deﬁne an objecHve funcHon 3.  Devise an algorithm to opHmise the objecHve funcHon  Next, we learnt about K-Means clustering, limitaHons of K-Means clustering and how to evaluate a clustering method.  K-means Clustering  1.  Stores k centroids 2.  A point is considered to be in parHcular cluster if it is closer to that cluster centroid  than any other centroid  EvaluaHon of Clustering  1.  External assessment - Compare clustering performance against a known clustering  (Ground truth or Gold standard)  a.  Rand Index  i.  Measure of similarity between two data clusters  b.  Purity  i.  Measure the purity for all clusters in terms of class labels of the data  in each cluster  c.  Mutual informaHon  i.  Mutual informaHon is a funcHon that measures the agreement of two clustering assignments in terms of how informaHve one is about the other, ignoring permutaHons  2.  Internal assessment - Determine if clustering follows certain intrinsic assumpHons  like cluster-to-cluster distance or cluster size  a.  SilhoueOe Coeﬃcient  i.  It is a measure of how similar an object is to its own cluster ii.  This method does not require the ground truth assignments  LimitaHons of K-means  1.  Random iniHalisaHon which means we end up with diﬀerent cluster each Hme 2.  We need to supply the number of clusters beforehand 3.  It cannot ﬁnd clusters of arbitrary shapes 4.  It cannot detect noisy data points         How to ﬁnd number of clusters in K-means  1.  Elbow method  a.  Run the Kmeans for a range of values and ﬁnd opHmal value  K-means++ clustering  1.  Choose one centroid uniformly at random from dataset 2.  Choose the new centroid based on probability value  Other clustering algorithms  1.  ParHHon based clustering 2.  Hierarchical based clustering - groups data points in a hierarchical structure  a.  AgglomeraHve clustering or boOom-up: each observaHon starts in its own cluster and pairs of clusters are merged as one moves up the hierarchy b.  Divisive or top-down: all observaHons start in one cluster and splits are  performed as one moves down the hierarchy  3.  Density based clustering - trying to ﬁnd cluster of arbitrary shapes within a dataset. Based on the density of the neighbouring points. We deﬁne epsilon - max distance between two points to form a cluster. We deﬁne min number of points within a cluster as a parameter input. We deﬁne "core point" as a point which has the deﬁned number of data points within the minimum deﬁned distance. We deﬁne "non-core point" as a point which has less than the deﬁned number of data points within the deﬁned distance but not 0.  4.  Grid based clustering 5.  Model based clustering 6.  Shape based clustering  