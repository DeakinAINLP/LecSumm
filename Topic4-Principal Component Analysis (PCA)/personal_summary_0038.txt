 Clustering – finds patterns in big data sets when data is not labelled. Dimensionality reduction – helps when number of features that describe difference in data become too large to manage.  Distance metrics are functions used in machine learning algorithms to define a distance between two data points. They must have properties: - distance between itself and an instance is 0. - distance between any pair of instances is non-negative and symmetric. - distance measure follows triangular inequality.  Examples of distance metrics are nearest neighbour classification and image retrieval.  Some types of distance measurements are: Euclidean (most common), cosine, Mahalanobis, Cityblock, Minkowski, Jaccard.  Clustering puts data points into groups based on similarity and difference of features/dimensions. The goal is to minimise intra-distance (same cluster) and maximise inter-distance (different cluster). Set-up of clustering includes: - define a distance metric between objects - create an objective function - develop algorithm to optimize the objective function  Kmeans is the most popular clustering algorithm. Kmeans stores the k centroids, which represents the centre points of each cluster. A cluster is a group of data points that are like each other in some way. A data point is part of a cluster when it is closest to a centroid.  Kmeans assigns data points to clusters based on the centroids and repeats until there are useful groupings.  Two main ways of evaluating machine learning algorithms. External assessment – evaluates performance on a known/predefined clustering. Internal assessment – evaluates performance on properties of the data such as cluster size or distance between cluster.  Silhouette coefficient and Dunn index are examples of internal assessment.  The Rand Index is a measure of similarity between two data clusters, comparing suggested assignments of data instances to different clusters by a clustering algorithm. The adjusted rand index takes chance into account and corrects bias introduced by chance.  Purity is a way of measuring how well a clustering algorithm groups similar data points together by looking at how many data points with the same true label/class end up in the same cluster.  Mutual information is a measure of how similar two clustering assignments are, and it helps evaluate clustering algorithms by calculating how much information one clustering assignment gives about the other.  The Silhouette Coefficient is a measure of how well each data point in a cluster is similar to the other points in that same cluster, compared to how similar it is to the points in the nearest neighbouring cluster.  Kmeans clustering limitations include: - the requirement to provide the number of clusters beforehand. - the possibility of different cluster assignments due to random initialization. - the inability to find clusters of arbitrary shapes or detect noisy data points.  The Elbow method is a way to find out the appropriate number of clusters.  Kmeans++ is a modified version of Kmeans that chooses the initial cluster centroids in a smarter way by monotonically decreasing the objective function with each iteration to always get closer to the best solution.  