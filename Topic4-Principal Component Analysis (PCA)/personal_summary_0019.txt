Tristen Clifton 221211319 SIT307 (Topic 3)  Summarize the main points that is covered in this topic.  Clustering is about grouping values based on their distance from other groupings,  This is achieved using distance metrics to define the distance, then distance measurements calculate the distance and finally grouping them based on this distance.  Examples of distance metrics: K-Nearest-Neighbor, Support Vector Machines (SVM), data visualization, information retrieval and ranking.  Examples of distance measurements: Euclidean distance, Cosine distance, Mahalanobis distance, Cityblock/Manhattan distance, Minkowski distance and Jaccard distance.  A clustering algorithm has 2 goals, Minimizing the intra-distance, that being the distance between points in a cluster, while maximizing the inter-distance, that being the distance from other clusters.  K-means is the most popular clustering algorithm, It works by selecting 2 or more points, grouping points to either cluster, then moving the initial points towards the average of the groups. It will keep repeating this until there is little movement between iterations.  The issue with K-Means is that it is very sensitive to the starting position, To work aground this Kmeans++ is used.  Kmeans++ works by choosing a random starting position initially, but it takes the end result and starts from a better position the second time, it repeats this improving each time.  Provide summary of your reading list – external resources, websites, book chapters, code libraries, etc.  https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics  https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html  Reflect on the knowledge that you have gained by reading contents of this topic with respect to machine learning.  This topic was mostly new content, however I have studied Kmeans in other classes, but I did not know how to evaluate the performance of Kmeans, nor did I fully understand the limitations.  I think the most important thing I learnt was the fact that I can use the metrics of Accuracy, Precision and recall to evaluate any clustering algorithm to evaluate its performance and determine if it was a good choice of algorithm.      Attempt the quiz given in topicly content (3.13) and add screenshot of your score (>85% is considered completion of the task) in this report.   Topic 3 Problem Solving  March 22, 2023  0.1 Tristen Clifton : 221211319 : Topic 3 Activitiy  0.1.1 Task explanation  In this topics activity I am tasked with calulating K means, As well as detmining the performance of its execution and successive executions  0.1.2 Load data from digitData2.csv” file. The last column of each file presents the  label and rest of the columns are features.  [33]: import pandas as pd  import numpy as np np.random.seed(333) import matplotlib.pyplot as plt import sklearn.metrics as metrics from sklearn.metrics import classification_report from sklearn.cluster import KMeans from yellowbrick.cluster import SilhouetteVisualizer from yellowbrick.cluster import KElbowVisualizer  digitData2 = pd.read_csv('digitData2.csv',header=None) print(digitData2.shape)  (1528, 65)  [34]: Feature_set = digitData2  X=Feature_set.iloc[:,0:-1] y_true=Feature_set.iloc[:,-1] print(X.shape) print(y_true.shape)  (1528, 64) (1528,)  In these code blocks i load needed librarys, I then Load the data from the given CSV file, And Then extract the feature set as well as the label set.  1  0.1.3 Selecting the optimum k value using Silhouette Coeﬀicient and plot the opti-  mum k values.  [35]: model = KMeans()  visualizer = SilhouetteVisualizer(model, colors='yellowbrick') visualizer.fit(X) visualizer.show()  [35]: <AxesSubplot:title={'center':'Silhouette Plot of KMeans Clustering for 1528  Samples in 8 Centers'}, xlabel='silhouette coefficient values', ylabel='cluster label'>  Using the Silhouette visualiser function of the yellowbrick library i was able to create a visualisation of the silhouette coeﬀicent values, from the given visualisation i can detrermine that 3 is the best quanity of clusters as it has the highest silhouette score  2  0.1.4 Create clusters using Kmeans and Kmeans++ algorithms with optimal k value found in the previous problem. Report performances using appropriate evalu- ation metrics. Compare the results.  [38]: def purity_score(y_true, y_pred):  # compute contingency matrix (also called confusion matrix) contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred) # return purity return np.sum(np.amax(contingency_matrix, axis=0)) / np.  ↪sum(contingency_matrix)  X=Feature_set.iloc[:,0:-1] y_true=Feature_set.iloc[:,-1] n_clusters=3  [39]: y_predRand=KMeans(n_clusters,init='random').fit_predict(X)  print(classification_report(y_true,y_predRand,zero_division=0)) print(  "purity_score: %0.3f" % purity_score(y_true,y_predRand)  )  precision  recall f1-score  support  0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0  accuracy macro avg weighted avg  0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.01 0.01  0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.02 0.02  0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.02 0.01 0.01  152 156 150 157 153 154 155 153 147 151  1528 1528 1528  purity_score: 0.301  [40]: y_predPlusPlus=KMeans(n_clusters,init='k-means++').fit_predict(X)  print(classification_report(y_true, y_predPlusPlus,zero_division=0)) print(  "purity_score: %0.3f" % purity_score(y_true, y_predPlusPlus)  )  3  precision  recall f1-score  support  0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0  accuracy macro avg weighted avg  0.00 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.01 0.01  0.00 0.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.02 0.02  0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.02 0.01 0.01  152 156 150 157 153 154 155 153 147 151  1528 1528 1528  purity_score: 0.301  For this specific run, It appears that the Kmeans++ performed worse as it had slightly lower average, however they are so close its almost redundant. being said usally kmeans++ is the more reliable choice if accuracy is important  0.1.5 Now repeat clustering using Kmeans for 50 times and report the average per- formance. Again compare the results that you have obtained in Q3 using Kmeans++ and explain the difference (if any).  [41]: accuracy_score = []  recall_score = [] precision_score = []  for x in range(0,50):  y_pred = KMeans(n_clusters,init='random').fit_predict(X)  accuracy_score.append(metrics.accuracy_score(y_true,y_pred)) recall_score.append(metrics.recall_score(y_true,y_pred,average='macro')) precision_score.append(metrics.  ↪precision_score(y_true,y_pred,average='macro',zero_division=0))  print("Average Accuracy Score: " + str(np.mean(accuracy_score))) print("Average Recall Score: " + str(np.mean(recall_score))) print("Average precision Score: " + str(np.mean(precision_score)))  Average Accuracy Score: 0.09414921465968586 Average Recall Score: 0.09403726045883941 Average precision Score: 0.029215597521039204  Comparing results with Q3 on average it seems they perform about the same, if not slightly worse. Hoever it is still better to go with Kmeans++ as the potentail for low performance outliers will  4  result in innacurate clustering  [ ]:  5   