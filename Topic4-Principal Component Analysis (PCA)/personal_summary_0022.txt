Summary (main points)  Measuring similarity or distances between different data points is fundamental to many machine learning algorithms. These algorithms are used both in supervised learning methods and unsupervised learning problems. Depending on the nature of the data point, various measurements can be utilised to measure distance.  Distance metrics  Distance metrics are used widely in machine learning algorithms. Distance measures are functions that define a distance between any two data instances for measuring how similar the instances are. The most related examples in machine learning are:  clustering algorithms (we looked at examples of clustering last topic)  Support Vector Machines (SVM)  ▪ ▪  K-Nearest-Neighbor ▪ ▪  data visualization ▪ ▪  information retrieval ranking  Distance measures satisfy the following three properties:    For any instance xi distance with itself is zero.   For an instance pairs xi  and xj, the distance is non-negative and symmetric.   Distance measures follow triangular inequality.  **Distance measures satisfying above properties are also known as Distance Metrics.    Euclidean distance  o  Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday) space. It represents the shortest distance between two vectors. It is the square root of the sum of squares of differences between corresponding elements.    Cosine distance  o  Cosine similarity is a measure of similarity between two non-zero vectors defined in an  inner product space.    Mahalanobis distance  o  The Mahalanobis distance (MD) is the distance between two points in multivariate space. The Mahalanobis distance is the distance of the test point from the center of mass divided by the width of the ellipsoid in the direction of the test point. It can be used to determine whether a sample is an outlier, whether a process is in control or whether a sample is a member of a group or not.  o  Mahalanobis distance can be thought of scaling each data dimension by its variance and adjusting for their relationships. When data are independent, such as an identity matrix, Mahalanobis distance becomes same as Euclidean distance.    Manhattan distance  o  Manhattan Distance is the sum of absolute differences between points across all the dimensions. To calculate Manhattan Distance, we will take the sum of absolute distances in both the x and y directions. In most cases, this distance measure yields results similar to the Euclidean distance. However, using City block distance, the effect of a large difference in a single dimension is dampened (since the distances are not squared).  o   Minkowski distance  o  Minkowski Distance is the generalized form of Euclidean and Manhattan Distance and  defines a distance between two points in a normed vector space.   Jaccard distance  o  The Jaccard distance is a distance used to measure diversity of any two sets.  Jaccard distance measures the dissimilarity between data sets and is obtained by subtracting the Jaccard similarity coefficient from 1. For binary variables, Jaccard distance is equivalent to the Tanimoto coefficient.  Clustering Algorithms  Goal of clustering algorithms are to:    Group unlabelled data objects with similar properties together   Discover interesting perhaps unexpected clusters in the data   Find a valid or useful organisation of the data  We can define two algorithmic goals. We need to find objective functions to:    Minimise intra-distance (distance between points in the same cluster)   Maximise inter-distance (distance between points from different clusters)  Define a generic set-up based on our current understanding from clustering methods:    Step 1: define a distance metric between objects   Step 2: define an objective function that gets us to our clustering goal   Step 3: devise an algorithm to optimise the objective function  K-means  k represents the centre points of clusters. You start off with these centroids and then measure teach data point to find its closest centroid. In other words, K-means stores k centroids for defining clusters. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-means searches for the best centroids by alternating between two methods:  1.  Assigning data points to clusters based on the current defined centroids (points which are  the centre of a cluster).  2.  Choosing centroids based on the current assignment of data points to clusters.  Step 1 and 2 repeat until you find a useful grouping of data points.  The most important limitations of simple Kmeans are:    Random initialisation means that you may get different clusters each time. As a solution, we  can use a Kmeans++ initialisation algorithm to initialise better.    We have to supply the number of clusters beforehand. We can use the Elbow method to    choose K, but it may not be straightforward. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)  The Elbow Method is a method for finding the appropriate number of clusters. The Elbow method interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset. Sometimes we might not be able to discern the elbow shape based on different numbers of clusters. In this case, the elbow method cannot help for obtaining the best value of K.  Kmeans++ is an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm. K-means++ starts with allocating one cluster centre randomly and then searches for other centres given the first one. So both algorithms use random initialisation as a starting point but in different ways. So Kmeans++:  ▪  Chooses one centroid M1 ▪  uniformly at random from the dataset ▪  Let D(x) be the shortest distance from a data point to the closest centroid we have already chosen.  ▪  Choose a new centroid from the dataset with probability of  Now repeat the previous step until we have initialised K centroids  In a Kmeans algorithm with a random starting number of centroids, the objective function monotonically decreases with each iteration of the algorithm. In other words every time the algorithm runs, it gets closer to (and not further away from) the best solution.  Evaluation of Clustering  All machine learning algorithms are required to be evaluated. Are the clusters useful? Evaluation of clustering methods is not easy. But, generally there are two main categories of evaluation methods for clustering:    External assessment:   compare clustering performance against a known clustering (often called Ground truth or Gold standard). Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.). Examples: Silhouette coefficient, Dunn index etc.  Rand Index   The Rand index, is a measure of the similarity between two data clusters, ignoring their permutations.  The adjusted rand index is the corrected-for-chance version of the Rand index. In other words the index takes chance into account and corrects any bias introduced by chance.  Purity  In evaluation methods of clustering, it is common practice to use more than one approach for evaluation because neither of the evaluation methods are comprehensive enough. Purity is a way of quality measurement in clustering methods.  Each cluster is assigned to the class label which has the majority in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances.  Based on the figure, the first cluster has 5 crosses and 1 circle, so the majority of the labels are cross. For the next cluster, we have 4 circles and 1 cross and 1 plus, so circle has the majority. And as for the last one we can see 3 pluses and 2 crosses which result in majority of pluses. Now we can calculate the Purity measurement of these obtained clusters as:  Disadvantages of this evaluation method. Must make sure a fair number of clusters are selected otherwise purity measurement would not be accurate.  Mutual Information  Mutual information is one of the most popular approaches in analysis of clustering. Mutual information is a measure of dependence or “mutual dependence” between two random variables. As such, the measure is symmetrical, meaning that I(X ; Y) = I(Y ; X).  It measures the average reduction in uncertainty about x that results from learning the value of y; or vice versa, the average amount of information that x conveys about y.  Silhouette Coefficient  The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments. The silhouette coefficient contrasts the average distance between the instances of the same cluster with the average distance between the instances of different clusters.  A high value of Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. If most objects have a high value, then the clustering configuration is appropriate. On the other hand, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters.  