1)  Studied about various kinds of distance formulas like Euclidean distance, jaccard distance,  manhattan distance etc  2)  Learnt about clustering i.e putting data points into groups. 3)  Learnt about working of k means. It stores centroids for defining clusters 4)  There are a lot of methods to evaluate clustering i.e purity, rand index, silhouette co efficient,  mutual information etc  5)  There are some limitations of k means for which it is difficult to find difficult clusters 6)  Apart of kmeans there are other clustering algorithms i.e hierarchical, dbscan, shape based  clustering etc  Summary of Reading List:  Part 2:  To handle the learning information, the K-implies calculation in information mining begins with a first gathering of haphazardly chosen centroids, which are utilized as the starting focuses for each group, and afterward performs iterative (redundant) computations to improve the places of the centroids. It stops making and streamlining groups when by the same token:  1)  The centroids have settled â€” there is no adjustment of their qualities in light of the fact that the  grouping has been fruitful.  2)  The characterized number of iterations has been accomplished.  https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning- 6a6e67336aa1  A bunch is a group of items that lie under a similar class,  objects with comparable properties are gathered in one group, and unique articles are gathered in another bunch. Furthermore, bunching is the most common way of ordering objects into various groups wherein each group, objects are basically the same as one another than those items in different groups. Essentially, fragmenting bunches with comparable properties/conduct and appoint them into groups. Being a significant examination technique in AI, grouping is utilized for recognizing examples and construction in marked and unlabelled datasets. Bunching is exploratory information examination methods that can recognize subgroups in information with the end goal that data of interest in every equivalent subgroup (group) are basically the same as one another and data of interest in discrete bunches have various attributes.  https://www.analyticssteps.com/blogs/5-clustering-methods-and-applications  Distance measurements are essentially used to represent closeness based calculations.  The distance measurements use has been available since its beginning. Essentially, Distance gives a likeness measure between two data of interest. One of the most famous instances of distance-based measurements is notable Closest neighbors rule for characterization, where another example is named with the greater part class inside its closest neighbors.  Calculations behind Closest Neighbor classifiers are the principal inspiration driving distance-based learning. These sorts of calculations have standard distance measurements like the Euclidean distance to quantify information comparability. The distance boundary that information carries as a correlation with non-comparative information can essentially expand the nature of these calculations.  https://medium.com/geekculture/7-important-distance-metrics-every-data-scientist-should-know- 11e1b0b2ebe3  Part 3: We use different distances formula to calculate the closeness of data points. All the points which are close to eachother form a cluster and those who are away form another cluster. In k means, k represents the centre points of clusters and means as the name suggests represents mean of data points. K-means searches for the best centroids by alternating between two methods:  1)  Assigning data points to clusters based on the current defined centroids (points which are the  centre of a cluster).  2)  Choosing centroids based on the current assignment of data points to clusters.  There are different kinds of clustering algorithms through which we can separate clusters on the basis of data points which are closed to eachother  