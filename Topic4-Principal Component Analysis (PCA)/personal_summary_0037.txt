One of the fundamental aspects of machine learning is measuring the similarities or distance between different data points, used in both supervised and unsupervised learning to measure how similar two instances are. A few distance metrics related to machine learning are clustering algorithms, K-Nearest Neighbor, data visualization and information retrieval.  Distance measures must follow three rules: 1. The distance between any instance and itself must be zero, 2. The distance between any two instances must be a positive number as well as symmetric, meaning the distance between x1 and x2 must be the same as the distance between x2 and x1, 3. It must follow triangular inequality. A few of the different distance measurements include Euclidean distance, Cosine distance, Mahalanobis distance, Manhattan distance, as well as Minkowski and Jaccard distance.  Clustering algorithms put data into groups by using similarities and difference in features in unclassified data. They are used quite a bit in unsupervised learning where there’s huge amounts of unclassified data but, are also used in supervised learning. The goals of these algorithms are to group unlabeled data objects that have similar properties together, discover interesting and unexpected clusters in data, and find a useful organization of the data. These help to minimize the distance between two data points within the same cluster and maximize the distance between two data points in separate cluster.  One of the most popular clustering algorithms is the K-Means algorithm. It finds the center of the clusters called centroids using two methods: Assigning data points to clusters based on the current defined centroids, and choosing centroids based on the current assignment of data points to clusters, the algorithm keeps repeating these two methods until a useful grouping of all data points is found.  In order to be sure that the clustering algorithms being used are useful, they have to go through some evaluations. Although it is not a simple task, there are two main categories of evaluating clustering algorithms: external and internal assessment. The external assessment is by comparing the clustering algorithm against a known clustering algorithm, while the internal assessment determines if the algorithm follows certain intrinsic assumptions like the size of the clusters or distance between clusters.  A few evaluation methods include the Rand Index, Purity, Mutual Information and Silhouette Coefficient.  The Rand Index measures the similarity between two data clusters, Purity checks the quality measurements in the algorithms by counting the number of correctly assigned instances and dividing by the total number of instances, Mutual Information is similar to the Rand Index as it measures the agreement between two algorithms, mainly, how informative is one algorithm about the other. Silhouette Coefficient on the other hand measures how similar an instance is to its own cluster and does not require ground truth. It compares the average distance between instances in the same cluster with the average difference between instances in another cluster.  Although K-Means is a very popular clustering algorithm, it has a number of limitations:    Random initialisation means that we may get different clusters each time   We have to supply the number of clusters beforehand    It is unable to find clusters of arbitrary shapes It cannot detect noisy data points, which are data points that should not be taken into consideration    One of the ways we can find the appropriate number of clusters for our algorithm is by using the Elbow Method, which interprets and validates consistency within a cluster analysis. It plots the sum of the distances between each point and the centre of its group against the number of groups and then looks for a point on the graph where the decrease in distance slows down, creating an elbow shape which is where it gets its name from.  Kmeans++ is an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm, however, unlike the K-Means algorithm, Kmeans++ starts by allocating one centroid randomly and then searches for other centroids based on the first one this make sure that the centroids are separated properly and can represent the data a lot better.  Other than K-Means, there are many clustering algorithms, including Hierarchical clustering, DBSCAN and shape-based clustering. Hierarchical clustering are algorithms find clusters that have a predetermined order and are of two types: agglomerative clustering which is a bottom-up approach and Divisive clustering which is top-down approach. DBSCAN clusters certain items in a group based on a given data point. For this algorithm we need to set a number of minimum data points as well as a distance, which will determine the resulting cluster. For shape-based algorithms we have VAT and IVAT, VAT transforms the distance matrix into a visual representation of a re-ordered matrix which is done in a way that emphasizes the dissimilarities between the data points in a way that reveals the underlying clustering structure of the data. iVAT is an extension of VAT that which involves reapplying the VAT algorithm to the re-ordered matrix until a stable clustering structure is obtained.     