Key Learnings:    Measuring similarity or distance between data points is essential in many machine learning  algorithms, both in supervised and unsupervised learning.    Distance metrics are functions used to define a distance between any two data instances and are  used  in  clustering  algorithms,  K-Nearest-Neighbour,  Support  Vector  Machines,  data visualisation, information retrieval, and ranking.    Distance measures should satisfy three properties: distance with itself is zero, the distance is  non-negative and symmetric, and the distance measure follows triangular inequality.    Euclidean  distance  is  the  ordinary  straight-line  distance  between  two  points  in  Euclidean space  and  is  calculated  using  the  square  root  of  the  sum  of  squared  differences  between corresponding features.    Cosine distance is used to calculate similarity between two vectors and is calculated as the  dot product of two vectors divided by the product of their magnitudes.    Mahalanobis  distance  is  the  distance  between  two  points  in  multivariate  space  and  is calculated  as  the  square  root  of  the  difference  between  the  two  points'  feature  vectors multiplied by the inverse of the covariance matrix of the data.    Cityblock/Manhattan  distance  is  the  sum  of  absolute  differences  between  corresponding  features of two vectors.    Minkowski distance is a generalisation of the Euclidean and Cityblock distances defined for    any n-norm. Jaccard distance is used to measure diversity of two sets and is calculated as the ratio of the size of the intersection of the two sets to the size of their union.    Clustering algorithms group unlabelled data points with similar properties, using similarity and differences  in features.  They are  commonly used  in unsupervised  learning  and  can also be useful  in  supervised  learning  to  create  groups.  The  goals  of  clustering  algorithms  are  to discover unexpected clusters in the data, find a valid organization of the data, and minimise intra-distance  while  maximising  inter-distance  between  points.  To  achieve  these  goals,  a distance metric is defined between objects, an objective function is created, and an algorithm is developed to optimise the function.    The most popular clustering algorithm in machine learning is K-means, known for being simple and fast. It uses centroids to define clusters and assigns data points to clusters based on their distance from the centroid. This process is repeated until a useful grouping of data points is achieved.    There are two main categories of evaluation methods for clustering: external assessment and  internal assessment.    Purity is a clustering evaluation metric that measures the proportion of correctly assigned data  points in a cluster. It ranges from 0 to 1, where 1 represents a perfect clustering.    Rand  index  is  a  measure  of  similarity  between  two  clusterings,  which  calculates  the percentage of  pairwise  agreements  between  the two  clusterings. Adjusted  Rand  index is  a corrected version of Rand index that considers the expected agreement between two random clusterings.    Mutual information is  a measure of  the  amount of information that two  clusterings share, which is based on the concept of entropy. It ranges from 0 to 1, where 1 represents a perfect clustering.    Silhouette coefficient is a clustering evaluation metric that measures how well each data point fits into its assigned cluster compared to other clusters. It ranges from -1 to 1, where values closer to 1 indicate better clustering.    Random initialisation can lead to different clusters each time. Using a Kmeans++ initialisation  algorithm can help to initialise better.    Kmeans requires the number of clusters to be supplied beforehand. The Elbow method is one  way to choose, but it may not always be straightforward.    Elbow method involves running Kmeans clustering for a range of values of k, computing the sum of squared error (SSE), and looking for the "elbow point" where the SSE begins to level off. However, this method may not always be helpful if the elbow shape is not discernible.   Kmeans++ is an algorithm that chooses initial cluster centers for Kmeans clustering in a better way  compared  to  random  initialisation.  It  chooses  centroids  from  the  dataset  with probabilities based on the distances to the closest previously chosen centroid, which gives a theoretical guarantee on convergence.    Hierarchical Clustering  is a clustering algorithm that creates  clusters  with  a predetermined order using a bottom-up or top-down approach, resulting in a dendrogram. Cutting the tree at different heights produces different levels of clustering.    DBSCAN is a density-based clustering algorithm that clusters data points based on a minimum number  of  data  points  and  distance  criteria,  with  core  points  and  border  points  forming clusters and noise points being ignored.    Shape-based  Clustering:  Visual  Assessment of  Tendency  (VAT) and  iterative  VAT  (iVAT) are useful techniques for exploratory data analysis, allowing data analysts to gain insight into the underlying  structure  of  the  data  and  to  identify  the  appropriate  number  of  clusters  for subsequent clustering algorithms.  