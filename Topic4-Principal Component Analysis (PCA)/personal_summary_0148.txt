Unsupervised learning is a type of algorithm that learns patterns from untagged data. The goal is that through mimicry, which is an important mode of learning in people, the machine is forced to build a concise representation of its world and then generate imaginative content from it.  We use the techniques of unsupervised learning in the algorithms of clustering and dimensionality reduction. Clustering refers to the algorithm of finding patterns and clustering the data into different groups based on the previously found patterns. Dimensionality reduction is useful when we have more than enough features that it becomes hard to manage all of them. It is a technique that allows the user to reduce the number of dimensions or features in the dataset.  Many machine learning algorithms work on finding the similarity between data points. In mathematical terms, we call this as finding the distance between the data points. We have a few different metrics we can use to find the distance between points.  Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday) space. The Manhattan distance, also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.  Kmeans is one of the most popular algorithms for clustering. It works by providing the initial number of clusters ‘k’, followed by the algorithm assigning each data point to a centroid which is closest to the data point.  The Rand index or Rand measure in statistics, and in particular in data clustering, is a measure of the similarity between two data clusters. The Rand index has a value between 0 and 1, with 0 indicating that the two data clusterings do not agree on any pair of points and 1 indicating that the data clusterings are exactly the same.  Within the context of cluster analysis, Purity is an external evaluation criterion of cluster quality. It is the percentage of the total number of objects (data points) that were classified correctly, in the unit range [0..1].  The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments. This method can be used to find the optimal value of ‘k’ in the kmeans clustering algorithm.  Hierarchical Clustering creates clusters in a hierarchical tree-like structure (also called a Dendrogram). Meaning, a subset of similar data is created in a tree-like structure in which the root node corresponds to the entire data, and branches are created from the root node to form several clusters.  Clusters are dense regions in the data space, separated by regions of the lower density of points. The DBSCAN algorithm is based on this intuitive notion of “clusters” and “noise”. The key idea is that for  each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points.  