Unsupervised Learning: We will learn and understand the application of unsupervised machine learning which are: Clustering Dimensionality Reduction Clustering: we use clustering to find patterns in large data sets when we don`t have target label. Dimensionality Reduction: when the number of features are too much, we can use this attribute to reduce the feature`s numbers and improve the performance and understanding of the datasets and model. Distances: Definitions and Properties: Measuring  distances  between  different  data  points  is  fundamental  to  many  machine  learning algorithms. These algorithms are used both in supervised and unsupervised learning problems. The most related examples in machine learnings are: Clustering algorithms KNN (K Nearest Neighbor) SVM (Support Vector Machine) Data Visualization Ranking Information retrieval There are different methods to measure the distance between 2 points and depending on the nature of the data points, we decide which method to use. Types of distance measurements: Euclidean Distance: It`s the ordinary straight-line distance between two points in Euclidean space. Mahalanobis Distance: The Mohalanobis  distance (MD) is the  distance  between two points  in multivariate space.Mahalanobis distance can be thought of scaling each data dimension by its variance and adjusting for their  relationships.  When  data  are  independent,  i.e.  M=I  (Identity  Matrix),  Mahalanobis  distance becomes sames as Euclidean distance. Cityblock/Manhattan Distance: In  most  cases,  this  distance  measure  yields  results  similar  to  Euclidean  distance.  However,  using Manhattan  distance,  the  effect of  a  large  difference  in  a  single  dimension  is  dampened  (since  the distances are not squared.) Minkowski Distance: The Minkowski distance defines a distance between two points in a normal vector space. Minkowski distance  is  a  generalization  of  Euclidean  and  Manhattan  distances  defined  for  any  p-norm. Jaccard Distance: The Jaccard distance is a distance used to measure diversity of any two sets. Clustering: Imagine a retail company wants to group the customers based on their usage and their income. So after modelling and identifying the groups, they categorize customers into the below groups: Cluster1: High usage and Income Cluster2: Low Usage and Income Cluster3: High usage and Low Income Cluster4: Low usage and High Income This was a real example of clustering. Clustering algorithms: With using clustering algorithms. We put data points into groups. It uses similarity and differences (based on distances) between different datapoints in attributes to create groups. Clustering algorithm`s goals: Group unlabelled data objects with similar properties together. Discover interesting perhaps unexpected clusters in the data Find a valid or useful organisation of the data A good objective function is the function that: Minimise intra-distance (distance between points in the same cluster) Maximise inter-distance (distance between points from different clusters) Now, we can define a generic set-up based on our current understanding from clustering methods: Step1: define a distance metric between objects Step2: define an objective function that gets us to our clustering goal Step3: devise an algorithm to optimise the objective function K_means: This is the most popular clustering algorithm that is fast and simple. “K” represents the centre points of clusters. In fact, it creates k clusters and “k” is the centre point. It selects randomly at first, then it starts and calculate the distance between each point to each centre point and find the nearest. Next with considering a perpendicular line between to centroids, consider new centroids and reappear the process to finally find the best centroid points and clusters. K_means searches for the best centroids by alternating between two methods: Assigning data points to clusters based on the current defined centroids Choosing centroids based on the current assignment of data points to clusters. These two steps repeat until the model find the best grouping of data. (with considering the min intra-distance and max inter-distance) Evaluation of Clustering: There are two main categories of evaluation methods for clustering: External assessment: compare clustering performance against a known clustering Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g. cluster to cluster distance or cluster size.) Examples: silhouette coefficient, Dunn index etc. Rand Index: The Rand index is a measure of the similarity between two data clusters. Purity:Purity is a way of quality measurement in clustering methods. What we do is measuring the purity for all clusters in terms of class labels of the data in each cluster. Mutual Information: Mutual information is one of the most popular approaches in analysis of clustering. It`s a function that measures the agreement of the two clustering assignments C and C` in terms of how informative one is about the other, ignoring the computations. In fact, how informative is c about C`. Silhouette Coefficient: It measures how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). The advantage of this method is that it does not need the ground truth cluster assignments. In fact, it contrasts the average distance between the instances of the same cluster with the average distance between the instances of different clusters.The final value ranges from -1 to 1. A higher value indicates that the object is well matched to its own cluster and poorly matched to neighbouring clustering. If most objects have a high value, then the clustering configuration is appropriate. On the other hand, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Limitations of K_means: The most important limitations of simple K_means are: Random initialisation means that you may get different clusters each time. We have to supply the number of clusters beforehand. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points. Finding a useful number of clusters: The “Elbow Method”  is a method for finding the appropriate number  of clusters.  It interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset. K_means++: K_means++ is an algorithm for choosing the initial cluster`s centre values. The point is that K_means++ starts with one cluster centre randomly and then searches for other centres given the first one. Other Clustering Algorithms: DBSCAN: DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that clusters certain items in a group based on a given data point. What we need to do is to set a number of data points  (minpts) and a distance (dis). The  resulting cluster is dependent on them. Hierarchical Clustering: There are two types of hierarchical algorithms which as: Agglomerative Clustering (bottom-up) Divisive Clustering (top-down) 