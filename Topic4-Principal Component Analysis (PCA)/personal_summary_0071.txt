 The learning goals are spread across topic 3 and 4 and are:  ▪  use clustering for revealing patterns from unlabelled data. ▪ ▪  learn techniques to reduce dimensionality. apply suitable clustering/dimensionality reduction techniques to perform unsupervised learning of data in a real-world scenario.  In topic 3 we focus on clusters,  Measuring distances  Here we look at the importance of distance metrics in machine learning algorithms, which are used to measure the similarity or distance between different data points.  We go through different types of distance metrics, including Euclidean distance, cosine distance, Mahalanobis distance, cityblock/Manhattan distance, Minkowski distance, and Jaccard distance. We also emphasizes the three properties that distance measures must satisfy:    distance with itself is zero.   distance is non-negative and symmetric between two instances.   the distance measure follows triangular inequality.  Some images to reflect measuring distance:  Euclidean distance:  Cosine Distance    Mahalanobis distance  Cityblock/Manhattan distance  There are 9 distance measures primarly used in data science:       Distance metrics  Distance metrics are essentially math functions we can use to measure the similarity or lack of similarity data. Often seen in machine learning algorithms such as clustering, classification, and regression. There are several types of distance metrics, including Euclidean distance, Manhattan distance, cosine distance, and Jaccard distance.  Clustering and its applications  Clustering is a machine learning technique used to group similar data points together based on their features, it used in a wide range of industries at present. An example of some more common clustering algorithms include K-means, DBSCAN, hierarchical clustering.  How Kmeans works?  K-means is a very common clustering algorithm that aims to partition a dataset into K distinct clusters based on the Euclidean distance between data points.  The algorithm starts by randomly selecting K initial cluster centroids and iteratively assigns each data point to the nearest centroid until convergence. We end up with  K clusters with their corresponding centroids.   Evaluation of clustering  There are several metrics that we can use to evaluate the performance of a clustering algorithm  silhouette score     purity   adjusted rand index.  The metrics access the quality by measuring the similarity of the data points within a cluster and the difference between clusters.  Limitations of Kmeans  K-means has several limitations that can affect its performance.  sensitivity to initial centroids     The assumption of equal-sized clusters   requirement of a pre-specified number of clusters  When dealing with complex data sets we can often get poor quality results that are not accurately reflecting the problem.  Clustering with Kmeans ++  K-means++ is an extension of K-means that addresses the issue of selecting initial centroids by introducing a more sophisticated initialization process.  It uses a probability-based approach to select the initial centroids. This can lead to better clustering results compared to random start ups.  Other clustering algorithms  There are several other clustering algorithms that can be used instead of K-means, such as DBSCAN, hierarchical clustering, spectral clustering as well as many more. Its important that we assess and pick the clustering algorithm that suits the task we are trying to complete.  Evaluating performance of Kmeans clustering  Evaluating the performance of K-means clustering can be done using different metrics such as:    elbow method   silhouette score  These metrics help to determine the optimal number of clusters for a given dataset and assess the quality of the clustering results.  DBSCAN and hierarchical clustering  DBSCAN and hierarchical clustering are two popular clustering algorithms that can be used instead of K-means.  DBSCAN is a density-based clustering algorithm that can discover clusters of arbitrary shapes and sizes. Hierarchical clustering is a clustering algorithm that builds a hierarchy of clusters by recursively merging or splitting them based on their similarity.  