Summary and Reflection of Topic 3: Clustering  Clustering  can  find  patterns  in  large  data  sets  when  the  data  is  not  labelled  whereas  dimensionality reduction helps us when the number of features, that describe the difference between the data, becomes too large to manage.  Python  –  packages  are  used  to  create  unsupervised  models,  reduce  dimensionality  of  the  data  and visualize high dimensional data.  Measuring similarity or distances between different data points is fundamental to many machine learning algorithms. Distance metrics are functions that define a distance between any two data instances. Related examples in machine learning:    Clustering algorithms   K-Nearest-Neighbor   Support Vector Machine (SVM)   Data visualization    Ranking  Information retrieval  Distance measure – a) for any distance with itself is zero, b) any pairs: the distance is non-negative and symmetric and c) distance follows a triangular inequality.  Types of distance measurements:  Euclidean distance – ordinary straight-line distance between two points in Euclidean (everyday) space. Appropriate for continuous numerical variable and we want to reflect absolute distances.  City block/Manhattan distance – similar to Euclidean distance however the effect of a large difference in a single dimension is dampened (distances are not squared).  Minkowski  distance  –  defines  a  distance  between  two  points  in  a  normed  vector  space.  It  is  a generalization of Euclidean distance (2 norm) and Cityblock distance (1 norm).  Cosine distance – essentially seeks the angle between 2 vectors (used when the magnitude of the vector does not matter).  Mahalanobis distance – distance between two points in multivariate space.  This can be thought of as scaling  each  data  dimension  by  its  variance  and  adjusting  for  their  relationships.  When  the  data  is independent, it becomes Euclidean distance. Removes redundancies – if we have repeated variable their repetitious effect will disappear.  Jaccard distance – distance used to measure diversity of any two sets.  Clustering algorithms:    Group unlabelled data objects with similar properties together   Discover interesting perhaps unexpected clusters in the data   Find a valid or useful organisation of the data  In other words, we can define two algorithmic goals. We need to find objective functions to:    Minimise intra-distance (distance between points in the same cluster)   Maximise inter-distance (distance between points from different clusters)      Clustering methods:  Define a distance metric between objects, define an objective function that gets us to our clustering goal and finally devise an algorithm to optimize the objective function.  Kmeans – the most popular clustering algorithm which starts with a random guess for the centroid, finds its closes centroid, updates the centroid and loops until converging.  Evaluation of clustering  Generally there are two main methods for clustering:  External assessment – compare clustering performance against a known clustering (often called a Ground truth or Gold standard)    Rand index – a function that measures the similarity of the two assignments   Purity - Each cluster is assigned to the class label which has the majority in the cluster, and then the  accuracy  of  this  assignment  is  measured  by  counting  the  number  of  correctly  assigned instances and dividing by the number of total instances    Mutual information – is a function that measures the agreement of the two clustering assignments  in terms of how informative one is about the other, ignoring permutations.    Silhouette coefficient - The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments. The silhouette coefficient  contrasts  the  average  distance  between  the  instances  of  the  same  cluster  with  the average distance between the instances of different clusters.  Internal assessment – determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.) Examples: Silhouette coefficient and Dunn index.  Limitations of Kmeans:    Random  initialisation  =  different  clusters  each  time.  Use  a Kmeans++ initialization  algorithm  to  initialize better.    We  have  to  supply  the  number  of  clusters  beforehand.  We  can  use  the Elbow  method to choose, but it may not be straightforward. The Elbow method is based on the concept of the sum of squared error (SSE) minimization. If the plot looks like an arm – at the bottom of the arm – is the best number of clusters but this is not useful. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)     Kmeans with Kmeans ++  Represents an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm. Unlike Kmeans, it however choses one centroid to start of with.        Other clustering algorithms    In the category of “Flat Clustering” where the goal of the algorithm is to create clusters that are coherent internally but clearly different from each other, there are two more clustering methods:  Hierarchical clustering  DBSCAN (density based)  Shape-based Clustering  Two  types  of hierarchical  clustering:  bottom-up  (agglomerative)  approach  and  top-down  (divisive). Under both we assign each observation to its own cluster and then compute the similarity (e.g., distance) between each of the clusters and join the two most similar clusters.  DBSCAN  -  a  clustering  algorithm  that  clusters  certain  items  in  a  group  based  on  a  given  data  point. It requires setting a minimum number of data points and a distance and is therefore dependent on this.  Shape-based  clustering,  VAT,  iVAT  –  useful  tools  for  exploratory  data  analysis;  gain  insight  into  the underlying structure of the data and to identify the appropriate number of clusters for subsequent clustering algorithms.  VAT – visualization technique transforming the distance matrix of a dataset into a visual representation emphasizing  the  dissimilarities  between  the  data  points  and  hence  revealing  the  underlying  clustering structure.  iVAT – extension of VAT which repeatedly applies the VAT algorithm to re-ordered matrix in order to refine the clustering structure.  Implementation with Python  KMeans, DBSCAN and hierarchical clustering is implemented using Scikit learn package.  Performance metrics via the model and the ‘ground truth’ can be assess by extraction the classification report as well as the purity score.         