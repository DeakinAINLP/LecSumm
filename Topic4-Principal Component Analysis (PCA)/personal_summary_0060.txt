   Measuring distances  Distance indicator  Distance metrics are widely used in machine learning algorithms. Distance measurement is a function that defines the distance (X, X  )  between any two data instances X I and X  To measure instance similarity.  The most relevant examples in machine learning are:  § § § § § §  Clustering algorithm (we saw a clustering example last topic) K Nearest Neighbors Support Vector Machine (SVM) Data visualization Information Exploration Ranking  Distance measurement satisfies three properties:  1.  2.  In any instance, xi, the distance to itself is zero. That is, d(x i, xi)=0. For instance pairs x i and  xj, the distance is non-negative and symmetric. That is, d(x i, x j)≥0 and d(x i, x j)=d(x j, x i).  3.  Distance measurement follows a trigonometric inequality. That is, d(x i, x k)≤d(x i, xj)+d(xj, xk).  Types of distance measurement  Measuring distance is very important in machine learning problems. Let's take a look at the different types of distance measurements.  Euclidean Distance  The Euclidean distance is the normal straight-line distance between two points in Euclidean (  everyday) space  . For any two data instances represented by the  d-dimensional feature vector xi,xj, their Euclidean distance is calculated as follows:  Cosine Distance  We introduced cosine distance earlier in Course 1. d− dimensional feature vectors x i, xj.   The cosine distance between these two feature vectors is calculated as follows:     Mahalanobis Distance  The Mahalanobis distance (MD)  is the distance between  two  points in a multivariate space.  For any two data instances represented by the d  −  dimensional feature vector xi,xj, their Mahalanobis distance is calculated as follows:  where M is the covariance matrix of the data. Intuitively, the covariance matrix generalizes the concept of variance to multidimensional. That is, the elements of the covariance matrix are the I,j positions of the i-th time and the elements of the j−th time vector. The covariance matrix will be discussed in more detail later.  Mahalanobis distances can be thought of as scaling each data dimension through variance and adjusting their relationships. If the data are independent, i.e. M=I (identity matrix), then the Mahalanobis distance is the same as the Euclidean distance.  City Block/Manhattan Distance  For any two data instances represented by , the distance of the d  −-dimensional feature vector x i, xj, and the city block is calculated as follows:  In most cases, this distance measurement gives a result similar to the Euclidean distance. However, using city distances mitigates the effects of large differences in one  dimension  (because distances are not squared).  Minkowski distance  The Minkowski distance defines the distance between two points in the normed vector space. The Euclidean distance (xi-xj with  2 norms)  and the Cityblock distance (  xi-xi with 1 norm)  are Minkowski distances It is a generalization of these distances defined for any distance.  p - standard.  §  When p=1, the distance is known as the Manhattan distance. §  When p=2 this distance is known as the Euclidean distance.  Note: If you don't remember the norm definition clearly,  we recommend that you review Topics 1 and 2.  Jacquard Distance  Note: There was a small error in this formula, but it has been fixed.  Jaccard distance is  the distance used to measure the diversity of any two sets. Consider any two instances of xi, and xj as binary vectors that indicate the presence or absence of features. The jacquard distance between xi and ji is defined as:        Various distance indicators  §  Euclidean Distance §  Cosine Distance §  Mahalanobis Distance §  City Block/Manhattan Distance §  Minkowski distance Jacquard Distance §    Clustering and its applications  Based on your current understanding from the clustering method, you can define a general setup.  § § §  Step 1: Define the distance metric between objects Step 2: Define an objective function to achieve your clustering goals Step 3: Devise an algorithm to optimize the objective function    How Kmeans works  The most common clustering algorithm in machine learning is called K-means. It is simple and fast.  In this algorithm, k represents the center point of the cluster. Starting from these centroids, measure the teach data points to find the closest centroid. That is, the centroid that defines the K-means store cluster. If a point is closer to the centroid of that cluster than any other centroid, the point is considered to be in a particular cluster. K-means  alternates  between the following two methods to find the optimal centroid:  1.  Assign data points to clusters based on the currently defined centroid (the point at  which the cluster is centered).  2.  Select a centroid based on the current assignment of data points to the cluster.  Repeat steps 1 and 2  until you find a group of useful data points.    Clustering Evaluation  All machine learning algorithms must be evaluated. Are clusters useful?  Clustering techniques are not easy to evaluate. In general, however, there are two main categories of clustering evaluation methods:  §  External evaluation:  Compares clustering performance to known clustering (  often referred to as ground truth or gold standard).        §  Internal evaluation: Determines whether clustering follows certain unique assumptions, such as distance between clusters or cluster size.  Land finger count  A land index is a measure of similarity between two clusters of data.  There is an allocation of data instances to different clusters suggested by the clustering algorithm (for example, "C'"). External assessments have knowledge of ground truth claster  assignments. (Say  C)  A land index is a  function that measures the similarity of two assignments.   C and C'", ignoring their permutations.  The Land Index is calculated as follows:  §  a=  Number of pairs of data instances in the same cluster on both C, C' ". §  b =  number of pairs of data instances  in different clusters of C and  C'" in different  §  clusters. c =  number of pairs of data instances in the same cluster in  C, but C  '" in different clusters.  §  d =  Number of pairs of data instances in different clusters of C, but in the same  cluster C'".  The Adjusted Land Index is a modified version of the Land Index potential. In other words, the index takes into account chance and corrects the bias introduced by chance.  Purity  For clustering evaluation methods, it is common to use multiple approaches to evaluation because neither evaluation method is comprehensive enough.  Purity is a method of quality measurement in clustering techniques. As the name implies, you want to measure the purity of all clusters in terms of the class label of the data in each cluster. Consider the following diagram as an example.    Shape.  Three clusters retrieved for three types of data: Cross, Circle, and Plus.  Each cluster is assigned to a class label that has a majority in the cluster . The accuracy of this allocation is measured by counting the number of correctly allocated instances and dividing by the total number of instances.  Based on the figure, the first cluster has five crosses and one circle, so the majority of labels are crosses. In the next cluster, there are  four  circles, one cross, and one  plus, so the circle is in the majority. And as for the last one, we see three pluses and two crosses, and we get most of the pluses.  Mutual intelligence  Mutual information is one  of the most common approaches to clustering analysis.  Measure the match between two clustering assignments, such as:  C and C'", so the aim is almost the same as the Land index. With reciprocal information, the main question is how useful the information is  C roughly   C'" or C'" is roughly C'.  How similar are they and similar in a useful way? What can you talk about C if you see C  '"?  Cross information is a function that measures the agreement between two clustering assignments C  and C'" in terms of how useful one is about the other, ignoring permutations. Simply put, how useful  C approximately   C’」.  Clustering partition C has K cluster and partition C' has  K  '" in this case these 2  The mutual information for the two clustering assignments is calculated as follows:  where P(i) indicates the probability that a randomly selected instance belongs  to the  I- partition th cluster CP'(j) and  P( I,j) is defined in the same way. As you can see, another benefit of mutual information is the number of clusters discovered by .   C and C' don't have to be exactly the same.  Assume that you have a ground truth claster   and corresponding assignments  (C) and K clusters. In this case, we also performed the following clustering techniques:  C' And we got the K' cluster. Then we can calculate the mutual information amount MI  (C,C').  So if our  C' clustering is very beneficial based on C  we think that the C method of clustering is working.  Number of silhouette clerks  How similar the silhouette value is to its  own cluster compared to  other clusters (separation/difference)  measure. This method has   the advantage that it does not require ground truth claster allocation. The silhouette factor contrasts the average distance between instances in the same cluster with the average distance between instances in different clusters.     Limitations of Kmeans Limitations of Kmeans  Kmeans clustering has many limitations.  The most important limitations of simple Kmeans are:  §  Random initialization means that you might get a different cluster each time. As a  §  solution, you can use the Kmeans++ initialization algorithm to improve initialization. You must specify the number of clusters in advance. You can use the Elbow method to make your choice, but it may not be easy. You can't find clusters of arbitrary shape.  § §  Noisy data points, that is, data points that should not be considered in cluster analysis, cannot be detected. (The k-median method is less affected, but it also cannot identify noisy data points.) )  Find useful cluster counts  As mentioned earlier,  one  of the challenges with Kmeans is the assumption that must be made about the number of clusters first. The elbow method  is a method for finding the right number of clusters. The Elbow method interprets and verifies consistency within cluster analysis to find the appropriate number of clusters in the dataset.  Elbow method  Watch the video for an overview of this concept. The idea of the elbow method is to run a Kmeans clustering algorithm on the following range of values  :  Calculate the sum of  K, squared error (SSE) as follows:    Kmeans with Kmeans++  Kmeans with Kmeans++  Kmeans++ is an algorithm  for selecting the initial cluster center value or centroid of the Kmeans clustering algorithm.  K-means++ starts by  randomly assigning one cluster center and searches for other centers by specifying the first cluster center. Therefore, both algorithms use random initialization as a starting point, but in different ways. So Kmeans++:       § § § §  Select one centroid 1 Uniformly and randomly from the dataset is the  shortest distance from the data point to the nearest centroid you have already selected. Select a new centroid from the dataset with the following probability  repeat the previous step until initialization is complete  Kmeans++ Assurance  §  §  §  In the Kmeans algorithm where the starting number of centroids is random  , the objective function decreases monotonically  with each algorithm iteration. This means that each time the algorithm runs, it gets closer to the optimal solution (but never moves away). For the best solution, let's say the objective function takes a value  X＝  Using KMEANS , the objective function converges as follows:  Y=  has no theoretical limit, but for  Kmeans with X/Y random initialization  , Kmeans++ initialization has the following theoretical guarantees of convergence:  2.  Provide summary of your reading list–external resources, websites, book  chapters, code libraries, etc.  3.  Reflecton the knowledge that you have gained by reading contents of this  topic with respect to machine learning.  Welcome	to	Unsupervised	Machine	Learning   