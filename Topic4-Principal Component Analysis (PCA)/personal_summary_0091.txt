This topic provided an in-depth coverage of clustering algorithms and how they are used in unsupervised machine learning. We started by exploring the math for calculating distances between data points and their respective metrics. We learned how to measure using  Euclidean distance, Cosine distance, Mahalanobis distance, Cityblock/Manhattan distance, Minkowski distance and Jaccard distance.  We then looked at clustering using K-means and explored how it works, looked at evaluation methods such purity, Rand Index and silhouette coefficient.  We analysed the limitations of Kmeans and how to find the optimal number of clusters using the Elbow method. We also briefly touched on Kmeans++ and how the initialisation and allocation in clusters differs.  Finally we explored density-based clustering algorithms such as DBSCAN and Hierarchical clustering.  The second part of the topic focused on the use of the Python programming language and how it can be used in order to execute and evaluate Kmeans clustering using Scikit Learn framework and the yellowbrick library.  