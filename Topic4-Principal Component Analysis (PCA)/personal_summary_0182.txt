 Clustering Algorithms   Clustering puts data points into groups and uses similarity and difference of features (or dimensions) to create groups that is unclassified and has no known targets.   Clustering can be useful for unsupervised learning.  The 2 main goals or objective functions are:   Minimise intra-distance measures of how close the data points are within each cluster (distance between points in the same cluster)   Maximise inter-distance measures of how far apart the clusters are from  each other (distance between points from different clusters)  A good clustering algorithm should minimize intra-distance and  maximize inter-distance  Clustering & Distance Metrics   Fundamental to Clustering algorithms are distance metrics.   Distance Metrics: Measuring similarity or distances between different data points in a given space is fundamental to many machine learning algorithms:  Cityblock/Manhattan distance  is the sum of the absolute differences of the  coordinates of two points.   Euclidean distance is the square root of the sum of the squared differences of the  coordinates of two points.   Chebyshev distance is the maximum of the absolute differences of the  coordinates of two points.   Minkowski distance defines a distance between two points in a normed vector  space and is a representation of Cityblock/Manhatten & Euclidean.   Cosine distance is the complement of the cosine similarity between two vectors,  which measures the angle between them.   Mahalanobis distance distance is a measure of how far a point is from the mean of  a distribution, taking into account the covariance matrix.   Jaccard distance is the ratio of the difference and the union of two sets, measures  diversity of any two sets  Types of Clustering Algorithms   Kmeans partitions the data into k clusters by minimizing the sum of squared distances within each cluster by searching for the best centroids clusters, but its accuracy is impacted by random initialization.   Kmeans++ is an algorithm for choosing the initial cluster’s centre values or  centroids for the Kmeans clustering algorithm improves kmeans to speed up convergence   Hierarchical based clustering: builds a hierarchical clusters by merging  smaller groups  Agglomerative clustering (bottom-up)  Divisive clustering (top-down)   Density based clustering: dives the datapoints into core, border and noise groups the data based on the density of points in a region by setting a minimum number of data points (minPts) and a distance (dis).  Shape Based Clustering : finds clusters of arbitrary shapes using  visualization techniques. Examples are VAT & iVAT.  Finding a useful number of clusters?   Finding the optimal number of clusters is a challenging task that  depends on the data and the objective of the analysis:   Elbow technique looks for a point where the (sum of squared distances of each data point to its cluster center against the number of clusters) SSD curve bends sharply, indicating that adding more clusters does not improve the fit significantly.  Hierarchical Clustering Dendrogram, builds a tree-like structure that shows how data points are merged or split into clusters at different levels of similarity. The dendrogram can be cut at a desired level of similarity to obtain the optimal number of clusters.  Evaluation of Clustering Performance   Clustering performance is the evaluation of how well a clustering algorithm  groups data points into meaningful clusters.   External assessment is the measurement of clustering performance based  on a predefined ground truth or or expert gold standard.   Internal assessment: is the measurement of clustering performance based  on the properties of the data and clusters.  Rand index is a function that measures the similarity (by counting the number of  pairs of data points that are in the same cluster or in different clusters) of the two assignments C and C′, ignoring their permutations.   Purity is measured by counting the number of correctly assigned datapoints (of  the majority class) and dividing by the number of total datapoints.   Mutual information is a function that measures the agreement of the two  clustering assignments C and C′ in terms of how informative one is about the other, ignoring permutations.  (can use joint & marginal probabilities).   Silhouette Coefficient, is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments.  