Task3.1P - Clustering  Unsupervised learning is a machine learning method where model supervision is not necessary. Instead, this kind of learning enables the model to operate autonomously under no supervision to ﬁnd previously unnoticed trends and information.  While  supervised learning, as  we  recall, works with  labelled data,  it  primarily  deals with  unstructured data.  Measuring similarity or distances between  diﬀerent  data  points  is  fundamental  to  many  machine  learning algorithms.  Distance Metrics: The methods used in machine learning frequently use distance measures. Distance measurements are functions that determine how far apart any two data examples are from one another and how comparable they are. The most related examples in machine learning are:  clustering algorithms (we looked at examples of clustering last topic)  Support Vector Machines (SVM)     K-Nearest-Neighbor    data visualization    information retrieval ranking  Types of distance measurements:  1.  Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday)  space. For any two data instances, represented by d-dimensional feature vectors xi, xj.  2.  Cosine distance for any two data instances represented by d-dimensional feature vectors xi, xj. 3.  The Mahalanobis distance (MD) is the distance between two points in multivariate space. For any two  data instances, represented by d-dimensional feature vectors xi, xj.  4.  For any two data instances, represented by d-dimensional feature vectors xi, xj. In  most cases,  this distance measure yields results similar to the Euclidean distance. However, using City block distance, the eﬀect of a large diﬀerence in a single dimension is dampened (since the distances are not squared). 5.  The Minkowski distance deﬁnes a distance between two points in a normed vector  space. Think of Euclidean distance (2 norm of xi – xj) and Cityblock distance as (1 norm of xi - xi) Minkowski distance is a generalization of these distances deﬁned for any p-norm.  6.  The  Jaccard  distance  is  a  distance  used  to  measure  diversity  of  any  two  sets.  Consider  any  two  instances xi and xj as binary vectors indicating presence or absence of features.  Clustering Algorithms:  The process of clustering combines data elements. It combines unidentiﬁed material with unknown objectives using characteristics that are like and diﬀerent from each other. Because it can handle enormous amounts of uncategorized data,  it  is  especially  useful  in  unsupervised  learning.  However,  because  it  generates  groups,  it  is  also  helpful  in supervised learning. In other words, we can deﬁne two algorithmic goals. We need to ﬁnd objective functions to:    Minimise intra-distance (distance between points in the same cluster)   Maximise inter-distance (distance between points from diﬀerent clusters)    K-means Algorithm:  The cluster centres are represented by k in this method. You begin with these centroids and measure each data point to determine which one is nearest to the centre. To put it another way, K-means keeps centroids for delineating groups. If a location is closer to one cluster's centre than any other centroid, it is said to be in that cluster. K-means alternates between two techniques to ﬁnd the optimal centroids:  7.  Allocating data points to groups based on the centroids as they are currently speciﬁed. (Points which  are the centre of a cluster).  8.  Selecting centroids based on the groups to which data points are currently assigned.  Generally, there are two main categories of evaluation methods for clustering:  1.  External assessment:  2.  compare clustering performance against a known clustering (o(cid:332)en called Ground truth or gold standard). Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g., cluster-to-cluster distance or cluster size etc.).  Limitations of K-means:  The primary drawbacks of basic K-means are:  1.  Random  initialisation implies that each  time, you  might  get  a diﬀerent  grouping. We can  use  a  K-means++  initialisation method as a ﬁx to beter initialise the system.  2.  The number of groups must be provided in advance. The Elbow technique can be used to select K, but it might  3. 4.  not be easy. It is unable to locate groups of random forms. It is unable to  identify noise data points, or points that shouldn't be considered  when performing a cluster analysis. (The K-median method is less aﬀected but cannot identify noisy data points either.)  K-means with K-mean++:  Kmeans++  is  an  algorithm  for  choosing  the  initial  cluster’s  centre  values  or  centroids  for  the  Kmeans  clustering algorithm.  As we have said before, K-means starts with allocating cluster centres randomly and then looks for beter solutions. K- means++ starts with allocating one cluster centre randomly and then searches for other centres given the ﬁrst one. So, both algorithms use random initialisation as a starting point but in diﬀerent ways.  In  a  Kmeans  algorithm  with  a  random  starting  number  of  centroids,  the  objective  function monotonically decreases with each iteration of the algorithm. In other words, every time the algorithm runs, it gets closer to (and not further away from) the best solution.  1.  Let us say, for the best solution, the objective function takes value joptimum. 2.  Let us say, when using Kmeans the objective function converges to jconverged.  Other Clustering Algorithms:  1.  Hierarchical Clustering:  Hierarchical clustering is a diﬀerent class of clustering methods. These methods locate groups that follow a preset  hierarchy.  There  are  two  varieties  of  hierarchical  clustering:  a  botom-up  strategy  and  a  top-down approach:  1.  Agglomerative clustering (botom-up): This method begins each observation in its own cluster, and as one  proceeds up the hierarchy, pairs of clusters are combined.  2.  Divisive clustering (top-down): This method starts with all data in one cluster and performs splits as one  descends the order.  2.  DBSCAN:  A  clustering  method  called  DBSCAN  (Density-Based  Spatial  Clustering  of  Applications  with  Noise)  groups particular objects based on a single data point. Se(cid:427)ng a distance and a minimal amount of data points (minPts) is necessary for this. (dis). These factors are user-deﬁned, so the cluster that is produced depends on them. 1.  Determine the separation between every point in the collection and every other point. If a point has at least the same amount of data points as it does within the speciﬁed range, it is referred to as a "core point". 2.  Border  points  are  data  elements  that  fall  outside  the  deﬁnition  of  a  core  point  but  are  still  within  its  speciﬁed distance. All additional information is viewed as "noise.".  3.  The next step is to combine all core and border points within dis of each other into a single cluster.  3.  Shape-based Clustering, VAT, iVAT:  A dataset's distance matrix is converted into a visual representation in the shape of a rearranged matrix using the VAT visualisation method. The data are rearranged so that the diﬀerences between the data points are highlighted, revealing the underlying grouping structure of the data. The reordered matrix will display block- like  structures  along  the  diagonal,  showing  the  existence  of  clusters,  if  the  data  has  a  distinct  clustering structure. In order to improve the clustering structure, the VAT method is frequently applied to the reordered matrix in iVAT, a variation of VAT. Iteratively computing the VAT on the reordered matrix is how the iVAT method works until a steady clustering structure is atained. This can help to identify the optimal number of clusters in the data.  Quiz-3 Atempt:      