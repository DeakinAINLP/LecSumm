 Distance measurement Euclidean distance: the straight-line distance between two points. Commonly used to deal with numerical data when the magnitude of the vectors is important (often applied in computer vision, and image processing)  Cosine distance: commonly used to deal with text data, high dimensional data, measures the angle between two vectors, where the magnitude of the vectors does not matter (often apply in natural language processing, text mining)  Mahalanobis distance: the distance between a point and a distribution and is sensitive to the correlation between variables. Commonly used to deal with correlated data, variables with different variances (often applied in classification, clustering)  Cityblock distance: the distance between two points by summing the absolute differences of feature values, commonly used to deal with features in different units and scales.  Jaccard distance: the ratio of the size of the intersection of two sets to the size of the union of two sets. Commonly used to deal with binary or categorical data, measures the similarity between two sets of data.  Clustering method Step 1: define a distance metric between objects Step 2: define an objective function that gets us to our clustering goal Step 3: devise an algorithm to optimise the objective function  Clustering with K-means and K-means++ K-means: an iterative method that repeatedly reassigns data points to the closest centroid and updates the centroid based on the new assignment. It starts with choosing randomly k cluster centroids. The algorithm converges when the assignment of data points to centroids no longer changes. To find an optimal number of clusters, we can use the Elbow method.  K-means++: a modified version of K-means to improve the issue of initialization. The initial centroids are chosen using a probabilistic method that selects centroids that are far apart from each other to ensure that each centroid represents a distinct part of the data space.  Other clustering algorithms Hierarchical clustering: group similar data points into clusters based on their pairwise similarity. The final output is a hierarchical tree diagram. There are two main types of this method: agglomerative and divisive. In agglomerative clustering, each data point starts as its own cluster, and the algorithm proceeds by successively merging the closest pairs of clusters until a single cluster containing all data points are formed. In divisive clustering, all data points start in a single cluster, and the algorithm proceeds by recursively separating the cluster into smaller clusters until each data point is in its own cluster.  DBSCAN: clusters data instances in groups based on two parameters defined by the user (eps – the maximum distance from each point while still belonging to the same cluster, minPts – a minimum number of data points to form a cluster)  Evaluation of clustering Rand Index: the percentage of data instances that are assigned to the same cluster in both the predicted and true clusters. Should be used to compare different clustering algorithms.  Purity: calculates the percentage of data points that are assigned to the majority class in each cluster. Should be used to evaluate the quality of individual clusters.  Mutual information: measures the amount of information shared between the true and predicted clusters. Should be used to evaluate the overall information of the clustering results.  Silhouette Coefficient: measures how well each data point fits into its assigned cluster, by calculating the distance between the point and other points in its cluster and in neighbouring clusters. The method does not require the true label.  