 A. Distance measurements:  a) General properties:  i. Distance with itself is zero. ii. For an instance pairs, the distance is non-negative and symmetric. iii. Distance measure follows triangular inequality.  b) Euclidean distance: ordinary straight-line distance between two points in  Euclidean (everyday) space.  c) Cosine distance d) Mahalanobis distance (MD): distance between two points in multivariate space;  it scales each data dimension by its variance and adjusting for their relationships.  e) Cityblock/Manhattan distance: absolute value, grid line f) Minkowski distance: distance between two points in a normed vector space.  (p=1, Manhattan; p=2, Euclidean).  g) Jaccard distance: distance used to measure diversity of any two sets.  B. Clustering algorithms:  a) Objective function: Minimise intra-distance; Maximise inter-distance. b) Step 1: define a distance metric between objects c) Step 2: define an objective function that gets us to our clustering goal d) Step 3: devise an algorithm to optimise the objective function  C. Kmeans:  a) Assigning data points to clusters based on the current defined centroids. b) Choosing centroids based on the current assignment of data points to clusters. c)  limitations of simple Kmeans: i. Random initialisation means that you may get different clusters each time.  (Kmeans++ initialisation algorithm may initialise better.)  ii. We have to supply the number of clusters beforehand. (Elbow method) iii. iv.  It cannot find clusters of arbitrary shapes. It cannot detect noisy data points.  d) Kmeans with Kmeans++: an algorithm to choose the initial clusterâ€™s centroids for Kmeans; every time the algorithm runs, it gets closer to the best solution.  D. Hierarchical clustering:  a) Agglomerative clustering (bottom-up) -- merge; b) Divisive clustering (top-down) -- split; c) Dendrogram: Cutting the tree at a different height will produce a selected  precision of clustering.  E. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  a) Calculate the distance from each point in the dataset to every other point. A point is considered a "core point" if it has at least the same number of data points within the defined distance.  b) Data points which cannot be considered core points but are under the defined  distance of the core point are called border points. All other points are regarded as "noise."  c) The next step is to combine all core and border points within dis of each other  into a single cluster.  F. VAT is a visualization technique that transforms the distance matrix of a dataset  into a visual representation in the form of a re-ordered matrix. iVAT is an  extension of VAT that involves repeatedly applying the VAT algorithm to the re- ordered matrix in order to refine the clustering structure.  