This topic we learned about clustering and various clustering algorithms. Clustering is the process of  grouping  similar  data  points  together  based  on  their  properties.  It  falls  under  unsupervised machine  learning,  as  there  is  no  predefined  label.  Similar  objects  are  clustered  together.  To measure similarity between objects in clustering problems, we can use the K-means method. We also learned about different distance metrics or data points, like triangular inequality, distance with itself is zero and the distance is non negative and symmetric. Few applications of clustering are nearest neighbor classification, image retrieval etcetera.  Different distance measurement types used in machine learning are Euclidean distance, Cosine, City Block or Manhattan distance. Manhattan distances are similar to  Euclidean in most cases. There  are  few  other  distance  measurement  types  as  well.  Chebyshev  distance  is  defined  as maximum among the distances along any coordinate dimensions. Minkowski distance generalizes distance metrics in a p norm form. Mahalonabis distance considers both distance and correlation between data points. Jacquard distance measures the similarity among two data sets.  Clustering algorithms can be used in machine learning for classification. The aim of the clustering algorithm is to group objects with similar properties in the same cluster so that the distance between points in the same cluster is minimum and the distance between data points in different clusters is maximum.  Kmeans  is  a  popular  clustering  algorithm.  In  this  algorithm,  it  tries  to  divide  data objects  into  K  clusters.  Clustering  can  be  evaluated  using  external  and  internal  assessments. External  assessment  compare  clustering  performance  against  a  known  clustering.  Internal assessment  determine  if  clustering  follows  certain  intrinsic  assumptions.  Some  examples  of evaluation criteria used in clustering are the Rand Index, Purity, Mutual Information, Silhouette Coefficient etcetera. Rand Index calculates the similarity between two clusters. Purity shows how correctly  data  points  are  assigned  to  correct  clusters  where  they  were  supposed  to  be  and  is measured  by  dividing  total  instances  with  the  number  of  correctly  assigned  instances.  Mutual Information  is  a  probabilistic  approach  to  measuring  similarity  based  on  mutual  information between  clusters  Silhouette  Coefficient  defines  how  similar  the  object  is  to  its  own  cluster compared to other clusters.  The Kmeans algorithm has a few limitations. As centroids are randomly defined, the algorithm might generate different clusters each time. Noisy data points cannot be detected, and the number of clusters needs to be given in advance. The Elbow method can be used to overcome the limitation of  finding  the  optimal  number  of  clusters.  The  Kmeans++  algorithm  is  used  to  address  the limitations of Kmeans to find the centroids. A few other commonly used clustering methods are hierarchical  based  clustering  (follows  a  hierarchy  base),  density  based  clustering  (DBSCAN considers  the  density  or  proximity  of  elements),  shape  based  clustering,  etcetera.  Shape  based clustering is commonly used in computer vision and image processing. DBSCAN can be utilized in clustering applications with more noisy data.  