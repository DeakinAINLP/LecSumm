Measuring distances  -  Measuring similarity or distances between data points is fundamental to machine learning  algorithms; these algorithms are used in both supervised learning methods and unsupervised learning problems.  Distance metrics  -  Widely used in machine learning algorithms -  Are functions that define a distance d(x(I), x(j)) between any two data instances xi and xj -  Examples  o  Clustering algorithms o  K-nearest-neighbor o  Support vector machines o  Data visualization o o  Ranking  Information retrieval  - -  Distance measures satisfy the following three properties:  o  For any instance xi, distance with itself is zero. That is, d(x(i),x(i)) = 0. o  For an instance pairs x(i) and x(j) the distance is non-negative and symmetric.  o  Distance measure follows triangular inequality. That is,  Distance measures satisfying the above properties are known as Distance Metrics.  Euclidean distance  -  Straight-line distance between two points in Euclidean (everyday) space. -  For any two data instances, represented by d-dimensional feature vectors x(i), x(j) , their  euclidean distance is computed as:  Cosine distance  -  We define cosine distance for any two data instance represented by d—dimensional feature  vectors x(I), x(j). The cosine distance for these two feature fectors are computed as:  -  -  Mahalanobis distance  -  MD is the distance between two points in multivariate space; for any two data instances,  represented by d- dimensional feature vectors x(I), x(j) their mahalanobis distance is computed as:  - -  M = covariance matrix of the data -  Elements of covariance matrix in I,j position is the covariance between the I-th and j-th elements  of a vector.  -  Scaling each data dimension by its variance and adjusting for their relationships  -  When data are independent, I.e. M = I (identify matrix), Mahalanobis  distance becomes same as  Euclidean distance  Cityblock / Manhattan distance  -  For any two data instances, represented by d- dimensional features x(I), x(j). Their cityblock  distance is computed as:  -  -  Using this distance, the effect of a large difference in a single dimension is dampened (since the  distances are not squared)  -  Minkowski distance  -  Between two points in normed vector space  o  Think of euclidean distance (2 norm of x(I) - x(j)) and cityblock distance as 1 norm of  x(I) - x(I)  -  Minkowski is a generalization of these distances defined for any p-norm  -  Jaccard distance  -  Distance used to measure diversity of any two sets; x(I) and x(j)\  -  Two instances xi and xj as binary vectors indicating presence or absence of features  -  -  Chebsyshev  -  Greatest of distances along any coordinate dimension  -  For any two data instances, represented by d-dimensional feature fectors x(I), x(j) their Euclidean  distance is  -  -  Schematic overview of popular distances  Clustering Algorithms  -  Goals  o  Group objects of similar properties o  Discover interesting clusters and groups in the data o  Find valid organization of the data  -  Two algorithmic goals:  o  Minimize intra-distance (distance between points in the same cluster) o  Maximize inter-distance (distance between points from different clusters)  Step 1:  -  Define a distance metric between objects  Step 2:  -  Define an objective function that gets us to our clustering goal  Step 3:  -  Devise an algorithm to optimize the objective function  How Kmeans works  -  Most popular clustering algorithm -  Stores k cnetroids -  A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any  other centroid  -  KMeans searches for the best centroids by alternating between two methods:  o  Assigning data points to clusters based on the current defined centroids o  Choosing centroids based on the current assignmnet of data points to clusters  Evaluation of Clustering  -  External assessment  o  Compare clustering performance against a known clustering (often called Ground truth or  Gold standard)  -  Internal assessment  o  Determine if clustering follows certain intrinsic assumptions (e.g., cluster-to-cluster  distance or cluster size etc.)  Rand index  -  Measure of the similarity between two data clusters -  Function that measures the similarity fo the two assignemnts C and C`, ignoring their  permutations  -  Rand index is computed as: -  A = number of pairs of data instances that are in the same cluster in both C,C` -  B = number of pairs of data instances that are in the different clusters in C and in different  clusters in C`  -  C = the number of pairs of data instances that are in the same cluster in C but in different clusters  in C`  -  D= the number of pairs of data instances that are in the different clusters in C but in the same  clusters as C`  Purity  -  Measure the purity for all clusters in terms fo class labels fo the data in each cluster:  - -  Q = number of clusters, A = number of correctly assigned elements in I cluster and n = number of  elements in each cluster  -  Purity is measured by counting the number of correctly assigned instances and dividing by the  number of total instances  -  Example:  o  Mutual information  -  Function that measures the agreement of the two clustering assignments C and C` in terms of how  informative one is about the other, ignoring permutations. e.g., how informative is C about C`  -  -  Silhouette Coefficient  -  Measure of how similar an object is to its own cluster (cohesion) compared to other clusters  (separation)  -  Does not require the ground truth cluster assignments  Limitations of Kmeans:  -  Random initialization means that you may get a different clusters each time -  We have to supply the number of clusters beforehand - -  It cannot find clusters of arbitrary shapes It cannot detect noisy data points  Finding the number of clusters  -  Elbow method  o  Run kmeans clustering algorithm for a range of values of K and for each value of K  compute the sum of squared error (SSE) as:  Clustering with Kmeans++  -  K-means begins with allocating cluster centers randomly then looks for superior solutions. K-  means++ begins with allocating one cluster center randomly but then searches for other centers given the first one.  -  Algorithm for choosing the initial cluster’s center values or centroids for the Kmeans clustering  algorithm  -  ▪  Choose one centroid uniformly at random from dataset ▪  Let D(x) be the distance from a data point to the closest centroid we have already  chosen  ▪  Choose a new centroid from the dataset with probability of ▪  Now repeat previous step until we have initialized K centroids  Guarantee of Kmeans++  -  The objection function monotonically decreasses with each iteration of the algorithm  o  (as the algorithm runs, it gets closer to the best solution)  o  -- additional algorithms relates to SIT720 students--  