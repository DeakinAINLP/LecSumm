I.  Summary Measuring distances: Measuring similarity or distances between different data points is fundamental to many machine learning algorithms.  -  Distance metrics: We can define function for a distance d(xi,xj) between  two any data point. There are some popular concepts relating to distance measuring: clustering algorithms, K-Nearest-Neighbour, Support Vector Machines (SVM), data visualization, information retrieval, ranking.  -  Types of distance measurements:  Euclidean distance:  Cosine distance:  Mahalanobis distance: can be thought of scaling each data dimension by its variance and adjusting their relationships. When data are independent M = I (identity matrix), Mahalanobis distance becomes the same as Euclidean distance.  *Where M is the covariance matrix of the data.  City block/Manhattan distance: yields similar results to Euclidean distance.  Minkowski distance: defines a distance between two points in a normed vector space.         *When p = 1, p = 2, the distance is known as the Manhattan distance and Euclidean distance respectively.  Jaccard distance: is a distance used to measure diversity of any two sets.  II.  Clustering: Clustering Algorithms: put data points into groups by using the similarity and difference of features (or dimensions). Clustering is particularly used in unsupervised learning since it copes with vast amounts of uncategorised data. Our goal is to find objective functions to minimise intra-distance (of points in the same clusters) and maximise inter-distance (of points from different clusters). This process takes 3 steps: Step 1: define a distance metric between objects. Step 2: define an objective function that gets us to our clustering goal. Step 3: devise an algorithm to optimise the objective function.  III.  K-Means: is the most popular clustering algorithm because of its simplicity and speed. How K-Means works: this algorithm starts with a random guess for centroids. Then for each point, it finds the closest centroid and then update the centroids. Keep looping until we achieve convergence. Reference: https://www.youtube.com/watch?v=4b5d3muPQmA  IV.  Evaluation of clustering: based on two main categories of evaluation methods: External assessment (Ground truth or gold standard) and Internal assessment (cluster-to-cluster distance or cluster size).  -  Rand Index: is a measure of similarity between data clusters. We can  compute Rand index by using the number of data pairs between Ground         truth (C) and clusters suggested by algorithms (C’).  -  Purity: measures the purity for all clusters in terms of class labels of the data in each cluster. And this is computed by counting the number of correctly assigned instances and dividing by the number of total instances. But we must make sure to select a fair number of clusters when we perform clustering on a set of data points.  -  Mutual data points: measure the agreement between two clustering assignments such as C and C′, so the aim is almost same as the Rand Index. And this is to answer the questions that how similar they are and are they similar in a useful way.  Where P(i), P’(j), P(i,j) are denoted as the probability of randomly selected instances to i-th and j-th cluster of the partition C. Assume that we also have the ground truth clusters and the corresponding assignment (C) with K clusters. We must perform a clustering method to find C’ and obtain K’ clusters. Only then can we calculate the mutual information of MI(C,C’) and if C’ clustering is highly informative based on C, this clustering method is well-performed.  -  Silhouette coefficient: measures of how similar an object is to its own  cluster (cohesion/similarity) compared to other clusters (separation/difference). One advantage of this method is that it does not require a ground truth. The silhouette coefficient contrasts the average distance between the instances of the same cluster with the average      distance between the instances of different clusters:  Where a(i) is the average distance of i-th instance with all other instances, and b(i) is the lowest average dissimilarity of i-th instance with all other clusters.  V.  Limitations of K-Means:  -  Finding useful number of clusters: We use Elbow Method to find the  appropriate number of clusters for iterations. This method interprets and validates consistency within a cluster analysis. The idea is to run K-Means algorithm for a range of values of K to compute sum of square error by this formula:  VI.  Clustering with K-Means++: Different from K-Means which starts allocating cluster centroids randomly and then looks for better solutions, K-Means++ starts allocating with just one cluster centroid randomly and then searches for other centroids:       By choosing one centroid as starting point, the objective function monotonically decreases which means the outcomes get closer to the best solution when the algorithm runs.  