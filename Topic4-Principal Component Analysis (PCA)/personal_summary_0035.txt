Unsupervised machine  learning Clustering can find patterns in large data sets when the data is not labelled and dimensionality reduction helps us when the number of features, that describe the difference between data, becomes too large to manage.  Measuring distances Measuring similarity or distances between different data points is fundamental to many machine learning algorithms  Distance metrics are used widely in machine learning algorithms. Distance measures are functions that define a distance d(xi, xj) between any two data instances xi and xj.  Distance Metrics  Clustering algorithms K-Nearest-Neighbour Support vector machines (SVM) Data visualisation Ranking  Information retrieval  Distance measures satisfy the following three properties:  1.  For an instance ùë•ùëñ distance with itself is zero 2.  For any instance pairs ùë•ùëñ and ùë•ùëó distance is non-negative and symmetric 3.  Distance measure follows triangular inequality.  For example, nearest neighbour classification. We can use the distance to find the nearest neighbour and classify the data to the class label of this neighbour  Types of distance measurements Euclidean distance Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday) space.  Cosine distance Cosine distance is a similarity metric used to measure the similarity between two vectors in a high - dimensional space. It measures the cosine of the angle between two vectors, which is a measure of how closely they are aligned.  Mahalanobis distance The Mahalanobis distance (MD) is the distance between two points in multivariate space .  Mahalanobis distance can be thought of scaling each data dimension by its variance and adjusting for their relationships. When data are independent Mahalanobis distance becomes same as Euclidean distance.  Cityblock/Manhattan distance In most cases, this distance measure yields results similar to the Euclidean distance. However, using City block distance, the effect of a large difference in a single dimension is dampened (since the distances are not squared).  Minkowski distance Minkowski distance is a generalization of other distance metrics, such as Euclidean distance and Manhattan distance, that measures the distance between two points in a multidimensional space . In addition to its use in measuring distances between points in a multidimensional space, the Minkowski distance is also used in machine learning algorithms such as k-nearest neighbors (KNN) and regression, where it is used to measure the similarity or dissimilarity between data points.  Jaccard distance Jaccard distance is a similarity metric used to compare the similarity between two sets.  Clustering Algorithms Clustering puts data points into groups. It uses similarity and difference of features (or dimensions) to create groups in material that is unclassified and has no known targets. It‚Äôs particularly used in unsupervised learning as it can deal with vast amounts of uncategorised data however it creates groups so it‚Äôs useful in supervised learning as well.  The goal of clustering algorithms are to:  Group unalbelled data objects with similar properties toge ther Discover interesting or perhaps unexpected clusters in the data Find a valid or useful organisation of the data  We can use objective functions to:  Minimise intra-distance (distance between 2 points in the same cluster) Maximise inter-distance (distance between points from different clusters)  One of the most popular clustering algorithms is K-Means  Algorithm:  1.  Start with a random guess of the centroids 2.  For each point find its closest centroid 3.  Update centroids 4.  Loop until converging.  Evaluation of clustering There are in general two main categoris of evaluation methods for clustering:  External assessment  Compare clustering performance against a known clustering   Internal assessment  Determine if clustering follow certain intrinsic assumptions  Rand Index The Rand index is a measure of the similarity between two data clusters. The rand index is a function that measures the similarity of two clustering‚Äôs usually a clustering against a known ground truth.  Purity In evaluation methods of clustering, it is common practice to use more than one approach for evaluation because neither of the evaluation methods are comprehensive enough. Purity is a way of quality measurement in clustering methods. Each cluster is assigned to the class label which has the majority in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances.  Mutual Information Like the Rand Index it measures the agreement between two clustering assignments. Mutual information is a function that measures the agreement of the two clustering assignments and in terms of how informative one is about the other, ignoring permutations. An advantage of mutal information is the number of clusters found by the two assignments aren‚Äôt required to be the same.  Silhouette Coefficient The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments. A high value of Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. If most objects have a high value, then the clustering configuration is appropriate. On the other hand, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters.  Limitations of k means  Random initialisation means you may get different clusters each time. (Can use Kmeans++ to  fix this)  Must supply the number of clusters beforehand (see below) Cannot find clusters of arbitrary shapes Cannot detect noisy data points  Finding a useful number of clusters The Elbow Method is a method for finding the appropriate number of clusters. The Elbow method interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset.  Kmeans++ Kmeans++ is an algorithm for choosing the initial cluster‚Äôs centre values or centroids for the Kmeans clustering algorithm. K-means++ starts with allocating one cluster centre randomly and then searches for other centres given the first one.       