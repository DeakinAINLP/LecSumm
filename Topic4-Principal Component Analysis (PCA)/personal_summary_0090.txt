Topic 3 Summary Points (Clustering)    In 3.2 we go over measuring distances and the importance of distance metrics and all the different types of distance measurements. These distance measurement types include: Euclidean distance, Cosine distance, Mahalanobis distance, Cityblock/Manhattan distance, Minkowski distance and Jaccard distance. All the theory and calculations behind these measurements are shown.    3.3 simply shows how to calculate the many different distance measurements and where  these can be applicable.    3.4 brief reintroduction to what clustering is; putting data points into groups based on  similarity and differences of features. Ideally used for unsupervised learning because of the high amount of unlabeled data.    3.5 goes over the most popular clustering algorithm which is K-means and how this works in an unsupervised learning environment. A simple overview of K-means algorithm is that it picks a random centroid for each cluster and then for each point it will find its nearest centroid and this will loop until you have distinguished clusters.    3.6 mainly goes over how we should evaluate clusters. There are two ways of evaluating  clusters, including external assessment (compares clustering performance against a known clustering) and internal assessment (where the cluster is evaluated with only the result itself). The Rand index is introduced which is shown to be a measure of similarity between 2 clusters. Purity is also summarized as a quality measurement in clustering methods where we measure all clusters for their purity percentage. Mutual information is another way of analyzing clusters and quite similar to the Rand index, it asks what one cluster says about another cluster and vice versa. Lastly the silhouette coefficient is shown and produces a value that shows how an object matches up to its own cluster.    3.7 goes over the limitation of K-means; these limitations include random initialization resulting in different clusters each time, having to use the Elbow method to find the appropriate number of clusters. Arbitrarily shaped clusters cannot be found using this method and it cannot detect noisy data points. In 3.8 K means++ is introduced as an algorithm similar to K means but the difference is in how the centroids are selected and also how K means is depending on the initialization of a centroid.      3.10 introduces us to K means clustering via Python using the scikit learn package. It shows how to merge two data frames and rename the header and how to define a number of clusters.    3.11 is mostly about the evaluation of performance in K means clustering. It very usefully  shows us how to find the purity score of a cluster.   