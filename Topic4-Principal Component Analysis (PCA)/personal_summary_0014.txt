This topic starts our journey into unsupervised machine learning. This topic specifically focuses on clustering in unsupervised learning. Clustering Clustering means to assign data points to different groups or “clusters” of data (hence the name clustering). This assignment is done based on the similarity of the data points. The similarity is often referred to as the distance given that if all data points are plotted on a scatter graph, those that are nearer together are more similar than those apart. The measurement of these distances results in data points known as distance metrics. Distance metrics are analysis results about data that can be attained using one of several algorithms and is found in many forms of machine learning such as clustering algorithms, K-Nearest-Neighbor and Support Vector Machines. Distance Measure Properties All distance measures satisfy the following properties. 1.  For any distance x, the distances with itself is 0. Thus, d(x1,x1) = 0. 2.  For any pair of data instances x1 and x2, the distance between them is non-negative and symmetric in that d(x1,x2) >= 0 and d(x1,x2) = d(x2,x1). 3.  Distance measure follows triangular inequality in that d(x1,x2) <= d(x1,x3) + d(x2,x3) Types of Distance Measurements There are a few different types of distance measurements. These are: 1.  Euclidean Distance 2.  Cosine Distance 3.  Mahalanobis Distance 4.  Cityblock/Manhattan Distance 5.  Minkowski Distance 6.  Jaccard Distance Each of these is suited to different applications based on the dimensionality of the datasets. Clustering Pt2 Unlike humans, computers are not naturally inclined to detect patterns in data. This is where clustering fits in. Through the use of specific algorithms, a machine learning model is able to detect patterns within the data set without needing human input to guide it on each dataset. As mentioned in the opening, clustering algorithms assign data points into groups or clusters based on the similarity of the data points (distance). The main objective of this is to classify otherwise unclassified or unlabeled data together. This will assist in discovering therwise unexpected clusters within the data and in finding a valid or useful way of organizing the data. Given this, it can be said that clustering algorithms have two goals: 1.  To minimize intra-distance, that is to keep the average distance between data points in the same cluster as low as possible. In other words, only have data points with high similarity in the same cluster. 2.  To maximize inter-distance, that is to keep the average distance of points in different clusters as high as possible. In other words, to keep clusters as unique as possible. K-means K-means is the most popular clustering algorithm within the machine learning world as it is simple to implement and runs quickly. K-means works to assign data points to clusters by first initialization k number of centroids. A centroid is the centre of a cluster and thus should be the mean(average) of all data points within the cluster. This is where the name “K-means” comes from as a random(k) number of centroids is initialized. The algorithm then does multiple loops through all data points, it starts by assigning each data point to one of the randomly generated centroids. Once this is done it updates the value of that centroid by calculating the average of all data points assigned to that centroid. After this, it will start the first loop again to see if any data points are more suited to a different centroid and then updates the centroids again. This is repeated until a useful group structure is found. Evaluation of Clustering As with all machine learning models, clustering models need to be evaluated to determine if they are running efficiently and providing useful output. In specific reference to the evaluation of clustering algorithms, there are two main categories of evaluation methods. These are: 1.  External assessment in which the performance of the clustering algorithm is compared against a known clustering. This known clustering is usually referred to a “Ground Truth” or “Gold Standard”. An example of external assessment is the Rand Index, Purity and Mutual Information. Internal assessment in which a further algorithm is run to determine if the clustering output follows a set of intrinsic assumptions such as cluster-to-cluster distance and cluster size. Some examples of this are the Silhouette Coefficient and the Dunn Index. 2. Limitations of K-means As mentioned in the intro, K-means is extremely popular due to it simplistic integration and usually speedy processing times. However, this simplicity does lead to a few limitations in the capabilities of K-means. The most significant of these limitations are: Random initialization results in the possibility of attaining different clusters each time the model is run. A solution to this is the use of the K-means++ initialization algorithm to initialize in a more strategic manner. The model has to be supplied with the number of clusters before it is run. The Elbow method can be used to choose K however it is usually not straightforward to implement this. The K-means algorithm cannot be used to find clusters of arbitrary shapes. The K-means algorithm is unable to detect noisy data points. Noisy data points are data points that should not be taken into account during the cluster analysis. These include data points that are outliers or are not valid. The K-median method can be used to combat this as it is slightly less affected but like K-means, it cannot identify noisy data points. Finding a useful K value Since K-means needs a predefined K value, it is important to make use of some form of method to find the most useful K value. The general method of doing this involves running the K-means algorithm several times, each time using a different K value. The results of each run are measured through some form of evaluation. As an example, the Elbow Method runs the algorithm several times, each with a different K value from within a range. After each run, the sum of squared error (SSE) value for the run is calculated and recorded. After this, all SSEs are plotted on a graph and from this the optimal K value can be chosen. The value that makes the “elbow” of the graph is the one which will be most useful. However, the optimal value of K cannot always be determined using the Elbow method as it is not always possible to deduce the elbow shape from the SSE values. In a case like this, the Elbow method cannot be used and another method must be used. K-means with K-means++ K-means++ is an algorithm that can be used to choose the initial values of the centroids to be used by the K-means algorithm. This is done by assigning one centroid value randomly and then searching for other centroids based on this initial centroid. Given this, both algorithms do use random initialization at first but in vastly different ways. Whereas K-means will first assign all centroid values and then begin to search for better values, K-means++ will assign an initial value chosen uniformly at random from the dataset. After this, it will choose a new value from the dataset to be a centroid based on a probability calculation that makes use of the shortest distance between each considered data point and the closest already assigned centroid. The guarantee of K-means++ is that the objective function will monotonically decrease with each iteration of the algorithm. This is to say that with each run of the algorithm – it will get closer to the best solution and will never start to move further away.                             Reflection This topic consisted of a set of extremely important and useful learning resources. Given its popularity, K-means is an extremely important implementation of clustering in unsupervised machine learning and having a good grasp of it is invaluable. The resources do brush over some of the maths related to the K-means algorithm but this can always be revised and brushed up on later down the line. Overall, I believe this topic’s content has provided a very good grounding to anyone that did not already have knowledge of K-means and to those that did, it provided a good refresher to the basics and an introduction to some of the more advanced aspects such as model evaluation as well as K- means++.       Quiz Result   