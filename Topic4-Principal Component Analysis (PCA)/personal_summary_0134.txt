 This topic I have learned about two aspects about Unsupervised learning, one being Clustering and Second being Dimensionality reduction.  Before understanding these topics, let’s get back and have ground understanding about some topics. First is Distance matrices, there are different mathematical functions, which measure the distance between two data or more data points, like Euclidean Distance, Cosine distance, Manhattan distance, Minkowski distance and Jaccard distance. These functions are crucial in many machine learning techniques and the one which we learned this topic, Kmeans.  The way Kmeans Clustering works is very simple, the objective is to find the clusters in the data, however, it is subjective to choose number of clusters which is “K”, the value of “K” can be determined using the total variance of clusters and plotting them in a line plot.  There are different ways to find the value of “K”, the easiest way is “Elbow method”, which plots the variance between clusters over the number of clusters, the value of “K” where the variance gradually decreases is the best “K” value for the data.  Another way to find the K value is “Silhouette Coefficient”, it measures the distance between different clusters and ideally for the best K value, the distance between clusters is infinite and for the worst K value it would be -1.  With all, Kmeans comes with its own demerits, you still need to find the optimum value of K, and It ignores the noise in the data.  Later, I learned how to apply all these in Python Programming language.     