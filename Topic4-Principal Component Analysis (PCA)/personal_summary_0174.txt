In this topic mainly focus on unsupervised learning. Clustering and Dimensionality reduction are the main categories of unsupervised learning. Clustering can find same patterns and trends in the large data sets and Dimensionality reduction is helps us when the number of features, that describe the difference between data, becomes too large to manage.  Distance metrics are used widely in machine learning algorithms. Distance measures for measuring how similar the instances are. Therefore, distance matrix gives huge advantages in machine learning.  Machine learning algorithm used.   clustering algorithms   K-Nearest-Neighbor   Support Vector Machines (SVM)   data visualization    information retrieval ranking  We have learnt the types of distance measurements, mainly discussing the Euclidean distance. The definition of the Euclidean distance is length of a line segment between the two points.  Apart from the Euclidean distance there are some other distances used in machine learning, we have learnt how to calculate below mentioned distance as well,    Cosine distance   Mahalanobis distance   Cityblock/Manhattan distance   Minkowski distance  Jaccard distance  Clustering Algorithms is the main part in unsupervised learning, therefore we have learnt the its applications used in. Clustering puts data points into groups. It uses similarity and difference of features (or dimensions) to create groups in material that is unclassified and has no known targets. It’s particularly used in unsupervised learning as it can deal with vast amounts of uncategorized data. However, it creates groups so it’s useful in supervised learning as well.  Main methods in clustering methods  Step 1: define a distance metric between objects.  Step 2: define an objective function that gets us to our clustering goal.  Step 3: devise an algorithm to optimize the objective function.  Then we learnt the how Kmean works then we learnt how to evaluate the clustering.     Evaluation of clustering methods is not easy. But, generally there are two main categories of evaluation methods for clustering:  External assessment:  compare clustering performance against a known clustering (often called Ground truth or Gold standard).  Internal assessment:  determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.).  Examples:  Silhouette coefficient, Dunn index etc.  The most important limitations of simple Kmeans are:    Random initialisation means that you may get different clusters each time. As a solution, we can  use a Kmeans++ initialisation algorithm to initialise better.    We have to supply the number of clusters beforehand. We can use the Elbow method to choose     , but it may not be straightforward. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.) Then we learnt the Limitations of Kmeans  To find the number of clusters we used elbow method.         