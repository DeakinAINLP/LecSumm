Main points covered in Topic 3  1.  Unsupervised learning  a.  In unsupervised learning, the dataset is unlabeled. It is difficult to draw meaningful insights  without  recognizing  patterns  or  applying  advanced  techniques  and strategies. Clustering and dimensionality reduction techniques play vital roles. b.  Clustering  is  the  form  of  grouping  similar  points  together.  Different  datasets  require different techniques to figure out appropriate clusters.  c.  Dimensionality reduction on the other hand is the technique to reduce complexity  but has meaningful and important data from the original dataset.  2.  Distances  a.  Measuring  distances  is  to  find  how  similar datapoints  are to  each other.  A  few popular applications are image recognition, voice recognition and face detection.  b.  But which one is a better distance metric?  i.  It all depends on the scenario of the problem or the nature of the dataset  we have.  c.  Types of distance measurements,  i.  Euclidean distance – Simple distance calculation for continuous data ii.  Cosine distance – Suitable for vectors of text data. iii.  Mahalanobis distance iv.  Manhattan distance – Distance measure in absolute values v.  Minkowski distance vi.  Jaccard  distance  –  Better  suited  for  sets.  Helpful  in  finding  the  diversity  between two sets.  3.  Clustering in detail  a.  The aim of clustering is to group unlabeled data objects with similar properties  together.  b.  Finding unknown clusters with valid and useful information c.  Intra-distance:  Intra  distance  in  a  cluster  should  be  reduced  as  these  are  the  distance between the points in a single cluster.  d.  Inter-distance: However, inter distance should be increased or be maximum  as  these are the distance between two different clusters  4.  A generic distance setup:  a.  Find the right distance metric to be applied. b.  Define an objective function to help reach clustering goal. c.  Design or apply the right algorithm to optimize the function.  Kmeans Clustering:  Kmeans clustering is  one of the most used algorithm that works well for unsupervised learning. The reason for the wide usage of Kmeans is due to its general accuracy to detect similarity in data points.  The algorithm for Kmeans is, ▪  Determine the number of clusters needed. However, in some cases we would prefer  the algorithm to determine the right number of clusters. Initialize the centroids randomly.  ▪ ▪  Assign each data point to the nearest centroid. ▪  Find the mean of the clusters and reiterate the process of assigning the data point to the nearest centroid Recalculate the centroids of each cluster based on the mean of all the points assigned to it.  ▪  Repeat steps 3-4 until the cluster assignments converge and do not change. ▪  Output the final k clusters.  K-means++ is an improvement over the basic K-means algorithm that helps to avoid poor initialization  and  increases  the  chances  of  finding  a  globally  optimal  solution.  The difference is the  choice of the centroids, that are chosen from the data points using a probability. The K-means++ algorithm helps to ensure that the centroids are well spread out and more representative of the entire data set. This, in turn, increases the chances of finding a globally optimal solution and improves the performance of the algorithm.  Disadvantages of KMeans:  The original positions of the initial centroids has an impact on the performance.  If the data  is  not  properly  clustered  to  begin  with,  this  might  be  an  issue.  It  cannot  manage clusters of various sizes and forms. Since not all clusters in real-world data are spherical, K-means  makes  this  assumption.  This  may  cause  certain  data  points  to  be  incorrectly classified  by  the  algorithm.  For  huge  datasets,  it  could  be  expensive  computationally. Calculating the distance between each data point and each cluster centroid is necessary for the k-means method. For big datasets, this might be a time-consuming operation.  Limitation of KMeans    Cannot find clusters in random shapes.   Doesn’t work well with outliers.   Need to define the number of clusters beforehand.  Other clustering algorithms  1.  Hierarchical clustering 2.  DBSCAN (density based) 3.  Shape-based Clustering  Hierarchical Clustering Hierarchical  clustering  is  a  method of  cluster  analysis  that  aims  to build  a  hierarchy of clusters (dendrogram). This type of clustering works by iteratively merging small clusters into larger ones until all data points belong to a single cluster. There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative clustering starts with each data point as a separate cluster and merges the closest pair of clusters until all data points belong to the same cluster. Divisive clustering, on the other hand, starts with all data points in a single cluster and recursively divides it into smaller clusters until each data point is in a separate cluster.  DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  DBSCAN  has  its  own  advantages,  such  as  the  ability  to  proceed  without  having  to predefine the number of clusters. This algorithm mainly works with eps and min_samples. This  algorithm  groups  together  data  points  that  are  close  to  each  other  in  a  high- dimensional space. DBSCAN works by defining a neighborhood around each data point, based on a distance metric and a density threshold. A point is considered a core point if it has at least a minimum number of neighbors within its neighborhood. Points that are not core points but are within the neighborhood of a core point are considered border points, while points that are not core points and are not within the neighborhood of any core point are considered noise points.  