This topic we learned about unsupervised learning. We learned about clustering and dimensionality reducFon. Clustering is good because within large data sets it could become tedious to ﬁnd paIerns, therefore clustering is a very helpful way of ﬁnding paIerns especially in data sets that have no labels.  We learn about distance metrics. It is a machine learning algorithm that is widely used within the programming realm. Basically, it measures the similarity between two instances. Some examples for machine learning include data visualisaFon, K nearest neighbour, and of course clustering algorithms.  The types of distance measurements that we have are Euclidean distance, cosine distance, Mahalanobis distance, Cityblock/ManhaIan distance, Minkowski distance and Jaccard Distance.  In general clustering algorithms get unlabelled data objects with similar properFes and group them even without labels. Basically, just ﬁnding unexpected clusters a giving meaning to it. In general, we want to minimise the intra distance within the same cluster and we would like to maximise the inter distance of diﬀerent clusters.  A collecFon of data points are divided into K clusters using the clustering method K-means, where K is the predetermined number of clusters. UpdaFng the cluster centroids iteraFvely while allocaFng data points to the closest centroid unFl convergence is how the algorithm operates. This is how it works simply put:  1.  Create K cluster centroids at random from data points 2.  Assign each data point to the nearest centroid, using any of the distance  measurements.  3.  Calculate the average of each of the following cluster, ﬁnding the new center 4.  Repeat 2 and 3 unFl the results stop changing  A[er the results we can then evaluate the clustering using silhoueIe coeﬃcient .  