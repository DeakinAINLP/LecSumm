Machine learning makes use of measuring distances or similarities between data points as these measurements can be used to classify clusters and define data. Clustering is the act of putting data points into groups using the similarities and differences of the features to identify the different groups. K-means is one of the more commonly used clustering algorithms in ML as it is fast and simple. It is used to through multiple iterations, find the centre of a cluster through random initialisation. After using a clustering method we need to evaluate the cluster to show if it is a good way of clustering the data or not. We use either external assessment to compare the Gold standard to the cluster or, Internal assessment to determine if the clustering follows certain intrinsic assumptions, such as size or distance from clusters. Kmeans is limited in its use due to its random initialisation, having to supply the number of clusters and its inability to find noise clusters or clusters of arbitrary shapes. K-means++ is an extension of the k-mean algorithm that is used to choose the initial clusters centre values. To measure distance between data points there are various formulas that are used in both supervised and unsupervised learning. Distance metrics are functions that define a distance between any two data instances. Distance measures must tick off 3 properties, for any distance, the distance with itself is zero. For any instance pairs the distance is nonnegative and reversible. And the distance measure follows triangular inequality. There are various types of distance measurements such as Euclidean distance, which is the ordinary straight line distance between two points. Mahalanobis distance  which is the distance between two points in multivariate space. Or Jaccard distance which is the distance used to measure the diversity of any two sets, using the points as binary vectors. Machine learning uses clustering algorithms to put data into groups, identifying similarities in features as to classify data into groups. It is used in unsupervised learning as it can manage lots of uncategorised data and create groups that can then be used in supervised learning too. Clustering aims to group unlabelled data objects with similar properties, discover unexpected clusters in data and find valid and useful organisations of data. The main objectives use the functions of intra distance, the distance between points in the same cluster and inter distance, the distance between points from different clusters to help validate that it has correctly clustered. K-means is the main clustering method that is used. It starts off at a random point and then classifies data points to each centroid by drawing a line perpendicular to the middle of the meeting point of the centroids. It will then reiterate and go to the centre of the new cluster and repeat this process until there is no meaningful changes in the centroids position. Evaluation of clusters is then done to determine how useful the cluster outcome is to be used. Using either external or internal assessment. Various assessments include the Rand Index which is a measure of similarity between 2 clusters, this is a external method which uses the ground truth. Purity measures the ratio of a class in the cluster and counts the number of each label in that cluster. It then compares them to find the “purity”. The silhouette coefficient measures how similar an object is to its own cluster compared to other clusters. And if the value is high then the object is well matches to its own cluster. K-means has limitations in its use, the main ones being its random initialisation that can change the outcome when run, needing to supply the number of clusters and that it cannot find clusters of arbitrary shapes or within noisy data points. The elbow method is used to find the appropriate number of clusters which interprets and validates consistency within a cluster to confirm the number of appropriate clusters. To solve one of k-means issues we use K-means++ where we can choose the starting point of the centroids to start clustering from. Kmeans++ chooses one centroid and then randomly searches for the other centres using the first assigned one. 