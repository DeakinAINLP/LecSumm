topic 3 covered This topic we were introduced to clustering. Clustering is a technique to find patterns in data by grouping cases together based on their features. This relation is referred to as distance or similarity between observations. Common metrics used to measure similarity between observations are euclidean distance (which i have used previously), manhattan distance, and cosine similarity (which i was also familiar with). a common algorithm and what we used (and its variation) was the kmeans algorithm. this has an iterative approach where it assigns observations to clusters and updates the cluster centres until it converges. the performance can evaluated using metrics such as the silhouette score (which we used to find the optimal k value) and adjusted rand score. limitations were its sensitivity to initial placement of the cluster centres and getting stuck in local minima. it also assumes clusters are spherical and equal in size, which isn't always the case in real world instances. we then learn't about the kmeans++ algorithm which overcomes some of these limitations. it uses a smarter initialisation method for the cluster centres, which helps to avoid getting stuck in local minima and generally leads to overall better clustering results. 