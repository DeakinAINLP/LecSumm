Introduction to Topic 3  ● By the end of topic 4 you will be able to: ● use clustering for revealing patterns from unlabelled data ● learn techniques to reduce dimensionality ● apply suitable clustering/dimensionality reduction techniques to perform unsupervised  learning of data in a real-world scenario.  3.2 Measuring distances  Distance metrics - ML examples: The most related examples in machine learning are:  ● clustering algorithms (we looked at examples of clustering last topic) ● K-Nearest-Neighbor ● Support Vector Machines (SVM) ● data visualization ● information retrieval ● Ranking ●  Types of distance measurements  Euclidean distance  Cosine distance  Mahalanobis distance  Cityblock/Manhattan distance  Minkowski distance  Jaccard distance  3.3 Example of different distance metrics  https://video.deakin.edu.au/media/t/0_v41nur27 Different distance metrics In this video we practise how to calculate some of the distance measurements that were introduced previously.  This is a great way to revise and consolidate your learning of these concepts. It explains how the calculations is implemented as well as where each measurement potentially can be used.  You will see the following distance measures being demonstrated:  The L1 (Manhat) norm that is calculated as the sum of the absolute values of the vector. The L2 (Euclid) norm that is calculated as the square root of the sum of the squared vector values.  ● Euclidean distance  ○ App when we have continues numerical variables. Reflect absolute distances  ● Cosine distance  ○ Used for metrics for measuring distance when the magnitude of the vector does  matter .. angle ● Mahalanobis distance  ○ continues numerical variables / Reflect absolute distances / remove redundancy  i.e repeated values  ● Cityblock/Manhattan distance  ○  ● Minkowski distance  ● Jaccard distance  Diverstiy of ANY two sets (measure similarity between sets)  3.4 Clustering and its applications  As with all the problems in machine learning, we need to answer the question:  Clustering Algorithms  Clustering puts data points into groups. It uses similarity and difference of features (or dimensions) to create groups in material that is unclassified and has no known targets. It’s particularly used in unsupervised learning as it can deal with vast amounts of uncategorised data however it creates groups so it’s useful in supervised learning as well. We will look at this idea in Course 3, 4 and 5.  As we discussed in topics 1 and 2, the goal of clustering algorithms are to:  ■ Group unlabelled data objects with similar properties together ■ Discover interesting perhaps unexpected clusters in the data  ■  Find a valid or useful organisation of the data  In other words, we can define two algorithmic goals. We need to find objective functions to:  ■ Minimise intra-distance (distance between points in the same cluster) ■ Maximise inter-distance (distance between points from different clusters)  Figure. difference between intra-distance and inter-distance  Now we can define a generic set-up based on our current understanding from clustering methods:  ■ Step 1: define a distance metric between objects ■ Step 2: define an objective function that gets us to our clustering goal ■ Step 3: devise an algorithm to optimise the objective function  3.5 How Kmeans works  the most popular clustering algorithm in machine learning is called K-means. It is simple and fast.  In the video we explore how K-means works on an unlabelled data set.  If maths is not your thing, skip to 2.34 and watch the example. If you become a data scientist or you want top marks you will need to dig in to the maths but right now it’s important to understand the concept.  In this algorithm, k represents the centre points of clusters. You start off with these centroids and then measure teach data point to find its closest centroid. In other words, K-means stores k centroids for defining clusters. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-means searches for the best centroids by alternating between two methods:  Assigning data points to clusters based on the current defined centroids (points which are the centre of a cluster). Choosing centroids based on the current assignment of data points to clusters. Step 1 and 2 repeat until you find a useful grouping of data points.  This method was independently discovered in 60s and 70s. Due to its reasonably accurate performance, this method still remains the algorithm of choice for analysis tasks.  E = Exception Step M = Maximisation Step  3.6 Evaluation of clustering  Evaluation of Clustering Now that we’ve learned about clustering methods and Kmeans, it is time to work on evaluation methods of clustering results. Is this a good way of clustering this data?  In this video we review how the clustering evaluation method purity can be used to evaluate the outcomes of a clustering experiment that has produced 3 centroids. It is a very simple example which highlights the fundamental principles of how the algorithm works.  As has been mentioned before, all machine learning algorithms are required to be evaluated. Are the clusters useful? Evaluation of clustering methods is not easy. But, generally there are two main categories of evaluation methods for clustering:  1. External assessment:  compare clustering performance against a known clustering (often called Ground truth or Gold standard). ● Internal assessment:  determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.).  ● Examples:  Silhouette coefficient, Dunn index etc.  The following figure illustrates a sample of ground truth (C) and the clustering partition found by a clustering algorithm  .  Figure. Ground truth VS Clustering partition found by algorithms  3.7 Limitations of Kmeans  Kmeans clustering has a number of limitations.  Watch the video to understand some of Kmeans limitations and possible solutions.  The most important limitations of simple Kmeans are:  ■ Random initialisation means that you may get different clusters each time. As a solution, we can use a Kmeans++ initialisation algorithm to initialise better. ■ We have to supply the number of clusters beforehand. We can use the Elbow  method to choose K, but it may not be straightforward.  ■ It cannot find clusters of arbitrary shapes. ■ It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)  Finding a useful number of clusters  As we have mentioned before, one of the challenges of Kmeans is the assumption we have to make about the number of clusters to start with. The Elbow Method is a method for finding the appropriate number of clusters. The Elbow method interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset.  Elbow Method  Watch the video for an introduction to this concept. The idea of elbow method is to run the Kmeans clustering algorithm for a range of values of k, compute the sum of squared error (SSE) as:  3.8 Clustering with Kmeans++  Kmeans++ is an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm.  As we have said before, K-means starts with allocating cluster centres randomly and then looks for better solutions. K-means++ starts with allocating one cluster centre randomly and then searches for other centres given the first one. So both algorithms use random initialisation as a starting point but in different ways. So Kmeans++:  3.9 Other clustering algorithms  Other clustering algorithms Kmeans is one of the most popular clustering methods in machine learning but it is not the only clustering algorithm.  In the category of Flat Clustering where the goal of the algorithm is to create clusters that are coherent internally but clearly different from each other, there are two more clustering methods:  ● Kmeans (as we know) ● Hierarchical clustering ● DBSCAN (density based) ● Shape-based Clustering ●  Hierarchical clustering  There is another type of clustering algorithms called hierarchical clustering. These algorithms find clusters that have a predetermined order.  There are two types of hierarchical clustering: one of them is a bottom-up approach and the other is top-down:  ■  ■  Agglomerative clustering (bottom-up): A “bottom up” approach in which each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive clustering (top-down): A “top down” approach in which all observations start in one cluster, and splits are performed as one moves down the hierarchy.  In this type of clustering, you end up with an hierarchical tree diagram or dendrogram. Cutting the tree at a different height will produce a selected precision of clustering.  Let’s look at this more closely.  Agglomerative clustering  Consider the following figure as an example. At the bottom of the tree, at the starting point each of the characters are assigned into a single separate cluster.  Figure. Agglomerative Clustering  DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that clusters certain items in a group based on a given data point.  For this, we need to set a minimum number of data points (minPts) and a distance (dis). Because these parameters are user-defined, the resulting cluster is dependent on them.  1.  2.  3.  Calculate the distance from each point in the dataset to every other point. A point is considered a "core point" if it has at least the same number of data points within the defined distance. Data points which cannot be considered core points but are under the defined distance of the core point are called border points. All other points are regarded as "noise." The next step is to combine all core and border points within dis of each other into a single cluster.  We keep repeating the above steps until we reach the  Shape-based clustering, VAT, iVAT VAT is a visualization technique that transforms the distance matrix of a dataset into a visual representation in the form of a re-ordered matrix. The re-ordering is done in such a way that the dissimilarities between the data points are emphasized in a way that reveals the underlying clustering structure of the data. If the data has a clear clustering structure, then the re-ordered matrix will exhibit block-like structures along the diagonal, indicating the presence of clusters.  iVAT is an extension of VAT that involves repeatedly applying the VAT algorithm to the re-ordered matrix in order to refine the clustering structure. The iVAT algorithm iteratively computes the VAT on the re-ordered matrix until a stable clustering structure is obtained. This can help to identify the optimal number of clusters in the data.  Both VAT and iVAT are useful tools for exploratory data analysis, allowing data analysts to gain insight into the underlying structure of the data and to identify the appropriate number of clusters for subsequent clustering algorithms.  