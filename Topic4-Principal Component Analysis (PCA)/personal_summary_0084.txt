Measuring Distances  Measuring both the similarities and differences between data points is a core concept in most machine learning algorithms, there are various different measurements that are used when measuring distance.  Distance Measures are functions that we use to define distance, between any two instances of data. Common examples being clustering algorithms, K-Nearest-Neighbour, Support Vector Machines, data visualisation and information retrieval. The three Distance Metrics that Distance Measures abide by are that for any instance of data the distance between that instance and itself must be zero, for a pair of two instances of data their distance must not be negative and must be symmetric lastly, distance measures must follow triangular inequality.  Euclidean distance is a distance measurement that is commonly used to measure the distance between data points, it measures the ordinary straight line distance between two data points.  Cosine distance is also a frequently used distance measurement that instead measures the distance between two instances of data based on the angle between them.  Mahalanobis distance measures the distance between two data points in multivariate space, it involves scaling each data dimension based on variance and adjusting based on relationships thus, if data is independent it is the same as Euclidean distance.  Cityblock/Manhattan distance measures the distance between dimensional vectors with similar results to Euclidean distance with the main difference being that Manhattan distance doesn’t square the distances so large dimensions have less of an effect on smaller dimensions.  Minkowski distance measures the distance of two instances of data in normed vector space, it can be thought of a generalisation of both Euclidean and Manhattan distance.  Jaccard distance measures the diversity between two sets of data, indicating the absence or presence of features between them.  Clustering and its Applications  Clustering utilises the similarity and difference of features to put data points that have no classification into groups. This is ideal for unsupervised learning as it often is dealing with large amounts of uncategorised data. Clustering algorithms have to main goals, to reduce intra-distance and maximise inter-distance, meaning that it wants to reduce distance between points within a cluster and increased distance between clusters.  From this we have defined the generic set up of clustering algorithms to involve three steps. 1. Define a distance metric between objects 2. Define an objective function to get us to our clustering goal 3. Devise an algorithm to optimise the objective function.  How Kmeans Works  Kmeans is one of the most popular clustering algorithms due to simplicity and speed. It involves assigning cluster centres (centroids) and measuring the closest centroid for each data points and assigning it to its closest centroid. Then based on the average of all the data points that are assigned to a centroid, the position of the centroid is moved, this happens for all the centroids. From here we measure distance and assign the data points to their nearest centroid before updating the centroid    again based on the average of the data points assigned to it. This loop continues until the centroid are no longer significantly changing their position.  Evaluation of Clustering  Generally there are two main categories of evaluation methods when it comes to clustering, that being External assessment and Internal assessment. External assessment involves comparing the clustering result with an already known clustering of the data. While Internal assessment involves seeing if the clustering has followed existing intrinsic assumptions.  Rand index is a measure of the similarity between two different data clusters, it considers all pairs that are assigned to different or the same clusters in the predicted and actual clustering. There is also the adjusted Rand index which corrects for any bias within the clustering that could have been introduced due to chance.  When evaluating clustering methods, we often use multiple different approaches as no one approach is usually sufficient. Purity is a way of quality measurement for clustering methods, it measures the purity of all clusters in terms of the class labels of the data within the clusters. Essentially it compares how many data points within a cluster actually match the label of the cluster, if only half the data points match the label it only has 50% purity. Although, we have to make sure that we have a reasonable amount of clusters as having a cluster for every single data point would technically have 100% purity.  Mutual information is one of the most popular methods of clustering analysis, it is similar to the rand index as it measures the agreement between two clustering assignments. It measures this agreement based on how informative one assignment is of the other (ignoring permutations)  Silhouette Coefficient measures the similarity of a data point to its own cluster compared to other clusters, it contrasts the average distance instances of the same clusters with the average distance between instances in other clusters. Meaning that a high value means that a data object is well placed in its own cluster wouldn’t place well in other clusters.  Limitations of Kmeans  Random initialisation at the start of Kmeans means that different clusters may be chosen each time, although this can be addressed by using Kmeans++ for choosing centroids.  We have to supply the numbers of clusters ourselves, while there are methods, we can use to calculate the number of clusters we should they aren’t always reliable or applicable.  Kmeans is unable to find clusters of arbitrary shapes.  Kmeans treats outliers as normal data points which can mess up the accuracy of the clustering, even Kmedian has this problem.  The elbow method interprets and validates the consistency within the cluster analysis to determine the appropriate number of clusters within the data. It runs Kmeans clustering for the range of data values and for each value compute the sum of squared error. Once plotted in a graph the data points should resemble an arm with the elbow being the data points that indicates how many centroids to choose.  Clustering with Kmeans++   Kmeans++ is an algorithm that chooses the centroids for the Kmeans algorithm. While Kmeans++ also starts with random initialisation it is different as it chooses one centroid randomly and then chooses the next centroid based on the distance from the first centroid, which is repeated until the set amount of centroids is reached (k).  