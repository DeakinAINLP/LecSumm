In topic 3 we learnt  about DISTANCE METRICS Distance  metrics  are  used  widely  in  machine  learning  algorithms.  Distance  measures  are functions  that  define  a  distance  d(xi,xj) between  any  two  data  instances  and Xi  and  Xj  for measuring how similar the instances are. The most related examples in machine learning are: ▪  clustering algorithms (we looked at examples of clustering last topic) ▪  K-Nearest-Neighbor ▪  Support Vector Machines (SVM) ▪  data visualization ▪  information retrieval ▪  ranking Types of distance measurements Distance  measurements  are  very  important  in  machine  learning  problems.  Let’s  look  at different types of distance measurements. ▪  Euclidean distance The Euclidean distance formula is a measure of the straight-line distance between two points  in  Euclidean  space.  In  two-dimensional  space  (x,  y),  the  formula  can  be expressed as:   d = sqrt((x2 - x1)^2 + (y2 - y1)^2) ▪  Cosine distance The cosine distance formula is a measure of dissimilarity between two vectors based on their cosine similarity. It is often used in machine learning and information retrieval to compare the similarity between documents or vectors in high-dimensional spaces. The cosine similarity between two vectors A and B is calculated as:  cosine_similarity = dot_product(A, B) / (||A|| * ||B||) ▪  Mahalanobis distance The  Mahalanobis  distance  is  a  measure  of  the  distance  between  a  point  and  a distribution.  It  takes  into  account  the  correlations  and  variances  of  the  variables involved. The formula for calculating the Mahalanobis distance between a point x and a distribution with mean μ and covariance matrix Σ is as follows:   Mahalanobis distance = sqrt((x - μ)ᵀ Σ⁻¹ (x - μ)) ▪  Cityblock/Manhattan distance The  Cityblock  distance,  also  known  as  Manhattan  distance  or  taxicab  distance,  is  a measure of distance between two points in a grid-like space. It calculates the sum of the absolute differences between the coordinates of the two points along each dimension. In a two-dimensional space (x, y), the Cityblock distance formula can be expressed as:   Cityblock distance = |x2 - x1| + |y2 - y1| ▪  Jaccard distance The  Jaccard  distance  is  a  measure  of  dissimilarity  between  two  sets.  It  is  calculated based on the Jaccard similarity coefficient, which measures the overlap between two sets. The Jaccard distance is defined as the complement of the Jaccard similarity. Given  two  sets  A  and  B,  the  Jaccard  distance  can  be  calculated  using  the  following formula: Jaccard distance = 1 - Jaccard similarity The Jaccard similarity between sets A and B is calculated as:   Jaccard similarity = |A ∩ B| / |A ∪ B| Clustering and its applications Clustering Algorithms Clustering  puts  data  points  into  groups.  It  uses  similarity  and  difference  of  features  (or dimensions)  to  create  groups  in  material  that  is  unclassified  and  has  no  known  targets.  It’s particularly used in unsupervised learning as it can deal with vast amounts of uncategorised data however it creates groups so it’s useful in supervised learning as well. We will look at this idea in Course 3, 4 and 5. As we discussed in topics 1 and 2, the goal of clustering algorithms are to: ▪  Group unlabelled data objects with similar properties together ▪  Discover interesting perhaps unexpected clusters in the data ▪  Find a valid or useful organisation of the data In other words, we can define two algorithmic goals. We need to find objective functions to: ▪  Minimise intra-distance (distance between points in the same cluster) ▪  Maximise inter-distance (distance between points from different clusters) How Kmeans Works K-means  is  an  iterative  clustering  algorithm  that  aims  to  partition  a  dataset  into  k distinct clusters. The algorithm works as follows: 1.  Initialization:  Randomly  select  k  points  from  the  dataset  as  the  initial  cluster centroids. 2.  Assignment: Assign each data point to the nearest centroid based on a distance metric (usually Euclidean distance). This step forms the initial clustering. 3.  Update: Recalculate the centroids of the clusters by taking the mean of all the data points assigned to each cluster. 4.  Repeat steps 2 and 3 until convergence: Repeat the assignment and update steps iteratively until the centroids no longer change significantly or a predetermined number of iterations is reached. 5.  Output: The algorithm terminates, and the final clusters are obtained. generally there are two main categories of evaluation methods for clustering: ▪  Externalassessment: ▪ compare  clustering  performance  against  a  known  clustering  (often  called Ground truth or Gold standard). Internalassessment: determine  if  clustering  follows  certain  intrinsic  assumptions  (e.g.  cluster-to-cluster distance or cluster size etc.). o  Examples: Silhouette coefficient, Dunn index etc.   Rand Index The  Rand  index,  is  a  measure  of  the  similarity  between  two  data  clusters.  We  have  the assignments of data instances to different clusters suggested by a clustering algorithm (say C' ). In external assessment, we have knowledge of the ground truth cluster assignments. (say C ) The Rand index is a function that measures the similarity of the two assignments  C and C’, ignoring their permutations.The Rand index is computed as: RI = (a + b) / (a + b + c + d)  Purity Mutual information is one of the most popular approaches in analysis of clustering. It measure the agreement between two clustering assignments such as C and C’ The Mutual Information (MI) between two clustering assignments can be calculated using the following formula: MI = ∑∑ p(i, j) * log(p(i, j) / (p(i) * p(j))) Silhouette Coefficient The  Silhouette  Coefficient  is  a  measure  of  how  well  an  individual  data  point  fits  into  its assigned cluster compared to other clusters. It takes into account both the cohesion (how close the data point is to other points in its own cluster) and the separation (how far the data point is from points in other clusters). The Silhouette Coefficient ranges from -1 to 1, where a higher value indicates a better fit. The formula to calculate the Silhouette Coefficient for an individual data point is as follows: s(i) = (b(i) - a(i)) / max(a(i), b(i)) Finding a useful number of clusters As we have mentioned before, one of the challenges of Kmeans is the assumption we have to make about the number of clusters to start with. The Elbow Method is a method for  finding  the  appropriate  number  of  clusters.  The  Elbow  method  interprets  and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset. Other clustering algorithms Kmeans is one of the most popular clustering methods in machine learning but it is not the only clustering algorithm. In the category of Flat Clustering where the goal of the algorithm is to create clusters that are coherent  internally  but  clearly  different  from  each  other,  there  are  two  more  clustering methods: ▪  Kmeans (as we know) ▪  Hierarchical clustering ▪  DBSCAN (density based) ▪  Shape-based  Clustering 