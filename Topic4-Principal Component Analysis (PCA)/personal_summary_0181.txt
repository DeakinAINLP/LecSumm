In topic 3 we largely examined clustering: the art of finding patterns in large data sets. In addition we also learnt how to use Python packages to implement some of the concepts we covered.  Measuring Distance  In order to break our dataset up into clusters, we need to have either some way of measuring the distances between data points, or some way of measuring the similarity between the the data points that belong to the dataset.  The distance between two data points can be viewed as a function: d(xi, xj), with xi and xj representing the two points. That distance function must satisfy the following conditions:   1. For any data point xi, the distance to itself is zero. That is: d(xi, xi) = 0.  2. For any pairs of data points, xi and xj the distance between them must be 0 or greater. That  is: d(xi, xj) ≥ 0.  3. For any pairs of data points, xi and xj the distance between them must be symmetric. That is:  d(xi, xj) = d(xj, xi)  4. The distance measure must obey the triangular inequality. That is:  d(xi, xk) ≤ d(xi, xj) +d(xj, xk)  Any distance function that obeys the above conditions is termed to be a distance metric.  Some distance metrics  Euclidean distance: the ordinary straight line distance between two points in Euclidean space. The formula is: dEuclidean(xi,xj) = ((xi1 -  xj1)2 + (xi2 -  xj2)2+ … + (xin -  xjn)2)1/2  In Python, using the numpy library this can written as:  def euclidean_distance(x: list[int], y: list[int]) -> float: import numpy as np x = np.array(x) y = np.array(y) return np.sqrt(np.sum((x - y) ** 2))  if __name__ == '__main__': row1 = [1, 1, 2, 1, 0] row2 = [0, 2, 2, 0, 2] print(euclidean_distance(row1, row2)) # 2.6457513110645907  Cosine distance: the angular cosine distance between two vectors xi and xj. The formula is:  In Python, using the scipy library, this can be implemented as:  def calculate_cosine_distance(a: list[int], b: list[int]) -> float: import scipy return float(scipy.spatial.distance.cosine(a, b))  if __name__ == '__main__': row1 = [1, 1, 2, 1, 0] row2 = [0, 2, 2, 0, 2] print(cosine_distance(row1, row2))  # 0.3453463292920229  Mahalanobis distance: a distance metric that finds the distance between a given point and the distribution created by other points in the data (Cansiz 2023). The formula is:   dMahalanobis(xi, xj) = (xi – xj )M-1(xi  – xj )T  with M being the covariance matrix of the data. Use when you have continuous numeric variables.  Cityblock/Manhattan distance: In this measure, the distance between points is sum of the absolute distance between their Cartesian coordinates. The formula is:  dCityblock(xi,xj) = |xi1 -  xj1| + |xi2 -  xj2|+ … + |xin -  xjn|  In Python, this can be written as:  def calculate_manhattan_distance(a, b): return sum(abs(e1 - e2) for e1, e2 in zip(a, b))  if __name__ == '__main__': row1 = [1, 1, 2, 1, 0] row2 = [0, 2, 2, 0, 2] print(calculate_manhattan_distance(row1, row2))  # 5  Minkowski distance: The distance between two vectors. A generalisation of both the Euclidean and the Manhattan distances, adding a parameter p that allows the different distance measurements to be calculated.  If p = 1, the formula yields the Manhattan distance.  If p = 2, the formula yields the Euclidean distance.  In Python this can be written as:  def calculate_minkowski_distance(x: list[int], y: list[int], p: int) -> float: return sum(abs(e1-e2)**p for e1, e2 in zip(x,y))**(1/p)  if __name__ == '__main__': row1 = [1, 1, 2, 1, 0] row2 = [0, 2, 2, 0, 2] print(calculate_minkowski_distance(row1, row2, 2)) # 2.6457513110645907  Jaccard distance: a distance that is used to measure the diversity of any two sets. If the two sets are identical, their Jaccard distance will be 0. If the two sets have no common elements, their distance will be 1. If there are some common elements between the two sets, the value will be somewhere n that interval: 0 ≤ Jaccard distance ≤ 1  dJaccard(xi, xj) = 1 - |xi ∩ xj|1/||xi  x∪ jr|1  Clustering  Humans see patterns everywhere. It’s hardwired into our brains, probably as a survival mechanism.   However, how do we get computers to find patterns in data?  One way to answer this question is to arrange our data into groups, using similarities and differences between the data points to form the groups. Clustering algorithms group data points with similar properties together. In doing this we may discover interesting or unexpected clusters in the data. A side affect is that the data becomes organised in ways that will be useful.  The algorithms attempt to minimise the distance between the points that make up a cluster (intra- distance), and maximise the distance between the points that belong to different clusters (inter- distance).  The steps are simple. Define:  1.  2.  3.  the distance metric to use  the function to use  the algorithm to optimise the function in use  Clustering algorithms are heavily used in unsupervised learning.  K-means clustering  The most popular clustering algorithm in use is named K-means. In this algorithm we define a target number, k. We then allocate k randomly selected centroids (a real or imaginary location representing the centre of a cluster). We then allocate all the data points to their nearest centroid, thus forming the clusters within the data (this is termed the ‘Expectation’ step). We then iterate, optimising the positions of the centroids based on the clusters they are in (this is termed the ‘Maximisation’ step). Then we re-allocating the data points to the new centroids. This cycle is repeated until either the centroid locations have stabilised or a defined number of iterations have been performed.  Limitations of K-means clustering   Random initialisation means that every run could yield different clusters. K-means++ offers  better a initialisation technique.   We have to supply the number of clusters. How do we know what value to chose? Cluster  evaluation may help us, but implies a lot of iteration.   Clusters of arbitrary shapes won’t be found.    It can’t detect cluster outliers.  Cluster Evaluation  Once the dataset has been divided into clusters, the clusters found need to be evaluated. We can either compare the found clusters to against already known clusters (termed Ground truth) or examine the found clusters using only the information contained within the clusters themselves.  Rand index: a measure of the similarity between two sets of clusters. We partition the data into tow sets of clusters using different clustering techniques. The formula is:  R = (a + b)/((n(n – 1))/2)  where:  a: The number of pairs of data instances that are in the same clusters in both partitions.  b: The number of pairs of data instances that in different clusters in both partitions.  n: The total number of pairs of data instances.  Essentially the Rand index is the number of agreements between the two clustering techniques divided by the number of disagreements.  The output value will be a number between 0 and 1.  0: the two sets of clusters do not agree on any pair of points  1: the two sets of clusters are exactly the same.  The Rand index can be adjusted (corrected) to take into account any chance grouping of the elements.  Purity (homogeneity): a way of measuring the quality of the clusters that have been found. We assign each class a label that is given by the most popular label of the data points that form the cluster. Then we sum the number of correctly assigned labels in each cluster, and divide by the number of total data points.  Mutual information: A very popular way of doing cluster analysis. Similar to the Rand Index in that it’s a measure of the similarity between two sets of clusters. It quantifies the “amount of information” about one random variable through observation of the other random variables.  The formula used is  , with:  C and C’: being the two partitions  K: the number of clusters in C  K’: the number of clusters in C’  P(i): the probability of the randomly selected instance belonging to the ith cluster  Silhouette coefficient: a measure of how similar a data point is to the others in its cluster. This measure is useful in that there is no other cluster assignments required for comparisons. The formula used is:  s(i) = (b(i) – a(i))/max[a(i), b(i)], where  a(i): the average distance of the ith instance with all other other instances in the same cluster  b(i): the lowest average dissimilarity of the ith instance with all the other clusters.  -1 ≤ s(i) ≤ 1:  1: Means clusters are well apart from each other and clearly distinguished  0: Means clusters are overlapping  -1: Means clusters are assigned poorly. Possibly to few clusters.  Elbow Method  Used as a technique to evaluate a K-means clustering run for a range of values for K. We compute the sum of the squared error for each K selected, and plot it on a graph. If we get an elbow shape in the plot, the optimal value for K is the one at the elbow inflexion point.  If we don’t get an elbow in the plot then this method was of no use to us.  K-means++  K-means++ chooses its initial centroid by allocating one randomly, and then searching for k others based on this first randomly chosen one. The advantages over K-means are:   Every iteration will get closer to a better solution   There is a theoretical guarantee on when the convergence will occur.  Other clustering algorithms  Hierarchical  In this clustering one builds a tree diagram finding clusters that have a predetermined order. A dendrogram plot consists of many U-shaped lines that connect data points in a hierarchical tree.  There are two types:   Bottom-up (Agglomerative) clustering: We start at the bottom, and assign each data point to its own cluster. Then we compute the distance between each of the clusters, and join the two most similar clusters. We repeatedly do this until we have built the complete tree. The higher up that you choose to ‘cut’ the tree, the fewer clusters you will have. But: how do you get the cluster pairs? There are four possible ways mentioned in the lecture notes:  1. The distance between the closest points (single-link)  2. The distance between the furthest points (complete-link)  3. The distance between the centroids (centroid)  4. The distance between the pairs of elements across cluster pairs (average-link)    Top-down (Divisive) clustering: The reverse of Agglomerative. All the data points are put into the same cluster. Then we split the cluster using any algorithm that produces at least two clusters. The process is repeated until each data point is in its own cluster.  Density-Based Spatial Clustering of Applications with Noise (DBSCAN)  K-means and hierarchical clustering both fail to when creating clusters of arbitrary shapes (Sharma 2020). DBSCAN can cluster complex shapes and detect noise in the data. DBSCAN is a density- based clustering algorithm that assumes that clusters are in fact dense regions separated by low density regions. Hence the name…  DBSCAN takes the parameters:  epsilon: The radius of the circle to create around each data point to check density. If too large a value is selected the clusters will merge. The value should be as small as is possible, but beware: if too small, a large part of the dataset will not be clustered.  minPoints: The minimum number of points required to be inside the circle defined by epsilon for the data point to be considered to be a core point. If there are fewer than minPoints inside that circle the point is classified to be a border point. If there are no other points inside that circle then the point is classified to be noise. If there are more than 2 dimensions in the data, choose minPts = 2*dim, where dim= the dimensions of your data set (Sander et al. 1998). For 2-dimensional data, use a default value of MinPts = 4 (Ester et al. 1996).  DBSCAN ordinarily uses Euclidean distance as its measurement, but other distance methods can be used. DBSCAN also will scan the entire dataset once. DBSCAN is also very sensitive to the selection of epsilon and minPoints.  Shape based, VAT, iVAT  Visual Assessment for Tendency (VAT) is a visual method for assessing if clusters are present in a dataset. Using this technique, the distance matrix of the dataset is transformed into a visual representation, in the form of a re-ordered matrix. If the data has clear clusters, then the re-ordered matrix should have a block-like structures appear along the diagonal.  iVAT (improved VAT) is more computationally intensive than VAT, and works by repeatedly applying the VAT algorithm to the re-ordered matrix in order to get a clearer picture of the clustering structure.  VAT and iVAT are not intended to find clusters: merely to indicate that clusters are present. So in using them you can avoid the expensive cost of conduction data analysis on datasets that don’t have clusters.  K-means clustering with Python  The scikit learn cluster package contains a K-means implementation.  