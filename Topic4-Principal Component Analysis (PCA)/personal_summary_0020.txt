Distance metrics A function used to measure the distance between two data instances (𝑥𝑖, 𝑥𝑗) to determine similarity. The distance is used to assign a class label to group instances in clusters. A few ways to measure distance are clustering algorithms, K-Nearest-Neighbour, and ranking. To be considered a distance metric, distance measurement must satisfy the following three properties:  1.  The distance between a data instance and itself (𝑥𝑖) is always 0. 𝑑(𝑥𝑖, 𝑥𝑖) = 0 2.  The distance between two pair instances is always positive and symmetrical/not dependent on order of instances. 𝑑(𝑥𝑎, 𝑥𝑏) ≥ 1 and 𝑑(𝑥𝑎, 𝑥𝑏) = 𝑑(𝑥𝑏, 𝑥𝑎)  3.  Follows triangular inequality, 𝑑(𝑥𝑎, 𝑥𝑐) ≤  𝑑(𝑥𝑎, 𝑥𝑏) +  𝑑(𝑥𝑏, 𝑥𝑐)  There are various types of distance measurements with each being calculated a different way: 1.  Euclidean distance: used to measure distance between two points in a Euclidean  space.  -  Hierarchical clustering: find clusters that have a predetermined order. The approach can either be agglomerative or divisive. Agglomerative is a ‘bottom down’ approach where pairs of clusters are merged when moving up the hierarchy. Divisive clustering is a ‘top down’ approach where clusters are split when moving down the hierarchy. This can be visualized as a tree diagram. How the clusters are split/merged is dependent on the distance. There are four possible distance types: 1.  Single-link: the distance between closest points 2.  Complete-link: the distance between the furthest points 3.  Centroid: the distance between centroids 4.  Average link: the average distance between pairs of elements from across  cluster pairs.  -  DBSCAN: clusters certain points items in a group based on a given data point. The minimum number of data points and the distance is user defined which will influence the cluster results.  -  Shape-based clustering: cluster datapoints based on the shape characteristics. The shape emphasizes the similarities and dissimilarities between data points.  Kmeans The most popular clustering algorithm is kmeans. ‘k’ represents the center point of each cluster (centroid). Centroids are first mapped, where the number of centroids must be defined beforehand. The data points are then mapped and kmeans searches for the best centroid for each data point. The cluster the data point will be a part of is determined by alternating between two methods:  1.  The distance of the unclustered data point to the centroid. 2.  The similarity between the data points already assigned to a centroid.  Limitations & solutions  -  Knowing how many clusters should be use can be difficult to determine. This can be resolved by the elbow method.  -  Random initialization means that the result of clusters can be different each implementation. To mitigate this, Kmeans++ initialization algorithm can be used.  Clustering evaluation Clustering evaluation methods are used to determine the efficiency of a clustering algorithm by evaluating the cluster results. There are two main categories of cluster evaluation methods:  1.  External assessment: comparing the cluster performance against a grounded truth (a known clustering).  2.  Internal assessment: checks if certain known criteria is met, e.g., cluster-to-cluster distance or cluster size.  Rand Index: measures the similarity between two data clusters Adjusted rand index is when any bias introduced by change is accounted for. Purity: used to measure the accuracy of the clusters by assigning a class label to the cluster and counting the accurately assigned instances and dividing this by the total number of instances. This determines the quality of the clustering method. Mutual information: measures the agreement between two clustering assignments. Silhouette coefficient: measures similarity of an object by measuring the distance to its own cluster and the distance between other clusters. Does not require grounded truth to be calculated.  