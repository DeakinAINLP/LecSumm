 - This topic we focused on clustering in machine learning. We looked at measuring distances. In this topic, we were introduced to various measurements and their formulae. Distance metric, it is the measure of the distance between two data points xi, xj. There are a few distance metrics such as: -    Euclidean Distance   Cosine Distance   Mahala Nobis Distance   Manhattan Distance   Minkowski Distance   Jaccard Distance  We learned how to calculate these distances using the formulae provided.  Then we learned about clustering and its applications. Clustering is used with unsupervised learning but is also used with supervised learning. It creates groups on unclassified data which have no targets. An important and widely used clustering algorithm is kmeans. Kmeans creates random points on unsupervised data and then groups the closest data points together and this creates clusters. Then we keep repeating this process till we get two good clusters which are usable.  Then we learned how to evaluate clustering. There are two ways of evaluating clustering:    Internal assessment: comparing cluster to cluster distance and size   External assessment: comparing against ground truth  Following are good evaluation methods for clustering: -    Rand Index       Purity   Mutual information   Silhouette coefficient  We learned about these in depth and how to calculate the evaluation scores for each one.  Then, we learned about the limitations of Kmeans which are: -    We must provide the number of clusters beforehand.   Random initialization means we might get different results each time.   It cannot detect noisy data points.   It cannot make clusters of arbitrary shapes.  A good method to find how many clusters we should make for a dataset is the elbow method, where we find the kmeans using a range of k values and then we plot them. Wherever we see an arm forming. We take the point which looks like an elbow as number of clusters value.  Then, we looked at kmeans ++ and how it is better. Unlike kmeans which chooses centroids at random, kmeans++ chooses the first centroid at random and then carefuly chooses the other centroids using probability. This results in better clustering.  Then, we looked at other clustering algorithms briefly such as: -    Hierarchical clustering   DBSCAN (Density-Based Spatial Clustering of Applications with Noise)   Shape-based clustering, VAT, iVAT  And lastly, we looked at how to implement kmeans using python on a dataset.   