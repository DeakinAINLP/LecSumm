   The focus of topic 3 is on unsupervised machine learning  mainly clustering and  dimensionality reduction    We define distance between different data points as how similar they are and the  smaller distance, the more we can claim that the two data points are very similar    There are various distance metrics which have been used in the content including  Euclidean  distance,  Cosine,  Mahalanobis,  Cityblock/Manhattan  and  Minkowski,  Jaccard distance.    Clustering is all about finding patterns and grouping those data points with similar  patterns together unlike supervised data where labels are already present.    For clustering, we mainly have 2 primary goals ; to maximise inter-distance and  minimise intra-distance as shown below:          To put things in simpler terms, Kmeans clustering is about randomly assigning k  centroids among a number of data points and then keep on updating the centroids  based  on  the  distances  of  the  clusters  being  formed  and  this  continues  until  the  centroids do not move as much.    To evaluate the effectiveness of clustering, there are two main methods ; external  assessment (ground truth) and internal assessment (based on intrinsic assumptions).    Here is something which summarises the above:      We can use Rand Index or purity for quality measurement in clustering methods.    Mutual information uses a very similar analogy to Rand Index while the Silhouette  Coefficient is used to measure how similar an object is to its own cluster.    We  have  equally  identified  various  flaws  with  using  Kmeans  algorithm  which  include we need to provide the number of clusters to the algorithm or we randomly  assign clusters which may lead to different clusters every time    The elbow method is used to calculate the sum of squared error (SSE) as :       Both Kmeans and Kmeans++ use the same principle of random initialisation but  what  is  different  with  Kmeans++  is  that  the  latter  starts  with  setting  one cluster  centre randomly and consequently looks for other centres given the first one    Other  clustering  algorithms  include  Hierarchical  clustering,  DBSCAN  or  shape-  based clustering.  External reading list  1.  https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html  2.  https://towardsdatascience.com/understanding-k-means-k-means-and-k-medoids-  clustering-algorithms-ad9c9fbf47ca  3.  How  to  Evaluate  the  Performance  of  Clustering  Algorithms  in  Python?  (Evaluation  of  Clustering) - YouTube  Reflection  This topicâ€™s content is more centralised towards clustering and the various methods of clustering  are explained in depth along with the python codes. We have been taught the basic concepts, how  to implement and also how to evaluate their effectiveness.       Quiz Screenshot     