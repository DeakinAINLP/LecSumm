 Clustering is part of unsupervised learning which is dealing with data that is unlabelled and the  overall goal is to find similarity between data points to find patterns using a variety of distance metrics.  The seminar went through numerous distance metrics which I will provide a brief dot-point based  summary below.  Distance Measurement type    Manhattan distance  o  Similar to Euclidean distances with differences in the formula used in calculation. o  Here, distance isn’t a direct diagonal distance like Euclidean but uses a grid like distance.    Euclidean distance  o  Is a straight-line distance (diagonal) as opposed to the grid like distance in Manhattan.    Chebyshev distance  o  Takes the max distance of the result of subtraction of coordinate points.    Minkowski distance  o  Generalizes distance metric using p-norm.    Manhalanobis distance  o  Takes into account distance and correlation of features o  As per the example from the seminar, we can look at distribution of the data points  ▪  Covariance matrix is used to do this    Jaccard distance  o  We use this for sets o  We are essentially looking at binary representation of features here where it is either  present or not present.  o  We look at union and intersection of the sets as part of the formulation of this distance  metric.         In terms of clustering algorithms, we have two main goals to consider which in minimising the  intra distance which is the data points within a cluster and maximizing the inter-distance which is data  points between different clusters. There are three steps which involve defining the distance metric,  defining objective function for the clustering goal, and optimizing the objective function.  One example of a clustering algorithm is KMeans which aims to partition data into a given number of  k clusters. The way it works is it will assign centroids to random data points. Assignment of data points to  the closest cluster will be done next, followed by calculating the mean of these points to find a new  centroid point. This process will repeat until the clustering becomes stable which in other words, means  that the centroids are not changing much anymore. This approach, however, creates a disadvantage  where clustering quality can depend on initialization due to the random assignment of centroids during  initialization which can result in different clusters being produced each time the algorithm is run.  Additionally, we also have to provide the cluster number. However, this can be overcome by using the  elbow method to find out the ideal cluster number where essentially, we test a range of k values and see  which k number marks the start of the quality score decreasing less.  To help overcome the initialization issue, we have another algorithm called Kmeans++. Here, we start  off with a single centroid being randomly assigned to a data point and the remaining centroids will be  assigned in a distributed manner with points that are ideally further away. Once all centroids have been  initialized, the process beyond that will be the same as Kmeans.  We have a few techniques to evaluate clustering quality. We have external assessment where we  look at the performance to known clustering (ground truth). We also have internal assessment which  looks at cluster to cluster distance and size. Some external assessment techniques include Rand Index  which looks at similarity between two clusters (the ground truth cluster and testing cluster). We also  have Purity which looks at the number of correctly assigned instances to the clusters. There is also  Mutual Information where we look at the agreement of two clustering assignments which to my  understanding means we are looking at how much information we can derive about one cluster using the  other cluster. In terms of internal evaluation, we have the Silhouette Coefficient which uses average  distances to its own cluster and other clusters to see how well it fits in its own cluster.  