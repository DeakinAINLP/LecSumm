Main points of this module:    Distance metrics are functions that define a distance between two data instances, common  examples are clustering algorithms, K-nearest-neighbour, support vector machines (SVM), data visualisations, information retrieval and ranking.    Distance measurements include Euclidean (linear), cosine, mahalanobis (multivariate space), city block/Manhattan (dimensional feature vectors), Minkowski (generalisation of Euclidean and Manhattan), Jaccard (diversity). Intra-distance is distance between points in the same cluster, inter-distance is between points in different clusters.      K-means involves assigning data points to clusters based on current defined centroids, then  choosing new centroids, and repeating.    An external assessment of clustering performance (against a known clustering) is considered to be the ‘ground truth’ or ‘gold standard’, an internal assessment follows certain intrinsic assumptions such as cluster-to-cluster distance or cluster size (silhouette coefficient, Dunn index).    The Rank index measures similarity of two assignments, ignoring their permutations.   Purity is a way of quality measurement in cluster methods.   Mutual information considers similarities between one cluster, if you look at another.   The Silhouette coefficient is a value of how similar an object is to its own cluster (cohesion)  compares to other clusters (separation).    Some limitations of K-means are random initialisation, number of clusters must be supplied beforehand, it cannot find clusters in arbitrary shapes, and cannot detect noisy data points.   The Elbow method involves plotting k-values to determine which number of clusters is the  most optimal, by analysing the ‘elbow’ of the line graph.    K-means++ chooses one centroid uniformly at random, then finding the shortest distance between this data point and the nearest centroid, and using probability to choose a new centroid, and repeating.    The guarantee of K-means++ is that it will monotonically decrease with each iteration.  