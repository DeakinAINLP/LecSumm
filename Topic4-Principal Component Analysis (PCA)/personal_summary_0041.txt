The third module focuses on unsupervised machine learning and some techniques to implement  it  in  python.  It  starts  with  distance  metrics,  clustering,  its  applications, performace  measure  techniques  and  usage  of  coding  in  Python  programming language.  Distance Metrics  Measuring  distance  is  an  important  part  of  clustering  that  further  helps  in  Machine Learning. It is because it tells us the level of similarity or dissimilarity between any two feature vectors. Now, since feature vectors are the data storers for the ML processing, it  makes  it  a  crucial  part  in  execution  of  algorithms.  Different  types  of  distance measuring matrices by name are: Euclidean distance, Cosine distance, Mahalanobis distance,  cityblock/  Manhattan  distance,  Minkowski  distance  and  Jaccard  distance. Details of all these measures can be found in the module contents along with examples and explanations. Overall, the main use of distance metrics in clustering is to decide the  cluster  label  for  a  particular  element  iterated  over  every  element  repeatedly  to assign it to the most similar group.  Clustering and clustering algorithms  The clustering algorithms are used to form clusters for the data passed that can be described  as  ML  classifiaction.  The  algorithm  popularly  used  for  the  purpose  is kmeans and kmeans++.  Kmeans and its limitations  KMeans is an unsupervised machine learning algorithm that helps in clustering data. The number of clusters represent the variable k and the algorithm starts with choosing k number of centroids and then using distance metrics to assign labels to the data. After  that  based  on  those  clusters,  it  redefines  centroids,  and  reassigns  labels.  It repeats this process until there are no significant changes occuring. The performance is highly dependent on the initialization of the cluster centres. This is a huge limitation of KMeans.  Kmeans++  Kmeans++ is an upgraded version of KMeans algorithm which helps us in overcoming the shortcomings of KMeans. It is because it starts with choosing one centre and then starts  clustering  the  neaby  data  points.  While  doing  that,  it  will  declare  the  other centroids  for  other  clusters  until  the  number  of  assigned  centres  reach  K.  Once,  it reaches  k,  the  data  is  then  clustered.  This  algorithm  when  applied  repeatedly  over numerous  iterations  on  the  resulted  dataset,  we  will  see  how  it  can  outperform  the normal kmeans, giving us better clustering results.  Performance evaluations  Now,  in  order  to  decide  value  of  k,  and  also  which  technique  should  be  used  for lustering, we have some methods and techniques that can evaluate the performance.  These include elbow method for choosing k , where we plot the values of k against the sum of squared error, which is a criteria that can be calculated using the formula given in  the  contents.  It  basically  involves  plotting  the  graph  and  then  choosing  the  point where the graph forms an elbow or a dented curve. Although, it gives us good results, it is not applicable in all the cases. Therefore, choosing the value of K is still a limitation in  K-Means.  As  for  an  algorithm  performance  evaluation,  there  are  two  types  of methods used including internal and external assessments. External assessments are based  on  comparison  against  a  known  clustering  called  Ground  truth  or  a  gold standard. Examples include rand index, purity and mutual information. On the other hand, internal assessment is based on following some certain intrinsic assumptions such  as  in  Silhoutteâ€™s  coefficient.  At  the  end  we  saw  implementations  of  studied concepts in python using different libraries and modules.  