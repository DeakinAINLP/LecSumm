Unsupervised machine learning  There are two aspects of unsupervised machine learning on two forms of unsupervised learning    Clustering: Clustering is a type of unsupervised learning where the goal  is to group similar data points together in clusters, based on the inherent structure of the data.    Dimensionality reduction: Dimensionality reduction, on the other hand, is a technique that reduces the number of features in a dataset while retaining the most important information.  Measuring distances  Measuring similarity or distances between different data points is fundamental to many machine learning algorithms. These algorithms are used both in supervised learning methods and unsupervised learning problems.  Depending on the nature of the data point, various measurements can be utilised to measure distance.  Distance metrics  Distance metrics are used widely in machine learning algorithms. Distance measures are functions that define a distance d (x1, x2) between any two data instances x1 and x2.  Euclidean distance  Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday) space. For any two data instances, represented by d- dimensional feature vectors xi, xj their Euclidean distance is computed as:  d = sqrt ((xi1 – xj1) ^2+……...+ (xiz - xjz) ^2) )  cosine distance  Cosine distance is a distance metric used in machine learning to measure the similarity between two vectors. It is commonly used in text classification and recommendation systems. Cosine distance is based on the cosine of the angle between two vectors. It is calculated as the dot product of two vectors divided   by the product of their magnitudes. Mathematically, cosine distance between two vectors A and B can be defined as:  cosine distance = 1 - (A. B) / (||A|| ||B||)  Mahalanobis distance  Mahalanobis distance is a measure of the distance between a point and a distribution. It considers the covariance of the variables, unlike the Euclidean distance. The formula for Mahalanobis distance between two points, x and y, in a dataset with mean, μ, and covariance, Σ, is:  d (x, y) = √[(x - y)'Σ^-1(x - y)]  where (x - y)' is the transpose of (x - y).  City block/Manhattan distance  Cityblock distance, also known as Manhattan distance, is a distance metric that measures the absolute differences between the coordinates of two points in n- dimensional space. It is named after the way a taxicab travels through city blocks, where the distance travelled is the sum of the lengths of the blocks, which are at right angles. In other words, it is the sum of the absolute differences of the coordinates along each dimension.  For example, in two-dimensional space, the cityblock distance between two points (x1, y1) and (x2, y2) is:  City block distance = |x1 - x2| + |y1 - y2|  Minkowski distance  Minkowski distance is a generalization of other distance metrics such as Euclidean distance and Manhattan distance.  Jaccard distance  Jaccard distance is a measure of the similarity between two sets. It is calculated as the difference between the size of the intersection and the size of the union of the two sets, divided by the size of the union:  J (A, B) = 1 - |A ∩ B| / |A ∪ B|    Clustering  Clustering is a type of unsupervised learning technique that is used to group similar data points together based on their similarities. The primary goal of clustering is to identify structures or patterns in data without any prior knowledge of the data or the labels. Clustering is a powerful technique that can be used to analyse and identify complex patterns in data that might be difficult to identify using other techniques.  How Kmeans Works  The most popular clustering algorithm in machine learning is called K-means. It is simple and fast. KMeans is an iterative algorithm that partitions a dataset into k clusters based on the proximity of data points to the centre of those clusters. The steps involved in the KMeans algorithm are:    Initialization: The first step is to randomly initialize k centroids in the  feature space of the dataset.    Assign data points to centroids: Each data point is assigned to the  nearest centroid based on the distance metric used, such as Euclidean distance or cosine distance.    Update centroids: After assigning data points to centroids, the centroid of each cluster is updated by computing the mean of all the data points assigned to that centroid.    Repeat steps 2 and 3: Steps 2 and 3 are repeated until the algorithm converges, which is when the centroids stop moving or the change in centroid positions falls below a certain threshold.    Output the clusters: Once the algorithm has converged, the final  centroids represent the centres of the clusters and the data points are assigned to the nearest centroid, forming the clusters.  Evaluation of Clustering  all machine learning algorithms are required to be evaluated. Are the clusters useful? Evaluation of clustering methods is not easy. But generally, there are two main categories of evaluation methods for clustering:     External assessment:   compare clustering performance against a known clustering (often called Ground truth or gold standard).  Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g., cluster-to-cluster distance or cluster size etc.).  Examples:  Silhouette coefficient, Dunn index etc.    Silhouette Coefficient: measures how well each data point fits into its assigned cluster based on the average distance between the point and other points in the same cluster compared to the average distance between the point and points in the nearest cluster.    Davies-Bouldin Index: measures the average similarity between each cluster and its most similar cluster, taking into account the size of the cluster and the distance between their centroids.    Calinski-Harabasz Index: measures the ratio of the sum of between- cluster dispersion and the sum of within-cluster dispersion for all clusters, indicating how well-separated the clusters are.    Rand Index: measures the similarity between two clustering results, by comparing the number of pairs of points that are classified in the same group in both the predicted and actual clustering results.    Adjusted Rand Index: a variation of the Rand Index that adjusts for  chance agreement between the predicted and actual clustering results.    Homogeneity, completeness, and V-measure scores: measures the  purity and completeness of each cluster, based on how much the points in each cluster belong to the same true class and how much of the true class is assigned to each cluster.  Here are some limitations of K-means clustering algorithm:    The most important limitations of simple Kmeans are:   Random initialisation means that you may get different clusters each time. As a solution, we can use a Kmeans++ initialisation algorithm to initialise better.    We must supply the number of clusters beforehand. We can use the  Elbow method to choose K, but it may not be straightforward.      It cannot find clusters of arbitrary shapes.   It cannot detect noisy data points, i.e., data points that should not be considered for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)  Finding a useful number of clusters  one of the challenges of Kmeans is the assumption we have to make about the number of clusters to start with. The Elbow Method is a method for finding the appropriate number of clusters. The Elbow method interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset.  Kmeans with Kmeans++  Kmeans++ is an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm. As we have said before, K- means starts with allocating cluster centres randomly and then looks for better solutions. K-means++ starts with allocating one cluster centre randomly and then searches for other centres given the first one. So, both algorithms use random initialisation as a starting point but in different ways.  Other clustering methods:  hierarchical clustering  There are two types of hierarchical clustering: one of them is a bottom-up approach and the other is top-down:  1.Agglomerative clustering (bottom-up):  A “bottom up” approach in which each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.  2.Divisive clustering (top-down):  A “top down” approach in which all observations start in one cluster, and splits are performed as one moves down the hierarchy.      DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that clusters certain items in a group based on a given data point.  Shape-based clustering, VAT, iVAT  VAT is a visualization technique that transforms the distance matrix of a dataset into a visual representation in the form of a re-ordered matrix. The re-ordering is done in such a way that the dissimilarities between the data points are emphasized in a way that reveals the underlying clustering structure of the data.  iVAT is an extension of VAT that involves repeatedly applying the VAT algorithm to the re-ordered matrix to refine the clustering structure. The iVAT algorithm iteratively computes the VAT on the re-ordered matrix until a stable clustering structure is obtained. This can help to identify the optimal number of clusters in the data.     