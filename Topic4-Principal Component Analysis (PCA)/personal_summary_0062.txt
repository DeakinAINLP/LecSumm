 Measuring similarity or distances between different data points is essential to many machine learning algorithms.  Distance metrics are utilized broadly in machine learning algorithms. Distance measures are functions that define a distance d(xi, xj) between any two data instances xi and xj.  Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday) space.  There are many distance measures such as:    Euclidean distance   Cosine distance   Mahalanobis distance   Cityblock/Manhattan distance   Minkowski distance   Jaccard distance  The most well known clustering algorithm in machine learning is called K-means. It is simple and fast.  In this algorithm, k represents the centre points of clusters. You begin off with these centroids and then measure teach data point to discover its closest centroid.  In other words, K-means stores k centroids for defining clusters.  A point is considered to be in a specific cluster in case it is closer to that cluster’s centroid than any other centroid.  The Rand index, is a measure of the similarity between two data clusters.  In evaluation strategies of clustering, it is common practice to utilize more than one approach for evaluation since neither of the evaluation strategies are comprehensive enough.  Purity could be a way of quality estimation in clustering strategies.  Mutual information is one of the most well known approaches in analysis of clustering.  The silhouette value could be a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference).  This strategy has the advantage that it does not require the ground truth cluster assignments.  A high value of Silhouette Coefficient demonstrates that the object is well coordinated to its own cluster and ineffectively coordinated to neighbouring clusters.     The limitations of simple Kmeans are:    Random initialisation implies that you may get different clusters each time. As a solution, we can utilize a Kmeans++ initialisation algorithm to initialise better.    We need to supply the number of clusters in advance. We can utilize the Elbow  strategy to select K, but it may not be direct.    It cannot discover clusters of arbitrary shapes.    It cannot identify noisy data points,  data points that should not be taken under consideration for cluster analysis.  Kmeans++ is an algorithm for choosing the starting cluster’s centre values or centroids for the Kmeans clustering algorithm.  K-means clustering could be a strategy for finding clusters and cluster centres in a set of unlabelled data.  Then we learnt and practiced the programming part of this module and did a lot of practice and completed the quiz and pass tasks.  