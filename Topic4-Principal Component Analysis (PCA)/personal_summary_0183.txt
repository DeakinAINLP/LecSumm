Topic 3 Notes:    Two forms of unsupervised learning:  1.  Clustering, to find patterns in large data sets when the data is not labelled 2.  Dimensionality reduction, to helps manage large number of features describing the  difference between data,    Clustering is based on measuring similarity or distances between different data points. Distance measures are functions that define a distance d(xi, xj) between any two data instances xi and xj    Three Properties of Distance Metrics:  1.  For any instance xi, distance with itself is zero. That is, d(xi,xi) = 0 2.  For an instance pairs xi and xj, the distance is non-negative and symmetric.  That is, d(xi,xj) >= 0 and d(xi,xj) = d(xj,xi).  3.  Distance measure follows triangular inequality. That is, d(xi,xk) <= d(xi,xj) + d(xj,xk).    Examples of distance metrics 4.  clustering algorithms 5.  K-Nearest-Neighbor 6.  Support Vector Machines (SVM) 7.  data visualization 8. 9.  ranking  information retrieval    Types of distance measurements  1.  Euclidean distance – ordinary straight line distance between two points  2.  Cosine distance – represent two data instance by d- dimensional feature vectors xi, xj  3.  Mahalanobis distance - distance between two points in multivariate space  4.  Cityblock/Manhattan distance – similar to Euclidean distance but dampens the effect of large  difference in single dimension  5.  Minkowski distance - a distance between two points in a p-normed vector space. Euclidean  distance (2 norms), Cityblock distance as (1 norms)  6.  Jaccard distance - distance used to measure diversity of any two sets    The goal of clustering algorithms are to:  1.  Group unlabelled data objects with similar properties together 2.  Discover interesting perhaps unexpected clusters in the data 3.  Find a valid or useful organisation of the data    Clustering methods:  1.  Step 1: define a distance metric between objects 2.  Step 2: define an objective function that gets us to our clustering goal  o  Minimise intra-distance (distance between points in the same cluster) o  Maximise inter-distance (distance between points from different clusters)  3.  Step 3: devise an algorithm to optimise the objective function    Kmeans - k represents the centre points of clusters. Steps:  1.  Start with picking random centroids and then measure each data point to find its closest  centroid. In other words, centroids for define the clusters.  2.  A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than  any other centroid.  3.  K-means searches for the best centroids by alternating between two methods:  o  Assigning data points to clusters based on the current defined centroids (points which  are the centre of a cluster).  o  Choosing centroids based on the current assignment of data points to clusters.    Evaluation methods for clustering  (is this a good way to clustering the data?):  1.  External assessment:  2.  compare clustering performance against a known clustering (often called Ground truth or Gold standard). Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.).    Rand index measures the similarity between two data clusters: a) suggested clustering algorithm  (say C’) and b) ground truth cluster assignment (say C).    Purity measures all clusters in terms of class labels of data in each cluster.  (5 crosses + 4 circles + 3 plus) divided by total data types (17) ~ 71%    Mutual Information measures the agreement of the two clustering assignments C and C’ in terms  of how informative one is about the other, ignoring permutations. To put it simple, how informative is C about C’.    Silhouette Coefficient measures how similar an object is to its own cluster (cohesion/similarity)  compared to other clusters (separation/difference)    Limitation of Kmeans 1.  Random initialisation means that you may get different clusters each time. Use  a Kmeans++ initialisation algorithm to initialise better.  2.  We have to supply the number of clusters beforehand. We can use the Elbow method to choose, but it may not be straightforward. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)    The Elbow Method is a method for finding the appropriate number of clusters. The Elbow  method interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset. The idea of elbow method is to run the Kmeans clustering algorithm for a range of values of K, compute the sum of squared error (SSE) as    Kmeans++ is an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm. Kmeans++ repeats the following steps until K centroids:    Other clustering algorithms  1.  Hierarchical clustering (Dendrogram) – finds clusters that have a predetermined order.  o  Agglomerative clustering (bottom-up):  starts in its own cluster, and pairs of clusters are  merged as one moves up the hierarchy  o  Divisive clustering (top-down): start in one cluster, and splits are performed as one  moves down the hierarchy.  2.  Divisive clustering - all data instances are put in the same cluster and split using any  clustering algorithm that produce at least 2 clusters until each data instance is assigned to its own cluster    DBSCAN (Density-Based Spatial Clustering of Applications with Noise) – a clustering that needs to  have a minimum number of data points (minPts) and a distance. 1.  Calculate the distance from each point in the dataset to every other point. A point is  considered a "core point" if it has at least the same number of data points within the defined distance.  2.  Data points which cannot be considered core points but are under the defined distance of  the core point are called border points. All other points are regarded as "noise."  3.  The next step is to combine all core and border points within distance of each other into a  single cluster.  4.  Repeat until all data points are finished.    Shape-based clustering  o  VAT o iVAT       