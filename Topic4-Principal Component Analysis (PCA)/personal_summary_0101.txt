 Overview In topic 3, we focused on the following learning goals:    Use clustering for revealing paterns from unlabeled data   Learn technologies to reduce dimensionality   Apply suitable clustering/dimensionality reduction techniques  to perform unsupervised learning of data in a real-world scenario.  Distance Metrics Functions that deﬁne a distance between any two data instances to know how similar they are.  Examples of distance metrics:  -  Clustering algorithms -  K-nearest neighbor -  Support vector machines (SVM) -  Data Visualization -  Information Retrieval -  Ranking  Types of Distance Measurements  -  Euclidean distance -  Cosine distance -  Mahalanobis distance -  Manhaten distance -  Minkowski distance -  Jaccard distance  Clustering and its Applications Clustering puts data points into groups. It uses similarity and diﬀerences of features to create groups in material that is unclassiﬁed and has unknown targets. It’s used in unsupervised learning as it deals with vast amounts of uncategorized data.  Goals of clustering  -  Group unlabeled data objects with similar properties -  Discover interesting (perhaps unexpected) clusters in data -  Find a valid or useful organization of the data  In other words, we need to ﬁnd objective functions to:  -  Minimize intra-distance (distance btw point in the same cluster) -  Maximize inter-distance (distance btw point from diﬀerent  clusters)  Generic steps for clustering  1. Deﬁne a distance metric btw objects 2. Deﬁne an objective function that gets us to our clustering goal 3. Devise an algorithm to optimize the objective function  How K-means Works This is the most popular clustering algorithm in machine learning.  It searches for the best centroids by alternating btw two methods:  1. Assign data points to clusters based on the current deﬁned  centroids  2. Choose centroids based on current assignment of data points to  clusters  Repeat the two steps until you ﬁnd a useful grouping of data points  Evaluation of Clustering In evaluating methods of clustering, it is common to use more than one approach for evaluation because neither of the evaluation methods are comprehensive enough.  Purity A way of quality measurement in clustering methods.  Consider this:  In the ﬁrst cluster, we have a majority of 5 to 1. The second cluster has a majority of 4 to 2. The third cluster has a majority of 3 to 2.  Now to calculate the purity measurement of these clusters:  Purity = (1/17) x (5+4+3) = 0.71 = 71% purity  Drawbacks of Purity:  Imagine we have 17 cluster with one point in each. In this case the result will be 100%. This is why we have to make sure to select a fair number of clusters when performing clustering on a set of data points.  Mutual Information One of the most popular approaches in analysis of clustering. It measures the agreement between two clustering assignments such as C and C’, in terms of how informative one is about the other.  The higher the Mutual Information, the beter the clustering  Silhouete Coeﬃcient A measure of how similar an object is to its own cluster compared to other clusters. The ﬁnal value ranges between -1 and +1. A high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.  does not require the ground truth cluster assignments  Limitation of K-means The most important limitations of simple K-Means:  -  Random Initialization means you may get diﬀerent clusters each time. Hence we can use Kmeans++ initialization algorithm to initialize beter.  -  We have to supply the number of clusters beforehand. We can use Elbow method to choose but it may not be straightiorward.  -  It can’t ﬁnd clusters of arbitrary shapes. -  It can’t detect noisy data points.  K-means Clustering in Python   