This topic's module covered important measures of distance and similarity often used in machine learning. We looked at how the measures are used in clustering algorithms and how machines use a combination of the two to understand data and make predictions.  The practical task involved using some sklearn built-in functions to calculate the ideal number of clusters for a given dataset based on the elbow method using both the distortion and silhouette metrics and then measuring the quality of those clusters with a purity test.  I found that performing the task practically in python really helped to drive home the concepts covered in the theory. Especially in regard to the evaluation of the quality of clusters using the purity test.  Distance and Similarity Measures  Measuring the similarity or distance between two or more data points or groups of points is fundamental to machine learning as it allows us to quantify these differences. Machines use these measures within various algorithms to learn patterns and make predictions.  There are a number of different methods to calculate distance or similarity and the chosen tool will largely depend on the problem we’re trying to solve. Three of the more common measures covered in this topic’s module are as follows:  Euclidean Distance Possibly one of the more common measures the Euclidean distance measures the distance between two points in a straight line. Using the example of two feature vectors the distance is calculated by squaring the difference between each corresponding point, adding those values together then taking the square root of that value.  = √(𝑥1𝑖 − 𝑥2𝑖)2 + ⋯ + (𝑥1𝑛 − 𝑥2𝑛)2  where x1 and x2 are feature vectors.  Cosine Distance Where the Euclidean distance measures a straight line between two points, the Cosine distance as you would expect measures the cosine of the angle between the two vectors. Again, using the example of two feature vectors, we calculate the cosine distance by subtracting from 1 the following.  = 1 −  (𝑥1𝑖∗ 𝑥2𝑖+⋯+𝑥1𝑛∗𝑥2𝑛) (√𝑥1𝑖+⋯+𝑥1𝑛) (√𝑥2𝑖+⋯+𝑥2𝑛)  Manhattan Distance (City Block) The Manhattan Distance is another measure that is extensively used in machine learning. It measures the distance between two points as the sum of the absolute differences between each point.  = |𝑥1𝑖 − 𝑥2𝑖| + ⋯ + |𝑥1𝑛 − 𝑥2𝑛|  We also covered the Mahalanobis, Minkowski, and Jaccard distances.  SIT307 Machine Learning       Clustering and it’s Applications  Clustering is used in unsupervised learning to group similar data points together. It involves finding groups, or clusters, of data points that are similar to each other in some way, typically based on distance or similarity calculations we discussed above.  The goal of clustering is to identify natural patterns or structures in the data without prior knowledge of their labels or categories. Clustering has various applications such as customer segmentation, image segmentation, anomaly detection, recommender systems, and data compression. It is an important tool in exploratory data analysis and can help in identifying hidden structures or relationships.  Kmeans The clustering algorithm we covered and used in the practical task was Kmeans. Kmeans is simple but very popular algorithm that works by to partitioning a given dataset into k clusters. The algorithm iteratively assigns each data point to the closest cluster centroid and updates the centroid based on the new assignment.  The value of k is provided to the algorithm and is either selected at random or optimally determined by using the elbow method which doesn’t always result in determining an optimal value for k. However, it is better than randomly taking a guess.  Evaluating the Clustering  Once Kmeans has finished assigning each data point to a cluster, we need to know how well the algorithm worked and whether the dataset is amenable to clustering. We covered a number of tests to measure the performance of the clustering. The simpler and more commonly used seems to be the Purity test. This test tells us the average percentage a particular data point dominates a cluster, the higher the percentage the better the quality of clustering.  