Topic 3 Summary Clustering Concepts  for identifying similar patterns in unstructured data by grouping or segmentation.     a type of exploratory analysis   Similarity and dissimilar between objects (distance Metrics (k-means method: unsupervised,  k-nearest neighbour: Supervised)    3 properties (Distance Metrics)  o  Distance within itself is zero. o o  Follows triangular inequality [ d(xi, xk) < d(xi, xj) + d(xj, xk) ].  the distance between 2 points (an instance pair) is non-negative and symmetric.  Distance Measurement Types:    Cityblock/Manhatten Distance (L1 Norm)  o  Similar to Euclidean distance. o  Formula: dCityblock(xi, xj) = |xi,1 – xj,1| + … + |xi,D – xj,D| o  The effect of a large difference in a single dimension is dampened. o  P = 1-norm   Euclidean Distance (L2 Norm)  o  Ordinary Straight-line distance between 2 points. o  Formula: d(xi, xj) = sqrt((xi,1 - xj,1)2 + … + (xi,D – xj,D)2) o  P = 2-norm    Minkowski Distance (LP Norm)  o  Distance between 2 points in a normed vector space. o o  Formula: d(xi, xj) = (Sum (xi - xj)p)1/p  Is a generalization of distances defined for any p-norm.    Chebyshev Distance (Max Norm)  o  Greatest of distances along any coordinate dimension o  Formula: d(x1,x2) = Max(xi1-xj1,xi2-xj2,…) o  p = infinity    Cosine Distance  o  Formula: dcosine(xi, xj) = 1 – ((xi  T xj) / (||xi||2 - ||xj||2))    Mahalanobis Distance (MD)  o  Distance between 2 points in multivariate space. o  Formula: dMahalanobis(x1, x2) = sqrt((xi - xj) M-1 (xi - xj)T, M = Covariance matrix. o  MD can be thought of as scaling each data dimension by its variance and adjusting  for their relationships.  o  When data are independent, (M = I (Identity Matrix), MD is the same as Euclidean  distance. Jaccard Distance    o  Distance used to measure the diversity of any 2 sets. o  xi and xj as binary vectors indicating the presence or absence of features.     o  Formula: d(xi, xj) = 1 – (|(xi N xj)|1 / |(xi U xjr)| 1) , N = Logical, U = logical or operators.  |x|1 = 1-norm  Clustering Algorithms    2 algorithmic goals (find objective functions to:)  o  Minimise intra-distance (distance between 2 points in the same cluster) o  Maximise Inter-distance (distance between 2 points from 2 different clusters)    3 steps  o  1) Define a distance metric between objects o  2) Define an objective function that gets us to our clustering goal o  3) Devise an algorithm to optimise the objective function    Kmeans (watch the video in 3.5) o  Simple and fast o  Partition data points into k clusters/centroids o  Kmean searches by alternating between 2 methods:  ▪  Assigning data points to clusters based on the currently defined centroids (E  (Expectation)-step).  ▪  Choosing centroids based on the current assignment of data points to  clusters (M(Maximisation)-step).  ▪  Algorithm method:    Start with a random guess for centroids.   For each point, find its closest centroid.   Update centroids   Loop until converging.  o  Limitation:  ▪  Random initialisation means that you may get different clusters each time  (circumvent by KMean++).  ▪  We have to supply the number of clusters beforehand. ▪ ▪  It cannot find clusters of arbitrary shapes. It cannot detect noisy data points. o  Elbow method (to find number of clusters)  ▪  Run kmeans clustering algorithm for a range of values of K, and for each  value of K, compute the sum of squared error (SSE)  ▪  Zik is a binary variable.    1 = xi is assigned to the cluster number K   0 = xi is not related to cluster number K  o  Kmeans++  ▪  Choosing one initial cluster’s centre values or centroids for the Kmeans  clustering algorithm  ▪  Starts with allocating one cluster centre randomly and then searches the  other centres given the first one.  o  Others:  ▪  Partition based. ▪  Hierarchy based.    Produces a hierarchical tree o dendrogram.   2 types:  o  Agglomerative Hierarchical Clustering (bottom-up)   ▪  Start from one observation as a cluster to clusters  merging into one.  o  Divisive clustering (top-down)  ▪  Opposite of agglomerative.    Finding distance metrics:  o  Single-link: the distance between closest points o  Complete-link: the distance between the furthest points o  Centroid: the distance between the centroids o  Average-link: the average distance between pairs of  elements from across cluster pairs.  ▪  Density based.    DBSCAN (Density-based Spatial Clustering of Applications with  Noise)  o  Clustering algorithm that clusters certain items in a group  based on a given data point.  o  Need to set a minimum data point inside the distance given. o  Core points: the point that has at least the same number of  data points within the defined distance.  o  Border points: data points that cannot be considered core  points but are under the defined distance of the core point.  o  Steps:  ▪  Calculate the distance from each point to every other point to classify core/border points. ▪  Combine all core and border points within a  distance of each other into a single cluster.  ▪  Grid Based. ▪  Shape Based.    VAT (Visual Assessment for Tendency)  o  A visualization technique that transforms the distance matrix of a dataset into a visual representation in the form of a reordered matrix.  o    iVAT (improved VAT)  o  extension of VAT that involves repeatedly applying the VAT algorithm to the reordered matrix in order to refine the clustering structure.  o  Helps to identify the optimal number of clusters.    Both VAT and iVAT are useful for exploratory data analysis.  ▪  Model-Based.    Evaluation of clustering  o  External Assessment  ▪  compare clustering performance against a known clustering (Ground truth or Gold Standard)  ▪  Rand Index    Measure of similarity between 2 data clusters   R = (a+b)/(n2) = (a+b)/(a+b+c+d)  o  a = same cluster in both C, C’    o  b = number of pairs in different clusters C and C’ o  c = number of pairs in the same cluster in C but different in  C’  o  d = number of pairs in different clusters in C but the same in  C’    Adjusted rand index = corrected-for-chance.  ▪  Purity    Measured by counting the number of correctly assigned instances  and dividing by the number of total instances.    Disadvantage:  o  Number of clusters does not equal the number of points  ▪  Mutual Information  allocated.    Measures the agreement of the two clustering assignments C and C’  in terms of how informative one is about the other, ignoring permutations (how informative is C about C’)    C’ clustering is highly informative based on C = conclude that the C  clustering assignment is doing good.  o  Internal  ▪  Determine if clustering follows certain intrinsic assumptions. ▪  Dunn index ▪  Silhouette Coefficient    Measure how similar an object is to its cluster (cohesion) compared  to other Clusters (Separation)    Does not require the ground truth cluster assignments.   Silhouette coefficient contrasts the average distance between the  instance of the same cluster.    Final value of the silhouette ranges from -1 to +1.  o  +1 / -1 = Similar o  ~ 0 = no difference.    High values of s(i) indicates that the object is well-matched to its  cluster and poorly matched to neighbouring clusters.  