1. Summarise the main points that are covered in this topic.  â— Clustering can identify patterns in large data sets of unlabelled data â— Dimensionality reduction helps us when the number of features becomes too many to  manage  â— Distance measurements are used for: clustering algorithms, K-Nearest-Neighbor,  Support Vector Machines (SVM), data visualization, information retrieval, and ranking.  â— Distance Metrics:  a. Distance from any point to itself = 0 b. Distance between two points is symmetric: the distance is the same in both  directions  c. A distance between any 2 points is <= the distance between those 2 points and a  3rd point  â— Euclidean distance =  ğ· âˆ‘ (ğ‘¥ ğ‘›=1  ğ‘–ğ‘›  âˆ’ ğ‘¥  2 ) ğ‘—ğ‘›  â— Cosine distance = the angle between vectors (used when the magnitude of the vectors is  not important)  â— Mahalanobis distance (MD) = (  ğ‘¥ ğ‘–  numerical variables)  ) ğ‘€ âˆ’ ğ‘¥ ğ‘—  âˆ’1  (ğ‘¥ ğ‘–  ğ‘‡  ) âˆ’ ğ‘¥ ğ‘—  (used when there are continuous  â— Cityblock/Manhattan distance =  âˆ’ ğ‘¥  | ) ğ‘—ğ‘›  ğ‘–ğ‘›  ğ· | âˆ‘ (ğ‘¥ ğ‘›=1 ) |ğ‘  1 ğ‘  âˆ’ ğ‘¥ ğ‘—  â— Minkowski distance =  â— Jaccard distance = 1 -  â— Clustering:   (used to measure the diversity of any two sets)  a. Unsupervised learning - categorizing data b. Supervised learning - creating groups  â— Clustering goals:  a. Minimize intra-cluster distance b. Maximize inter-cluster distance  â— Clustering process:  a. Define a distance metric b. Define an objective function c. Define an algorithm for optimizing the objective function  â— K-means:  a. Choose initial centroids b. Divide the space in half and assign data points to each centroid c. Assign each data point to the nearest centroid d. Calculate new centroids as the average of their data points e. Repeat steps b, c, and d until the centroids stop moving  â— Rand index = similarity between two cluster sets  â— Adjusted Rand index = corrected-for-chance version of the Rand index â— Mutual Information = measures the agreement between two clustering assignments â— Silhouette Coefficient = measure of how similar an object is to its own cluster  (cohesion/similarity) compared to other clusters (separation/difference) â— Purity / Homogeneity = the percentage of correctly categorized objects â— K-means limitations:  a.  random initialisation may get different clusters each time - use K-means++ initialisation algorithm  b. need to supply the number of clusters beforehand - use the Elbow method c. cannot find clusters of arbitrary shapes d. doesnâ€™t cope well with noisy data - K-medians is less effected  â— Elbow method: compute the sum of squared error (SSE) â— Agglomerative clustering: every object starts in its own cluster, then pairs of clusters are  merged  â— Divisive clustering: all objects start together in a single cluster, then clusters are split in  two  â— DBSCAN: set a maximum distance between objects and a minimum number of objects â— VAT: shape-based clustering â— iVAT: iterative VAT â— Ng-Jordan-Weiss: uses graphs, Laplacian algorithm, and eigenvectors/eigenvalues for  dimension reduction  