 In topic 3, I have learnt about various distance/similarity measures used in developing machine learning and understand the clustering concept.To measure the distance between data points, we can use various measurements. Firstly, distance matrices, which is used to measure between data instances xi and xj. Examples for this are clustering algorithms, K-Nearest-Neighbor, Support Vector Machines (SVM), data visualization, information retrieval and ranking.  I’ve learnt the properties of distance measures and types of distance measurements.  o  Euclidean distance is the ordinary straight-line distance between two points in Euclidean space. o  Cosine distance is used for any two data instance represented by dimensional feature vectors o  The Mahalanobis distance (MD) is the distance between two points in multivariate space. o  Cityblock/Manhattan distance is used For any two data instances, represented by dimensional feature vectors so that the effect of a large difference in a single dimension is dampened. o  The Minkowski distance defines a distance between two points in a normed vector space. o  The Jaccard distance is a distance used to measure diversity of any two sets.  Next, in topic 3, we discussed the goals of clustering and followed by that, defined two algorithmic goals. They are, minimize intra-distance; the distance between points in the same cluster and maximize inter-distance; the distance between points from different clusters.  The most simple, fast and popular clustering algorithm in machine learning is called K-means. To evaluate clusters, there are two methods, external and internal assessments. They compare clustering performance against a known clustering and determine if clustering follows certain intrinsic assumptions.  To measure similarity between two clusters, we use the Rand index. Purity is a way of quality measurement in clustering methods. To analyze the clustering, we can use mutual information. The silhouette value is a measure of how similar an object is to its own cluster compared to other clusters. The silhouette coefficient contrasts the average distance between the instances of the same cluster with the average distance between the instances of different clusters. As well as its advantages, there are some limitations of Kmean which are discussed during this topic.  Kmeans++ is an algorithm for choosing the initial cluster’s center values or centroids for the Kmeans clustering algorithm. Also, there are some other clustering methods such as DBSCAN, shape based clustering and Hierarchical clustering. Hierarchical clustering algorithms find clusters that have a predetermined order. DBSCAN which means Density-Based Spatial Clustering of Applications with Noise is a clustering algorithm that clusters certain items in a group based on a given data point.  When it comes to shape-based clustering, VAT is a visualization technique that transforms the distance matrix of a dataset into a visual representation in the form of a re-ordered matrix and   iVAT is an extension of VAT that involves repeatedly applying the VAT algorithm to the re-ordered matrix to refine the clustering structure.  It is important to learn how to apply Kmean in Python. Thus, this topic we learnt how to implement KMeans clustering using the Scikit learn package. Firstly, how to load csv file for Kmean and merge two data frames and rename the header name, Target as a ground truth of KMean clustering and how to define number of clusters. Then we can evaluate the performance of clustering by getting the performance metrices. So that we can het the performance of the DBSCAN model and hierarchical clustering by purity score.  Topic 3 included some advanced learning compared to topic 1 and 2. To understand clustering, it was important to do self-research as the theory will be not sufficient to learn. W3school helped me to understand clustering from the very beginning and in “greeksforGreek” website, there were many resources to do the self-studies.      