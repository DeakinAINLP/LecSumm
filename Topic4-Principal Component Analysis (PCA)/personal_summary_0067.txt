During topic 3 the main points are:  Measuring distances: Measuring distances is a fundamental concept that is used in many machine learning algorithms, such as clustering, nearest neighbor, and ranking. Measuring distances is the process of calculating the distance between two or more data points in a feature space. It is a way of calculating the similarity or dissimilarity between two data points. There are several distance measures that you should know, including:  Euclidean  distance:  This  is  the  most  common  distance  measure  used  in  machine  learning.  It calculates  the  straight-line  distance  between  two  points  in  Euclidean  space.  In  other  words,  it measures the length of the vector connecting two points. Cosine distance: This measures the cosine of the angle between two vectors. It is commonly used for text similarity measures. Mahalanobis distance: This is a measure that takes into account the covariance of the data. It is useful when the variables are correlated. Cityblock/Manhattan distance: This is a measure of the distance between two points in a city block grid-like  street  pattern.  It  is  calculated  by  adding  up  the  absolute  differences  between  the coordinates of two points. Minkowski  distance:  This  is  a  generalization  of  the  Euclidean  and  Manhattan  distances.  It  is calculated as the nth root of the sum of the nth powers of the differences between the coordinates of two points. Jaccard distance: This is a measure of the similarity between two sets. It is calculated as the ratio of the size of the intersection of the sets to the size of the union of the sets.  Clustering Algorithms: Clustering  is  a  type  of  unsupervised  learning  algorithm  in  machine  learning  that  aims  to  group similar data points together based on certain criteria. As discussed in topic 1 and 2, the goal of clustering algorithms are to: a.  Group unlabelled data objects with similar properties together b.  Discover interesting perhaps unexpected clusters in the data c.  Find a valid or useful organisation of the data So  we  can  define  two  algorithmic  goals  which  are  minimise  intra-distance  and  maximise  inter- distance.  There are 3 steps to define a generic set-up a.  define a distance metric between objects b.  define an objective function that gets us to our clustering goal c.  devise an algorithm to optimise the objective function  Kmeans: KMeans is a popular unsupervised machine learning algorithm used for clustering data points.  KMeans is used for clustering - that is, dividing data points into groups (clusters) based on their        similarities. The algorithm requires the user to specify the number of clusters (k) beforehand. The algorithm works by initializing k centroids (representative points) randomly, and then iteratively updating  the  centroids  until  they  converge  to  the  best possible  location.  The  distance  between each data point and the centroid is calculated using a distance metric (usually Euclidean distance).  Each data point is assigned to the cluster with the nearest centroid. The process is repeated until the centroids no longer move or until a maximum number of iterations is reached. KMeans has some limitations, such as being sensitive to initial centroid placement and not being suitable for all types of data. However, KMeans is a popular and efficient algorithm for clustering large datasets, and  is  often  used  in  areas  such  as  customer  segmentation,  image  processing,  and  anomaly detection.  Kmeans++: Kmeans++ is an algorithm for initializing the centroids in the K-means clustering algorithm.  Kmeans++ selects the first centroid randomly from the dataset. It then selects the next centroids by choosing them from the remaining data points with a probability proportional to the squared distance  from  the  nearest  centroid  already  chosen.  This  increases  the  chances  of  selecting centroids that are far away from each other, leading to better cluster assignments. The process of selecting centroids continues until K centroids have been selected. The resulting centroids are used as the initial starting points for the K-means algorithm, which then iteratively assigns each data point to the nearest centroid and updates the centroids until convergence.  Kmeans++ is a more efficient initialization method than random selection of K because it leads to faster convergence and better clustering results. It is widely used in practice, and many machine learning libraries implement it as the default initialization method for K-means.  Hierarchical clustering: Hierarchical  clustering  is  a  widely  used  unsupervised  machine  learning  technique  that  aims  to group similar data points or objects into clusters based on their similarity or distance metrics. There are two approaches: Hierarchical clustering can be performed using either agglomerative (bottom-up) or divisive (top-down) methods. Agglomerative clustering starts with each data point as a separate cluster and successively merges the closest clusters, while divisive clustering starts with one large cluster and successively splits the most heterogeneous clusters. To decide which clusters to merge or split, hierarchical clustering uses linkage criteria. The most common linkage methods are single-link (minimum distance), complete-link (maximum distance), average-link  (average  distance),  and  centroid  (the  distance  between  the  centroids  which  is minimum varicance).  DBSCAN (Density-Based Spatial Clustering of Applications with Noise):  DBSCAN  (Spatial  Aggregation  Classes  for  Confidentiality-Based  Microsound  Applications)  is  an aggregation algorithm that aggregates certain items in a group based on a given data point.          To do this, we need to set the minimum number of data points (minPts) and the distance (dis). Because these parameters are determined by the user, the resulting collection depends on them.  A. Compute the distance from every point in the dataset to every other point. A point shall be "viewed" if it has at least the same amount of data within the determined correct distance. b. Data points that cannot be regarded as core points but are below a certain defined core point distance are called boundary points. All other points are considered "noise". C. The next step is to merge all the core points and boundary points inside the dis into a cluster.  Shape-based clustering, VAT, iVAT: VAT is a visual technique that converts a dataset's distance matrix into a graphical representation by reorganizing the matrix. This reorganization emphasizes dissimilarities between data points to unveil the inherent cluster patterns within the data. When the dataset exhibits clear clustering, the reordered  matrix  will  show  block-like  patterns  along  the  diagonal,  signifying  the  existence  of clusters.  iVAT builds upon VAT by continuously applying the VAT method to the reordered matrix to further refine the clustering patterns. The iVAT algorithm iterates the VAT process on the reordered matrix until  a  consistent  clustering  structure  emerges,  aiding  in  determining  the  optimal cluster  count within the data.  Both VAT and iVAT serve as valuable tools for exploratory data analysis, enabling data analysts to understand the underlying data structure and identify the most suitable number of clusters for subsequent clustering algorithms.  