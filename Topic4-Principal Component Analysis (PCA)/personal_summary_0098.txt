Learning Summary Similarity and distance measuring among various data points is key to several machine learning algorithms. Such  algorithms are  used for  supervised and  unsupervised learning.  The nature  of data point is a key factor by which measurements can be used for measuring distance.  Distance Metrics:  Widely used in machine learning, these are functions which define distance among any two data instance xi and xj. For the measurement on how similar these instances are, the examples that relate to machine learning are:    K-Nearest Algorithm   Data Visualization   K-Nearest Neighbor   Clustering Algorithms and many more…  Distance Measure/ Distance Metric satisfies 3 properties they are:    For any instance xi the distance towards itself is 0.   An instance pair xi and xj the distance is symmetric and non-negative.   The distance measure follows a triangular inequality.  Types of Distance Measures: The various kinds of distance measures are as follows:    Euclidean Distance: A distance between two points is said to be an ordinary straight line in Euclidean space. Its  representation is  done by  d  dimensional feature  vectors  xi and xj.  The as: distance  computed  is    Cosine Distance:  The distance can be defined as by d dimensional feature vectors xi and xj.  Which can be computed  as:    Mahalanobis Distance: The distance between two points in a multivariate space is known as Mahalanobis Distance. Its representation is done by d dimensional feature vectors  xi and xj.  The distance is computed as:    Cityblock/Manhattan Distance: Its representation is done by d dimensional feature vectors  xi and xj. The distance is computed as: Where in majority cases the distance measure yields results in a similar Euclidean distance. However when this distance in use, the effect of large difference within a single dimension is is dampened.    Minkowski Distance: in this, the distance between two points is in a normed vector space. Where Euclidean distance (2 norm of  xi – xj)  and Cityblock distance (1 norm of  xi and xj)  is     generalized of such distance that is defined for any p-norm. Where P = 1 distance is known as Cityblock distance, whereas P = 2 is known as Euclidean distance. Jaccard Distance: This distance is used for measuring diversity of any two sets. Considering instance  xi and  xj  as  binary  vectors  which  indicates  absence  or  presence  of  features.  The    distance is computed as:  Clustering: A way of putting data into groups is known as clustering. Where the usage of similarity and difference of features (or dimensions) are used for the creation of groups in material which is unclassified comprising of unknown targets. Particularly used in uns upervised learning due to its ability  of  dealing with  large  amounts  of uncategorized  data,  however  on the  other  hand  it creates groups making it useful for supervised learning as well. The objectives of clustering are as follows:    Grouping of unbiased data objects comprising of similar properties together.   Discovering unexpected or interesting clusters within the data.   Finding out useful or valid organization of the data.  Hence, in other words the algorithmic objective of clustering is finding out the objective function that, minimizes intra and inter-distance, which is said to be the distance among points within the same and different clusters respectively.  K-Means: The most popular clustering algorithm which is used in machine learning is K-Means due to its simplicity and swiftness. Within the algorithm, the representation of  K are the center points of the cluster, where one initiates with these centroids and measures teach data points for finding its closest centroids. The consideration of a point in a  particular cluster is when it’s closer to the cluster’s centroid compared to any other centroid. The algorithm searches for the best centroid by alternating among two methods,  which are:    Assigning data points to the cluster on the basis of current defined centroids.   Selecting centroids on the basis of the current assignment of data points  to the cluster.  These two methods are repeated until and unless a useful grouping of data points are found.  Evaluation of Clustering: When the evaluation of clustering is to take place, there are mainly two main categories on how clustering takes place. The categories are:    External Assessment: An assessment where the clustering performance is compared against  a known clustering (known as Gold Standard or Ground truth).     Internal Assessment:  An  assessment where  it determines  if clustering  follows any  certain intrinsic assumptions (for instance, cluster size, cluster-to-cluster distance etc.)  The various evaluation methods are as follows:    Rand Index: The measure of similarity among two data clusters is known as Rand Index. The assignment of data instance  to various clusters are  suggested by the clustering  algorithm. Within  the  external  assessment,  we  have  knowledge  regarding  the  ground  truth  cluster assignments. It is a function that is used for measuring similarity of C and C’. This is computed  as:  o  Where: A is the number of data instances pair within the same clusters in C and C’. o  B is the number of data instances pair which is in different clusters in C and in different  clusters in C’.  o  C is  the number  of data  instances pair  that are  within the  same cluster  in  C  but  in  different clusters in C’.  o  D is the number of data instances pair which are in different clusters in C but in the  same clusters in C’.  The index, takes chances into account and rectifies any bias that is introduced by the chance.    Purity: It is a quality measurement in clustering methods. Where one would want to measure the  purity  for  all  clusters  in  terms  of  class  labels  of  data  within  each  cluster.  Where  it’s accuracy is measured by counting the number of correct assigned instances and divide them by the number  of total instances.    Mutual Information: The most popular approach used in clustering. Where the agreement is measured among two clustering assignments like C and C’. Hence, making the aim similar to  that of Rand Index. It is computed as:    Silhouette  Coefficient:  A  measure  on  how  similar  an  object  is  towards  its  own  clusters, comparing to other clusters. The advantage with this methods is that it doesn’t require any ground truth cluster assignments. It contrasts the average distance among the instance of the same cluster  with  the  average  distance  among the  instances  of  different  clusters.  This  is  computed as: A High value Silhouette Coefficient indicates the object being well matched to its respective cluster and matched poorly with its neighboring clusters where if majority of the objects comprise of a high value then the configuration of the cluster is appropriate.    Limitations of K-Means: There are several limitations with K-Means, which are as follows:    Due to random initialization there is a potentiality of getting different clusters everytime. It’s  solution is using KMeans++ initialization algorithm for better initialization.    The number of clusters are required to be supplied beforehand. The usage of Elbow method  can be done for choosing K but might not be straightforward.    Clusters of arbitrary shapes cannot be found.   Unable to detect noisy data points, hence data points shouldn’t be taken into consideration  for cluster analysis.  Elbow  Method:  A  method  used  for  finding  out  the  appropriate  number  of  clusters  which interprets  and  validates  consistency  within  a  cluster  analysis  for  finding  out  the  appropriate number of clusters within a dataset. Where the K-Means clustering algorithm runs for a range of  values of K computing the sum of squared error  as:  K-Means++: An algorithm used for selecting the initial cluster’s center values or centroids for the K-Means clustering algorithm. As mentioned before K-Means start with allocating cluster center on a random basis then looks out for the better solutions. Whereas, K-Means++ starts allocating one cluster center randomly and searcher for other centers given the first one. Hence, both the algorithms use random initialization as a starting point but in different ways. Where K-Means++:    Selects one centroid,   Uniformly  at random from a dataset,   Let  D(x)  become  the  shortest  distance  from  a  data  point  to  the  closest  centroid  we  have  chosen already.    Choose a new centroid from a dataset with the probability of   This process is repeated until and unless K centroids have initialized.  Guarantee of K-Means++: There are various things that are guaranteed when K-Means++ is used, and they are as follows:    Where  K-Means  has  a  random  starting  number  of  centroids,  decreasing  the  objective function monotonically  with each  iteration of  the algorithm.  So, whenever  the algorithm  is running it gets closer to the best solution.    Where for assumption the best solution, the function takes value joptimum.   Where  for  assumption,  when  K-Means  is  being  used  the  objective  function  converges  to  jconverged.    Although no sort of theoretical bound on jconverged/ joptimum for K-Means with random initialization  is  there.  K-Means++  initialization  has convergence.  theoretical  guarantee  on  Other Clustering algorithms: There are various other clustering algorithms, they are as follows:    Hierarchical  Clustering:  This  cluster  algorithm  finds  out  clusters  comprising  of  a  pre- determined  order.  The  Hierarchical  clustering  algorithm  comprises  of  a  two  approaches Bottom-Up  and Top-Down  approach:  o  Agglomerative clustering (Bottom-Up): Where each observation starts within its own  cluster, and pairs of clusters are merged as one moves up to the hierarchy.  o  Divisive  clustering  (Top-Down):  Where  all  observations  start  in  one  cluster,  and  perform  splits as one moves down within the hierarchy.    DBSCAN  (Density-Based  Spatial  Clustering  of  Applications  with  Noise):  A  clustering algorithm which clusters certain items within a group, on the basis of a given data point. For which, minimum number of data points  (minPts) and  distance (dis) is required to be set  as the parameters are user-defined resulting the cluster being dependent on them.    Shape-based  Clustering  (VAT,  iVAT):  VAT  a  visualization  technique  which  transforms  the distance matrix of a dataset into a visual representation in the form of a re-arranged matrix. The re-arrangement is done in a manner where the dissimilarities among the data points are emphasized in way revealing underlying clustering structure of the data. If the data comprises of a clear clustering structure then the re-arranged matrix exhibits block-like structure along the diagonal indicating that a cluster is present. iVAT an extension of VAT involving repeated application of VAT to the re-arranged matrix in order for  refining  the  clustering  structure. The  algorithm  computes  the  re-arranged  matrix until  a  stable  clustering  structure  is  obtained.  Helping  in  identifying  optima l  number  of clusters within the data. Hence, making  the aforementioned  as useful  tools  for exploratory  data analysis,  by  which data analysts can gain insight to underlying structure of data and identify appropriate number of clusters for subsequent clustering algorithms.  