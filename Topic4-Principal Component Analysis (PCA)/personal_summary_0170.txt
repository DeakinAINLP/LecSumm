ï¶  Unsupervised machine learning:  UNSUPERVISED LEARNING:  unlabelled data is used to predict possible outcomes using following methods:  CLUSTERING  (useful to handle large datasets  with unlabelled data)  DIMENTIONALITY REDUCTION  (useful to manipulate features efficiently when data becomes too large to handle)  ï¶  Measuring distances using algorithms: ï‚·  Distance metrics is extensively used in both supervised and unsupervised machine learning  algorithms to find the similarity between the data points.  ï‚·  Properties of distance metrics:  1)  For any instance, the distance within itself is 0. 2)  For instance pairs, the distance is non-negative and symmetric. 3)  Follows the triangular inequality.  ï¶  Types of distance measurements:  1)  Euclidean Distance:  ï‚·  This represents the shortest distance between two vectors.  Itâ€™s represented by the square root of the sum of squares of differences between data points.  ï‚·  Euclidean distance is stated as:  2)  Cosine distance:  ï‚·  Cosine similarity is used as a metric for measuring distance when the magnitude of the  vector does not matter. It takes a unit length vector to calculate the dot products.  ï‚·  ï‚·  ğ‘‘ğ¶ğ‘œğ‘ ğ‘–ğ‘›ğ‘’(xğ‘–, xğ‘—) = 1 âˆ’  ğ‘‡xğ‘— xğ‘– ||xğ‘–||2.||xğ‘—||2  3)  Mahalanobis distance:  ï‚·  This is the distance between two points in multivariate space. For uncorrelated variables, the Euclidean distance equals to the Mahalanobis distance. But for more correlated variables the plotting them in regular space becomes a problem.  ï‚·  Thus, the Mahalanobis distance solves the problem by measuring distance between points,  even for correlated variables for multivariate space.  ï‚·  ğ‘‘ğ‘€ğ‘â„ğ‘ğ‘™ğ‘ğ‘›ğ‘œğ‘ğ‘–ğ‘ (xğ‘–, xğ‘—) = (xğ‘– âˆ’ xğ‘—)ğ‘€âˆ’1(xğ‘– âˆ’ xğ‘—)ğ‘‡  4)  Manhattan distance: It is the sum of absolute differences between data points across all dimensions.  ï‚· ï‚·  Manhattan distance is calculated as:  5)  Minkowski Distance:  ï‚·  It is the generalized form of Euclidean and Manhattan distance.  p-> order of norm  6)  Jaccard distance:  ï‚·  This distance is used to measure the diversity of any two sets.  ğ‘‘ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘(xğ‘–, xğ‘—) = 1 âˆ’  âˆ£ xğ‘– âˆ© xğ‘— âˆ£1 âˆ£ xğ‘– âˆª xğ‘—ğ‘Ÿ|1  ï¶  Clustering Algorithm:  Aim:  ï‚·  Group unlabeled data with similar properties together. ï‚·  Discover interesting patterns/clusters in data. ï‚·  Group the cluster to get useful insights.  Steps:  ï‚·  Define distance metric between objects. ï‚·  Define an objective function that satisfies the clustering goal. ï‚·  Devise an algorithm to optimize the function.  K-means:  ï‚·  This is an unsupervised ML clustering algorithm that is fast and easy to implement. k is the  ï‚·  center point of the cluster. It is an iterative algorithm that divides the unlabeled dataset into k clusters in such a way that they belong to group having similar characteristics.  ï‚·  Elbow method is used to find optimal no of clusters.  ï¶  Evaluation of clustering:  ïƒ˜  Rand index:  ï‚·  ï‚·  It is the measure of similarity between two clusters. It returns 1.0 value for identical clusters and 0.0 for random labeling. ğ‘+ğ‘   ïƒ˜  Silhouette coefficient:  ï‚·  This is the measure of how similar an object is to its own cluster compared to other  clusters.  ï‚·  A higher Silhouette coefficient value indicates that the data point is better matched to its  own cluster and poorly matched to other clusters.  ï‚·  The best score value is 1 and -1 is the worst.  ï¶  Clustering with k means++:  ï‚·  To overcome the drawbacks of k means, k means++ has been introduced. ï‚·  This algorithm uses the concept of smarter initialization of the centroids and improve the  quality of clustering.  ï‚·  Apart from initialization rest of the algorithm remains the same.  ï¶  Other clustering algorithms:  ï¶  Hierarchical clustering:  ï‚·  These algorithms find clusters that have predetermined order.  