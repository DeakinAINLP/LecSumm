Types of distance measurements:  Euclidean distance:  The Euclidean distance is a measure of the distance between two points in a two- or multi- dimensional space. It is the straight-line distance between the two points, and is calculated by taking the square root of the sum of the squares of the differences between each corresponding coordinate.  Cosine distance:  Cosine distance (also known as cosine similarity) is a measure of similarity between two vectors in a multi-dimensional space. It is often used in machine learning and information retrieval to compare the similarity between two documents or text snippets.  The cosine distance is calculated by taking the dot product of the two vectors and dividing it by the product of their magnitudes:  cosine distance = dot product / (magnitude of vector A * magnitude of vector B)  Mahalanobis distance:  The Mahalanobis distance is a measure of the distance between a point and a distribution.  We then calculate the covariance matrix of the variables in the distribution. Finally, we calculate the Mahalanobis distance between the point and the distribution as:  D^2 = (x - mu)' S^-1 (x - mu)  where x is the vector of values for the point, mu is the mean vector of the distribution, and S^-1 is the inverse of the covariance matrix of the variables in the distribution.  Cityblock/Manhattan distance:  The Cityblock distance (also known as Manhattan distance or Taxicab distance) is a measure of the distance between two points in a two- or multi-dimensional space. It is the distance between two points measured along the axes at right angles to each other.  In two-dimensional space, the Cityblock distance between two points (x1, y1) and (x2, y2) can be calculated using the formula:  distance = |x2 - x1| + |y2 - y1|  In three-dimensional space, the formula becomes:  distance = |x2 - x1| + |y2 - y1| + |z2 - z1|  Minkowski distance:      It is a measure of the distance between two points in a two- or multi-dimensional space and can be used to define a family of distance metrics.  In two-dimensional space, the Minkowski distance between two points (x1, y1) and (x2, y2) is defined as:  distance = (|x2 - x1|^p + |y2 - y1|^p)^(1/p)  where p is a parameter that determines the "order" of the distance metric. When p=1, the Minkowski distance is equivalent to the Cityblock distance (Manhattan distance), and when p=2, it is equivalent to the Euclidean distance.  Jaccard distance:  The Jaccard distance is a measure of the similarity between two sets of data. It is the complement of the Jaccard similarity, which is a measure of the overlap between two sets.  The Jaccard distance is defined as the ratio of the size of the intersection of two sets to the size of their union, subtracted from 1:  Jaccard distance = 1 - (size of intersection / size of union)  The Jaccard distance ranges from 0 to 1, where a value of 0 indicates that the two sets are identical and a value of 1 indicates that the two sets have no common elements.  Clustering and its applications:  Clustering puts data points into groups. It uses similarity and difference of features (or dimensions) to create groups in material that is unclassified and has no known targets. It’s particularly used in unsupervised learning as it can deal with vast amounts of uncategorised data however it creates groups so it’s useful in supervised learning as well.  How Kmeans Works:  K-means is a clustering algorithm that partitions a dataset into K clusters based on the similarity of their data points.  In this algorithm, k represents the centre points of clusters. You start off with these centroids and then measure teach data point to find its closest centroid. In other words, K-means stores k centroids for defining clusters. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-means searches for the best centroids by alternating between two methods:  ➢  Assigning data points to clusters based on the current defined centroids (points which are  the centre of a cluster  ➢  Choosing centroids based on the current assignment of data points to clusters.  Step 1 and 2 repeat until you find a useful grouping of data points      Evaluation of Clustering: Evaluating the performance of a clustering algorithm is an important step in understanding the quality of the clustering results.  ➢  External assessment: compare clustering performance against a known clustering (often  ➢  called Ground truth or Gold standard). Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.).  Rand Index: The Rand Index is an external evaluation metric for clustering algorithms that measures the similarity between the clustering results and the ground truth labels or class membership information. The Rand Index is defined as the proportion of pairs of data points that are either in the same cluster or in the same class in the ground truth labels.  Purity: Purity is an internal evaluation metric for clustering algorithms that measures the quality of the clustering results based on how well the clusters correspond to the ground truth labels or class membership information. Purity is calculated as the ratio of the total number of correctly classified data points to the total number of data points in the dataset.  Mutual Information: Mutual Information is an external evaluation metric for clustering algorithms that measures the amount of information shared between the clustering results and the ground truth labels or class membership information. Mutual Information is a non-negative measure that quantifies the similarity between the two sets of labels, taking into account both the number of data points and the number of clusters or classes.  Mutual Information can be calculated using the following formula:  MI = sum(sum(P_ij * log(P_ij / (P_i * Q_j))))  where P_ij is the joint probability of a data point belonging to cluster i and having ground truth label j, P_i is the marginal probability of a data point belonging to cluster i, and Q_j is the marginal probability of a data point having ground truth label j.  Silhouette Coefficient: The Silhouette Coefficient is an internal evaluation metric for clustering algorithms that measures the quality of the clustering results based on how well separated the clusters are and how well each data point fits within its assigned cluster. The Silhouette Coefficient ranges from -1 to The Silhouette Coefficient for a single data point is calculated as follows:  s(i) = (b(i) - a(i)) / max(a(i), b(i))  where a(i) is the average distance between data point i and all other data points in the same cluster, and b(i) is the average distance between data point i and all data points in the nearest cluster that is different from its own. 1, where a higher value indicates a better clustering result.        Limitations of Kmeans:  ➢  Random initialisation means that you may get different clusters each time. As a solution, we  can use a Kmeans++ initialisation algorithm to initialise better.  ➢  Number of clusters needs to be specified: K-means requires the number of clusters to be specified before running the algorithm, which may not be known in advance or may be difficult to determine. Choosing an inappropriate number of clusters can lead to poor clustering results. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis  ➢ ➢  Elbow Method:  The idea of elbow method is to run the Kmeans clustering algorithm for a range of values of K, and compute the sum of squared error. The elbow method suggests that the optimal number of clusters can be determined by identifying the "elbow" point in the plot between SSE and number of clusters.  Clustering with Kmeans++:  K-means++ is an algorithm for selecting the initial centroids in K-means clustering.  The K-means++ algorithm works as follows:    Choose the first centroid at random from the data points.   For each remaining data point, compute the distance to the nearest centroid that has already been chosen. This distance is weighted by the number of data points already assigned to that centroid.    Choose the next centroid at random from the remaining data points, with probability  proportional to the weighted distance to the nearest existing centroid.    Repeat steps 2 and 3 until k centroids have been chosen.   Run the standard K-means algorithm with the chosen centroids as the initial centroids.  The K-means++ algorithm ensures that the initial centroids are well separated from each other and are likely to be close to the true cluster centers. This can result in faster convergence and better clustering results, especially for high-dimensional data or datasets with complex structures. The K- means++ algorithm is now the default initialization method in many K-means implementations.  Other clustering algorithms:          Hierarchical clustering:  Hierarchical clustering is a clustering algorithm that groups similar data points into clusters based on the similarity or distance between the data points. The algorithm builds a hierarchy of clusters, which can be represented as a tree-like structure called a dendrogram.  There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative clustering starts with each data point as its own cluster and then successively merges the two closest clusters until a single cluster containing all the data points is obtained. Divisive clustering starts with all the data points in a single cluster and then successively splits the cluster into two smaller clusters until each data point is in its own cluster.  DBSCAN (Density-Based Spatial Clustering of Applications with Noise):  DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular density-based clustering algorithm used in machine learning and data mining. The algorithm groups together data points that are close to each other in a high-density region, while labeling data points that lie in low- density regions as noise or outliers.  The algorithm works by defining a neighborhood around each data point based on a specified radius or distance metric. A data point is considered a core point if it has at least a specified number of other data points within its neighborhood. A cluster is formed by grouping together all the core points and their neighbors, recursively.           