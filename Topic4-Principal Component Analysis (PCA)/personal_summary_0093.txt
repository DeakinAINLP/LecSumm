  Summary/Reflection of the main points during topic 3:    Unsupervised learning: Identifying patterns in data without labeled outcomes; explores  underlying data structure    Distance metrics:    Euclidean: √(Σ(xi - yi)^2)    Cosine: 1 - (Σ(xi * yi) / (√Σxi^2 * √Σyi^2))    Mahalanobis: √((x - y)' * S^(-1) * (x - y)) where S is covariance matrix    Manhattan: Σ|xi - yi|    Minkowski: (Σ|xi - yi|^p)^(1/p)    Jaccard: (A∩B) / (A∪B)        Clustering goals: Discover hidden structures, reveal relationships, facilitate decision-making,  compress and represent data    Clustering process: Assign data points to groups based on similarity; groups should have high  intra-group similarity and low inter-group similarity        K-means:    Algorithm for clustering; iteratively assigns points to centroids, updates centroids    Requires pre-defined number of clusters, sensitive to initialization    Elbow method: Plot inertia vs. cluster number; pick "elbow" point indicating  diminishing returns    K-means++:      Improved initialization for better convergence, reduces likelihood of poor clustering  Selects initial centroids more evenly across data, minimizes outliers' impact    Evaluation of clustering:    External assessment: Comparing clustering results to known ground truth    Rand Index: Measures agreement of two partitions; (TP+TN) /  (TP+TN+FP+FN)    Purity: Proportion of data points assigned to correct cluster         Mutual Information: Measures information shared between two partitions;  ΣΣP(i, j) * log(P(i, j) / (P(i) * P(j)))    Internal assessment: Measures clustering quality without ground truth    Silhouette Coefficient: (b - a) / max(a, b), where a is average intra-cluster distance, b is average distance to nearest other cluster    Alternative clustering algorithms: DBSCAN, hierarchical, spectral; each suited for different  data distributions, noise levels, and cluster shapes    Comparing K-means and K-means++:    K-means: Random initial centroids, prone to local optima    K-means++: Careful initial centroid selection, improves convergence, reduces  chances of poor clustering   