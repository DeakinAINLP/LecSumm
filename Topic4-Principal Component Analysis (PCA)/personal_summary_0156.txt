The topic stared off with an introduction to the various concepts relating to clustering such as understanding the similarity or distance between data points thus creating a basis for the topic's learning.  I was able to understand distance metrics such as Euclidean distance, Cosine distance, Cityblock/Manhattan distance and Jaccard distance which are crucial to the machine learning goals of minimizing intra-distance (distance between points in the same cluster) and maximizing inter-distance (distance between points from different clusters).  Another takeaway from the topic's study was the Kmeans and Kmeans++ clustering algorithms which involve the measure of distance of datapoints from a centroid(central data point). I got to realize that a data point is considered to be in a particular cluster if it is closer to that clusterâ€™s centroid than any other centroid based on the distance metrics.  To add to this, the concept of cluster evaluation came to light. This involved the use of evaluation techniques such as purity tests which measure the quality of an evaluation technique using the labels of data in each cluster. Other than this, another evaluation technique, the Silhouette Coefficient, is used to test how similar a data point is to its own cluster compared to other neighbouring clusters. Since evaluation techniques are not comprehensive in solving datapoint to cluster assignment, it is advised that one uses several evaluation techniques to make a more reliable choice of clustering method.  Finally, I got introduced to the use of python libraries including pandas, sklearn and matplotlib to automate the clustering techniques.  