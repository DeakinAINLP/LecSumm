Lesson Review: Clustering Module Learning Objectives  1.  Use clustering for revealing patterns from unlabelled data. 2.  Learn techniques to reduce dimensionality. 3.  Apply suitable clustering/dimensionality reduction techniques to perform unsupervised  learning of data in a real-world scenario.  Summarising the content: Distance Metrics – functions that define a distance d(xi, xj) between any two data instance xi and xj for measuring how similar the instances are. Distance metrics are measure that satisfy the following three properties:  Distance metrics are using in machine learning algorithms such as: clustering, K-Nearest-Neighbour, Support Vector Machines (SVM), data visualization, information retrieval, ranking.  Euclidean Distance – is the ordinary straight-line distance between two points in Euclidean space. For any two data instances, their Euclidean distance is calculated as:  Cosine Distance – is the distance for any two data instances represented by feature vectors.  Mahalanobis Distance (MD) – is the distance between two points in multivariate space.  Cityblock/Manhattan Distance – For data represented in feature vectors. Cityblock is similar to Euclidean however the effect of a large distance in a single dimension is dampened as the distances are not squared.  Minkowski Distance – defines a distance between two points in a normed vector space. Euclidean distance is 2 norm and city block is as 1 norm – Minkowski is a generalisation of these distances for any p-norm.  Jaccard Distance – is used to measure the diversity of any two sets. Where any two instances as binary vectors the Jaccard distance can indicate the presence or absence of features.  Clustering – is putting data into groups. It uses similarity and difference of features or dimensions to create groups in material that is unclassified and has no know targets. The goal of clustering algorithms is to;  ▪  Group unlabelled data objects with similar properties together ▪  Discover interesting perhaps unexpected clusters in the data. ▪  Find a valid or useful organisation of the data.  In regard to Distance metrics the objective function of a Clustering algorithm is to; ▪  Minimise intra-distance (distance between points in the same cluster) ▪  Maximise inter-distance (distance between points from different clusters)  K-Means Clustering – In K-means, k represents the centre point of clusters. Each data point is measured to find its closest centroid. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-means algorithms switch between assigning data points to clusters based on the current defined centroids, and choosing centroids based on the current assignment of data points to cluster.  Evaluation of Clustering – involve methods of determining how a clustering algorithm has performed. External assessment is evaluating the result against a know result or ‘ground truth’. Internal assessment looks to determine if clustering follows certain intrinsic assumptions (eg. Cluster-cluster distance or cluster size etc.).  Rand Index – is the measure of the similarity between two data clusters.  Purity – is a way of quality measurement in clustering methods. Each cluster is assigned to a class label which holds the majority in a cluster. The accuracy is measured by counting the number of correctly assigned instances within and then dividing by the number of total instances.  Mutual Information – is a function that measures the agreement of the two clustering assignments in terms of how informative one is about the other (ignoring permutations).  Silhouette Coefficient – indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. The Silhouette coefficient contrasts the average distance between the instances of the same cluster with the average distance between instances of different clusters.  