I have been introduced to unsupervised learning this topic. I have studied distance measurements and how can they be used with examples. I have covered the calculation of Euclidean distance with an example, cosine distance with an example, Mahalanobis distance, cityblock/Manhattan distance with an example, Minkowski distance (how it is related to cityblock and Euclidean distance), and Jaccard distance. I have studied clustering algorithms like Kmeans, evaluation methods like calculation of rand index, homogeneity, mutual information, silhouette coefficient, and how clustering is related to machine learning. I have seen the limitations of the Kmeans, how to use the elbow method, and why it can be useless with real-world data (because with real-world data, the cost value of the cluster function usually doesn’t increase drastically while trying to find the elbow shape). I have understood Kmeans can be useless because of many reasons (mainly the centres of the clusters can be different each time Kmeans is calculated) and we should use Kmeans++. I have also understood basically how Kmeans++ is calculated. I have seen other clustering methods like hierarchical clustering (agglomerative and divisive), DBSCAN, and shape-based clustering.  In the programming part, I have seen how Kmeans works, how the elbow method can be used, how to train and test Kmeans clustering (with purity score calculation), and how hierarchical clustering can be done by using Python.  While solving the quiz, I couldn’t understand this question.  I thought the hierarchical clustering shouldn’t be a density-based clustering algorithm. Maybe k- means can be because we are trying the find centroids and the distance between centroids and points. I must study more about those algorithms.       