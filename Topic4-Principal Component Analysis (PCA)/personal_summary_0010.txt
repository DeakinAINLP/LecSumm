In machine learning, distance or similarity measures play a critical role in clustering and classification tasks. These measures quantify the similarity between two samples or feature vectors in a dataset. Common measures include Euclidean distance, cosine similarity, and Mahalanobis distance. Clustering is an unsupervised learning technique used to group similar data points together based on some similarity or distance measure. In this task, various clustering approaches were explored, including KMeans, a popular and widely used algorithm. KMeans clusters data points into k clusters based on the minimum sum of squared distances between the data points and their respective cluster centers. However, KMeans has limitations such as sensitivity to the initial choice of centroids and difficulty in handling non-convex data. To evaluate the performance of KMeans clustering, Silhouette Coefficient was used, and the optimal k value was selected using KElbowVisualizer. Additionally, KMeans++ was used to overcome the limitations of KMeans. KMeans++ initializes cluster centroids using a smarter approach than KMeans, which leads to better performance and less sensitivity to the initial choice of centroids even though my numbers weren’t too different. As I have highlighted in the previous reports as well, the most important part of every topic’s module is the implementation of those concepts in python. 