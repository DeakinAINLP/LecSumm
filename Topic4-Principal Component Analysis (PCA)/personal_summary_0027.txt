Measuring Distances The distance between two data points characterises the similarity of those data points and is regularly used in both supervised and unsupervised machine learning approaches. The distance between two data points is measured using a distance metric d( xi , xj ) between data points xi and xj A distance metric must have the following properties: The distance between a data point and itself is 0. The distance is not negative and is symmetric, A number of diﬀerent distance metrics exist: Euclidean distance. The straight line distance between two data points. Mahalanobis distance. The distance between multivariate data points and the centroid of a cluster. Let M is the covariance matrix of the data: dMahalanobis Xi Xj Clustering Clustering is the process of grouping data points by their similarity. Speciﬁcally, clustering seeks to minimise the distance between any two points in the same cluster, and maximise the distance between any two points in diﬀerent clusters. It allows trends in data to be identiﬁed without a requirement for supervision. Where a signiﬁcant quantity of data is available, this is useful as it allows otherwise impossible to detect relationships to be identiﬁed. Once clustering has been the result must be evaluated for suitability. A range of evaluation techniques exist. External assessment compares a cluster against a ground truth to determine its success, whereas internal assessment evaluates the clustering algorithm output Rand Index comparison. Compares the similarity of clusters in the clustered data and ground truth data. Purity comparison. Identiﬁes a ratio of correctly clustered data points where correctly is that the data point is in the majority for that cluster. Invalid where number of clusters approaches number of data points. Mutual Information comparison. This evaluation measures the degree to which a ground truth cluster provides information about an output cluster and vice versa. Where predictions are strong the model is better. Silhouette Coeﬃcient. Measure of how similar a data point is to its own cluster, and diﬀerent it is from other clusters. Each data point is given a value from -1 to 1. High values indicate suitable clustering. Low or negative values indicated too few or too many clusters. s(i) = b(i) − a(i) maxa(i), b(i) average distance of point i to other instances of same cluster is lowest average dissimilarity of point i to other clusters a(i) b(i) K means Clustering K means clustering is an algorithm that ﬁnds clusters in a given set of data. In plain terms, it works by selecting K random centroids, and adding each data instance to a cluster with the centroid it is closest to. It then updates the centroids by taking the mean of the data instances in each cluster. Each data point is then updated to join the cluster of the centroid it is closet to. This process continues until the centroid update operation does not cause signiﬁcant change to the centroids, indicating that an optimum clustering has been reached for the number of clusters selected and their initial values. K means is fast and simple, but has some disadvantages: Selecting a useful number of clusters requires trial and error. Often performed using the elbow method. The elbow method ﬁnds the total sum of squared error for a range of values of K, and selects the 'elbow' of their graph as the optimal k value. Where no elbow is given, the graph does not assist choosing k and the values of k should be expanded, or a diﬀerent method used. An alternate method is to calculate the average silhouette coeﬃcient for each value of K and select the peak. Random nature of start point selection means that algorithm is not deterministic. It is possible to arrive at sub-optimal local maximums. K means ++ Clustering K means ++ is an improvement to the K Means clustering algorithm that reﬁnes initial data point selection. It guarantees that the subsequent K means algorithm will monotonically decrease (ie improve with each iteration until it converges on the optimum) The algorithm to select initial centroids is as follows: Choose initial centroid randomly D(x) let be the distance from a data point to the closest centroid already chosen Choose another centroid with probability D2 xi ( ) ∑i D2 xi ( ) Continue until k centroids have been identiﬁed DBSCAN Clustering Density Based Spatial Clustering of Applications with Noise searches a set for a core of samples with high density and then expands them to create clusters. It is useful when density is evenly spread across clusters, and is eﬀective at ﬁnding clusters with any shape, as opposed to KMeans which assumes convex shaped clusters. DBScan is deterministic - it always generate the same clusters based on a given set of data with a consistent order Hierarchical Clustering Hierarchical clustering creates pairs of most similar clusters together continuously from a single cluster to clusters with a single data point. This creates a hierarchy of cluster similarity It can be performed bottom up from single data point clusters (Agglomerative hierarchical clustering) or top down from the single cluster (divisive hierarchical clustering) The advantage of hierarchical clustering over KMeans is that the number of clusters does not have to be pre-determined. 