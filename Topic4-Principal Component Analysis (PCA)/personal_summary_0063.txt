Topic 4 – Clustering  Learning Goals    Clustering for revealing patterns from unlabeled data   Reduce dimensionality.   Apply suitable clustering/dimensionality reduction techniques to perform unsupervised learning  of data in a real-world scenario  Measuring distances    Measuring similarity or distances between different data points is fundamental to many machine  learning algorithms.    Distances metrics  o  Define a distance D(xi, xj) between any two data instances xi and xj o  These can be measured using algorithms like clustering algorithms, k nearest  neighbours, support vector machines o  They must satisfy the following properties  ▪  For any instance xi, distance with itself is 0; d(xi, xi) = 0 ▪  For an instance of pairs xi and xj, the distance is non-negative and symmetric;  d(xi, xj) >= 0 and d(xi, xj) = d(xj, xi)  ▪  Distance measure follow triangular inequality; d(xi, xk) <= d(xi, xj) + d(xj, xk)  o  Examples of distance measures in algorithms  ▪  Nearest neighbours    Close a data value is to a category on a graph, the more alike that data  point is to that category  ▪  Image retrieval  o  Type of distance measurements  ▪  Euclidean distance     Ordinary straight-line distance between 2 points   D represents the in-dimensional feature vectors xi, xj  ▪  Cosine distance    ▪  Mahalanobis distance    Distance between 2 points in multivariate space (2 or more variables)     Generalizes the notion of the variance to multiple dimensions.  ▪  Cityblock/Manhattan distance    Distance measure yields results to the Euclidean distance, however the  effect of a large difference in a single dimension is dampened.    Examples of different distance metrics    Euclidean distance  o  For any 2 data instances represented by d-dimensional feature vectors I and j = distance  exists  o  Used in the case when there are continuous demerit variables; this will reflect the  absolute distance between vectors.    Cosine distance   o  For any 2 data instances represented by d-dimensional feature vectors I and j = distance  exists  o  Cosine similarities are generally used to measure distances if the magnitude is not of  concern, but the angle of the 2 vectors    Mahalanobis distance  o  For any 2 data instances represented by d-dimensional feature vectors I and j = distance  o  exists Can be used when there are continuous numeric values and reflect absolute distance (by also removing redundancies); E.g repeat data.    Cityblock distance  o  For any 2 data instances represented by d-dimensional feature vectors I and j = distance  exists    Minkowski distance  o If Euclidean distance is 2 norm of xI – xj. and Cityblock is 1 norm of that o  This distance is a generalization of both distances (defined for any p-norm)    Jaccard distance  o  Measure the diversity of any two sets o o o  Measures the similarity between 2 sets  E.g if xi and xj were represented as binary vector (presence or absence of features) ||x||1 means 1-norm  Clustering and its application Clustering algorithms    Data points into groups  Similarities and differences of features (dimensions) to categorize   Used in unsupervised learning, as data is not originally categorized   Goals include.  o  Group unlabeled data objects with similar properties o  Unexpected cluster of data discovery o  Valid or useful organization of data o  Objective functions include  ▪  ▪  Intra-distance; this should be minimized as this is the distance between data in the same cluster Inter-distance; this should be maximized as this is the distance between data from one cluster to another  ▪    Steps were taken for a generic setup (based on current understanding)  o  Step 1: define a distance metric between objects o  Step 2: define an objective function that gets us to our clustering goal o  Step 3: devise an algorithm to optimize the objective  K-means    K means is a popular clustering method   For the following example will apply k means on an unlabeled data set o  K is a representation of the center points of clusters o  These K centroids are initiated, which then measures teach data points to find its closest  centroid.  o  For example if we pick a data point at (4, 3), and the original centroids, where k is 2, is (3, 4) and (3, 9), it measures the distance from point x and y with the data point provided. Ever one is closer in terms of distance, then it will be allocated and grouped into that centroid. K clustering can be more complex based on the k value, therefore optimizing this algorithm would lead to cleaner and more reliable clusters. o  So k means searches for the best centroid by alternating between two method  ▪  Assigning data points to clusters based on the currently defined centroid, ▪  Choosing centroids based on the current assignment of data points to clusters. ▪  Step 1 and 3 are repeated until all data is grouped based on the k value  How does k means work  Source: [1]    Initializing the centroids values, in the example, it will be randomly allocated; however having a nice cluster function will optimize to choose the best centroids (u1, u2, ….. uK)    Loop until all centroids stop updating (this is due to centroids changing in the nested loop)  o  Expectation step: Nested for loop which traverses all points i = 1 to n, in which it will  find the nearest/closet centroid to that data point  o o  argmin will return the closest centroid for a specific data point based on the distance  function if zik = 1 for k, then xi belongs to kth cluster  o  ▪  Another for loop; Maximization expectation step: updates the centroid based on  average values of all within that cluster  ▪ ▪  Summation of all x values which exist in cluster k  Evaluation of clustering    Evaluation of the clustering results   Purity  o  A clustering evaluation method that evaluates the outcomes of the clustering experiment    2 main categories of evaluation methods for clustering  o  External assessment  ▪  Compare clustering performance against a known clustering  o  Internal assessment  ▪  Determine if clustering follows certain intrinsic assumptions; examples include  silhouette coefficients, dunn index  Rand index is a measure of the similarity between two data clusters    Assignments of data instances to different clusters suggested by a clustering algorithm (C)   Using external assessment, we know of the ground truth cluster assignment (C’)  o  Rand index is a function that recognizes the similarity of the two assignments C and C’  o  a = the number of pairs of data instances that are in the same cluster  ▪ ▪  b = the number of pairs of data instances that are different in C’ than clusters in  ▪  C c = the number of pairs of data instances that are the same in cluster C’ but different in clusters in C  ▪  d = the number of pairs of data instances that are different clusters in C’ but in  Purity  the same clusters in C  a way of quality measurement in clustering methods     measure purity based on all clusters in terms of class labels in each cluster   accuracy by counting the number of correctly assigned instances and dividing by the number of total instances    e.g  o  1st has 5 crosses, 1 circle; the majority are crosses o  2nd has 4 circles and 1 cross, 1 plus and 1 cross; the majority is circles o  3rd has 3 plus and 2 crosses; the majority is pluses o  Therefore, add all majorities together from each cluster, and then divide by the total  number of data instances. (5 + 4 + 3) / 17 = 0.71    However, in the cases, we want 17 clusters, the purity will return as 100% (17/17 = 1)  Mutual information    How informative C is about C’ or C about C   How similar are they and the similarity relevance   Is a function that measures the agreement of the two clustering assignments C and C’ in terms of how informative one is about the other ignoring permutations.    o  Where K is the clustering partition for C and K’ is the clustering partition for C’ o  P(i) is the probability of randomly selecting an instance that belongs to (i)th cluster for  partition C.  Silhouette Coefficient    Measurement of the similarity of an object to its own cluster(similarities/cohesion), in  comparison to surrounding clusters (separation/difference)    Mathematical; compares the mean distance between the instances of the same cluster with the  average distance between the instances of another cluster.  o  In this equation, an (i) is the average distance of the (i)th instance with all other instances of the same cluster.  o  b(i) lowest average dissimilarity of (i)th instance with all other clusters. o  The resultant of the function has a range from -1 to 1, where the higher the number, the  closer it is to the matched cluster  o  Lower numbers indicate poorly matched to neighbouring clusters.  Limitations of K-means    K means clustering initialize random clusters at the beginning (however this can be fixed using    the K means++ initialization algorithm) Supplying the number of clusters is manual (therefore, it would require the user to know about the different amounts of possible expected groups, which could take a very long time)    Does not work on arbitrary shapes   Detecting and clustering data based on noisy data points can cause  Finding a useful number of clusters    Since means requires the assumption of the number of clusters in a data set, the Elbow method could counter this by actually identifying the set amount of required clusters  o  Run k-means clustering for a range of values of K o  Then compute the sum of squared error (SSE)  o o  Finds the distance (||xi -uk||^2) between xi (data instance) and its corresponding  centroid (uk) in the cluster  o  Zik is a binary that returns 1, if xi is below to respective k, otherwise 0, if it is not related  Kmeans with Kmeans++[1]    Choosing the best at selecting the best possible centre values or centroids for the K-means    clustering algorithms Instead of initialising multiple random cluster centres (unlike the normal k means), ++ starts by allocating one cluster centre randomly and then searches for other centres given the first one.    Procedure  o  Select centroid (u1) at random from the data o  Let D(x) be the shortest distance from a data point to the closest centroid probability  Initialize a new centroid from the dataset with a probability of  o o  The above steps are repeated (excluding 1)  All description to answers is in the python file, in the form of comments  