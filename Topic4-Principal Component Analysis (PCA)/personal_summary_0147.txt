  Summary  o  Different distance metrics:  ▪  Euclidean distance: The distance metric Euclidean distance estimates the distance  between  two  locations  in  Euclidean  space.  This  statistic  is commonly  used  in  machine  learning  and  data  analysis.  The  Euclidean distance  is  determined  as  the  square  root  of  the  total  of  the  squared differences between the vector components. The formula for Euclidean distance  between  two  locations  in  two  dimensions  is  provided  by  the square root of the sum of the squared differences between their x and y coordinates.  ▪  Cosine  distance:  The  cosine  distance  is  a  measure  for  comparing  the similarity of two vectors in high-dimensional space. It is used to compute the cosine of the angle between two vectors rather than their distance. Cosine distance, which is quantified on a scale of -1 to 1, is commonly used for text data and high-dimensional data like as photos and movies. Cosine distance is defined as 1 minus cosine similarity, where cosine similarity is estimated as the dot product of the two vectors divided by the product of their magnitudes.  ▪  Mahalanobis  distance:  The  Mahalanobis  distance  is  a  multivariate distance measure that considers the data's covariance matrix in addition to  the  distance  between  two  data  points.  Unlike  Euclidean  distance,  it takes into consideration the correlations between the various dimensions of the data as well as the variances of each dimension. The Mahalanobis distance  formula  includes  calculating  the  difference  between  two  data points, multiplying it by the inverse of the covariance matrix, and dividing the  result  by  the  square  root.  The  Mahalanobis  distance  is  useful  for spotting  outliers,  grouping,  and  classification  issues  in  which  the  data exhibits correlations and varying variances across dimensions.  ▪  Cityblock/Manhattan  distance:  The  Manhattan  distance,  also  known  as the cityblock distance, is a distance metric used to quantify the distance between two places in an n-dimensional environment. It computes the distance in terms of the distance a car would have to go if driving on a city block grid-like layout from point A to point B. In other terms, the distance is  the  total  of  the  absolute  differences  between  the  two  places' coordinates. Cityblock distance is frequently utilised in machine learning techniques such as KNN, clustering, and anomaly detection.  ▪  Minkowski distance: Minkowski distance is a distance metric that is used to calculate the distance between two points in an n-dimensional space. It is an extension of existing distance metrics such as Manhattan distance (when p=1) and Euclidean distance (when p=2). The Minkowski distance formula contains an order parameter 'p' that may be changed to highlight bigger  or  smaller  disparities  between  two  locations'  coordinates.  In   machine  learning  applications  such  as  clustering,  classification,  and outlier identification, Minkowski distance is often employed.  ▪  Jaccard  distance:  The  Jaccard  distance  is  a  distance  metric  used  to compare the similarity of two sets. It computes the ratio of the size of the sets'  intersection  to  the  size  of  their  union.  In  machine  learning,  the text  analysis,  picture Jaccard  distance segmentation, and grouping. The Jaccard distance formula is 1 - (|A ∩ B| / |A ∪ B|), where A and B are the two sets being compared, and |A ∩ B| and |A ∪ B| are the sizes of their intersection and union, respectively. The Jaccard distance is a number between 0 and 1, with 0 indicating that the sets are identical and 1 indicating that the sets are entirely distinct.  frequently  used  for  is  o  Clustering  Algorithms:  Clustering  techniques  are  used to  organise  data  points with  comparable  properties  or  qualities  into  clusters  or  segments.  These  are unsupervised  algorithms,  which  means  they  do  not  require  labelled  data  to train.  Clustering  methods  include  K-means  clustering,  hierarchical  clustering, density-based  clustering,  and  model-based  clustering.  These  algorithms  are like  as  segmentation,  anomaly  detection,  and  pattern used  for  tasks identification image  analysis,  and bioinformatics.  in  disciplines  such  as  marketing,  ▪  K-means: K-means is a clustering technique that divides data points into k clusters,  with  the  centroid  representing  each  cluster.  It  attempts  to minimise  the  sum  of  squared  distances  between  data  points  and  the cluster  centroid.  The  method  operates  by  initialising  k  centroids, clustering  data  points  depending  on  their  closeness  to  the  centroid, updating centroids based on the mean of data points in each cluster, and repeating until convergence is reached. After the centroids stop moving, the algorithm has reached its conclusion. Because the first centroids are picked  at  random,  it is  typical to run  the  procedure  with  various initial centroids numerous times to reach the final clustering result. o  Evaluation of Clustering: The assessment of clustering algorithms is necessary to guarantee  the  algorithm's  efficacy  and  that  the  clustering  result  satisfies  the desired conclusion. Silhouette score, Dunn index, Davies-Bouldin index, Calinski- Harabasz index, and eye examination are some typical approaches for analysing clustering.  The  assessment  technique  used  is  determined  by  the  problem's needs and the features of the dataset.  ▪  Rand Index: The Rand Index is a similarity metric between two collections of  data  points  that  is  widely  used  to  evaluate  clustering  techniques.  It calculates the proportion of data points accurately assigned to the same or distinct clusters by the algorithm and the ground truth. The Rand Index goes from 0 to 1, with 1 indicating perfect agreement and 0 indicating no agreement  beyond  chance.  Although  it  is  simple  and  can  be  applied  to both binary and multi-class classification problems, it is not sensitive to cluster size or form and can be modified by cluster number.    ▪  Purity: Purity is a clustering algorithm quality metric that evaluates how successfully  the  algorithm  distributes  data  points  to  the  right  clusters based on a given ground truth. The fraction of data points in a cluster that belong  to  the  same  class  or  category  is  computed.  Purity  is  a  number between 0 and 1, with 1 signifying that all data points in a cluster belong to  the  same  class.  Nevertheless,  purity  has  restrictions,  such  as disregarding overlap across classes and favouring bigger clusters.  ▪  Mutual Information: Mutual Information is a metric that measures how much information two clusterings share and is frequently used to assess clustering methods. It estimates the amount of knowledge received about one clustering by knowing the other and goes from 0 (no agreement) to 1 is (complete  agreement)  (perfect  agreement).  Mutual commonly  employed  in  machine  learning  and  information  theory, although it is impacted by the size of clusters and may be biased towards bigger clusters.  information  ▪  Silhouette Coefficient: The Silhouette Coefficient is a clustering method evaluation  metric  that  evaluates  how  well  each  data  point  fits  into  its allocated cluster and how different each cluster is from other clusters. It has a value between  -1 and 1, with a greater value suggesting a better clustering  outcome.  The  Silhouette  Coefficient  is  frequently  used  to calculate the ideal number of clusters. It does, however, have limitations because it does not take into account the size and structure of the clusters and can be sensitive to the distance measure utilised.  is  a  popular  clustering  technique,  however  o  Limitations of K-Means: Because of its simplicity, scalability, and effectiveness, K-means it  has  significant disadvantages.  These  limitations  include  sensitivity  to  initial  centroids,  the requirement to specify  the  number of clusters  ahead of time,  the  inability to handle non-linear clusters, the limitation to numeric data, sensitivity to outliers, difficulty clustering data with varying densities, and the assumption of spherical clusters with equal variance. When using the K-means algorithm in real-world applications, several restrictions must be recognised.  o  Clustering  with  K-Means++:  K-Means++  is  a  modified  version  of  the  K-Means clustering method that tries to lessen K-Means' sensitivity to cluster centroids' initial location. The K-Means++ method initialises the centroids in such a way that the algorithm is more likely to converge to a satisfactory solution. It works by randomly selecting the first centroid from the data points and then selecting further centroids based on distances from the nearest centroid. K-Means++ can produce superior clustering results and perform better than the traditional K- Means method, particularly for big datasets with a large number of dimensions.  o  Other Clustering Algorithms:  ▪  Partition  Based  Clustering:  Partition-based  clustering,  also  known  as centroid-based  or  partitional  clustering,  is  a  form  of  clustering  method that splits data points into non-overlapping parts or clusters. The method    begins  with  a  random  selection  or  initialisation  of  centroids  using  a specified  approach,  and  then  data  points  are  allocated  to  the  nearest centroid  to  build  an  initial  partitioning.  The  centroids  are  updated depending  on  the  mean  value  of  each  cluster's  data  points,  and  the procedure  is  repeated  until  convergence  or  a  stopping  threshold  is satisfied.  Partition-based  clustering  is  computationally  efficient  and scalable,  and  the  results  are  simple  to  comprehend.  Nevertheless,  it  is sensitive to the number of clusters and the initial choice of centroids, and it  may  not  perform  well  for  data  with  non-linear  or  irregularly  shaped clusters.  ▪  Hierarchical  Based  Clustering:  Hierarchical  clustering  is  a  clustering technique that builds a hierarchy of groups by merging or splitting them based  on  their  similarity.  Hierarchical  clustering  is  classified  into  two types: agglomerative and divisive. Agglomerative clustering begins with each data point as its own cluster then combines the most comparable clusters until only one remains. Divisive clustering takes a single cluster of data  points  and  separates  it  into  smaller  subclusters.  Hierarchical clustering  produces  a  dendrogram-like  visual  representation  of  the clusters, but it is computationally intensive and sensitive to the similarity measure and linking mechanism utilised.  ▪  Density  Based  Clustering:  Density-based  clustering  is  a  clustering technique  that  divides  data  points  into  clusters  based  on  their  local density, with DBSCAN being the most prevalent. Data points in DBSCAN are classified as core points, border points, or noise points based on their distance from nearby points. DBSCAN can detect clusters of various forms and sizes, deal with data noise, and is sensitive to two hyperparameters. It  may  struggle  with  clusters  of  different  densities,  though,  and  huge datasets can be computationally costly.  ▪  Grid Based Clustering: Grid-based clustering is a sort of clustering method that divides data points into grid-like cells, with each cell representing a cluster.  This  approach  can  handle  noise  and  outliers  effectively  and  is excellent for studying huge datasets with multiple dimensions. STING and WaveCluster are two common grid-based clustering methods that employ statistical  measurements  and  wavelet  analysis,  respectively,  to  locate clusters. Grid-based clustering has the benefit of being able to manage noise and outliers and scaling to big datasets, although the choice of grid size  might  impact  the  clustering  results,  and  some  algorithms  may  not function well for datasets with irregularly shaped clusters.  ▪  Model Based Clustering: Model-based clustering is a clustering algorithm that  uses  statistical  models  to  group  data  points  into  clusters.  The algorithm fits a probabilistic model to the data and assigns data points to clusters based on the parameters of the model. Gaussian Mixture Models (GMMs)  and  Latent  Dirichlet  Allocation  (LDA)  are  examples  of  model-    based clustering algorithms. GMM-based clustering models each cluster as a Gaussian distribution with its own mean and covariance matrix, while LDA  is  used  for  text  data  to  represent  each  document  as  a  mixture  of topics. Model-based clustering can discover complex clusters, but it can be  computationally  expensive  and  requires  the  user  to  specify  the number  of  clusters  and  the  model,  which  can  be  difficult  for  some datasets.    Reading list: Lecture Slides, Lecture Recordings, Learning Contents.   My reflections: This topic's topic is alternative distance metrics, clustering methods, and  clustering  assessment  methodologies.  We  began  by  looking  at  six  different distance  metrics:  Euclidean  distance,  cosine  distance,  Mahalanobis  distance, cityblock/Manhattan  distance,  Minkowski  distance,  and  Jaccard  distance.  Each distance metric is utilised in various machine learning applications and data analysis, and each has its own formula for calculating distances between data points. Then we looked  at  four  different  clustering  algorithms:  K-means  clustering,  hierarchical clustering, density-based clustering, and model-based clustering are all methods for grouping data. These algorithms are often used for segmentation, anomaly detection, and  pattern  recognition,  and  they  aggregate  data  points  into  clusters  according  on their  features  or  attributes. Lastly,  we  investigated  clustering  algorithm  assessment methodologies  such  as  the  Silhouette  score,  Dunn  index,  Davies-Bouldin  index, Calinski-Harabasz  index,  and  eye  examination.  We  also  spoke  about  two  particular evaluation tools this topic: the Rand Index and Purity. The Rand Index compares the similarity  of  two  sets  of  data  points,  whereas  Purity  assesses  how  effectively  data points are clustered based on a specific ground truth.  