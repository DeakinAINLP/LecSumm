Clustering Data  A method of unsupervised learning is clustering data. This involves analysing a dataset, finding points with similar attributes and grouping these similar datapoints together. Without labels to this data, what these similarities are is impossible to know, but with clustering this context becomes irrevelent.  The similarity of these datapoints is measured by their distance from each other.  One popular method of clustering is the K-means algorithm. This sets a random number of points at random places on the visualised graph of data and calculates the distance of each datapoint to these ‘cluster centres’. The closest cluster centre to each datapoint becomes it’s cluster. This is done incrementally until reaching an accurate representation of data clusters within the dataset. To accurately determine the correct clusters for the data, the number of the cluster centres (k) placed on the graph needs to be adjusted and evaluated.  The below figures show a simple illustration of data before and after clustering.  The distance between each data point within each cluster is known as the ‘intra-distance’ and the distance between two points from different clusters is known as the ‘inter-distance’.  In order to successfully cluster data based on similar properties:    Minimise intra-distance (distance between points in the same cluster)   Maximise inter-distance (distance between points from different clusters)  Measuring distance can be done using a variety of methods and the correct method (or more accurately the more useful method) depends on the context and layout of the data.  After clustering data we need evaluate the clusters in order to determine if they were successful in providing useful insight or if we need to adjust the model. Assessment can be either external, meaning to compare the results against a known metric, or internal, meaning to determine if the clustering follows intrinsic assumptions of the data.  different clusters in C′. c = the number of pairs of data instances that are in the same cluster in C  but in different clusters in C′.    d = the number of pairs of data instances that are in the different clusters in C but in the  same clusters in C′.  With most tools that we use, K Means also has some limitations. The most important of these are:    The clusters are randomly initialised, so they may be different each time.   We have to provide a K number as the number of clusters. We can use methods such as the elbow method to predict what this value should be but it may not be reliable. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis.  