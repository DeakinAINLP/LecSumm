Clustering  During this lesson we discuss about the clustering algorithms, Measuring distances, Distance metrics, K-mean algorithm, Kmean++ and other clustering algorithms.  Clustering  A machine learning technique called clustering involves putting instances into clusters based on how similar they are. It uses unlabeled data because it is an unsupervised learning method. Market segmentation, social network analysis, search result grouping, medical imaging, picture segmentation, and anomaly detection are a few examples of applications for clustering.  Different clustering algorithms exist, each of which is appropriate for a specific data distribution. Centroid-based clustering, density-based clustering, distribution-based clustering, and hierarchical clustering are a few popular techniques.  Data is grouped into non-hierarchical groupings using centroid-based clustering. K-means is the most used centroid-based method. High example density locations are linked together into clusters using density-based clustering. Data are thought to be made up of distributions, such as Gaussian distributions, according to distribution-based clustering. A tree of clusters is produced through hierarchical clustering.  Distance Metrics  Metrics for measuring distance and similarity between data points are known as distance metrics. They are essential to many machine learning methods, including as clustering and classification. By precisely assessing the similarity between data points, an efficient distance metric can enhance the performance of a machine learning model.  There are various distance metrics, including the Euclidean, Manhattan, Minkowski, and Hamming distances. Each measure has a unique method for measuring the separation between data points and may be better suited for particular sorts of data or issues.  KMean Clustereing  A popular centroid-based grouping algorithm is K-means. It seeks to divide n observations into k clusters, where each cluster is a prototype for the cluster and each observation belongs to the cluster with the closest mean (cluster centres or cluster centroid).  Each data point is repeatedly assigned to the closest cluster centroid, which is then updated to reflect the mean of all the data points in the cluster. Up until convergence, or when the assignments of points to clusters stop changing, this process is repeated.  How K-mean Works  A dataset is divided into k clusters using the iterative K-means clustering technique. The k cluster centroids are initially initialised at random to start the procedure. The method then assigns each data point, in each iteration, to the closest cluster centroid depending on the distance between the data point and the centroid. The centroids are updated to reflect the mean of all the data points in the cluster after each data point has been assigned to a cluster. Repeat this procedure until convergence, which occurs when the cluster assignments of points remain constant  DBSCAN  Density-Based Spatial Clustering of Applications with Noise is known as DBSCAN. It is a clustering technique based on density that clusters points that are closely spaced apart while flagging as outliers those that are isolated in low-density areas. One of the most popular clustering methods, DBSCAN has attracted a lot of attention in both theory and application.  DBSCAN finds high density core samples and expands clusters from them. It works well with data that includes clusters with a comparable density.  