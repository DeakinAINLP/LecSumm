 In  topic  3  I  learned  about  unsupervised  machine  learning  and  its  two  form  i.e.,  Clustering  and Dimensionally reduction. I got I knowledge of why we need distance metrics i.e., distance between two instances to measure the how similar these tow instances are. We have number of methods to measures  the  distance  between  two  points.  These  are  like  Euclidean  Distance,  Cosine  Distance, Mahalanobis Distance, Cityblock/Mahattan Distance, Minkowski Distance, and Jaccard Distance.  I came to know about clustering algorithms i.e., clustering algorithm is used for grouping of similar objects together based on their characteristics. The purpose of clustering algorithm is dividing the data into groups and the data points within each cluster are same to each other as compared to those are in other cluster.  I got the information about K-means and how it works. The K-means is most clustering algorithm in machine learning and its simple and fast. Also, we need to use evaluation methods on the clustering results. The evaluation method is used to evaluate the outcomes to see the clustering result is useful or not. There are two main categories of evaluation methods for clustering i.e., External assessment, and internal assessment. External assessment compares the clustering performance against a known clustering.  Also  known  as  ground  truth  or  gold  standard.  The  internal  assessment  determines  if clustering certain intrinsic assumptions e.g., cluster-to-cluster distance or cluster size etc. Silhouette coeﬃcient, Dunn index are examples of internal assessment.  There  are  number  of  evaluation  methods  are  available  and  these  are  like,  Rand  Index,  Purity,  and Silhouette Coeﬃcient. Also, there are some limitations of K-means and one of them is assumption we have  to  make  about  the  number  of  clusters  to  start  with.  We  have  elbow  method  to  ﬁnd  the appropriate number of clusters.  Also, I came to know about Kmeans++ and how it is diﬀerent from Kmeans. Kmeans++ is an algorithm starts with allocating one cluster centre randomly and then searches for other centres given the ﬁrst one. On the other hand, Kmeans starts with allocating cluster centre randomly and looks for better solution.  Answer to Python:  4. when we run Kmeans iterate 50 times the result varied when you run multiple times. The result sometimes same with Kmeans++ and sometimes slightly diﬀerent. This is because Kmeans is sensitive to the initial positions of the centroids, and the algorithm can converge to slightly diﬀerent local optima depending on the starting positions.  On the other hand, the Kmeans++ reduces the likelihood of the algorithm converging to suboptimal solution by selecting the initial centroids in more eﬃcient way.        