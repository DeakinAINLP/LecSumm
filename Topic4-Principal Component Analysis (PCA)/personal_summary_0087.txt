Summary: Through my study of Topic 3 I learnt by reading the content, watching the videos, taking notes and working through the activities. Evidence can be seen below.  Activity 3.1:  1.  Unsupervised learning  Activity 3.2:  2.  Measuring distances  a.  Euclidean distance b.  Cosine distance c.  Mahalanobis distance d.  Cityblock/Manhattan distance e.  Minkowski distance f. Jaccard distance g.  Activity 3.3: Different distance metrics examples through a video of  1.  Euclidean distance 2.  Cosine distance 3.  Mahalanobis distance 4.  Cityblock/Manhattan distance 5.  Minkowski distance 6.  Jaccard distance  Activity 3.4:  1.  Clustering and its applications and ducks and jaguars. 2.  Clustering Algorithms  a.  Maximise inter-distance b.  Minimise intra-distance  Step 1: define a distance metric between objects Step 2: define an objective function that gets us to our clustering goal Step 3: devise an algorithm to optimise the objective function  Activity 3.5: 1.  K-means. 2.  Expectation maximisation  Activity 3.6:  1.  Evaluation of Clustering. 2.  Ground truth 3.  Rand Index 4.  Purity – There needs to be a reasonable amount of clusters in this method otherwise  it will result in an uninformative outcome.  5.  Mutual Information 6.  Silhouette coefficient  Activity 3.7:  1.  Limitations of Kmeans  a.  Random initialisation b.  Specify number of clusters c.  Arbitrary shapes d.  Detection of noisy data  2.  Elbow Method and it’s sum squared error  Activity 3.8: Kmeans with Kmeans++  Activity 3.9: Other clustering algorithms.    Hierarchical clustering   DBSCAN (Density-Based Spatial Clustering of Applications with Noise)   Shape-based clustering, VAT, iVAT  Activity 3.10: Kmeans Clustering in Python  Activity 3.11: Train and test KMeans clustering Performance metrics    Purity   Mutual Information   Silhouette Coefficient  f1  recall   accuracy   precision  Activity 3.12: DBSCAN Hierarchical clustering   Activity 3.13:  