DBSCAN DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expand clusters from them. Good for data which contains clusters of similar density. [More details] from sklearn.cluster import DBSCAN X=Feature_set.iloc[:,0:-1] y_true=Feature_set.iloc[:,-1] model = DBSCAN(eps=0.3, min_samples=10) y_pred=model.fit_predict(X) print(     "purity_score: %0.3f"     % purity_score(y_true, y_pred) ) Performance of the DBSCAN model: purity_score: 0.519 Hierarchical clustering  import numpy as np from matplotlib import pyplot as plt from scipy.cluster.hierarchy import dendrogram from sklearn.cluster import AgglomerativeClustering def plot_dendrogram(model, **kwargs):     # Create linkage matrix and then plot the dendrogram     # create the counts of samples under each node     counts = np.zeros(model.children_.shape[0])     n_samples = len(model.labels_)     for i, merge in enumerate(model.children_):         current_count = 0         for child_idx in merge:             if child_idx < n_samples:                 current_count += 1  # leaf node             else:                 current_count += counts[child_idx - n_samples]         counts[i] = current_count     linkage_matrix = np.column_stack(         [model.children_, model.distances_, counts]     ).astype(float)     # Plot the corresponding dendrogram     dendrogram(linkage_matrix, **kwargs) X=Feature_set.iloc[:,0:-1] y_true=Feature_set.iloc[:,-1] # setting distance_threshold=0 ensures we compute the full tree. model = AgglomerativeClustering(distance_threshold=0,linkage="complete", affinity="cosine",n_clusters=None) model = model.fit(X) plt.title("Hierarchical Clustering Dendrogram") # plot the top three levels of the dendrogram plot_dendrogram(model, truncate_mode="level", p=3) plt.xlabel("Number of points in node (or index of point if no parenthesis).") plt.show() model = AgglomerativeClustering( linkage="complete", affinity="cosine",n_clusters=2) y_pred = model.fit_predict(X) print(     "purity_score: %0.3f"     % purity_score(y_true, y_pred) ) Clustering and its applications Humans are hard wired to find patterns in everything. Imagine you are walking out into the garden on a fine day. You look up to the clouds and suddenly you realise there is a puffy white duck in the sky. Did I just see a huge puffy white duck in the sky? Figure. Patterns in the sky (Susan, 2015) Yes you did! Our brains always look for patterns! Why do we see patterns in the clouds? The traditional explanation is that it was very important to in individual’s survival to spot a jaguar in the undergrowth. Our brains cluster unconsciously. In fact, we are encoded to see patterns in everything such as shopping, traffic, clothes, sport etc. As with all the problems in machine learning, we need to answer the question: How do we get a computer to find patterns so it doesn’t get attacked by a jaguar? Let’s explore clustering as it relates to machine learning. Clustering Algorithms Clustering puts data points into groups. It uses similarity and difference of features (or dimensions) to create groups in material that is unclassified and has no known targets. It’s particularly used in unsupervised learning as it can deal with vast amounts of uncategorised data however it creates groups so it’s useful in supervised learning as well. We will look at this idea in Course 3, 4 and 5. As we discussed in topics 1 and 2, the goal of clustering algorithms are to: Group unlabelled data objects with similar properties together Discover interesting perhaps unexpected clusters in the data Find a valid or useful organisation of the data In other words, we can define two algorithmic goals. We need to find objective functions to: Minimise intra-distance (distance between points in the same cluster) Maximise inter-distance (distance between points from different clusters) Figure. difference between intra-distance and inter-distance Now we can define a generic set-up based on our current understanding from clustering methods: Step 1: define a distance metric between objects Step 2: define an objective function that gets us to our clustering goal Step 3: devise an algorithm to optimise the objective function Definition and Properties Measuring similarity or distances between different data points is fundamental to many machine learning algorithms. We will be looking at these issues this topic. These algorithms are used both in supervised learning methods and unsupervised learning problems. We will go through how to select a metric as we go through this topic. Depending on the nature of the data point, various measurements can be utilised to measure distance. Distance metrics Distance metrics are used widely in machine learning algorithms. Distance measures are functions that define a distance d(xi,xj){"version":"1.1","math":"\(d(x_i,x_j)\)"} between any two data instances xi{"version":"1.1","math":"\(x_i\)"} andxj{"version":"1.1","math":"\(x_j\)"}  for measuring how similar the instances are. The most related examples in machine learning are: clustering algorithms (we looked at examples of clustering last topic) K-Nearest-Neighbor Support Vector Machines (SVM) data visualization information retrieval ranking Distance measures satisfy the following three properties: For any instance xi{"version":"1.1","math":"\(x_i\)"}, distance with itself is zero. That is, d(xi,xi)=0{"version":"1.1","math":"\(d(x_i,x_i)=0\)"}. For an instance pairs xi{"version":"1.1","math":"\(x_i\)"} and xj{"version":"1.1","math":"\(x_j\)"}, the distance is non-negative and symmetric. That is, d(xi,xj)≥0{"version":"1.1","math":"\(d(x_i,x_j) \geq 0\)"} and d(xi,xj)=d(xj,xi){"version":"1.1","math":"\(d(x_i,x_j) = d(x_j,x_i)\)"}. Distance measure follows triangular inequality. That is, d(xi,xk)≤d(xi,xj)+d(xj,xk){"version":"1.1","math":"\(d(x_i,x_k) \leq d(x_i,x_j)+d(x_j,x_k)\)"}. Distance measures satisfying above properties are also known as Distance Metrics. Now let us work on two examples. Example 1: Nearest Neighbor Classification Figure. Using distance to find the label of the new data point (the red triangle). We can use the distance to find the nearest neghbour and classify the data to the class label of this neighbour. So what do you think is the correct label of the red triangle in the Figure? Square or circle? Example 2: Image retrieval Another interesting example is in image retrieval. Consider the NUS Wide Animal dataset, which contains 269,648{"version":"1.1","math":"\(269,648\)"} images of 13 animal types. Figure. Animal types in NUS Wide Animal dataset. Now, given a new image like the image of a cat, can we fetch all cat images from the dataset? Apparently yes! with the help of distance measurements this is possible. Types of distance measurements Distance measurements are very important in machine learning problems. Let’s look at different types of distance measurements. Euclidean distance Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday) space. For any two data instances, represented by d{"version":"1.1","math":"\(d\)"}-dimensional feature vectors xi,xj{"version":"1.1","math":"\(\textbf{x}_i,\textbf{x}_j\)"} their Euclidean distance is computed as:  dEuclidean(xi,xj)=((xi,1−xj,1)2+...+(xi,D−xj,D)2)12{"version":"1.1","math":"d_{Euclidean}(\textbf{x}_i,\textbf{x}_j) = \big ( (\textbf{x}_{i,1} - \textbf{x}_{j,1})^2+...+(\textbf{x}_{i,D} - \textbf{x}_{j,D})^2)^{\frac{1}{2}}"} For example, consider these two vectors: x1=[11210]Tandx2=[02202]T{"version":"1.1","math":"\textbf{x}_1 = \begin{bmatrix} 1\\ 1\\ 2\\ 1\\ 0 \end{bmatrix}^T \mathrm{and}\quad \textbf{x}_2 = \begin{bmatrix} 0\\ 2\\ 2\\ 0\\ 2 \end{bmatrix}^T"} We can calculate the distance as: =(1−0)2+(1−2)2+(2−2)2+(1−0)2+(0−2)2{"version":"1.1","math":"=\sqrt{(1-0)^2+(1-2)^2+(2-2)^2+(1-0)^2+(0-2)^2}"} =1+1+0+1+4{"version":"1.1","math":"=\sqrt{1+1+0+1+4}"} =7≈2.65{"version":"1.1","math":"= \sqrt{7} \approx 2.65"} Cosine distance We previously introduced cosine distance in course 1. But as a reminder, we define Cosine distance for any two data instance represented byd−{"version":"1.1","math":"\(d-\)"} dimensional feature vectors xi,xj{"version":"1.1","math":"\(\textbf{x}_i,\textbf{x}_j\)"}.  The Cosine distance for these two feature vectors are computed as: dCosine(xi,xj)=1−xiTxj||xi||2.||xj||2{"version":"1.1","math":"\(d_{Cosine}(\textbf{x}_i,\textbf{x}_j) = 1-\frac{\textbf{x}_i^T \textbf{x}_j}{||\textbf{x}_i||_2.||\textbf{x}_j||_2}\)"} Lets see an example: x1=[11210]Tandx2=[02202]T{"version":"1.1","math":"\textbf{x}_1 = \begin{bmatrix} 1\\ 1\\ 2\\ 1\\ 0 \end{bmatrix}^T \mathrm{and}\quad \textbf{x}_2 = \begin{bmatrix} 0\\ 2\\ 2\\ 0\\ 2 \end{bmatrix}^T"} We can calculate the cosine distance as: =1−(1×0+1×2+2×2+1×0+0×2)(12+12+22+12+02)×(02+22+22+02+22){"version":"1.1","math":"= 1 - \frac{(1\times0+1\times2+2\times2+1\times0+0\times2)} {\sqrt{(1^2+1^2+2^2+1^2+0^2)}\times \sqrt{(0^2+2^2+2^2+0^2+2^2)}}"} =0.3453{"version":"1.1","math":"=0.3453"} Mahalanobis distance The Mahalanobis distance (MD) is the distance between two points in multivariate space. For any two data instances, represented by d−{"version":"1.1","math":"\(d-\)"} dimensional feature vectors xi,xj{"version":"1.1","math":"\(\textbf{x}_i,\textbf{x}_j\)"} their Mahalanobis distance is computed as: dMahalanobis(xi,xj)=(xi−xj)M−1(xi−xj)T{"version":"1.1","math":"d_{Mahalanobis}(\textbf{x}_i,\textbf{x}_j) = (\textbf{x}_i-\textbf{x}_j)M^{-1}(\textbf{x}_i-\textbf{x}_j)^T"} Where M{"version":"1.1","math":"\(M\)"} is the covariance matrix of the data. Intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. In other words, the elements of covariance matrix in the i,j{"version":"1.1","math":"\(i,j\)"} position is the covariance between the i−th{"version":"1.1","math":"\(i-th\)"} andj−th{"version":"1.1","math":"\(j-th\)"} elements of a vector. We will later explain covariance matrix in more detail. Mahalanobis distance can be thought of scaling each data dimension by its variance and adjusting for their relationships. When data are independent, i.e. M=I{"version":"1.1","math":"\(M=I\)"} (identity matrix), Mahalanobis distance becomes same as Euclidean distance. Cityblock/Manhattan distance For any two data instances, represented by d−{"version":"1.1","math":"\(d-\)"} dimensional feature vectors xi,xj{"version":"1.1","math":"\(\textbf{x}_i,\textbf{x}_j\)"}, their Cityblock distance is computed as: dCityblock(xi,xj)=|xi,1−xj,1|+...+|xi,D−xj,D|{"version":"1.1","math":"d_{Cityblock}(\textbf{x}_i,\textbf{x}_j) = |\textbf{x}_{i,1} - \textbf{x}_{j,1}|+...+|\textbf{x}_{i,D} - \textbf{x}_{j,D}|"} In most cases, this distance measure yields results similar to the Euclidean distance. However, using City block distance, the effect of a large difference in a single dimension is dampened (since the distances are not squared). Let us compute the Cityblock/Manhattan Distance for two sample vectors: x1=[11210]Tandx2=[02202]T{"version":"1.1","math":"\(\textbf{x}_1 = \begin{bmatrix} 1\\ 1\\ 2\\ 1\\ 0 \end{bmatrix}^T \mathrm{and}\quad \textbf{x}_2 = \begin{bmatrix} 0\\ 2\\ 2\\ 0\\ 2 \end{bmatrix}^T\)"} Cityblock distance=|1−0|+|1−2|+|2−2|+|1−0|+|0−2|=5{"version":"1.1","math":"Cityblock\ distance = |1-0|+|1-2|+|2-2|+|1-0|+|0-2| = 5"} Minkowski distance The Minkowski distance defines a distance between two points in a normed vector space. Think of Euclidean distance (2 norm of xi−xj{"version":"1.1","math":"\(\textbf{x}_i-\textbf{x}_j\)"}) and Cityblock distance as (1 norm of xi−xi{"version":"1.1","math":"\(\textbf{x}_i - \textbf{x}_i\)"}) Minkowski distance is a generalization of these distances defined for any p{"version":"1.1","math":"\(p\)"}-norm. d(x,y)=(∑i=0n−1|xi−yi|p)1p{"version":"1.1","math":"d(\textbf{x},\textbf{y}) = (\sum_{i=0}^{n-1} |x_i - y_i|^p)^\frac{1}{p}"} When p=1{"version":"1.1","math":"\(p=1\)"}, the distance is known as the Manhattan distance. When p=2{"version":"1.1","math":"\(p=2\)"} the distance is known as the Euclidean distance. Reminder: If you do not clearly remember the definition of norm, you may want to have a review of topics 1 and 2. Jaccard distance Note: there was a small error in this equation that has been fixed. The Jaccard distance is a distance used to measure diversity of any two sets. Consider any two instances xi{"version":"1.1","math":"\(\textbf{x}_i\)"} andxj{"version":"1.1","math":"\(\textbf{x}_j\)"} as binary vectors indicating presence or absence of features. Jaccard distance between xi{"version":"1.1","math":"\(\textbf{x}_i\)"} and xj{"version":"1.1","math":"\(\textbf{x}_j\)"} is defined as: dJaccard(xi,xj)=1−∣xi∩xj∣1∣xi∪xjr|1{"version":"1.1","math":"d_{Jaccard}(\textbf{x}_i,\textbf{x}_j) = 1 - \frac{\lvert\textbf{x}_i \cap \textbf{x}_j\rvert_1}{\lvert\textbf{x}_i \cup \textbf{x}_jr\vert_1}"} Where ∩{"version":"1.1","math":"\(\cap\)"} denotes logical and while ∪{"version":"1.1","math":"\(\cup\)"} denotes logical or operators. The ∣x∣1{"version":"1.1","math":"\(\lvert\textbf{x}\rvert_1\)"} is 1-norm. Let us assume that xi=[1,0,1]{"version":"1.1","math":"\(\textbf{x}_i = [1,0,1]\)"} and xj=[1,1,0]{"version":"1.1","math":"\(\textbf{x}_j = [1,1,0]\)"}, the Jaccard distance calculated as:  dJaccard(xi,xj)=1−13=23{"version":"1.1","math":"d_{Jaccard}(\textbf{x}_i,\textbf{x}_j) = 1 - \frac{1}{3} = \frac{2}{3}"} Note that: Dimensionality in Data There are many issues that arise when analysing and organising data in high-dimensional spaces. Let us look at some typical dimensions of the data we’re dealing with: Text data: Imagine a News website. If you start crawling the news on the website for a short period of time such as a topic, depending on the number of documents you crawl, it is typical to have more than 10,000 dimensions. This number is the size of the dictionary you have to build based on the words you extracted from the News documents. We need to represent each document based on the words in a dictionary (remember. the feature vector covered in topics 1 and 2: Data representation). Image data: Imagine we would like to use pixels as features, just an 64×64{"version":"1.1","math":"\(64\times64\)"} image would have 4,096 dimensions! Genomic data: Take Parkinsons disease case-control data as an example. It has 408,803  Single-nucleotide polymorphisms (SNPs) and Alzheimer’s disease has 380,157 SNPs. Now that we’ve learned about clustering methods and Kmeans, it is time to work on evaluation methods of clustering results. Is this a good way of clustering this data? In this video we review how the clustering evaluation method purity can be used to evaluate the outcomes of a clustering experiment that has produced 3 centroids. It is a very simple example which highlights the fundamental principles of how the algorithm works. As has been mentioned before, all machine learning algorithms are required to be evaluated. Are the clusters useful? Evaluation of clustering methods is not easy. But, generally there are two main categories of evaluation methods for clustering: External assessment:compare clustering performance against a known clustering (often called Ground truth or Gold standard). Internal assessment:determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.). Examples:Silhouette coefficient, Dunn index etc. The following figure illustrates a sample of ground truth (C{"version":"1.1","math":"\(C\)"}) and the clustering partition found by a clustering algorithm C′{"version":"1.1","math":"\(C^\prime\)"}. Figure. Ground truth VS Clustering partition found by algorithms Rand Index The Rand index, is a measure of the similarity between two data clusters. We have the assignments of data instances to different clusters suggested by a clustering algorithm (say C′{"version":"1.1","math":"\(C^\prime\)"}). In external assessment, we have knowledge of the ground truth cluster assignments. (say C{"version":"1.1","math":"\(C\)"}) The Rand index is a function that measures the similarity of the two assignments C{"version":"1.1","math":"\(C\)"} and C′{"version":"1.1","math":"\(C^\prime\)"}, ignoring their permutations. The Rand index is computed as R=a+ba+b+c+d=a+b(n2){"version":"1.1","math":"\(R = \frac{a+b}{a+b+c+d} = \frac{a+b}{\left(\begin{array}{c} n \\ 2 \end{array}\right)}\)"}, where a{"version":"1.1","math":"\(a\)"}= the number of pairs of data instances that are in the same cluster in both C{"version":"1.1","math":"\(C\)"}, C′{"version":"1.1","math":"\(C^\prime\)"}. b{"version":"1.1","math":"\(b\)"} = the number of pairs of data instances that are in the different clusters in C{"version":"1.1","math":"\(C\)"} and in different clusters in C′{"version":"1.1","math":"\(C^\prime\)"}. c{"version":"1.1","math":"\(c\)"} = the number of pairs of data instances that are in the same cluster in C{"version":"1.1","math":"\(C\)"} but in different clusters in C′{"version":"1.1","math":"\(C^\prime\)"}. d{"version":"1.1","math":"\(d\)"} = the number of pairs of data instances that are in the different clusters in C{"version":"1.1","math":"\(C\)"} but in the same clusters in C′{"version":"1.1","math":"\(C^\prime\)"}. The adjusted rand index is the corrected-for-chance version of the Rand index. In other words the index takes chance into account and corrects any bias introduced by chance. Purity In evaluation methods of clustering, it is common practice to use more than one approach for evaluation because neither of the evaluation methods are comprehensive enough. Purity is a way of quality measurement in clustering methods. As the name suggests, we would like to measure the purity for all clusters in terms of class labels of the data in each cluster. Consider the following figure as an example. Figure. Three obtained clusters for three types of data: Cross, Circle, and Plus. Each cluster is assigned to the class label which has the majority in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances. Based on the figure, the first cluster has 5 crosses and 1 circle, so the majority of the labels are cross. For the next cluster, we have 4 circles and 1 cross and 1 plus, so circle has the majority. And as for the last one we can see 3 pluses and 2 crosses which result in majority of pluses. Now we can calculate the Purity measurement of these obtained clusters as: Purity=117×(5+4+3)≈0.71{"version":"1.1","math":"Purity = \frac{1}{17} \times (5+4+3) \approx 0.71"} Which results in approximately 71%{"version":"1.1","math":"\(71 \%\)"} purity. Now let us explain one of the disadvantages of this evaluation method. Consider the example we just solved, what if a particular outcome groups the points into 17{"version":"1.1","math":"\(17\)"} clusters? One cluster for each point. It may not sound like a clustering approach but in this case the purity measurement would result in 100%{"version":"1.1","math":"\(100 \%\)"}  purity since there is only one single point in each cluster. But it does not make sense. Why? This is why we have to make sure that we are selecting a fair number of clusters when we perform clustering on a set of data points. Mutual Information Mutual information is one of the most popular approaches in analysis of clustering. It measure the agreement between two clustering assignments such as C{"version":"1.1","math":"\(C\)"} and C′{"version":"1.1","math":"\(C^\prime\)"}, so the aim is almost same as the Rand Index. In mutual information the main question is how informative is C{"version":"1.1","math":"\(C\)"} about C′{"version":"1.1","math":"\(C^\prime\)"} or C′{"version":"1.1","math":"\(C^\prime\)"} about C{"version":"1.1","math":"\(C\)"}. How similar are they and are they similar in a useful way? What can you tall about C{"version":"1.1","math":"\(C\)"} if you look at C′{"version":"1.1","math":"\(C^\prime\)"}? Mutual information is a function that measures the agreement of the two clustering assignments C{"version":"1.1","math":"\(C\)"} and C′{"version":"1.1","math":"\(C^\prime\)"} in terms of how informative one is about the other, ignoring permutations. To put it simple, how informative is C{"version":"1.1","math":"\(C\)"} about C′{"version":"1.1","math":"\(C^\prime\)"}.  Let’s assume that clustering partition C{"version":"1.1","math":"\(C\)"} has K{"version":"1.1","math":"\(K\)"} clusters and the partition C′{"version":"1.1","math":"\(C^\prime\)"} has K′{"version":"1.1","math":"\(K^\prime\)"} clusters, in this case the Mutual Information of these two clustering assignments are computed as:  MI(C,C′)=∑i=1K∑j=1K′P(i,j)logP(i,j)P(i)P′(j){"version":"1.1","math":"MI(C,C^\prime) = \sum_{i=1}^{K}\sum_{j=1}^{K^\prime} P(i,j) log \frac{P(i,j)}{P(i)P^\prime(j)}"} Where P(i){"version":"1.1","math":"\(P(i)\)"} denoted the probability of randomly selected instance to belong to i−{"version":"1.1","math":"\(i-\)"}th cluster of the partition C{"version":"1.1","math":"\(C\)"}. P′(j){"version":"1.1","math":"\(P^\prime(j)\)"} and P(i,j){"version":"1.1","math":"\(P(i,j)\)"} are also similarly defined. As you can see another advantage of mutual information is the number of clusters which is found by C{"version":"1.1","math":"\(C\)"} and C′{"version":"1.1","math":"\(C^\prime\)"} are not required to be exactly the same. Assume that we have the ground truth clusters and the corresponding assignments (C{"version":"1.1","math":"\(C\)"}) with K{"version":"1.1","math":"\(K\)"} clusters. In this case, we also performed a clustering method such as C′{"version":"1.1","math":"\(C^\prime\)"} and we obtainedK′{"version":"1.1","math":"\(K^\prime\)"} clusters. Then we can calculate the mutual information of MI(C,C′){"version":"1.1","math":"\(MI(C,C^\prime)\)"}. So if our C′{"version":"1.1","math":"\(C^\prime\)"} clustering is highly informative based on C{"version":"1.1","math":"\(C\)"} we can conclude that the C{"version":"1.1","math":"\(C\)"} clustering method is doing well. Silhouette Coefficient The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments. The silhouette coefficient contrasts the average distance between the instances of the same cluster with the average distance between the instances of different clusters: s(i)=b(i)−a(i)max{a(i),b(i)}{"version":"1.1","math":"s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}"} In the above formula, a(i){"version":"1.1","math":"\(a(i)\)"} is the average distance of i−{"version":"1.1","math":"\(i-\)"}th instance with all other instances of the same cluster, and b(i){"version":"1.1","math":"\(b(i)\)"} is the lowest average dissimilarity of i−{"version":"1.1","math":"\(i-\)"}th instance with all other clusters. The final value of Silhouette calculation ranges from −1{"version":"1.1","math":"\(-1\)"} to +1{"version":"1.1","math":"\(+1\)"}. A high value of Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. If most objects have a high value, then the clustering configuration is appropriate. On the other hand, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters. View transcript SPEAKER 1: In this tutorial, you're going to see an example of evaluating a clustering algorithm on a simple problem. As we have said before, a good evaluation of your trained method is obligatory in machine learning. Let's say you ran a clustering algorithm on some data, and this is your output. So you clustered that data into three clusters, which the first one is this one, the second one is this, and this is the last one. Now how can we evaluate the performance of your clustering method? In this example, we're going to talk about purity or homogeneity, which is one of the evaluation methods of clustering. Purity says each cluster is assigned to the class which has the majority in the cluster. In this video we practise how to calculate some of the distance measurements that were introduced previously. This is a great way to revise and consolidate your learning of these concepts. It explains how the calculations is implemented as well as where each measurement potentially can be used. You will see the following distance measures being demonstrated: Euclidean distance Cosine distance Mahalanobis distance Cityblock/Manhattan distance Minkowski distance Jaccard distance View transcript SPEAKER 1: In this tutorial, we're going to show you the different measurements of distance. The Euclidean distance is the first one. For any 2 data instances represented by d-dimensional feature vectors xi and xj, the Euclidean distance is computed as this formula. So you just need to subtract the values in each dimension and take them to power 2. Consider this example. x1 is 1, 1, 2, 1, 0, and x2 is 0, 2, 2, 0, 2. So first, we subtract 1, 0, power 2. Again, 1 to power 2. 2 to the power of 2, 1, 0 power 2, and 0 minus 2, power of 2. So the root square of this value is the Euclidean distance. The Euclidean distance is appropriate when we have continuous numerical variables and we want to reflect absolute distances. The next distance is cosine distance. For any 2 data instances represented by d-dimensional feature vectors xi and xj, the cosine distance is computed as xi transposed, xj, and the norm 2 of xi and the norm 2 of xj. Consider this example. As you can see in here, the xi transpose xj would be the multiplication of these values. 1 times 0, 1 plus 1 times 2, 2 times 2, and so on. Also the norm 2 of this vector is just 1 times 1, plus 1 times 1, plus 2 times 2, 1 times 1, and 0 times 0. And also, the square root of this 1 multiplied by the norm 2 of these vector. So then as you can see, the final value of cosine distance for these vectors is 0.3453. The cosine similarity is generally used as a metric for measuring distances when the magnitude of the vector does not matter. It's basically in here we are seeking for the angle between 2 vectors. The next distance we are going to see is Mahalanobis distance. Consider any 2 data points represented by d-dimensional feature vectors xi and xj. The Mahalanobis distance is computed as this, which is a pretty simple formula, xi minus xj, the inverse of the covariance matrix, and xi minus xj transposed. Consider this example. First, look at these data points. This is the independent variable 1, and this is the independent variable 2. The mean of this data is 500, 500, and it should be a point around here. So we would like to find the distance of x, which is 410 and 400 in 2d, from the mean of this data. So first we need to construct x minus m, which is 410 minus 500, 400 minus 500, which is -90 and -100. Also we find the covariance matrix between these 2 vectors, which is something like this. So we can find the Mahalanobis distance by multiplying x minus m, which is -90, -100, to covariance matrix, and then again multiplied by the transpose of this vector, which as you can see, finally, the value is 1.825. The Mahalanobis distance is appropriate when we have continuous numerical variables, and we want to reflect absolute distance, but we want to remove redundancies. If we have repeated variables, there repetitious effect will disappear. The next distance is Cityblock, or Manhattan distance. For any 2 data instances represented by d-dimensional feature, vector xi and vector xj, the their Cityblock distance is computed as this, which is pretty simple and is the absolute difference of xi in dimension 1 minus xj in dimension 1, and so on until dimension d. Consider this simple example. So as you can see, the Manhattan distance is just 1 minus 0, absolute value, plus 1 minus 2, and so on. So as you can see, the final value of Manhattan distance is 5. The Minkowski Distance is a generalisation of these 2 distances we have just seen, the Euclidean distance and Cityblock distance. So as you remember, the Euclidean distance was in norm 2, and the Cityblock distances is in norm 1. So the Minkowski distance is a generalisation of these distances defined for norm p. So it's basically the same concept. The last distance is Jaccard distance. The Jaccard distance is a distance used to measure diversity of any 2 sets. So it's very important that Jaccard distance is used to measure the diversity of sets. So now we are talking about sets. Consider any 2 instances, such as xi and xj, as the binary vectors. The most popular clustering algorithm in machine learning is called K-means. It is simple and fast. In the video we explore how K-means works on an unlabelled data set. If maths is not your thing, skip to 2.34 and watch the example. If you become a data scientist or you want top marks you will need to dig in to the maths but right now it’s important to understand the concept. In this algorithm, k represents the centre points of clusters. You start off with these centroids and then measure teach data point to find its closest centroid. In other words, K-means stores k{"version":"1.1","math":"\(k\)"} centroids for defining clusters. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-means searches for the best centroids by alternating between two methods: Assigning data points to clusters based on the current defined centroids (points which are the centre of a cluster). Choosing centroids based on the current assignment of data points to clusters. Step 1 and 2 repeat until you find a useful grouping of data points. This method was independently discovered in 60s and 70s. Due to its reasonably accurate performance, this method still remains the algorithm of choice for analysis tasks. View transcript References Steinhaus, H 1956, Sur la division des corp materiels en parties. Bulletin of acad. polon. sci., IV(C1. III), 801–804. Lloyd, S 1982, Least squares quantization in PCM, IEEE transactions on information theory, 28, 129–137. Originally as an unpublished Bell laboratories Technical Note (1957). K-means clustering is a method for finding clusters and cluster centres in a set of unlabelled data. Watch the video and follow the Python code to understand how to implement KMeans clustering using the Scikit learn package. Intuitively, we can think of a cluster as comprising a group of data points whose distances within the cluster are small compared with the distances to points outside of the cluster. The Scikit learn package contains an implementation for KMeans. KMeans Load CSV file for KMeans clustering Download  digitdata0.csv and digitData1.csv import pandas as pd import numpy as np digit_zero=pd.read_csv("data/digitData0.csv",header=None) digit_one=pd.read_csv("data/digitData1.csv",header=None) digit_zero.shape digit_zero.head() The output of the code is as follows: Merge two data frames and rename the header name  Feature_set=pd.concat([digit_zero,digit_one],join="inner") Feature_set.columns=["feature_"+str(i+1) for i in range(Feature_set.shape[1]-1)] Feature_set.head() The output of the above code: Target as a ground truth of KMean clustering cols=["feature_"+str(i+1) for i in range(Feature_set.shape[1]-1)] cols.append("Target") Feature_set.columns=cols Feature_set.head() The output of the above code: How to define a number of clusters, K? Elbow methodThe elbow method shows the optimum k value from a range of values using distance metrics. from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from yellowbrick.cluster import KElbowVisualizer # Generate synthetic dataset with 8 random clusters X=Feature_set.iloc[:,0:-1] y=Feature_set.iloc[:,-1] # Instantiate the clustering model and visualizer model = KMeans() visualizer = KElbowVisualizer(     model, k=(2,12), metric='distortion', timings=False ) #distortion same as Euclidean distance visualizer.fit(X)        # Fit the data to the visualizer visualizer.show()  Kmeans with Kmeans++ Kmeans++ is an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm. As we have said before, K-means starts with allocating cluster centres randomly and then looks for better solutions. K-means++ starts with allocating one cluster centre randomly and then searches for other centres given the first one. So both algorithms use random initialisation as a starting point but in different ways. So Kmeans++: Chooses one centroid μ1{"version":"1.1","math":"\(\mu_1\)"}  uniformly at random from the dataset Let D(x){"version":"1.1","math":"\(D(x)\)"} be the shortest distance from a data point to the closest centroid we have already chosen. Choose a new centroid from the dataset with probability of D2(xi)∑iD2(xi){"version":"1.1","math":"\(\frac{D^2(x_i)}{\sum_{i} D^2(x_i)}\)"} Now repeat the previous step until we have initialised K{"version":"1.1","math":"\(K\)"} centroids For further understanding of Kmeans you can read the article k-means++: The Advantages of Careful Seeding by Arther and Vassilviskii. Guarantee of Kmeans++ In a Kmeans algorithm with a random starting number of centroids, the objective function monotonically decreases with each iteration of the algorithm. In other words every time the algorithm runs, it gets closer to (and not further away from) the best solution. Let us say, for the best solution, the objective function takes value joptimum{"version":"1.1","math":"\(j_{optimum}\)"} Let us say, when using Kmeans the objective function converges to jconverged{"version":"1.1","math":"\(j_{converged}\)"} Kmeans clustering has a number of limitations. Watch the video to understand some of Kmeans limitations and possible solutions. The most important limitations of simple Kmeans are: Random initialisation means that you may get different clusters each time. As a solution, we can use a Kmeans++ initialisation algorithm to initialise better. We have to supply the number of clusters beforehand. We can use the Elbow method to choose K{"version":"1.1","math":"\(K\)"}, but it may not be straightforward. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.) Finding a useful number of clusters As we have mentioned before, one of the challenges of Kmeans is the assumption we have to make about the number of clusters to start with. The Elbow Method is a method for finding the appropriate number of clusters. The Elbow method interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset. Elbow Method Watch the video for an introduction to this concept. The idea of elbow method is to run the Kmeans clustering algorithm for a range of values of K{"version":"1.1","math":"\(K\)"},  compute the sum of squared error (SSE) as: SSEK=∑i=1n∑k=1Kzik||xi−μk||2{"version":"1.1","math":"SSE_K = \sum_{i=1}^{n} \sum_{k=1}^{K} z_{ik} ||x_i - \mu_k||^2"} So ∥xi−μk∥2{"version":"1.1","math":"\(\|x_i - \mu_k\|^2\)"} finds the distance of each point (xi{"version":"1.1","math":"\(x_i\)"}) to its corresponding centroid (μk{"version":"1.1","math":"\(\mu_k\)"}) in the cluster. zik{"version":"1.1","math":"\(z_{ik}\)"} is a binary variable which is 1{"version":"1.1","math":"\(1\)"} when xi{"version":"1.1","math":"\(x_i\)"} is assigned to the cluster number k{"version":"1.1","math":"\(k\)"} and it is 0{"version":"1.1","math":"\(0\)"} when xi{"version":"1.1","math":"\(x_i\)"} is not related to cluster number k{"version":"1.1","math":"\(k\)"}.  As you can see in the following figure (left image), based on the elbow method, it looks 6 is the best cluster number for this case. Figure. Illustration of Elbow method with different number of clusters.* Sometimes we might not be able to discern the elbow shape based on different numbers of clusters. In this case, the elbow method cannot help for obtaining the best value of k{"version":"1.1","math":"\(k\)"}. View transcript SPEAKER 1: In this tutorial, we're going to talk about K in Kmeans. As you know, we're required to specify K when we would like to perform Kmeans. But how challenging is it to find K? Consider these data points. As you can see, we have a specified two clusters. K equals the two four these data points. So we have one centroid in here, another centroid in there. So probably, these two groups will add up in one cluster. And these three groups will add up in another cluster. If we specified three clusters, we're going to have something like this. So this group is a cluster. These two groups will be another cluster. Also the remaining two is the last cluster. Now what if we increase the number of clusters to four? As you can see, these two groups will end up to be one cluster. And the remaining groups will be each one cluster. Now if you jump from five and go directly to six, we can see we have one centroid or one cluster for each of these groups, but two clusters or centroids for this one, which looks to be redundancy because they look to be only in one group. But if you select five clusters, we can see it really makes sense. So we have a centroid and cluster for each group of the data. Also, you can see in here the distribution of data points and the cluster we found. This plot shows the value of the objective function, which we already talked about with respect to the number of clusters. As you know this, value will reduce as the number of clusters increases, because the value of inter-intro distances will be reduced. But how can we find the best number of clusters, because usually it's not possible to illustrate or visualise data like this? There is a method called elbow method. This method is used for finding the number of cluster. The idea of elbow method is to run Kmeans clustering algorithm for a range of values of K. And for each value of K, we will compute the sum of a squared error as this. Sum of a squared error is exactly same as the J or the objective function we had in Kmeans. So as you can see, we're calculating the distance of Xi from the centroid of its cluster which it belongs to. Also we have zed IK as a binary value, which is 1 when Xi belongs to a cluster K. And it's 0 when Xi does not belong to cluster K.       This module is optional for SIT307 students.  SIT720 student must complete this module.  Other clustering algorithms Kmeans is one of the most popular clustering methods in machine learning but it is not the only clustering algorithm. In the category of Flat Clustering where the goal of the algorithm is to create clusters that are coherent internally but clearly different from each other, there are two more clustering methods: Kmeans (as we know) Hierarchical clustering DBSCAN (density based) Shape-based  Clustering If you’re interesting in extending your knowledge there are articles attached below on DBSCAN and Spectral clustering. Hierarchical clustering There is another type of clustering algorithms called hierarchical clustering. These algorithms find clusters that have a predetermined order. There are two types of hierarchical clustering: one of them is a bottom-up approach and the other is top-down: Agglomerative clustering (bottom-up):A “bottom up” approach in which each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. Divisive clustering (top-down):A “top down” approach in which all observations start in one cluster, and splits are performed as one moves down the hierarchy. In this type of clustering, you end up with an hierarchical tree diagram or dendrogram. Cutting the tree at a different height will produce a selected precision of clustering. Let’s look at this more closely. Agglomerative clustering Consider the following figure as an example. At the bottom of the tree, at the starting point each of the characters p,q,r,s,t{"version":"1.1","math":"\(p,q,r,s,t\)"}  are assigned into a single separate cluster. Figure. Agglomerative Clustering As we go up to the higher levels, the closest characters are formed another cluster. i.e. s,t{"version":"1.1","math":"\(s,t\)"} and p,q{"version":"1.1","math":"\(p,q\)"}. At the next level we can notice r,s,t{"version":"1.1","math":"\(r,s,t\)"} are making a cluster. And finally at the top of the tree, all the characters are in one single cluster p,q,r,s,t{"version":"1.1","math":"\(p,q,r,s,t\)"}.  So in In Agglomerative or bottom-up clustering method we assign each observation to its own cluster. Then, compute the similarity (e.g., distance) between each of the clusters and join the two most similar clusters. We do this until we get to the top of the tree. Cutting the tree at a given height will give a partitioning clustering at a selected precision. If you cut the tree at deeper levels, you will get more clusters than cutting in the tree in higher levels. But the question which arises here is: How to find the closest cluster pairs? i.e. how to find the distance between two sub-clusters in the middle of the tree? For example what is the distance of s,t{"version":"1.1","math":"\(s,t\)"} cluster and p,q{"version":"1.1","math":"\(p,q\)"} cluster? We are mentioning four possible ways you can use for finding the distances: Single-link: the distance between closest points Complete-link: the distance between the furthest points Centroid: the distance between the Centroids Average-link: the average distance between pairs of elements from across cluster pairs Divisive clustering Similar to Agglomerative clustering in this method all data instances are put in the same cluster For splitting, we can use any clustering algorithm that produces at least two clusters (e.g. Kmeans) The process is continued until each data instance is separate and assigned to its own cluster DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that clusters certain items in a group based on a given data point.  For this, we need to set a minimum number of data points (minPts) and a distance (dis). Because these parameters are user-defined, the resulting cluster is dependent on them.  Calculate the distance from each point in the dataset to every other point. A point is considered a "core point" if it has at least the same number of data points within the defined distance.  Data points which cannot be considered core points but are under the defined distance of the core point are called border points. All other points are regarded as "noise."  The next step is to combine all core and border points within dis of each other into a single cluster.   We keep repeating the above steps until we reach the   Shape-based clustering, VAT, iVAT VAT is a visualization technique that transforms the distance matrix of a dataset into a visual representation in the form of a re-ordered matrix. The re-ordering is done in such a way that the dissimilarities between the data points are emphasized in a way that reveals the underlying clustering structure of the data. If the data has a clear clustering structure, then the re-ordered matrix will exhibit block-like structures along the diagonal, indicating the presence of clusters. iVAT is an extension of VAT that involves repeatedly applying the VAT algorithm to the re-ordered matrix in order to refine the clustering structure. The iVAT algorithm iteratively computes the VAT on the re-ordered matrix until a stable clustering structure is obtained. This can help to identify the optimal number of clusters in the data. Both VAT and iVAT are useful tools for exploratory data analysis, allowing data analysts to gain insight into the underlying structure of the data and to identify the appropriate number of clusters for subsequent clustering algorithms. References: Bezdek, James C., and Richard J. Hathaway. "VAT: A tool for visual assessment of (cluster) tendency." Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No. 02CH37290). Vol. 3. IEEE, 2002. Havens, Timothy C., and James C. Bezdek. "An efficient formulation of the improved visual assessment of cluster tendency (iVAT) algorithm." IEEE Transactions on Knowledge and Data Engineering 24.5 (2011): 813-822. Reference Erman, Nusa & Korosec, Ales & Suklan, Jana. (2015). ‘Performance Of Selected Agglomerative Hierarchical Clustering Methods’. Innovative Issues and Approaches in Social Sciences. 8. 180-204. 10.12959/issn.1855-0541.IIASS-2015-no1-art11. SEE ALSO from sklearn.cluster import KMeans from sklearn.datasets import make_blobs from yellowbrick.cluster import KElbowVisualizer # Generate synthetic dataset with 8 random clusters X=Feature_set.iloc[:,0:-1] y_true=Feature_set.iloc[:,-1] model = KMeans(n_clusters=2) y_pred=model.fit_predict(X) performance metric Purity Mutual Information Silhouette Coefficient f1 recall accuracy precision from sklearn.metrics import classification_report print(classification_report(y_true, y_pred)) Performance of the KMeans: Purity from sklearn import metrics def purity_score(y_true, y_pred):     # compute contingency matrix (also called confusion matrix)     contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)     # return purity     return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)  print(     "purity_score: %0.3f"     % purity_score(y_true, y_pred) ) 