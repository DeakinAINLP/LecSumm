Various aspects of unsupervised learning such as clustering and dimensionality reduction are studied. When the dataset contains unsupervised data, the process of identifying patterns, grouping various features, and analysing the data becomes complex. This is when clustering comes into picture. In other words, clustering is used to group or classify unlabelled or unsupervised data.  One of the important factors in machine learning is finding the distance between data points. These metrics which are used in machine learning are called Distance metrics. Nearest neighbour classification is the process in which the nearest neighbour point based on the distance is identified and this point is then given the class label. Image Retrieval is also possible with the help of distance metrics.  Various distance measurements and their properties are discussed as follow.  Euclidean distance is a distance measurement technique where there is a straight line between two points that gives the distance between them.  Cosine distance measures the cosine of angle between two vectors.  Mahalanobis distance is the measurement of point p and the distribution D between the two points considered for measurement. It can also be explained as the distance between two multiverse spaces.  Cityblock/Manhattan is the distance measured by calculating right angle between two axes.  Minkowski distance is the normed vector space and is a generalization of Euclidean distance and Manhattan distance.  Jaccard distance is calculated by subtracting Jaccard coefficient from one and is used to calculate the dissimilarity between two data points.  Clustering and its application  Clustering is the process of classifying unlabelled data according to various factors for example based on distance between data points, that is the distance metrics in such a way that all the data in one cluster is similar in some way when compared to other cluster that is defined. For example, clustering can be used to classify a dataset of vehicles. That is cars can be separated in one cluster and trucks can be grouped into one cluster. Clustering has vast range of applications.  Various parameters associated with clustering are studied. Rand index is one of them which measures the similarity between two indices. Purity is another measure of clustering that is a way of measuring the quality of clusters. Mutual information is like rand index, but the number of clusters need not be equal for this. Silhouette Coefficient high value means the cluster is well matched to its own cluster and poorly matched to its neighbouring cluster.  Limitations of K means are discussed. This includes random initialization of data while clustering, the number of clusters needs to be predefined, it is difficult to locate clusters with random shape and noisy data points are not detected. To overcome the problem of initialising number of clusters various methods like elbow method, K-mean, k-mean++, Hierarchical clustering, DBSCAN, Shape based clustering, VAT and IVAT. Elbow method is used to find the number of clusters that have a predetermined order. K-mean and k mean++ is used to find the number of clusters in a dataset. Hierarchical clustering finds clusters that have a pre-determined order. Two types of Hierarchical clustering methods are present. One is bottom up and the other is top-down tree method. DBSCAN is a density-based clustering where parameters are user defined and resulting cluster is dependent. Shape based clustering, VAT is a rearranging matrix, and I VAT is a process where VAT process is repeated multiple times. This way any dataset which is not classified is grouped successfully and can be used for further analysis and processing.  