Learning summary  Topic 3: Clustering  Learning objective:  -  Use clustering for revealing patterns from unlabeled data - -  Apply suitable clustering/dimensionality reduction techniques to perform unsupervised learning  Learn techniques to reduce dimensionality  of data in a real-world scenario.  Learning summary:  -  Measuring distances: use clustering for revealing patterns from unlabeled data learn techniques to reduce dimensionality apply suitable clustering/dimensionality reduction techniques to perform unsupervised learning of data in a real-world scenario. use clustering for revealing patterns from unlabeled data learn techniques to reduce dimensionality apply suitable clustering/dimensionality reduction techniques to perform unsupervised learning of data in a real-world scenario.  Distance metrics: Distance measures are functions that define a distance d(xi, xj)  between any two data instances xi and xj for measuring how similar the instances are.  The most related examples in ML:  ▪  clustering algorithms, K-Nearest-Neighbor, Support Vector Machines (SVM), data visualization, information retrieval, ranking  There are three properties in distance measures:  For any instance xi, distance with itself is zero. That is, d(xi, xj) = 0 For an instance pairs xi and xj, the distance is non-negative and symmetric. That  is, d(xi, xj) ≥ 0 and d(xi, xj) = d(xj, xi).  Distance measures follow triangular inequality: d(xi, xk) ≤ d(xi, xj) + d(xj, xk).  -  There are different types of distance measurements:  Euclidean distance: Euclidean distance is the ordinary straight-line distance between two  points in Euclidean (everyday) space. This could be computed as:  dEucliden(xi, xj)2 = d(xi,1 - xj,1)2 + … + d(xi,D – xj,D)2  Cosine distance: We previously introduced cosine distance in course 1. But as a  reminder, we define Cosine distance for any two data instances represented by d- dimensional feature vectors xi, xj. The Cosine distance for these two feature vectors are computed as:  Mahalanobis distance:  The Mahalanobis distance (MD) is the distance between two points in  multivariate space. For any two data instances, represented by d-dimensional feature vectors their Mahalanobis distance is computed as:  Cityblock/Manhattan distance: For any two data instances, represented by d-  dimensional feature vectors , their Cityblock distance is computed as:  Minkowski distance: The Minkowski distance defines a distance between two points in a  normed vector space. Think of Euclidean distance (2 norm of xi - xj) and Cityblock distance as (1 norm of xi - xj) Minkowski distance is a generalization of these distances defined for any p-norm.  o  Jaccard distance: The Jaccard distance is a distance used to measure diversity of any two sets. Consider any two instances xi and xj as binary vectors indicating presence or absence of features. Jaccard distance between xi and xj is defined as:  -  Clustering and its applications: in simple words, clustering is acknowledgement of the pattern on  objects.  Clustering Algorithms: Clustering puts data points into groups. It uses similarity and  difference of features (or dimensions) to create groups in material that is unclassified and has no known targets.  There are three main steps of clustering methods:  Step 1: define a distance metric between objects Step 2: define an objective function that gets us to our clustering goal Step 3: devise an algorithm to optimize the objective function  -  Evaluation of clustering  Rand index: The Rand index, is a measure of the similarity between two data clusters Purity: Purity is a way of quality measurement in clustering methods. Mutual Information: Mutual information is a function that measures the agreement of the two clustering assignments C and C’ in terms of how informative one is about the other, ignoring permutations.  Silhouette Coefficient: The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference).  -  Kmeans and limitation:  Kmeans clustering is a method of vector quantization, originally from signal processing,  that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.  Kmeans limitations:  Random initialisation means that you may get different clusters each time. As a solution, we can use a Kmeans++ initialisation algorithm to initialise better. We have to supply the number of clusters beforehand. We can use the Elbow  method to choose, but it may not be straightforward. It cannot find clusters of arbitrary shapes.   It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)  -  There are few other clustering algorithms:  Kmeans (as we know) Hierarchical clustering DBSCAN (density based) Shape-based  Clustering      