The main focus of this topic was clustering, particularly using the Kmeans algorithm.  Distance metrics  A function which measures ‘distance’ between two data points in order to classify how similar they are. Examples of distance measurements include Euclidean, Cosine, Mahalanobsis, Cityblock/Manahttan, Minkowski and Jacard Distance.  Clustering algorithms  Clustering algorithms are used to organise data points into groups. These algorithms use the similarity of data points to organise them into logical groups, using the distance metrics mentioned above. The purpose of the clustering algorithm is to put data points which are the most similar into groups – this helps to classify and to discover patterns in unlabelled data.  Kmeans algorithm  Kmeans is one of the most popular and widely used clustering algorithms.  The K in kmeans refers to the number of clusters you would like to form from the given data points.  This process starts with a random selection of cluster centre points (centroids) and tries to fit the data points to those clusters. Once clusters are formed, the centroids are updated and the process is run again on that new information. The result of this process will depend on the initial random selection of centroids.  The Kmeans algorithm operates as follows  1.  Assign K random values/data points to as the beginning centroids 2.  For each data point, calculate which centroid is closest and assign the data set to that cluster 3.  For each cluster, update the centroid to the centre point of that cluster 4.  Repeat steps 2 and 3 until there is no more changes to the centroids  Limitations of Kmeans  Selecting the number of clusters  Kmeans algorithm requires you to provide the number of clusters desired – but this isn’t always obvious or easy to determine. The Elbow Method is one process that can help which calculates the sum of squared error (SSE) of Kmeans algorithm results for a range of K values. The value which forms the ‘elbow’ of this line when plotted is the optimal number of clusters for the given dataset.  Selecting the initial centroid values  As mentioned, the performance of the Kmeans algorithm depends on the initial randomised values of the centroids. The Kmeans++ algorithm can be used to help select better starting values. This process starts with the random selection on the first centroid, but uses an algorithm involving probability to help determine the starting values of the rest.  Evaluation  After a clustering algorithm has been used to classify data, the results must be evaluated to determine if this resulted in a good way to cluster the data. There are two main types of evaluation:  External assessment involves comparing the clustering result to known previously grouped data (known as the ground truth) to see if the algorithm achieved the same result. Methods of evaluation includes Rand index, Purity and Mutual Information.  Internal assessment uses a measure such as the Silhouette Coefficient which determines how similar a data point is to its own cluster as compared to other clusters.  