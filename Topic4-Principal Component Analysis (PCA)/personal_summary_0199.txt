 DISTANCES  These algorithms are used both in supervised learning methods and unsupervised learning problems. We will go through how to select a metric as we go through this topic.  DISTANCE METRIC  Distance  measures  are  functions  that  define  a  distance 𝑑(𝑥!, 𝑥")  between  any  two  data instances 𝑥!	and𝑥" for measuring how similar the instances are.  The most related examples in machine learning are:    clustering algorithms (topic 2)    K-Nearest-Neighbour    Support Vector Machines (SVM)    data visualization    information retrieval  ranking  Distance measures or Distance Metrics satisfy the following three properties:  1. For any instance 𝑥!, distance with itself is zero. That is, 𝑑(𝑥!, 𝑥!) = 0 2. For  an  instance  pair 𝑥! and 𝑥", the  distance  is non-negative and symmetric.  That  is, 𝑑(𝑥!, 𝑥") ≥ 0 and 𝑑(𝑥!, 𝑥") = 𝑑(𝑥", 𝑥!)  3. Distance measure follows triangular inequality. That is, 𝑑(𝑥!, 𝑥#) ≤ 𝑑(𝑥!, 𝑥") + 𝑑(𝑥", 𝑥#)  NEAREST NEIGHBOUR  We can use the distance to find the nearest neighbour and classify the data to the class label of this neighbour.  IMAGE RETRIEVAL  We  can  use  a  data  set  of  images  and  bring  a  new  image  to  identify/extract  similar  images  using distance measurements.  Type of Distance Measurements  Understanding the field of distance measures is more important than you might realize. Take k-NN for example, a technique often used for supervised learning. As a default, it often uses Euclidean distance. By itself, a great distance measure.  However, what if your data is highly dimensional? Would Euclidean distance then still work? Or what if your data consists of geospatial information? Perhaps haversine distance would then be a better alternative! (Grootendorst, 2021).  Disadvantages  Although it is a common distance measure, Euclidean distance is not scale in-variant which means that distances computed might be skewed depending on the units of the features. Typically, one needs to normalize the data before using this distance measure.  Moreover, as the dimensionality increases of your data, the less useful Euclidean distance becomes. This has to do with the curse of dimensionality which relates to the notion that higher-dimensional space  does  not  act  as  we  would,  intuitively,  expect  from  2-  or  3-dimensional  space.  For  a  good summary, see this post (Grootendorst, 2021)  Use Cases  Euclidean  distance  works  great  when  you  have  low-dimensional  data  and  the  magnitude  of  the vectors is important to be measured.  Methods like kNN and HDBSCAN show great results out of the box if Euclidean distance is used on low-dimensional data.  Although many other measures have been developed to account for the disadvantages of Euclidean distance, it is still one of the most used distance measures for good reasons. It is incredibly intuitive to use, simple to implement and shows great results in many use-cases (Grootendorst, 2021)  Cosine distance  Disadvantages  One main disadvantage of cosine similarity is that the magnitude of vectors is not taken into account, merely their direction. In practice, this means that the differences in values are not fully taken into account. If you take a recommender system, for example, then the cosine similarity does not take into account the difference in rating scale between different users. (Grootendorst, 2021)  Use Cases We use cosine similarity often when we have high-dimensional data and when the magnitude of the vectors is not of importance. For text analyses, this measure is quite frequently used when the data is represented by word counts. For example, when a word occurs more frequently in one document over another this does not necessarily mean that one document is more related to that word. It could be the case that documents have uneven lengths and the magnitude of the count is of less importance. Then, we can best be using cosine similarity which disregards magnitude (Grootendorst, 2021)  Mahalanobis distance  This  is  a  measure  of  the  distance  between  a  point  and  a  distribution,  taking  into  account  the covariance structure of the data. The distance between two points in multivariate space  Mahalanobis distance can be thought of scaling each data dimension by its variance and adjusting for their relationships. When data are independent, i.e. 𝑀 = 𝐼 (identity matrix), Mahalanobis distance becomes same as Euclidean distance.  Manhattan distance  Also known as taxicab distance or city block distance, this is the distance  between  two  points  measured  along  the  axes  at  right angles.  Imagine vectors that describe objects on a uniform grid such as a chessboard.  Manhattan  distance  then  refers  to  the  distance between two vectors if they could only move right angles. There is  no  diagonal  movement  involved  in  calculating  the  distance (Grootendorst, 2021).  Disadvantages  Although Manhattan distance seems to work okay for high-dimensional data, it is a measure that is somewhat less intuitive than Euclidean distance, especially when using in high-dimensional data.  Moreover, it is more likely to give a higher distance value than Euclidean distance since it does not the shortest path possible. This does not necessarily give issues but is something you should take into account (Grootendorst, 2021).  Use Cases When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes. Take Euclidean distance, for example, would create a straight line between two vectors when in reality this might not actually be possible (Grootendorst, 2021).  Minkowski distance  Zero Vector — The zero vector has a length of zero whereas every other vector has a positive length.  For  example,  if  we  travel  from  one  place  to  another,  then  that  distance  is  always positive. However, if we travel from one place to itself, then that distance is zero.  Scalar Factor — When you multiple the vector with a positive number its length is changed whilst keeping its direction. For example, if we go a certain distance in one direction and add the same distance, the direction does not change.    Triangle Inequality — The shortest distance between two points is a straight line.  Disadvantages  Minkowski  has  the  same  disadvantages  as  the  distance  measures  they  represent,  so  a  good understanding of metrics like Manhattan, Euclidean, and Chebyshev distance is extremely important.  Moreover, the parameter p can actually be troublesome to work with as finding the right value can be quite computationally inefficient depending on your use-case. (Grootendorst, 2021).  Use Cases The upside to p is the possibility to iterate over it and find the distance measure that works best for your use case. It allows you a huge amount of flexibility over your distance metric, which can be a huge benefit if you are closely familiar with p and many distance measures. (Grootendorst, 2021).  Jaccard distance  The Jaccard distance is a distance used to measure diversity of any two sets. Consider any two instances x! and x" as binary vectors indicating  presence  or  absence  of  features. It  is  the  size  of  the intersection divided by the size of the union of the sample sets.  A major disadvantage of the Jaccard index is that it is highly influenced by the size of the data. Large datasets can have a big impact on the index as it could significantly increase the union whilst keeping the intersection similar  (Grootendorst, 2021).  Use Cases  The Jaccard index is often used in applications where binary or binarized data are used. When you have a deep learning model predicting segments of an image, for instance, a car, the Jaccard index can then be used to calculate how accurate that predicted segment given true labels.  Similarly, it can be used in text similarity analysis to measure how much word choice overlap there is between documents. Thus, it can be used to compare sets of patterns (Grootendorst, 2021).  CLUSTERING ALGORITHMS  Clustering puts data points into groups. It uses similarity and difference of features (or dimensions) to create  groups  in  material  that  is  unclassified  and  has  no  known  targets.  It’s  particularly  used  in  unsupervised  learning  as  it  can  deal  with  vast  amounts  of  uncategorised  data  however  it  creates groups so it’s useful in supervised learning as well.  The goal of clustering algorithms are to:    Group unlabelled data objects with similar properties together    Discover interesting perhaps unexpected clusters in the data    Find a valid or useful organisation of the data  In other words, we can define two algorithmic goals. We need to find objective functions to:    Minimise intra-distance (distance between points in the same cluster)    Maximise inter-distance (distance between points from different clusters)  GENERIC SET-UP  Step 1: define a distance metric between objects  Step 2: define an objective function that gets us to our clustering goal  Step 3: devise an algorithm to optimise the objective function  K MEAN  K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.  Typically,  unsupervised  algorithms  make  inferences  from  datasets using  only  input  vectors  without  referring  to  known,  or  labelled, outcomes (Education Ecosystem (LEDU), 2018).  “the  objective  of  K-means  is  simple:  group  similar  data  points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.”  KNN is a supervised learning algorithm mainly used for classification problems, whereas K-Means (aka K- means clustering) is an unsupervised learning algorithm. K in K-Means refers to the number of clusters, whereas K in KNN is the number of nearest neighbours (based on the chosen distance metric).  In this algorithm, k represents the centre points of clusters. You start off with these centroids and then measure teach data point to find its  closest  centroid.  In  other  words,  K-means  stores  centroids  for defining clusters. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-means searches for the best centroids by alternating between two methods:    Assigning data points to clusters based on the current defined centroids (points which are the  centre of a cluster).    Choosing centroids based on the current assignment of data points to clusters.  Steps 1 and 2 repeat until you find a useful grouping of data points.  Limitations of K Mean  The most important limitations of simple Kmeans are:    Random initialisation means that you may get different clusters each time. As a solution, we can  use a Kmeans++ initialisation algorithm to initialise better.    We  have  to  supply  the  number  of  clusters  beforehand.  We  can  use  the Elbow  method to    choose k, but it may not be straightforward.  It cannot find clusters of arbitrary shapes.  It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)  Finding a useful number of clusters  Elbow Method  As expected, the plot looks like an arm with a clear elbow at k = 3.  Unfortunately, we do not always have such clearly clustered data. This means that the elbow may not be clear and sharp.  For Dataset A, the elbow is clear at k = 3. However, this choice is ambiguous for Dataset B. We could choose k to be either 3 or 4 (Mahendru, 2019).  Evaluation of Clustering  There are two main categories of evaluation methods for clustering:    External  assessment:  compare  clustering  performance  against  a  known  clustering  (often  called Ground truth or Gold standard).    Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g. cluster- to-cluster distance or cluster size etc.).  The following figure illustrates a sample of ground truth (𝐶) and the clustering partition found by a clustering algorithm 𝐶'.  Rand Index  Rand Index (RI, ARI) measures the similarity between the cluster assignments by making pair- wise comparisons. A higher score signifies higher similarity.  For each pair, it is considered correct if the pair is predicted to be in the same cluster when they are in the same cluster (somewhat like “true positive”) and correct if the pair is predicted to be in different clusters when they are indeed in different clusters (somewhat like “true negative”) (Wong, 2022).  𝑎 = the number of pairs of data instances that are in the same cluster in both 𝐶, 𝐶’ .   b = the number of pairs of data instances that are in the different clusters in 𝐶 and in different clusters  in 𝐶'.  c  = the number of pairs of data instances that are in the same cluster in 𝐶 but in different clusters in 𝐶'.   d = the number of pairs of data instances that are in the different clusters in 𝐶 but in the same clusters  in 𝐶'.  Rand Index does not consider chance; if the cluster assignment was random, there can be many cases of “true negative” by fluke. Ideally, we want random (uniform) label assignments to have scores close to 0, and this requires adjusting for chance.  The adjusted rand index is the corrected-for-chance version of the Rand index. In other words the index takes chance into account and corrects any bias introduced by chance.  When to use Rand Index    You want interpretability: RI is intuitive and easy to understand.    You  are unsure  about  cluster  structure:  RI  and  ARI  do  not  make  assumptions  about  the  cluster  structure and can be applied to all clustering algorithms.    You  want  a basis  for  comparison:  RI  is  bounded  between  the [0,  1] range,  and  ARI  is  bounded between  the [-1,  1] range.  The  bounded  range  makes  it  easy  to  compare  the  scores  between different algorithms.  When to NOT use Rand Index  You do not have the ground truth labels: RI and ARI are extrinsic measures and require ground truth cluster assignments.  Purity  Purity is a way of quality measurement in clustering methods. As the name suggests, we would like to measure the purity for all clusters in terms of class labels of the data in each cluster.  Each cluster is assigned to the class label which has the majority in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances (Yıldırım, 2021).  Consider the following figure as an example.  Based on the figure, the first cluster has 5 crosses and 1 circle, so the majority of the labels are cross. For the next cluster, we have 4 circles and 1 cross and 1 plus, so circle has the majority. And as for the last one we can see 3 pluses and 2 crosses which result in majority of pluses.  In general, purity increases as the number of clusters increases. For instance, if we have a model that groups each observation in a separate cluster, the purity becomes one. Consider the example we just solved, what if a particular outcome groups the points into 17 clusters? One cluster for each point. It may not sound like a clustering approach but in this case the purity measurement would result in 100%  For this very reason, purity cannot be used as a trade-off between the number of clusters and clustering quality.  Mutual Information  Mutual information is one of the most popular approaches in analysis of clustering. It measure the agreement between two clustering assignments such as 𝐶and	𝐶', so the aim is almost same as the Rand Index. In mutual information the main question is how informative is 𝐶 about 𝐶’ or 𝐶’ about 𝐶. How similar are they and are they similar in a useful way? What can you talk about 𝐶if you look at C'?   Silhouette Coefficient  The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require  the  ground  truth  cluster  assignments.  The  silhouette  coefficient  contrasts  the  average distance between the instances of the same cluster with the average distance between the instances of different clusters:  𝑠(𝑖) =  𝑏(𝑖) − 𝑎(𝑖) 𝑚𝑎𝑥{𝑎(𝑖), 𝑏(𝑖)}  In the above formula, 𝑎(𝑖) is the average distance of 𝑖−th instance with all other instances of the same cluster, and b(𝑖)is the lowest average dissimilarity of 𝑖−th instance with all other clusters. The final value of Silhouette calculation ranges from −1 to +1. A high value of Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. If most objects have a high value, then the clustering configuration is appropriate. On the other hand, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters.  Hierarchical clustering  There  is  another  type  of  clustering  algorithms  called  hierarchical  clustering.  These  algorithms  find clusters that have a predetermined order.    Agglomerative clustering (bottom-up): A “bottom up” approach in which each observation starts  in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.    Divisive clustering (top-down): A “top down” approach in which all observations start in one  cluster, and splits are performed as one moves down the hierarchy.  Agglomerative clustering  Consider the following figure as an example. At the bottom of the tree, at the starting point each of the characters 𝑝, 𝑞, 𝑟, 𝑠, 𝑡 are assigned into a single separate cluster.  As  we  go  up  to  the  higher  levels,  the  closest  characters  are  formed  another  cluster. i.e. 𝑠, 𝑡 and 𝑝, 𝑞. At the next level we can notice 𝑟, 𝑠, 𝑡 are making a cluster. And finally at the top of the tree, all the characters are in one single cluster 𝑝, 𝑞, 𝑟, 𝑠, 𝑡.  So in In Agglomerative or bottom-up clustering method we assign each observation to its own cluster. Then,  compute  the  similarity  (e.g.,  distance)  between  each  of  the  clusters  and  join  the  two  most similar clusters. We do this until we get to the top of the tree. Cutting the tree at a given height will give a partitioning clustering at a selected precision. If you cut the tree at deeper levels, you will get more clusters than cutting in the tree in higher levels.  Finding the distances:  1.  Single-link: the distance between closest points 2.  Complete-link: the distance between the furthest points 3.  Centroid: the distance between the Centroids 4.  Average-link: the average distance between pairs of elements from across cluster pairs  Divisive clustering    Similar to Agglomerative clustering in this method all data instances are put in the same cluster   For splitting, we can use any clustering algorithm that produces at least two clusters (e.g. Kmeans)   The process is continued until each data instance is separate and assigned to its own cluster  DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  A clustering algorithm that clusters certain items in a group based on a given data point.  For this, we need to set a minimum number of data points (minPts) and a distance (dis). Because these parameters are user-defined, the resulting cluster is dependent on them.  1.  Calculate the distance from each point in the dataset to every other point. A point is considered a  "core point" if it has at least the same number of data points within the defined distance.  2.  Data points which cannot be considered core points but are under the defined distance of the  core point are called border points. All other points are regarded as "noise."  3.  The next step is to combine all core and border points within dis of each other into a single cluster.  We keep repeating the above steps until we reach the  Shape-based clustering, VAT, iVAT  VAT  is  a  visualization  technique  that  transforms  the  distance  matrix  of  a  dataset  into  a  visual representation  in  the  form  of  a  re-ordered  matrix.  The  re-ordering  is  done  in  such  a  way  that  the dissimilarities between the data points are emphasized in a way that reveals the underlying clustering structure of the data. If the data has a clear clustering structure, then the re-ordered matrix will exhibit block-like structures along the diagonal, indicating the presence of clusters.  iVAT is an extension of VAT that involves repeatedly applying the VAT algorithm to the re-ordered matrix in order to refine the clustering structure. The iVAT algorithm iteratively computes the VAT on the  re-ordered  matrix  until  a  stable  clustering  structure  is  obtained.  This  can  help  to  identify  the optimal number of clusters in the data.  Both VAT and iVAT are useful tools for exploratory data analysis, allowing data analysts to gain insight into  the  underlying  structure  of  the  data  and  to  identify  the  appropriate  number  of  clusters  for subsequent clustering algorithms.  