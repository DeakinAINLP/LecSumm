Summary In this module we covered topics in unsupervised machine learning, focusing on the concept of clustering. Subtopics include finding distances, clustering algorithms, performance evaluations.  Distances Distances between data points are used to find similarity. Long distance means low similarity and short distance means high similarity. Both supervised and unsupervised learning involves finding distance metrics. Distances have three important properties: any data instance has a distance of 0 with itself, the distance is measured as the magnitude between two data pairs so it is always positive and symmetrical, and distances follow triangular inequality.  Common distance functions include: Euclidean distance, cosine distance, mahalanobis distance, cityblock distance, minkowski distance, and jaccard distance. Euclidean distance finds the magnitude of the straight line between two datapoints. Cosine distance finds the angle between two vectors. Jaccard distance finds the diversity between two sets.  Clustering The goal of clustering is to recognize patterns and put data into unlabelled groups. Thus it is highly useful in unsupervised learning as it can find similarity in unclassified data. An important part of clustering is minimising intra-distance (distance between points in a cluster) and maximising inter- distance (distance between clusters) to create well-defined clusters.  The process is > distance metric > objective function > optimising algorithm. The hope is to organise the data by matching similar data sets and if possible find unexpected clusters.  Clustering needs to be evaluated to determine its performance. The primary two types of evaluation are external assessment and internal assessment. External compares the clusters against a known cluster (ground truth). Internal does not rely on ground truth, instead relying on internal measurements such as intra-cluster distance, cluster size. Examples of evaluation methods are Rand Index, Mutual Information, Purity Score and Silhoette Coefficient.  Some clustering algorithms include K-Means, DBSCAN, Hierarchical, Shape-based (VAT/iVat).  Clustering: K Means K-means is popular clustering algorithm based on finding K centroids (center of a cluster) and then grouping data points into their closest centroid. It starts by randomly assigning K centroids. Then it assigns the data points to their closest centroid. After that the centroids are reassigned to the average centres of their respective groups. The previous two steps repeat until the centroids no longer move significantly. Determining the number of clusters can be achieved with several methods. One popular method is the Elbow Method.  K-Means has some drawbacks. The major issue is cluster initialization – the starting position of each cluster can drastically impact the performance. A bad starting position can lead to inaccurate and inconsistent clustering.  KMeans++ solves this problem by initializing cluster starting points in a more logical manner. Only the first cluster starting point is assigned manually, the rest are determined based on this initial cluster. Kmeans++ guarantees convergences in under 8(logK + 2) iterations, where K is the number of clusters. Standard KMeans has no such guarantee.  Another major issue is the number of clusters required is not known ahead of time. One must determine the ideal number of clusters first. Other issues are that Kmeans cannot generate arbitrary shapes, only blobs. The algorithm is also susceptible to noise and has no way of detecting it.  Clustering: DBSCAN A clustering algorithm that can tackle noise is DBSCAN. DBSCAN method is a density-based algorithm that does not require a pre-selected number of clusters. It has two user-defined features, minimum number of data points (or minimum samples) and distance (or epsilon). Data points are broken into three groups – core points, border points, and noise. Any point with the minimum points within the distance are considered core points. Points that are within a core-points distance but themselves do not have the minimum number of points are the border points. The remaining points are classified as noise.  The algorithm first selects a random point and checks if it’s a core point. If it is, then each of the close points are visited to determine if they’re core or non-core. This continues until there are no more core points within range of any of the visited core points. Then the core and non-core points are categorized as a cluster. Then another non-clustered point is chosen and the cycle continues until only noise points are left, which are ultimately ignored. All the core and border points will be in clusters.  Clustering: Hierarchical Another method of clustering is Hierarchical clustering. Hierarchical clustering is split into two types, agglomerative clustering and divisive clustering. Agglomerative clustering is a bottom-up approach while divisive clustering is top-down.  Both types of clustering form a tree diagram known as a dendogram. In agglomerative clustering, each data point begins is assigned to its own cluster, forming the bottom layer of the tree. The closest data-points are grouped into new clusters of pairs, forming the next layer. From this point at each layer the distance between sub-clusters is calculated and the closest are paired together, forming new layers until reaching the top of the tree with one single cluster.  In divisive clustering all data-points start in the same cluster. This time clusters are split in two instead of merged. Any clustering algorithm tat generates two or more clusters can be used for this stage. This repeats until data-points are in individual clusters. This process forms the tree from the top down.  The resulting tree contains layers of cluster groups, so the desired number of clusters can be taken by cutting the tree at the desired height.  Different methods of determining sub-cluster distance include single-link (distance between closest points), complete-link (distance between furthest points), centroid (distance between centroids) and average-link (average distance from clusters).       