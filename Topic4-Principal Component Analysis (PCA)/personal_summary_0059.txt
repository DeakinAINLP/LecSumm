Topic3. Clustering(unsupervised machine learning)  -  Distance metrics: measuring how similar the instances are  : clustering algorithms, K-Nearest-Neighbor, Support Vector Machines(SVM), data visualization, information retrieval, ranking.  -  Distance to itself is o d(x,x) = 0 -  The instance between two pairs is non negative and symmetric. D(x,x) >= 0 and d(x1,x2) d= d(x2, x1)  -  Distance measure follows triangular inequality -  Types of distance measurement 1.  Euclidean distance(2norm)  𝑑𝐸𝑢𝑐𝑙𝑖𝑑𝑒𝑎𝑛(𝑋𝑖, 𝑋𝑗) = {(𝑋1 − 𝑋2)2 + ⋯ + (𝑋𝑖 − 𝑋𝑗)2}  1 2  2.  Cosine distance  = 1- cosine similarity  cosine similarity = cos 𝜃 =  √∑  ∑  𝑛 𝑖=1  𝐴𝑖𝐵𝑖  𝑛 2 𝑖=1 √∑ 𝐴𝑖  𝑛 𝑖=1  2 𝐵𝑖  3.  Mahalanobis distance(MD)-useful when having covariance  : can be thought of scaling each data dimension by its variance and adjusting for their relationship.  4.  Cityblock/Manhattan distance: the effect of a large difference in a single dimension is  dampened(since the last distances are not squared)-1 norm  5.  Minkowski distance: a generalization of these distance defined for any p norm  6.  Jaccard distance: a distance used to measure diversity of any two sets.  -  Clustering Algorithms  : puts data points into groups,  uses similarity and difference of features(dimensions) : minimize intra distance(distance between points in the same cluster) : maximise inter distance(distance between points from different cluster) 1)  Define a distance between objects 2)  Define an objective function that gets us to our clustering goal 3)  Devise an algorithm to optimize the objective function K means 1)  K: the centre points of clusters 2)  Start off with centroids 3)  Measure teach data point to find its closet to centroid  -  : K means searches for the best centroids by alternating these, assigning data points to clusters based on the current defined centroids or choosing centroids based on the current assignment of data points to clusters.  -  Evaluation of clustering 1.  External assessment  Compare clustering performance against a known clustering        2.  Internal assessment Determine if clustering follows certain intrinsic assumptions(cluster-to-cluster distance or cluster size) 1)  The rand index  : a measure of the similarity between two data clusters, ignoring their permutations  R =  𝑎+𝑏 𝑎+𝑏+𝑐+𝑑  =  𝑎+𝑏 (𝑛 2)  2)  Purity  : quality measurement : each cluster is assigned to the class label which has the majority in the cluster : then accuracy of this assignment is measured by counting number of correctly assigned instances and dividing by the number of total instances.  3)  Mutual information : most popular : measure the agreement between two clustering assignment in terms of how informative one is about the other, ignoring permutations.  4)  Silhouette Coefficient  : a measure how similar an object is to its own cluster(cohesion/similarity) compared to other clusters(separation/difference) : does not require the ground truth cluster assignment : contrasts the average distance between the instances of same cluster with average distance between instances of different clusters  -  Limitation of Kmeans 1.  Random initialization means that you may get different clusters each time- K means ++  initialization algorithm to initialize better  2.  Have to supply the number of clusters beforehand- we can use Elbow method to choose  -  K It cannot find clusters of arbitrary shapes It cannot detect noisy data points.  3. 4. Kmeans with Kmeans++ : an algorithm for choosing the initial cluster`s centre values or centroids for the Kmeans clustering algorithm. 1)  Choose one centroid 2)  Uniformly at random from the dataset 3)  Let D(x) be the shortest distance from a data point to the closest centroid we have  already chosen  4)  Choose a new centroid from the dataset with probability 5)  Repeat the previous step until we have initialized K centroids  : guarantee of Kmeans++  : the objective function monotonically decreases with each iteration of the algorithm  -  Hierarchial clustering  : find cluses that have a predetermined order 1.  Agglomerative clustering(bottom-up)  : each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy   : assign each observation to its own cluster, then compute the similarity(distance) between each of the clusters and join the two most similar clusters. Do this until get to the top of the tree. Cutting the tree at a given height will give a partitioning clustering at a selected precision. If the tree was cut at a deeper level, more clusters than cutting in the tree in higher level. 1)  Single-link: distance between closes points 2)  Complete-link: distance between the furthest points 3)  Centroid: distance between centroids 4)  Average-link: average distance between pairs of elements from across cluster pairs  2.  Divisive clustering(top-down)  : all observations start in one cluster, and splits are performed as one moves down the hierarchy. For splitting any clustering algorithms can be used to produce at least two clusters(Kmeans), process is continued until each data instance is separate and assigned to its own cluster.  -  DBSCAN(Density-Based Spatial Clustering of Applications with Noise)  : clusters certain items in a group based on a given data point 1)  Calculate the distance from each point in the dataset to every other point. A point is considered a “core point” if it has at least the same number of data points within the defined distance  2)  Data points are under defined distance but not core points- border points, and all other  points are “noise”  -  3)  Combine all core and border points within dis of each other into a single cluster. Shape based clustering (VAT, iVAT) : useful tool for exploratory data analysis, allowing data analysts to gain insight into the underlying structure of the data and to identify the appropriate number of clusters for subsequent clustering algorithms  