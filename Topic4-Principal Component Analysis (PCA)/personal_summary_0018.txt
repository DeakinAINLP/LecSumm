Lesson Overview – Clustering  Clustering is one of two forms of unsupervised machine learning.  Clustering is used to find patterns from a large dataset which is not labelled and putting these data points into groups. The  measurement  of  distance  between  two  data  points  is  often  used  in  both  supervised  and unsupervised  machine  learning  to  measure  how  similar/different  two  instances  are.    Through  this topic’s  learning,  different  types  of  distance  measurements  were  introduced,  including,  Euclidean distance, cosine distance, Mahalanobis distance, Manhattan distance, Jaccard distance and Minkowski distance.  Euclidean  distance  measures  the  distance  in  a  straight  line  between  two  points. Mahalanobis distance measures the distance between two points but taking into consideration also the correlation between variables.  Cosine distance measures the angle created between two points, but as this indicates the similarity between these two points instead, the difference between the two points is calculated by 1 minus the cosine similarity.  Manhattan distance is the sum of the absolute difference of the vector points.  Jaccard distance is used to measure distance between two sets.  Clustering  in  unsupervised  machine  learning  attempts  to  group  unlabelled  data  points  into  groups using similarity and differences of features.  The goal of unsupervised machine learning is to minimise distance between points in the same cluster (intra-distance) and maximise distance between points from different clusters (inter-distance). K-mean is the most popular clustering algorithm, it aims is to separate dataset into different number of clusters (k clusters).  In K means k is also referred to as centroid and these are the centre points of clusters.    Centroids  are  initially  placed  randomly;  each  data  point  is  then  assigned  to  the  closest centroid forming k number of clusters, centroids are then recalculated and placed closer to the centre of each cluster until the sum of squared distances between each point and their respectively centroid is  minimised.    Despite  its  usefulness,  K  means  does  come  with  several  limitations;  firstly,  as  the initialisation  of  centroid  is  random  and  therefore  results  achieved  can  be  different  each  time, secondly, number of clusters is required to be predefined, thirdly, KMeans is unable to find clusters of arbitrary shapes, lastly, KMeans is unable to detect noisy data points.  To overcome issue of initialising centroids randomly, KMeans++ improves the process by starting with allocating one cluster centroid only and then selecting subsequent centroids using this centroid until all centroids have been located.  In  addition  to  K-means  and  K-means++  algorithm,  other  clustering  algorithms  include  hierarchical clustering, DBSCAN (density based) and shape-based clustering.  There are two types of hierarchical clustering,  agglomerative  (bottom  up)  clustering  and  divisive  (top-down)  clustering.    Bottom-up clustering begin with each data point being its own cluster and are then merged as they move up the hierarchy.   On another hand, the top-down approach considers all data points as one cluster and are separated into various clusters as it moves down the hierarchy.   DBSCAN (density-based clustering) groups data points based on how dense a region is and separates clusters by regions with low density.  Different evaluation methods are used to evaluate the effectiveness of various clustering algorithm. These methods include, Rand Index, Purity method, mutual information, and silhouette coefficient. The utilisation of multiple evaluation methods is often required to obtain a comprehensive evaluation of the effectiveness of a clustering solution. Learning this topic also included performing various clustering algorithm and evaluation algorithm on Python.  