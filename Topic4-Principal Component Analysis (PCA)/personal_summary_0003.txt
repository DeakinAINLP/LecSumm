This topic covered distance measures as used by several clustering algorithms to find similar data points within the feature set. The different distance measures focus on different features eg Minkowski with a high p will weight large differences between points more strongly than Manhattan distance. Others ignore distance and use angles or set inclusion. These will result in the clustering algorithm selecting different points as being similar enough for inclusion in the cluster. Clustering algorithms differ in terms of complexity, determinism, and performance guarantees. They produce different patterns when the state space is visualised. They are sensitive to their parameter k, the number of clusters to find. If this is not already known from a gold standard, it can be measured visually (Elbow method) or by testing different values in a range to find which value of k least violates assumptions about the data eg clustered points are close together, points from other clusters do not intrude into another clusterâ€™s space, clustered points should be similar to each other and differ from others. This makes clustering partly an art, requiring both knowledge of the data being clustered and the performance of the clustering algorithms, as well as trial and error to find a suitable method. 