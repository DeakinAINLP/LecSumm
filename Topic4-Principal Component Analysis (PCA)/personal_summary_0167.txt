Unsupervised Machine Learning:  In general, machine learning algorithms work with datasets that have labelled inputs and outputs in order for the algorithm to discover the underlying connection between input and output (i.e., Supervised learning algorithm). But what if we don't know the output and the sources aren't labelled?  Such an algorithm is incapable of detecting the fundamental pattern. In such situations, we employ Unsupervised algorithms that work with such datasets.  Unsupervised learning concentrates on finding similarities and grouping data points based on their distance, which is known as Clustering. The distance between these datapoints is used to determine their commonalities.  Jaccard Distance    Manhattan/Cityblock Distance  Euclidean Distance  Mahalanobis Distance  Cosine Distance  Minkowski Distance  These metrics are used to evaluate the data elements. These measures are also employed in the supervision of machine learning.  Clustering algorithms, Hierarchical Clustering, and Data visualisation are the most common Unsupervised machine learning algorithms.  Clustering:  The main goal of clustering algorithms is to group datapoints according to how far apart they are from their centres and to divide them up into fictitious segments so that they don't intersect. The cluster method is one of the most straightforward and popular ones. The clustering programme typically performs the following actions:  1. The algorithm concentrates on choosing datapoints to a centroid at random. The algorithm then focuses on quantifying the distances between centroids and datapoints using metrics distances to determine how similar and different they are.  2. After that, a clustering algorithm based on the separation between datapoints and the  centroid allocates them to a cluster.  3. The algorithm concentrates on finding new centroid based on the cluster centroid after data  points have been clustered together.  4. The clustering algorithm iteratively continues steps 3 and 4 until the centroids stop moving.  One of the most popular Clustering algorithms is Kmeans.  Based on the distance between a datapoint and the centroid, the Kmeans method classifies a datapoint as belonging to a particular cluster. The K-means algorithm focuses on choosing the cluster's centroid that best describes the cluster. This algorithm concentrates on switching back and forth between two iterative steps.  1. Designating the closet cluster centre for each data point.  2. After that, each cluster is set to represent the mean of the datapoints given to it.  The K-means++ algorithm is another widely used cluster algorithm. As opposed to K-means, which prioritises arbitrarily selecting data points as its centre. While the K-means++ algorithm originally chooses the centroid at random, it then chooses the centroid based on the empirical probability distribution of the points. This provides the algorithm with an improved initial starting point.  Hierarchical clustering:  The closest clusters are combined to form one cluster using this technique, which first treats each cluster as an individual cluster. This procedure is done until only one cluster remains after all the datapoints have been clustered into distinct groups based on their distance from one another.  There are two different hierarchical clustering approaches:  1. Divisive clustering: Using the resources of divisive clusters, this method separates the data points from a single, massive cluster into smaller clusters while maximising the space between the data points.  2. Agglomerative Clustering: In this bottom-up method, data points are treated as separate clusters and combined into one large cluster to create a hierarchy from bottom to top.  DBSCAN:  Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density.  Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a base algorithm for density- based clustering. It can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers.  The DBSCAN algorithm uses two parameters:   minPts: The minimum number of points (a threshold) clustered together for a region to be  considered dense.    eps (ε): A distance measure that will be used to locate the points in the neighborhood of any point.  Distance function: The choice of distance function is tightly linked to the choice of ε, and has a major impact on the outcomes. In general, it will be necessary to first identify a reasonable measure of similarity for the data set, before the parameter ε can be chosen. There is no estimation for this parameter, but the distance functions need to be chosen appropriately for the data set.  Evaluation of Clustering Algorithm:  It is very difficult to evaluate the clustering algorithm. There are two different kinds of evaluations that are used.  