Clustering This second module related to some of the more important concepts related to unsupervised machine learning.  Use Clustering for revealing patterns from unlabelled data Fundamental to many machine learning algorithms is measuring similarity/distance between data points  Functions that define a distance between any two data instances to measure how similar instances are. Different distance metrics used in clustering, K-Nearest-Neighbor, SVM, data visualization, information retrieval, ranking  Properties of Distance Metrics: Zero distance with itself Non-negative and symmetric distance Follows triangular inequality  Euclidean distance: straight-line distance between two points Cosine distance: measures the cosine of the angle between two vectors Mahalanobis distance: distance between points in multivariate space, accounts for correlations Cityblock/Manhattan distance: sum of absolute differences of coordinates Minkowski distance: generalization of Euclidean and Cityblock distance (depends on parameter p) Jaccard distance: measures diversity of two sets (based on intersection and union)  Clustering and its applications Pattern recognition.  The goal at first:  §  Group unlabelled data objects with similar properties together §  Discover interesting perhaps unexpected clusters in the data §  Find a valid or useful organisation of the data  find objective functions to:  §  Minimise intra-distance (distance between points in the same cluster)  §  Maximise inter-distance (distance between points from different clusters)  Techniques to reduce dimensionality  Kmeans  K-means is an unsupervised machine learning algorithm used for clustering data points into 'k' groups based on similarity. It helps to identify patterns, trends, or structures in the data.  How it works:  a. Initialize 'k' centroids randomly or by using a specific method (e.g., k-means++) b. Assign each data point to the nearest centroid, creating 'k' clusters c. Update the centroids by calculating the mean of all data points within each cluster d. Repeat steps b and c until convergence (i.e., centroids don't change significantly) or a maximum number of iterations is reached  Points to note:    K-means is sensitive to the initial placement of centroids; it may converge to a local   minimum It's essential to choose an appropriate value for 'k'; methods like the elbow method or silhouette analysis can help    K-means works best with continuous numerical data and Euclidean distance    Scaling and normalization of features may improve clustering results  It assumes that clusters are spherical, equally sized, and have similar densities  Evaluation of clustering  §  External assessment: compare  clustering  performance  against  a  known  clustering  (often  called Ground truth or Gold standard). Internal assessment: determine  if  clustering  follows  certain  intrinsic  assumptions  (e.g.  cluster-to-cluster distance or cluster size etc.).  Rand Index Is commonly employed to assess the performance of clustering algorithms, especially when the  true  labels  for  the  data  points  are  available.  The  Rand  Index  measures  the  agreement between two cluster assignments, considering all possible pairs of data points.  Purity  Purity  is  an  external  evaluation  metric  used  to  assess  the  quality  of  clustering  results  in unsupervised machine learning algorithms. It is a simple and intuitive measure that computes the  extent  to  which  each  cluster  contains  data  points  belonging  to  a  single  class.  Purity  is especially helpful when the true labels for the data points are available. To calculate purity, follow these steps:  1.  Assign each data point in a cluster to the class that occurs most frequently within that  cluster.  2.  Calculate the total number of correctly assigned data points across all clusters. 3.  Divide the total number of correctly assigned data points by the total number of data points in the dataset.  Mutual Information This method quantifies the amount of information shared between the two clusterings, with higher  values  indicating  greater  similarity.  Unlike  Purity,  Mutual  Information  takes  into account both the size of clusters and the number of clusters, making it a more robust evaluation metric.  Silhouette Coefficient The  Silhouette  Coefficient  is  an  internal  evaluation  metric  used  to  assess  the  quality  of clustering results in unsupervised machine learning algorithms. It measures the cohesion and separation of the clusters, providing an indication of how well each data point fits within its assigned  cluster.  The  Silhouette  Coefficient  does  not  require  true  labels  for  the  data  points, making it particularly useful when ground truth is unavailable.  Limitations of Kmeans  Some limitations:  §  Random initialisation means that you may get different clusters each time. As a solution, we can use a Kmeans++ initialisation algorithm to initialise better. §  We have to supply the number of clusters beforehand. We can use the Elbow  method to choose (cid:0), but it may not be straightforward. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points. Elbow Method The Elbow method helps to assist in finding the appropriate number of clusters. The  method  involves  plotting  the  variance  explained  by  each  cluster  against  the  number  of clusters, and identifying the "elbow point" where the decrease in variance explained begins to level off. The  idea  behind  the  elbow  method  is  that  as  the  number  of  clusters  increases,  the  variance explained should decrease, but at some point, adding more clusters will only capture small, insignificant variations in the data, and the decrease in variance explained will begin to level off.  The  point  at  which  this  occurs  is  the  elbow  point,  and  is  considered  to  be  the  optimal number of clusters for the given dataset  Clustering with Kmeans++ In  the  standard  k-means  algorithm,  the  initial  centroids  are  chosen  randomly  from  the  data points, which can lead to poor performance or convergence to suboptimal solutions. The k- means++ algorithm addresses this issue by using a more intelligent initialization process. The k-means++ algorithm works as follows:  1.  Choose the first centroid uniformly at random from the data points.  2.  For each data point, compute the distance to the nearest centroid that has already been chosen.  3.  Choose  the  next  centroid  from  the  data  points  with  probability  proportional  to  the square of the distance to the nearest centroid.  4.  Repeat steps 2 and 3 until k centroids have been chosen.  By choosing the centroids in this way, k-means++ ensures that the initial centroids are well- spaced and spread out across the data space, which can lead to faster convergence and better clustering results.  Other Clustering Algorithms  Hierarchical clustering is a clustering technique that creates a hierarchy of clusters. It starts with each data point in its own cluster and then merges the closest clusters until a single cluster is formed. Hierarchical clustering can be either agglomerative (bottom-up) or divisive (top- down).  DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed together, while also identifying outliers as noise points. It requires two input parameters: a radius epsilon, and a minimum number of points required to form a cluster.  Shape-based clustering is a family of clustering algorithms that group together objects with similar  shapes.  The  shape  of  an  object  is  typically  described  by  a  set  of  features,  such  as curvature,  size,  and  orientation.  Examples  of  shape-based  clustering  algorithms  include  the popular k-shape algorithm and the Hough transform-based clustering algorithm.  The rest of the module focused on Python skills for loading and manipulating datasets with Kmeans.  2. Provide summary of your reading list – external resources, websites, book chapters, code libraries, etc. For this unit I read the provided materials, Python libraries and did the activities throughout the module in a Jupyter notebook.  I  also  supplemented  my  reading  with  Code  Academy  Machine  Learning  course  Learn  the Basics of Machine Learning | Codecademy  And some additional YouTube videos: Machine Learning Tutorial Python - 13: K Means Clustering Algorithm Machine Learning Tutorial Python - 13: K Means Clustering Algorithm - YouTube  How to find Optimal K with K-means Clustering ? The Elbow and Silhouette methods How  to  find  Optimal  K  with  K-means  Clustering  ?  The  Elbow  and  Silhouette  methods  - YouTube  3. Reflect on the knowledge that you have gained by reading contents of this topic with respect to machine learning.  After reading the contents of this topic related to machine learning, I have gained a deeper understanding of several important concepts and techniques in the ML field. In particular, I have  learned  about  different  types  of  clustering  algorithms,  such  as  hierarchical  clustering, DBSCAN,  and  shape-based  clustering,  which  can  be  used  to  group  similar  data  points  or objects based on different criteria.  These were areas I was not familiar with before. I also found the elbow method useful when working with clustering. Overall, I found this topic's contents on machine learning to be both informative and practical for our Ontrack tasks. I can see that the concepts and techniques presented are applicable to a wide range of real-world problems, from image recognition to customer segmentation, and can be used to develop more accurate and efficient machine learning models.  