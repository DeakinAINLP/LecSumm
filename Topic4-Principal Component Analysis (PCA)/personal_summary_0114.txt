Topic 3 - Clustering  Clustering : It can find patterns in large data sets when the data is not labelled.  Dimensionality reduction : It helps when the number of features that describe the  difference between data, becomes too large to manage.  Distance metrics : These are functions that define a distance between any two  data instances for measuring how similar the instances are.  clustering  k-nearest neighbour  SVM  data visualization  information retrieval  ranking  There are different types of distance measurements such as :    Euclidean distance Cosine distance  Mahalanobis distance Cityblock/Manhattan distance  Minkowski distance Jaccard distance  Clustering Algorithms : It puts data points into groups. It uses similarity and difference of features to create groups in material that is unclassified and has no known targets.  It has two algorithmic goals :    Minimise intra-distance (distance between points in the same clusters). Maximise inter-distance (distance between points from different clusters)  Steps involved in clustering.  Define a distance metric between objects. Define an objective function that gets us to our clustering goal.  Devise an algorithm to optimise the objective function.  K-means : In this algorithm, k represents the centre points of clusters. We start off  with these centroids and then measure the distance of data points to find its  closest centroid. A point is considered to be in a particular cluster if it is closer to  that cluster's centroid than any other centroid.  Evaluating clustering : Generally there are two main categories of evaluation  methods for clustering  External assessment : compare clustering performance against a known  clustering  Internal assessment : determine if clustering follows certain intrinsic  assumptions  Rand Index : Its a measure of similarity between two clusters.Â  For example in  external assessment, we have the knowledge of ground truth assignments and  hence using Rand Index we can measure its similarity with the algorithm generated cluster.  Purity : It is a way of quality measurement in clustering methods. Each cluster is assigned to the class label which has the majority in the cluster and then the  accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances.  Mutual information : It is a function that measures the agreement of the two clustering assignments in terms of how informative one is about the other, ignoring permutations.  Silhouette Coefficient : It is a measure of how similar an object is to its own cluster  compared to other clusters. A high value of Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters.  Limitations of K-means :  Random initialisation means we may get different clusters each time. Number of clusters need to supplied beforehand. It cannot find clusters of arbitrary shapes. It cannot detect data points that should not be taken into account for cluster  analysis.  Elbow method : As per this method, run K-means clustering algorithm for a range  of values of K, and for each value of K, compute the sum of squared error and plot  it. It decreases as K increases. If the plot looks like an arm, then the elbow of the  arm indicates the right value of K.  K-means++ : Its an algorithm for choosing the initial cluster's centre values or  centroid for the K-means clustering algorithm. Unlike K-means, K-means++ starts  with allocating one cluster centre randomly and then searches for other centres  given the first one.  Other clustering algorithms : In addition to K-means there are other clustering  algorithms. They are as follows :  Hierarchical clustering  DBSCAN (density based)  Shape-based Clustering   