The two forms of unsupervised learning are clustering and dimensionality reduction. When the data is not labelled Clustering can find patterns in large data sets whereas dimensionality reduction helps us when the number of features, that describe the difference between data becomes too large to manage.  Measuring similarity or distances between different data points is fundamental to many machine learning algorithms. These algorithms are used both in supervised learning methods and unsupervised learning problems. Depending on the nature of the data point, various measurements can be utilised to measure distance. Distance measures are functions that define a distance d(xi,xj) between any two data instances xi and xj for measuring how similar the instances are. Types of distance measurements: Euclidean distance (Euclidean distance is the ordinary straight-line distance between two points in Euclidean (everyday) space) Cosine distance Mahalanobis distance (The Mahalanobis distance (MD) is the distance between two points in multivariate space) Cityblock/Manhattan distance Minkowski distance (The Minkowski distance defines a distance between two points in a normed vector space). Jaccard distance  (The Jaccard distance is a distance used to measure diversity of any two sets.).  Clustering puts data points into groups. It uses similarity and difference of features (or dimensions) to create groups in material that is unclassified and has no known targets. The goal of clustering algorithms are to: Group unlabelled data objects with similar properties together, Discover interesting perhaps unexpected clusters in the data, Find a valid or useful organisation of the data. We can define two algorithmic goals. We need to find objective functions to:  Minimise intra-distance (distance between points in the same cluster)  Maximise inter-distance (distance between points from different clusters)  The most popular clustering algorithm in machine learning is called K-means. It is simple and fast. K- means stores k centroids for defining clusters. A point is considered to be in a particular cluster if it is closer to that cluster’s centroid than any other centroid. K-means searches for the best centroids by alternating between two methods:  1.  Assigning data points to clusters based on the current defined centroids (points which are  the centre of a cluster).  2.  Choosing centroids based on the current assignment of data points to clusters.  Step 1 and 2 repeat until you find a useful grouping of data points.  External assessment:  There are two main categories of evaluation methods for clustering:  compare clustering performance against a known clustering (often called Ground truth or Gold standard).  determine if clustering follows certain intrinsic assumptions (e.g. cluster-to-cluster distance or cluster size etc.). Examples: Silhouette coefficient, Dunn index etc.  Internal assessment:  The Rand index, is a measure of the similarity between two data clusters. The adjusted rand index is the corrected-for-chance version of the Rand index. Purity is a way of quality measurement in clustering methods. In this we measure the purity for all clusters in terms of class labels of the data in each cluster. We have to make sure that we are selecting a fair number of clusters when we perform clustering on a set of data points. Mutual information is one of the most popular approaches in analysis of clustering. It measure the agreement between two clustering assignments such as C. and C’, so the aim is almost same as the Rand Index. Mutual information is a function that measures the agreement of the two clustering assignments C and C’ in terms of how informative one is about the other, ignoring permutations. To put it simple, how informative is C about C’. The silhouette value is a measure of how similar an object is to its own cluster (cohesion/similarity) compared to other clusters (separation/difference). This method has the advantage that it does not require the ground truth cluster assignments. The silhouette coefficient contrasts the average distance between the instances of the same cluster with the average distance between the instances of different clusters. A high value of Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. If most objects have a high value, then the clustering configuration is appropriate. On the other hand, if many points have a low or negative value, then the clustering configuration may have too many or too few clusters.  The most important limitations of simple Kmeans are:  o  Random initialisation means that you may get different clusters each time. As a solution, we  can use a Kmeans++ initialisation algorithm to initialise better.  o  We have to supply the number of clusters beforehand. We can use the Elbow method to  o o  choose K, but it may not be straightforward. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)  The Elbow Method is a method for finding the appropriate number of clusters. The Elbow method interprets and validates consistency within a cluster analysis to find the appropriate number of clusters in a dataset.  Kmeans++ is an algorithm for choosing the initial cluster’s centre values or centroids for the Kmeans clustering algorithm. K-means++ starts with allocating one cluster centre randomly and then searches for other centres given the first one.  