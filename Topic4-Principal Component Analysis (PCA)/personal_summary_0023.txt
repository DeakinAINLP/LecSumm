learning, Clustering. What is Clustering? Clustering is a technique where you input a dataset and then group datapoints into smaller group, or we can say categorizing the datapoints. By categorizing groups, we can use it to for patterns and predict future outcomes. But how do we group them? We measure distance from one datapoint to another point and see if the two belongs with each other or not. If a dataset is small and obvious to the eyes, it’s easy for us to just point us the number of clusters. But what if there’s like thousands of data points, we would require computer knowledge to group them. Here is where KMeans comes in. Before we look into KMeans, lets see how we measure distance between two datapoints.  By studying matrix and vector, we all should have basic knowledge towards distance calculation. One most common formula is the Euclidean distance. To find the distance between two vectors, we use this formula:  Distance Metric  We can then cluster dataset by finding distance between each point, this is also one technique  that kmeans use to group datapoints. Beside Euclidean distance, there’s also a bunch of other types that we can use to find distance such as Cosine, Mahalanobis, Cityblock etc... But I believe that the most important/required is just Euclidean.  KMeans Before this, we need to understand intra-distance and inter-distance. Intra distance is the  distance between two points in the same cluster and Inter-distance is the distance between two points in a different cluster. The goal of the algorithm is to minimize intra and maximize inter-distance. So how does KMeans work? Let’s label k as the number of clusters inside the datapoint. The algorithm randomly selects a range of k cluster (starts from 1 - k). It selects a random datapoint, then attempt to converge by finding distance between datapoint and putting them into groups. We then update the centroids and keep looping until it converges.  There are a lot of clustering algorithm that can help us find the number of clusters in a data  point. But the two main evaluation is External and Internal. External is to compare with a known cluster, and internal is to use algorithm and figure out the best cluster number for that dataset.  KMeans Limitation  The problem about KMeans algorithm is that it always starts with a random centroid given the  random range of k clusters. So, if the algorithm were to have a bad initialization, it would have a very long run-time before it converges. However, that’s why we have clusters algorithm like elbow and silhouette to find the good number of clusters first before using the KMeans algorithm to converge the clusters. Another thing about KMeans is that if the cluster has an arbitrary shape, its hard for it to minimize intra and maximize inter distance, which is another disadvantage of the algorithm. Lastly is data that is corrupted or noisy type, Kmeans could not detect it as well.  Although there’s some sort of limitation, we also created KMeans++ to fill in some limitation. KMeans++ allows the algorithm to have a better initialization of the centroids. Although it stills pick a random centroid, after the first convergence, it picks the centre of that cluster, then re attempt to converge a new cluster. The algorithm help decreases the amount of iteration since it gets closer to the center every iteration instead of further away.  