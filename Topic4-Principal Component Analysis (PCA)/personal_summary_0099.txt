Topic 3 – Clustering  3.1 – Welcome to unsupervised machine learning  Where a dataset is not labelled, clustering is used to find correlations and patterns in an unstructured data.  3.2 – Measuring Distances  Distance metrics are used to define distance d(xi,xj). This is used to measure how similar a set of data instances are.  Cosine distance is the angle between two vectors, whereas Euclidean distance is the length of the straight line between two points.  (Insert image from topic 1 content to show distance calculations)  Mahalanobis distance is used in multivariate space.  M Represents covariance matrix.  Cityblock/Manhattan distance is like Euclidean distance is calculated as follows:   Minkowski distance is a middle ground between Euclidean and Manhattan distance, when p = 1 the distance is known as Manhattan distance and when p = 2 then it is Euclidean:  Jaccard distance is used to find diversity between two sets:  Example from the topicly Quiz:  Consider two baskets of fruits denoted by sets A = {'banana','orange','grapes','pear'} B = {'apple', 'banana', 'grapes', 'pear'} The Jaccard distance (to measure similarity) between A and B will be:  The intersection between these is: A ∩ B = {'banana', 'grapes', 'pear'} The union between these is: A ∪ B = {'banana', 'orange', 'grapes', 'pear', 'apple'}  J (A, B) = |A ∩ B| / |A ∪ B| = 3 / 5 = 0.6 = Jaccard similarity coefficient  1 – 0.6 = 0.4  Therefor J = 0.4  3.4 – Clustering and its applications  Clustering at its core clusters. It groups up unclassified data from differing features based on similarity or differences. It is primarily useful in discovering clusters and ways to organise a dataset. Clustering is ultimately used to discover similarities between different data points and to then group them together in a way which can be interpreted.  Clustering is done by getting the objective function to:  -  Minimise intra-distance, which is the distance between points in the same cluster. -  Maximise inter-distance, which is distance between points in different clusters.  I.e. group things which have a good ratio of intra-distance between internal cluster points to inter-distance between different clusters.  Step 1: Get a metric by which objects can be clustered. What is the clustering going to be based off?  Step 2: Create objective function that can achieve results.  Step 3: Create optimisation algorithm for the function.  3.5 – How K-means works  In Kmeans k represents the centre point of clusters, so Kmeans has k clusters. Certain points are determined to be a part of a certain cluster if its point is closest to that k then any other. There are two methods that Kmeans uses to determine the best centroids, these are:  Of which these points continue and cycle until a useful grouping of data points is found.  (inset K means workbook image)  3.6 – Evaluation of Clustering  As with all machine learning methods, Kmeans needs to be evaluated for finding optimum k values. The two main methods of evaluation are:  -  External assessment: compare results with an established clustering. Known as Gold  standard or ground truth. Internal assessment: Determine if certain assumptions are followed.  -  Rand Index is the function that compares the gold standard/ground truth or known/established clustering with the results. So, comparing the clusters from the algorithm C’ with the clusters from the ground truth cluster C.  Purity is an evaluation technique of clustering that measures the accuracy by calculating the number of correctly assigned instances and then putting that over the total number.  Mutual Information is like rand index however it determines both how similar C and C’ are and is the way in which they are similar useful at all.  Silhouette Coefficient is a way to measure how similar a point is to its cluster.  3.7 – Limitations of Kmeans  Elbow Method is used to determine the number of clusters, which is one of the main challenges for Kmeans.  3.8 – Clustering with Kmeans++  Kmeans++ is used to select centroids. Kmeans++ selects the first centroid at random but then selects the rest based off of the first one.  