Machine learning (Pass Task- Topic 3)  Unsupervised Learning  The main two forms of unsupervised learning are,  ‚ñ™  Clustering ‚ñ™  Dimensionality reduction  Distance measures  Distance measures are functions that define a distance d (xi, xj) between any two data instances xi and xj. Distance matrix is a square matrix containing pairwise distances.  The properties of distance matrix are,  There are different types of distance measurements used in the field of machine learning. Some of them are as follows,  1.  Euclidean Distance  ordinary straight-line Distance between the two points  2.  Cosine Distance  Cosine Distance = 1 ‚Äî Cosine Similarity          The intuition behind this is that if 2 vectors are perfectly the same then the similarity is 1 (angle=0 hence ùëêùëúùë†(ùúÉ)=1) and thus, distance is 0 (1‚Äì1=0).  Given two n-dimensional vectors of attributes, A and B, the cosine similarity, cos(Œ∏), is represented using a dot product and magnitude as  3.  Mahalanobis Distance  The Mahalanobis distance (MD) is the distance between two points in multivariate space.  Where ‚ÄòM‚Äô is the covariance matrix of the data. The elements of covariance matrix in the i, j position is the covariance between the i-th and j-th elements of a vector.  Mahalanobis distance can be thought of scaling each data dimension by its variance and adjusting for their relationships. When data are independent, i.e., M=I (identity matrix), Mahalanobis distance becomes same as Euclidean distance.  4.  Manhattan Distance  This distance measure yields results like the Euclidean distance. However, using Manhattan distance, the effect of a large difference in a single dimension is dampened (since the distances are not squared).  5.  Minkowski Distance  The Minkowski distance defines a distance between two points in a normed vector space.  ‚ñ™  When P=1, the distance is known as the Manhattan distance. ‚ñ™  When P=2 the distance is known as the Euclidean distance.         6.  Jaccard Distance  The Jaccard distance is a distance used to measure diversity of any two sets. Consider any two instances xi and xj as binary vectors indicating presence or absence of features.  Clustering Algorithms  Clustering is a technique where without any prior knowledge of the labels or categories, related data points will be grouped together based on their characteristics. It measures the similarity between data points and group them in order to find natural clusters in the data.  The goal of the algorithm is to,  ‚ñ™  Minimize intra-distance (distance between points in the same cluster) ‚ñ™  Maximize inter-distance (distance between points from different clusters)  There are various types of clustering algorithms. Namely,    Hierarchical Clustering   K-Means Clustering   Dbscan   Spectral Clustering, Etc.  These algorithms differ in terms of the way they measure similarity between data points, the way they group data points together, and the number of clusters they produce.  K-Means Clustering  The algorithm chooses k initial cluster centroids at random (k is the desired number of clusters). Then, it iteratively assigns each data point to the cluster whose centroid is closest and changes each cluster's centroid to reflect the mean of the data points that are given to it.  Evaluation of Clustering  There are two main categories of evaluation methods for clustering:        ‚ñ™  External assessment: compare clustering performance against a known  ‚ñ™  clustering (often called Ground truth or gold standard). Internal assessment: determine if clustering follows certain intrinsic assumptions (e.g., cluster-to-cluster distance or cluster size etc.).  o  Examples:  Silhouette coefficient, Dunn index etc.  1.  Rand Index  It is a measure of the similarity between two data clusters.  2.  Purity  Purity is a way of quality measurement in clustering methods. The accuracy of this assignment is measured by counting the number of correctly assigned instances and dividing by the number of total instances.  3.  Mutual Information  It is a measure of the dependence between two random variables. Mutual information is a function that measures the agreement of the two clustering assignments A and A‚Ä≤ in terms of how informative one is about the other, ignoring permutations. To put it simple, how informative is A about A‚Ä≤.  Let‚Äôs assume that clustering partition A has K clusters and the partition A‚Ä≤ has K‚Ä≤ clusters, in this case the Mutual Information of these two clustering assignments are computed as: MI (A, A‚Äô)  4.  Silhouette Coefficient  This takes into account both the cohesion and separation of clusters. The Silhouette Coefficient ranges from -1 to 1, with a higher value indicating better clustering results.            The Silhouette Coefficient of a data point i is defined as follows:  s(i) = (b(i) - a(i)) / max(a(i), b(i))  where a(i) is the average distance between i and all other data points in the same cluster, and b(i) is the average distance between i and all data points in the nearest cluster (i.e., the cluster with the smallest average distance to i) that is different from the one i belongs to.  The clustering result is good, with well-separated clusters and high cohesiveness within each cluster, according to a silhouette coefficient that is near to 1. When the Silhouette Coefficient is near to 0, the clustering outcome is not clearly better or worse than a random clustering.  Limitations of Clustering  The most important limitations of simple Kmeans are:  ‚ñ™  Random initialization means that you may get different clusters each time. As a solution, we can use a Kmeans++ initialization algorithm to initialize better. ‚ñ™  We have to supply the number of clusters beforehand. We can use the Elbow  ‚ñ™ ‚ñ™  method to choose K, but it may not be straightforward. It cannot find clusters of arbitrary shapes. It cannot detect noisy data points, i.e. data points that should not be taken into account for cluster analysis. (The K-median method is less affected but cannot identify noisy data points either.)  Finding a useful number of clusters ‚Äì Elbow Method  Elbow method is to run the Kmeans clustering algorithm for a range of values of K, compute the sum of squared error (SSE) as:    Clustering with Kmeans++  Kmeans++ is a modification of the original Kmeans clustering algorithm that aims to improve its convergence rate and quality of results, especially for high-dimensional datasets.  The Kmeans++ algorithm works as follows:  o  Choose the first centroid uniformly at random from the dataset. o  For each subsequent centroid, choose the next centroid from the remaining data points with probability proportional to the square of its distance to the nearest already chosen centroid.  o  Run the standard Kmeans algorithm with the chosen centroids as initial cluster  centers.  The key idea behind Kmeans++ is that by selecting centroids that are far away from each other, the algorithm is less likely to converge to a suboptimal solution where some clusters are too large or too small. By choosing the next centroid with a probability proportional to its distance to the nearest already chosen centroid, the algorithm favors data points that are far away from existing centroids, which helps to spread out the centroids evenly.   