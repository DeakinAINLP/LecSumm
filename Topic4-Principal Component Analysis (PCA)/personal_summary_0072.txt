Evidence of learning Summary of this topic Two forms of unsupervised learning covered by the unit:  clustering, dimensionality reduction  Distance metrics Distance metrics are used to quantifiably determine how similar two instances are to each other  They are used often in both supervised and unsupervised learning models  Mathematical definition: A function ùëë(ùë•ùëñ, ùë¶ùëó) that defines a distance between instances xi and yj  All distance measures must satisfy three properties:  1.  The distance between an instance and itself is 0 2.  For any two different instances, the distance must be non-negative 3.  Distance follows triangular inequality. I.e.: Between 3 instances, the distance between  instances 1 and 3 is always less than or equal to the sum of the distances between instances 1 and 2 and between instances 2 and 3  Types of distance measurements:  -  Euclidean distance: The distance between two points if a straight line was drawn between them  -  Cosine distance: The angle of difference between two points relative to the origin -  Mahalanobis distance -  Cityblock/Manhattan distance -  Minkowski distance - Jaccard distance  Clustering The goal of clustering algorithms is to:  -  Group unlabelled data objects with similar properties together -  Discover clusters in the data -  Find a valid or useful organisation of the data  The goals of objective functions in clustering algorithms are to:  -  Minimise the intra-distance between points in the same cluster -  Maximise the inter-distance between points from different clusters  K-means This algorithm works in the following way:  Initialize k cluster centroids randomly.  1. 2.  Assign each data point to the nearest centroid. 3.  Recompute the centroids as the mean of the data points assigned to them. 4.  Repeat steps 2-3 until convergence (i.e., until the centroids no longer move significantly).  Limitations:  -  The algorithm may produce different clusters each time because the cluster centroids are randomly initialised. We can use the Kmeans++ algorithm to improve initialisation.  -  -  -  The algorithm needs to be given the number of clusters in the dataset beforehand. We can use the Elbow method to choose the number of clusters but it is not always straightforward. The algorithm can‚Äôt find arbitrarily shaped clusters due to it using the mean of points as the centroid of clusters. It can‚Äôt detect noisy data points (i.e.: outliers) that should not be used for cluster analysis. K- median is less affected by noisy data points but it can‚Äôt detect them either.  Evaluation of clustering All machine learning models need to be evaluated to check if they are useful  There are two main categories of evaluation for clustering:  -  -  External assessment: Comparing the clustering against a ground truth, i.e.: an answer that is known to be correct Internal assessment: Evaluate whether the clustering follows certain assumptions such as distance to other points, checking the number of points in the cluster, etc. (examples: Silhouette coefficient, Dunn index)  Rand Index The Rand index is a function that compares the clustering of points from an algorithm (C‚Äô) to the correct clustering of points according to a ground truth (C).  The Rand index is computed as:  Where:  - a = the number of pairs of data instances that are in the same cluster in both C, C‚Äô -  b = the number of pairs of data instances that are in the different clusters in C and in  -  different clusters in C‚Äô c = the number of pairs of data instances that are in the same cluster in C but in different clusters in C‚Äô  -  d = the number of pairs of data instances that are in the different clusters in C but in the  same clusters in C‚Äô  Purity Purity is measured as follows:  1.  Determine the majority type of point in each cluster. 2.  Count the number of points in each cluster that belong to the majority type. 3.  Sum the total number of counted points and divide by the total number of points to obtain  the purity result.  It is important to select a fair amount of clusters when performing clustering on data points, otherwise if there were N points and they were clustered into N clusters, the purity would be 100% with exactly 1 point in each cluster.   Mutual information Mutual information is a function that measures the agreement between the clustering from the algorithm (C‚Äô) compared to the clustering of the ground truth (C‚Äô) in terms of how informative one is about the other.  That is, mutual information quantifies the extent to which knowing the clustering assigned by the algorithm helps us predict the true clustering, and vice versa.  Silhouette coefficient A measure of how similar an object is compared to its assigned cluster (cohesion/similarity) compared to other clusters (separation/difference)  No ground truth cluster assignments are needed for this to work.  As a function, the value of the silhouette coefficient function ranges from -1 to +1, where -1 means that the object likely belongs to another cluster and +1 means the object is well matched to its current cluster.  The formula is:  Where:  a(i) is the average distance of the i-th object with all other objects in the same cluster  - -  b(i) is the lowest average dissimilarity of the i-th instance with all other clusters.  Elbow method The Elbow method is used to find the number of clusters to input for the K-means algorithm.  It works by running the K-means algorithm with a range of different K values each time, then computing the sum of squared error (SSE) for that value of K.  SSE formula:  After plotting the values of SSE against the number of clusters chosen, K we can sometimes see an elbow-like shape forming where the gradient changes rapidly:     If there is no clear rapid change in the gradient as in the second graph, then the Elbow method can‚Äôt help with finding the best value for K.  Kmeans++ Steps for the algorithm:  1.  Choose the first centroid randomly 2.  Choose the next centroid based on a probability function, where points far away from any existing centroid are more likely to be chosen as the next centroid. This ensures that centroids are spread away from each other, while avoiding being placed where there are no data points.  3.  Repeat step 2 until K clusters are initialised. 4.  Continue with the K-means algorithm as normal  The probability function used is:  Where D(x) is the shortest distance from a data point to the closest centroid that‚Äôs already been chosen.  Other clustering algorithms -  Hierarchal clustering -  DBSCAN (density based) - Shape-based clustering  Hierarchical clustering Used for finding clusters with a predetermined order.  Two types:  1.  Agglomerative clustering (bottom-up): Each object starts with its own cluster, then pairs of  clusters are merged as the algorithm moves up the hierarchy.  2.  Divisive clustering (top-down): All objects start as part of one cluster, then clusters are split  as the algorithm moves down the hierarchy.    Hierarchical clustering forms a hierarchical tree diagram or dendrogram. Cutting the tree at different heights will produce different precisions of clustering.  In agglomerative clustering, clusters are merged with its next most similar cluster. This is repeated until we have a single cluster.  There are several ways to find the similarity between two clusters:  1.  Single-link: the distance between closest points 2.  Complete-link: the distance between the furthest points 3.  Centroid: the distance between the centroids 4.  Average-link: the average distance between pairs of elements from across cluster pairs  In divisive clustering all objects starts as part of the same cluster, then a clustering algorithm that produces at least two clusters (such as Kmeans) is used to recursively split the cluster into smaller clusters. This is repeated until each object is assigned to its own cluster.  