Distance Measurement  Describes similarity or distances between data points. Distance measurement types include:    Euclidean – output is the absolute distance between two data points in normal space   Cosine – angle measured between two vectors where magnitude doesn’t matter   Maholanobis – uses a covariance matrix to calculate absolute distance between two vector  points in multivariate space. It removes the effect of repetitive variables.    Cityblock/Manhattan – measures differences in absolute values, where the effect of a  potentially large vector distance in one dimension is tempered    Minkowski – absolute distance derived from Euclidean & Manhattan distances   Jaccard – measures distance (similarity or diversity) between two sets  Data Clustering  Find patterns or property similarities in large data sets when data is not labelled and groups them together. The most popular method of clustering an unlabelled set uses k-means.  K-means  Is where the distance of (teachable) data points from an average centre point k (centroid) of a particular cluster is measured. Data points are then assigned to the cluster with the closest centroid. This process is repeated/looped for all data points in the set, where the centroid is then adjusted to the new cluster average. Once no more significant changes to centroids are observed, convergence has occurred and the clustering process is stopped.  K-means++  Method of randomly defining an initial cluster centroid value, searching for others based on this selection before iterating and continually optimising towards convergence. In other words, this method improves algorithmic performance.  Limitations of K-means  An assumed number of starting data clusters can lead to centroid redundancies (if k too high) or alternatively centroids are shared between data clusters (if k too low). Also, randomised initialisation may result in a different cluster numbers each time. The elbow method is used to help visually determine (chart) an appropriate k value within a data set.  Other Clustering Algorithms  Hierarchical  There are two types:    Agglomerative (bottom up) – clusters are merged   Divisive (top down) – clusters are split  DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  Data with many nearby neighbours is clustered together and expanded upon, with data points in low density regions being designated outliers (or noise).  Shape Based Clustering (VT & iVAT)  Provides an insight into the underlying structure of data which helps identify an appropriate or optimal number of clusters. This method creates a visual “map” indicating density though colour, providing a “suggestion” of clustering.  Clustering Evaluation  Multiple methods are used to assess the effectiveness of the clustering algorithm.    External (supervised) evaluation – gold standard employed where a labelled ‘benchmark’    dataset is used to evaluate performance Internal (unsupervised) evaluation – intra and inter clustering distances are considered. For example, a cluster with data points close to one another (intra) is considered a good result, likewise data points from separate clusters with large (inter) is also considered a good result.  Rand Index  Is a measure of the similarity of two data clustering’s. It has a value between 0 and 1, where 1 indicates that the clustering’s are the same.  Purity  A calculation which uses the number of correctly assigned instances within each cluster, then divides this by the total number of known instances for the entire data set.  Mutual Information  Measures the agreement of mutual variables between two clusters. It quantifies information obtained from one random variable through observation of another random variable. If it’s informative, then the clustering method is working well.  Silhouette Coefficient  Measures the similarity of a variable to its own cluster with a range of -1 to +1. In other words, the average distance between each instance of the same cluster is compared/measured against that of other clusters. A higher value indicates that the clustering method is suitable.     