 1.  SVM  Support  vector  machines  are  another  supervised  learning  algorithm  used  for  both classification and regression. The fundamental idea behind SVM is to create a hyperplane as a line or a plane dividing two classes of data. SVM work well with small datasets and high dimensional dataset.  Though  SVMs  are  accurate,  can  be  used  to  solve  several  problems,  and  are  easy  to interpret and train, SVMs are computationally complex and expensive. And are sensitive to outliers.    Linear SVM  Linearly separable data  Image source: https://www.baeldung.com/cs/nn-linearly-separable-data    Non-Linear SVM  Image Source: https://jamesmccaffrey.wordpress.com/2019/04/27/the-difference-between-linearly-separable-data-and-a-linear-classifier-in-machine-learning/  As it is evident from the placement of data, the first picture depicts the linearly separable Data, which is being divided by a line. However, the second picture has different clusters and are unable to separate.  2.  Popular Terms  Some of the popular terms of SVM are, Distance  margin  -  The  distance  between  the  hyperplane  and  the  nearest  data  points called the distance margin. The better the gap between the two classes, the greater the distance margin.  Finding  a  hyperplane  that  divides  the  two  classes  with  a  zero  distance  margin  is  the objective  of  hard  margin  SVMs.  This  indicates  that  no  data  points  are  located  on  the incorrect side of the hyperplane. Finding a hyperplane that divides the two classes with the greatest distance margin is the aim of soft margin SVMs. The distance margin is still as wide as it can be, even though some data points are on the incorrect side of the hyperplane.  The function that converts data from a lower-dimensional space to a higher-dimensional one is known as a kernel in support vector machines (SVMs). By doing this, the data is transformed into a space in which the classes can be distinguished linearly.  The simplest kernel function is the linear kernel. It merely translates the information to a higher-dimensional space in which the classes may be distinguished linearly. Polynomial kernel: This kernel function uses a polynomial function to translate the data to a higher-dimensional space. Problems that cannot be resolved linearly in the original space can be resolved using this method.  RBF kernel: This kernel function uses a Gaussian function to translate the data to a higher- dimensional space. This kernel function is incredibly strong and can be applied to several scenarios.  SVR â€“ Support Vector Regression  This is the model that is used to predict continuous values. It is a subtype of the more versatile support vector machine (SVM), which may be applied to both classification and regression.  Finding  a  function  that  minimizes  the  prediction  error  and  roughly approximates  the  relationship  between  the  input  variables  and  a  continuous  target variable is the aim of SVR. By increasing the space between the training data points and the decision border, the function is discovered. The training data points are divided into the positive class and the negative class by a hyperplane called the decision boundary.  Reflection on the knowledge gained.  A type of machine learning method called support vector machines (SVMs) can be applied to both classification and regression problems. SVMs operate by locating the hyperplane that divides the data points into two classes most effectively. The data is split into two parts by a hyperplane, which is a line or plane. The two categories for classification are "positive" and "negative." The two classes in regression are "actual" and "predicted."  A line or plane known as a hyperplane separates the data into two sections. A hyperplane could be a line that separates the data into two zones, one above and one below, in a two-dimensional space.  Support  vectors:  The  data  points  that  are  closest  to  the  hyperplane  are  the  support vectors. By maximizing the space between the support vectors and the decision boundary, the hyperplane is discovered. The data are divided into two classes by a line or plane known as the decision boundary.  Margin: The margin is the separation between the nearest data points and the hyperplane. The model will be more accurate the wider the margin.  