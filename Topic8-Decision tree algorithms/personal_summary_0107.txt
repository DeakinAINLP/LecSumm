 ● SVM stands for Support Vector Machines. SVMs are machine learning methods that are used for the classification use cases especially when we face high-dimensional data as we have seen previously with the curse of dimensionality.SVMs come under supervised machine learning models that can be classified into linear and non-linear.  ● Furthermore, they are also useful in cases where the number of features exceeds the  number of data points in the dataset. The support vectors in SVM are a subset of all the training points that are used to decide where an unseen data point may be classified in the test phase. This makes the models extremely memory efficient since they don’t need to take into account all the data points at once. SVM models also have various kinds of kernel that it can operate upon making it a viable choice for multiple use cases. ● The linear SVM models are a good introduction to how support vectors are able to  separate the data into different classes. This is achieved by fitting a hyperplane between the sets of data and when a new data point is added, depending on which side of the hyperplane it lands, it is classified accordingly.  ● The non-linear SVM, as the name suggests, cannot simply classify the data points using a linear hyperplane, but instead needs to find a non-linear boundary that fits between the classes of data. One can imagine a concentric dataset that has a group of data near the core of the circle while another group of data points is on the outer rim, much like an atomic model where the nucleus is the core group and electrons are the outer group. Then a boundary between the electrons and the nucleus would be the hyperplane for that model.  ● But It is not easy to achieve this boundary in the dataset as it is because what we are looking for ultimately is the hyperplane that distinctly separates the dataset. In such cases, we need to use the famous kernel trick in SVM.  ● The Kernel trick is a technique in which the lower-dimension data is transformed into a  higher-dimensional space and plotted in a manner where a plane can cut through the two, now separated spaces. This hyperplane can then be lowered back to the lower-dimensional space where the original data resides. This conversion from lower to higher dimensions is the kernel trick in SVM  ● It is also possible to classify more than two classes in SVM. This can be achieved through one vs all or one vs one technique. As per the names, SVM just traverses through all the possible classes and attempts to classify them one by one.  