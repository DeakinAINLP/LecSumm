Topic 7 overview  SVM (Support Vector Machine) is a popular machine learning algorithm used for classification tasks. SVM aims to find the hyperplane that separates the data points of different classes with the maximum margin. It is classified into two categories. They are    Linear SVM   Non-linear SVM  SVM formulation:  In the case of SVM, formulation involves defining the problem of finding a hyperplane that separates the data points of different classes with the maximum margin. The formulation specifies the objective function and the constraints that need to be satisfied to achieve this goal. It also identifies the parameters that need to be optimized, such as the weight vector w and the bias term b.  We have also learnt SVM formulation for linear saperable data and linear non-saperable data.  Kernal trick:  Kernel trick is a mathematical technique used in machine learning to extend the feature space of a dataset without computing the coordinates of the data points in the higher- dimensional space. It is commonly used in algorithms like SVM, which operate in high- dimensional feature spaces, to efficiently handle complex datasets and improve classification accuracy.  Nonlinear SVM  Nonlinear SVM (Support Vector Machines) are a type of machine learning algorithm used for classification and regression tasks. Unlike linear SVM, nonlinear SVM can handle data that is not linearly separable by transforming the input data into a higher-dimensional space.  Nonlinear SVMs use a kernel function to transform the input data into a higher-dimensional feature space, where it becomes more likely that the data will be linearly separable. The most used kernel functions are polynomial kernels and radial basis function (RBF) kernels.  Multiclass classification in SVM:  SVM can also be extended to multi-class classification problems, where the goal is to classify data into more than two classes.  There are two common approaches to solving multi-class classification problems using SVM:  One-vs-One (OvO) approach:  In this approach, SVM trains a separate binary classifier for every pair of classes in the data set. For example, if there are K classes, K(K-1)/2 classifiers will be trained. During testing, each classifier is used to predict the class of a given input, and the class with the most number of votes is assigned as the final output. The main advantage of this approach is that  it is computationally efficient, but it may lead to ambiguous results if some of the classifiers predict conflicting classes.  One-vs-All (OvA) approach:  In this approach, SVM trains a separate binary classifier for each class in the data set, where each classifier is trained to distinguish that class from all the other classes. During testing, each classifier is used to predict the likelihood of a given input belonging to its class, and the class with the highest likelihood is assigned as the final output. The main advantage of this approach is that it is more reliable than the OvO approach, but it is more computationally expensive since it requires training K separate classifiers.  Polynomial kernel function:  The polynomial kernel function maps the input data into a higher-dimensional feature space by computing the inner product of the input vectors in a polynomial form. The degree of the polynomial is a parameter that can be tuned during training to find the best fit for the data. The polynomial kernel function is defined as:  K (x, y) = (x.T * y + c) ^d  Where:    x and y are input data vectors   T is the transpose operation   d is the degree of the polynomial   c is a constant that can be used to shift the decision boundary  RBF kernel function:  The RBF kernel function maps the input data into a higher-dimensional feature space by computing the similarity between input data points. The RBF kernel function is defined as:  K (x, y) = exp(-gamma * ||x-y||^2)  Where:    x and y are input data vectors   ||x-y|| is the Euclidean distance between the input vectors.   gamma is a parameter that controls the shape of the decision boundary. A larger  gamma value results in a more complex decision boundary.  We also learnt about statistical learning theorem of SVM.     