The widely used supervised machine learning technique known as the Support Vector Machine (SVM) may be applied to both classification and regression applications. SVMs function by locating the perfect line or hyperplane in a high-dimensional space that can separate the various classes of data points. (Wikipedia Contributors, 2019)  One of the fundamental ideas in the Support Vector Machine (SVM) method is the maximum margin hyperplane (MMH). The hyperplane is the boundary that, with the greatest feasible margin, divides the two classes of data points in the feature space. The margin is defined as the separation between the nearest data points from each class and the hyperplane. (Penfold, 2017)  Soft edge When the data cannot be separated linearly, the Support Vector Machine (SVM) algorithm is extended with SVM. The goal of soft margin SVM is to maximise the margin between the decision border and the nearest data points while yet allowing for some misclassifications. (Rishabh Misra, 2019)  Kernel trick and non-linear SVM â€“ In this section we learn about the kernel technique, which transforms the input data into a higher-dimensional feature space to enable SVMs to handle non- linearly separable data. (Nimmaturi, 2019)  The Support Vector Machine (SVM) Statistical Learning Theory is a theoretical framework that offers a strong foundation for the creation and study of SVM algorithms. The theory is founded on the ideas of structural and empirical risk minimisation.  SVMs, or support vector machines, were first created to solve binary classification issues. However, SVM may be used to multi-class classification issues utilising several techniques, including direct multi-class classification, one-vs-one (OVO), and one-vs-all (OVA).  