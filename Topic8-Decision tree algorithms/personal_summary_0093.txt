Topic 7 Summary Notes – SVM Models    We are introduced to SVM which is a regression analysis algorithm. It is different to    traditional linear regression techniques in that it will find a hyperplane to fit data points in a continuous space. In terms of using SVM with linear non-separable data, it is quite a bit different, and we must accept that there will be some trade-offs with the margin and the number of errors. It can depend on the data but generally we shouldn’t interfere with the boundary despite having some outliers. It can be okay to have some of the data points misclassified.    Generally in SVM we prefer model complexities that are simple with small errors to use. We can get an idea of the complexity of a hypothesis class by understanding the number of instances it can shatter which is also called its Vapnik-Chervonenkis dimension. We can also see that maximizing margins within a model will result in a less complex model and therefore a better fit for SVM.    For using SVM in Python we must use the libraries StandardScaler and train_test_split for data scaling. We should also use the plot_decision_regions library along with PCA to build the SVM model. It is shown how to regularization in SVM within Python. Lastly we try to use an RBF kernel which is quite different to the linear and polynomial kernel we used previously.  