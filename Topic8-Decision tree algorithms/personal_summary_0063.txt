 The optimization problem is frequently solved by developing an identical problem called a dual problem.  In quadratic programming, the initial optimization problem is called the primal problem. The solution to the dual problem gives a lower bound to the solution of the primal problem.  The soft margin dual problem is defined when we change the primal problem with soft margins to dual. It remains the same but that there's an upper bound on the Lagrange multipliers.  The linear equation should permit us to outline and consider relationships between two continuous (quantitative) variables.  The difference between what we anticipated and the true value or output of that point, is considered to be the error.  Structural risk minimization seeks to prevent over-fitting by joining a penalty on the model complexity. This means, it prefers easier functions over more complex functions.  Suppose we pick n occasions and assign labels of + and - to them arbitrarily. In case our hypothesis class is rich enough to learn any association of labels to the data, it is sufficiently complex.  Lastly we learnt and practiced the python programming related to this module.  