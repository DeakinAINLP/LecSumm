Summarising the content: Quadratic Programming (QP) – is the process of solving certain mathematical optimisation problems involving quadratic functions.  Specifically, the aim is to optimise a multivariate quadratic function subject to linear constraints on the variables. In QP the original optimisation problem is called the primal problem, the solution to the dual problem provides a lower bound to the solution of the primal problem.  Support Vector Machine (SVM) -  are a set of supervised learning methods used for classification, regression, and outlier detection. SVM is an example of quadratic programming where the aim is to find a hyperplane so that the margin is maximised while satisfying a constraint.  Linear SVM – Where data can be linearly separable with a narrow margin, or for instance where it may not be linearly separable – it is preferred to not interfere with the boundary. It is acceptable to have  large  margins  even  though  some  of  the  constraints  are violated. This is the trade off between the margin and the number of errors in classifying the training data.  Soft  Margin  concept  –  Allows  an  SVM  model  to  make  a  certain number of mistakes and keep the margin as wide as possible  so that other points can still be classified correctly. The soft margin dual problem is defined when we change the primal problem with soft  margins  to  dual.  With  an  upper  bound  on  the  LaGrange multiplier as an exception.   Structural risk minimisation – seeks to prevent over-fitting by incorporating a penalty on the model complexity. This means it prefers simpler functions over more complex functions. The general idea is to minimise the structural risk where h(f) is the complexity of hypothesis function f and λ is a penalty parameter:  Probabilistic Guarantee - Minimise can be achieved by increasing the number of training sample, or it can be done by minimising h which is the complexity of the model. The equation states by reducing the  complexity  of  the  model,  you  have  a  higher  chance  for  smaller  test  values  or  smaller  upper bounds.  Multi-class Classification in SVM – One  vs  All  –  For  each  class  a  binary  SVM  classifier  is  trained  with  samples  from  that  class  being positive samples, and samples from other classes as being viewed as negative examples.  One  vs  One  –  the  SVM  algorithm  trains  multiple  binary  classifiers,  each  trained  to  distinguish between two classes. During prediction – each test sample is passed to all classifiers and a voting scheme is applied on the output to determine the final class label.  