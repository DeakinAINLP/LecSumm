 Support Vector Machines Overview   Support vector machines (SVMs) are supervised learning models for classification and regression.   Can represent non-linear functions.  Based on statistical learning theory.  Achieve high accuracies with large data.  SVM formulation and solution for linearly separable data   SVMs construct a hyperplane or a set of hyperplanes that maximise the margin of  separation between classes.  A hyperplane is a decision boundary that separates the data points of different classes.  A margin is the distance between the hyperplane and the closest data points, called support  vectors.   The primal problem in SVM is to find the optimal hyperplane that separates the data  points with the maximum margin. It can be written as a quadratic optimization(programming) problem with linear constraints, which can be computationally expensive for large datasets.   Using Lagrange multipliers we can convert a constrained optimization (primal problem) into an unconstrained optimization problem. This is called the lagrange dual function.  The dual optimization problem of SVM is to maximize the Lagrange dual function with respect to the Lagrange multipliers, subject to some bounds and linear constraints. Solving the dual optimization problem can yield the optimal values of the Lagrange multipliers, which can then be used to find the optimal hyperplanes and margin. Dual optimization in SVM is computationally more efficient than primal optimization when the number of features is large compared to the number of training points.  SVM formulation and solution for linearly non-separable data   Soft margin concept is used for data that is not linearly separable. It is used in  cases with outliers, noisy or difficult to classify instances.   Soft margin SVM allows some data points to be misclassified or within the  margin of the decision boundary, giving the support vectors, where there are data points that lie on or within the margin of the decision boundary.   However, while this soft margin concept effectively increases the margin width to reduces the classification errors, it also reduces the generalization ability.   Soft margin SVM introduces a regularization parameter (penalizing  misclassification) that controls the trade-off between maximizing the margin and minimizing the classification error.   The soft margin dual problem is also a quadratic optimization problem that  involves finding the optimal values of the Lagrange multipliers for the support vector data points.   However,  Kernel trick and non-linear SVM   The soft margin dual problem only depends on the dot  products of the data points, which allows for using kernel tricks to handle non-linearly separable data.   Kernel trick: SVMs use a kernel function to map the data  into a high-dimensional feature space where linear separation may be possible. Kernel function is a function that computes the dot product of two mapped vectors   Thus, Non-linear SVM can then classify non-linearly  separable data using this kernel trick.  Support Vector Regression   Support Vector Regression (SVR) is a machine learning  technique for regression problems.   SVR uses the concept of support vectors to find a function that minimizes the error between the predicted and actual values.   SVR can handle nonlinear and high-dimensional data by  using kernel functions.   SVR has some advantages over other regression methods, such as robustness to outliers and sparsity of the solution.  Statistical learning theory of SVM   A hypothesis class is a set of functions that can be used to model a learning problem. The  complexity of the hypothesis class can be characterized by looking at how many instances it can shatter. Shattering means that for any possible labeling of the points, there exists a function in the class that separates them perfectly. The number of instances a hypothesis class can shatter is called its Vapnik-Chervonenkis (VC) Dimension.   Based on the concept of Vapnik-Chervonenkis (VC) dimension, which measures the  complexity of a hypothesis class. It can be used to derive a framework for error bounds and generalization guarantees for learning algorithms.   SVMs seek to minimize the structural risk, which balances the empirical risk and the VC  dimension. Maximising margins will result in having a less complex mode. Structural risk minimization attempts to prevent over-fitting by incorporating a penalty on the model complexity.   Probabilistic Guarantee is a property of Statistical Learning Theory that ensures a bound  on the generalization error of SVM.  The upper bound on the generalisation error increases with higher complexity.  The upper bound on the generalisation error reduces with larger training sets .  Multi-class classification in SVM   Multi-class classification: a problem where the output variable can take  more than two values   To extend SVM to multi-class classification, there are two main approaches:  One-vs-all: For each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples. In the prediction phase, the test sample is passed to each model (classifier) and the predicted class is determined based on the highest score obtained from the models  One-vs-one: In this method, the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. A voting scheme is applied on the output of individual binary classifier outputs to determine the final class label.  