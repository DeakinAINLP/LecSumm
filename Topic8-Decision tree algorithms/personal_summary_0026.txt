Topic 7 provided an depth discussion on linear and non-linear Support Vector Machine (SVM) algorithms.  In  a  linear  context  it  is  essentially  a  means  of  optimisation.  There  is  dual  optimisation  elements,  the former of which is known as the primal problem.  The primal problem is maximising the Margin.  The rationale is producing a balance in applying a linear separation philosophy but having a means of the acceptable amount of noisy points (or outliers) or the minimum amount of separation (i.e narrow and close dataset).  The C parameter is the control (hyperparameter) for steering the acceptable level of margin. It provides that balance-point between large margins and fitting the training data.  High values of C will highly penalise misclassification (which may be beneficial if it’s noise/outliers but the overall model remains accurate), conversely a low value will permit a higher level of misclassification.  There are two ways of conducting SVM algorithms one vs one or one vs many. This is a method of down- selecting a model that has performed the best.  One vs One pits the binary classifiers against the others individually. One vs Many is one binary classifier vs the remainder.  Influencing this will be the size of some binary classifiers in comparison to others which may lead to an overpowering bias or if there is sufficiently different sizes of datapoints for features.  In Python, there are 3 main kernals (or hyperparameters) to vary the implementation of a SVM model.    Linear (Similar to Linear Regression, but differences in the method of selection/optimisation)   Polynomial – While a step above a linear regression in complexity, it provides a nominal benefit  in the ability to capture noise / minor outliers. Facilitates a closer margin of separation.    RBF  (Radial  Basis  Function)  is  a  an  exponential  driven  function  which  is  highly  complex  and computationally expensive, but allows for circular “zones” to be fitted over data. But this has to be  balanced  with  the  margin.  It  can  overfit  for  a  given  C  value  if  gamma  is  reduced.  It  has similarities to the K-NN algorithm.  The second portion of the topics learning demonstrated the pros and cons of complexity in models.  There was 2 ways to decrease test error which is bounded by the training error + a variable.  That  variable  is  controlled  by  the  complexity  of  the  model  and  the  training  samples  available.  One  is controllable by the machine learning model utilised and the other is subject to the dataset (out of the modeller’s direct control).  As a consensus, complexity should be reduced as far as practicable. 