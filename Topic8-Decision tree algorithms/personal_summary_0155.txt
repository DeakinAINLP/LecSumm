 1)  In SVM at times, information can be directly separable however with a tight edge. At different  times, because of noise, a portion of the examples may not be directly separable 2)  It is by and large didn't like to disrupt the limit even with little noisy data of interest or  anomalies. It is satisfactory to have huge edges despite the fact that a portion of the limitations are disregarded.  3)  The soft margin dual problem is defined when we change the base issue with soft margins to double. It continues as before with the exception of that there is an upper bound on the Lagrange multipliers  4)  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity. This means, it prefers simpler functions over more complex functions 5)  Multiclass classification in SVM can be done via two methods i.e One vs all and One vs One 6)  In 'One Vs All', for each class, a parallel SVM classifier is trained with samples from that class  being seen as positive models and samples from different classes being seen as negative models  7)  In 'One vs One', the SVM calculation trains different binary classifiers, each prepared to  recognize two classes  Part 2:  1)  When contrasted with AI, conventional measurements is more connected with deduction of signs in the information; though AI is more centered around forecast. Of the multitude of models there are to browse for AI, linear regression is quite possibly of the least complex model you can utilize and is ordinarily utilized as a benchmark while building new AI models in light of persistent information (a logistic regression would be the same on the off chance that you are working with straight out information). You can see the reason why the two insights and AI are essential devices as an information proficient. A decent suggestion to recollect is that insights inclines more towards derivation and AI towards forecast. That being said, it's likewise important that measurements gave the bedrock to AI to be made in any case  https://towardsdatascience.com/how-to-simplify-hypothesis-testing-for-linear-regression-in-python- 8b43f6917c86  2)  Depending on the number of features you have you can either choose Logistic Regression or  SVM. SVM works best when the dataset is small and complex. It is usually advisable to first use logistic regression and see how does it performs, if it fails to give a good accuracy you can go for SVM without any kernel (will talk more about kernels in the later section). Logistic regression and SVM without any kernel have similar performance but depending on your features, one may be more efficient than the other  https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for- beginners/  Part 3:  Support vector machines (SVM) are a famous and strong AI method for order and regression errands. SVM models depend on the idea of finding the ideal hyperplane that isolates the information into various classes. One of the vital elements of SVMs is the capacity to utilize different kernel functions to model non-linear relationships between the input factors and the output variable.  One such bit is the spiral premise capability (RBF) piece, which is a well known decision for SVMs because of its adaptability and capacity to catch complex relationships between the information and result factors. The RBF part has two significant boundaries: gamma and C (additionally called regularization boundary). Gamma is a boundary that decides the width of the portion capability, and C is a regularization boundary that controls the compromise between accomplishing a solid match to the preparation information and a basic choice limit. The decision of these boundaries can fundamentally affect the presentation of a SVM model, making it essential to carefully tune them       