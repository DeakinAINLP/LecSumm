This  topic  I  learned  about  linear  SVM (Support  Vector Machine),  SVM  formulation  and solution  for  nearly  separable  data,  SVM  formulation  and  solution  and  solution  for  linearly  non- separable data, soft margin dual problem, Kernal trick and non-linear SVM, and VC.  Letâ€™s talk about above mentioned topics briefly.  SVM: This is a machine learning algorithm used for the classification and regression analysis. In the case  of  classification,  SVM  finds  the  best  decision  boundary  (hyperplane)  that  separates  the  data points belonging to different classes, while maximising the margin between the classes.  SVM formulation and solution for linearly separable data: The SVM formulation for linearly separable data involves finding the hyperplane that maximise the margin between the two classes.  SVM  formulation  and  solution  for  linearly  non-separable  data:  Once  we  assumed  that  the  data  is linearly separable, what if the  data is  not linearly separable?  Sometimes,  data may  not be  linearly separable, and it is preferred not to interface with the boundary even with small noisy data points or outliers. We  need  a trade-off  between  the  margin  and  number  of  errors  in  classifying the  training instances and this trade-off brings us to the soft margin concept.  Soft  margin  dual  problem:  The  soft  margin  dual  problem  is  defined  when  we  change  the  primal problem with soft margins to dual.  Kernel trick and non-linear SVM: The kernel trick is a technique used in machine learning to implicitly map input the data into high-dimensional feature space, allowing linear models to perform as a non- linear classification. The SVM handle the non-separable data efficiently by mapping the data to higher dimensional space where it can be linearly separable.  The  non-linear  SVM  formulation  involves  using  a  kernel  function  to  transform  the  input  data  into higher dimension space, where the data becomes linearly separable.  Vapnik-Chervonenkis: The Vapnik-Chervonenkis (VC) is a theory in statistical learning that provides a framework for analyzing the learnability of a model. It defines the capacity of a model as the maximum number of training points that it can classify perfectly.  