Topic 7 summary  This topic covered SVM models by exploring SVM as a technique when the output values of the feature vectors are binary. And how to use python programming to implement both linear and non- linear SVM. In machine learning, support vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.  In SVM, we have so far assumed that data is linearly separable, sometimes data can be linearly separable but with a narrow margin. At other times, due to noise, some of the instance may not be linearly separable. It is generally preferred not to interfere with the boundary even with small noisy data points or outliers. It is acceptable to have large margins even though some of the constraints are violated. In practice, we need a trade-off between the margin and the number of errors in classifying the training instances.  This brings up the soft margin concept, it is defined when the training instances are not linearly separable.  The goal of this post is to explain the concepts of Soft Margin Formulation and Kernel Trick that SVMs employ to classify linearly inseparable data. Soft margin formulation is based on a simple premise: allow SVM to make a certain number of mistakes and keep margin as wide as possible so that other points can still be classified correctly. This can be done simply by modifying the objective of SVM. Kernel functions are generalized functions that take two vectors (of any dimension) as input and output a score that denotes how similar the input vectors are. A simple Kernel function you already know is the dot product function: if the dot product is small, we conclude that vectors are different and if the dot product is large, we conclude that vectors are more similar.  Multiclass classification in SVM can be done as follows:  1.  One vs all: In this approach, for each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples.  2.  One vs One: In this method, the SVM algorithm trains multiple binary classifiers, each  trained to distinguish between two classes.   