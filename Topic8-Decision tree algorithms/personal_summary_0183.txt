Introduction  This topic we were introduced to Linear Support Vector Machines (SVM) and Non-linear SVM. A Support Vector Machine is a supervised algorithm that is well suited to smaller but complex data sets.  The primary goal of a SVM is to find the best possibly hyperplane that will separate the data into two classes. In two dimensional data that hyperplane will simply be a line (linear). In higher dimensions it will be represented by a linear equation (non-linear).  An advantage of SVM’s is that they are statistically robust to outliers.  When working with SVM’s the term “margin” is used to describe the distance between the hyperplane and the closest points from each of the separated classes.  The name “support vectors” is given to the points that are closest to the hyperplane. They are termed support vectors because if their position shifts, the hyperplane will shift as well. Hence they “support” the hyperplane – this shows that hyperplane only depends on the support vectors and not on any other observations.  An image might make this all clearer:  Image Source: (Saini 2021)  SVM formulation and solution for linearly separable data  There can be a large number of hyperplanes between the classes to choose from: but the best hyperplane is the one that maximises the margin (the distance between the classes), and it is found by using an optimisation algorithm for the hyperplane by minimising the associated cost function  subject to the constraint  , where:  xi is the feature vector of the ith training sample, yi is the corresponding class label (+1 or -1),     w represents the weight vector of the hyperplane, and  b is the bias term.  The above optimisation problem is known as the primal problem. The computational complexity of the primal problem is O(d3) (with d being the dimension of the feature space – essentially a quadratic programming optimisation problem).  Lagrange Multipliers are useful here, as they give us a strategy to find the local minima and maxima of a function subject to equality constraints (‘Lagrange multiplier’ 2023). This allows us to re- express the primal problem in a way that has a computational complexity of O(n3) with n being the number of instances in the training set. This re-expression is known as the dual problem.  It can be seen that if n > d then solving the dual problem will be more expensive than solving the primal problem. That said, the dual problem is a popular approach as it allows optimisations that can improve its computational efficiency. In addition a “kernel trick” allows the dual problem to work with non-linearly separable data.  SVM formulation and solution for linearly non-separable data  If the data is not easily linearly separable, then workarounds have to be introduced. One such work around consists of widening the margins, knowing that a number of data points will then be wrongly classified. This relaxation of the strict separation requirement is termed creating a “soft margin”.   The slack variables ξi and a regularisation parameter C are introduced. C controls the trade off between maximising the the margin and minimising the classification errors. A smaller value of C gives a wider margin, and allows more training samples to be misclassified. A larger value of C results in a narrower margin and puts more emphasis on correctly classifying the training samples.  The Kernel trick and non-linear SVM  The “kernel trick” describes a technique in dual problem SVM’s whereby the input data is mapped from a lower dimensional input space to a higher dimensional feature space by a kernel function. This allows the input classes to more easily separated. Commonly used kernel functions are the:   Linear Kernel: K(x, y) = x·y  This kernel corresponds to a linear  transformation and is suitable for linearly separable data.   Polynomial Kernel: K(x, y) = (x·y + c)ᵈ  This kernel function allows for non-linear  decision boundaries by considering polynomial transformations of the input features.   Gaussian (RBF) Kernel: K(x, y) = exp(-γ ||x - y||²) This kernel allows for complex non-  linear decision boundaries and is widely used for non-linearly separable data.  Linear Kernel  The formula for a simple regression line is: y = wo + w1x1 with wo  being the intercept of the line and w1 being the slope of the line. This equation has a closed form solution: [wo,w1] = ([1, x1]T[1, x1])- 1[1, x1]T. This closed form solution means we can solve linear regressions without the need to iteratively improve our optimisation functions.  Page 3 of 7  Martin Paulo  SIT720 – Machine Learning  ID: 223587421  Example of a Linear Regression  Doing a worked example in Python, with x=[1, 2, 3, 4] and y=[1, 2, 2, 4]:  Knowing that W is [0.5 0.6] allows us to make predictions. What would y be for an x of 2.5?  y=(1 2.5)(0.5  0.6)=1×0.5+2.5×0.6=0.5+1.5=2   Statistical learning theory of SVM  The statistical learning theory of Support Vector Machines gives us a theoretical framework for understanding the the generalisation performance and error bounds of SVM’s. It is rooted in the principles of statistical learning and focuses on the concept of empirical risk minimisation and structural risk minimisation.  The Vapnik-Chervonenkis (VC) Dimension is a measure of the capacity or complexity of a hypothesis space. It characterises the maximum number of points that can be ‘shattered’ (fitted perfectly for all label assignments) by a given set of hypotheses. The VC dimension plays a crucial role in determining the generalisation properties and bounds of SVM’s.  The statistical learning theory provides theoretical bounds on the generalisation error of SVM’s based on the VC dimension and the number of training samples. These bounds guarantee the performance of SVM’s on unseen test data, allowing for a principled understanding of the model's behaviour and its ability to generalise from the training data.  In short, for best performance, we should try to increase the number of training samples: and at the same time, minimise the complexity of our model.  Multi-class classification in SVM  All the Support Vector Machines examples we have looked at so far have been binary classification problems. However, SVM’s can be extended to handle multi-class classifications. The two main approaches are:   One-vs-All   One-vs-One  One-vs-All  In the one-vs-all approach, also known as one-vs-rest, a separate binary SVM classifier is trained for each class, treating that class as the positive class and the rest as the negative class. If there are N classes, N classifiers are trained. During prediction, each classifier assigns a confidence score for its corresponding class, and the class with the highest confidence score is selected as the final prediction. This approach requires training multiple classifiers but is often computationally efficient and more suitable for large-scale multi-class problems.  One-vs-One  In the one-vs-one approach, a separate binary SVM classifier is trained for every pair of classes. If there are N classes, N(N-1)/2 classifiers are trained. During prediction, each classifier casts a vote for its corresponding class, and the class with the maximum number of votes is selected as the final prediction. This approach requires training multiple classifiers but is generally computationally efficient for moderate-sized multi-class problems. It is thus suitable for problems with a smaller number of classes: but may become computationally expensive as the number of classes increase.  