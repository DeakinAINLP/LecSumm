   The SVM aims to find a hyperplane that maximizes the margin while satisfying the constraint, which can be formulated as a quadratic programming problem. Using Lagrange multipliers, the primal problem is converted into the dual problem, which is solved to obtain the support vectors and classification function, and the dual problem is preferred as it allows the use of arbitrary kernels for non-linear SVM boundaries.    When  data  is  not  linearly  separable,  SVM  uses  the  concept  of  a  soft  margin,  where  slack variables are added to allow misclassification of outliers or difficult instances. The soft margin concept involves a trade-off between the margin and the number of errors in classifying the training instances, and the dual problem with an upper bound on Lagrange multipliers is used to find the support vectors and classification function. In linear regression, the goal is to find a line that summarises the relationship between two continuous variables, where the line has two parameters: the slope and the intercept. The error, which is the difference between the predicted and true value, is minimised by taking the derivative of the error function with respect to the parameters and finding the Moore- Penrose pseudo-inverse of the feature matrix.    The maximum margin makes sense theoretically as it helps to minimise the complexity of the model by maximising the margin, which is an upper bound on the probability that the test error  will  exceed  the  training  error  by  some  value.  The  Vapnik-Chervonenkis  dimension characterises the complexity of the hypothesis class and its upper bound is proportional to the margin, meaning that by maximising the margin, we can minimise the complexity of the model.    Multi-class classification in SVM can be done using One vs All or One vs One approach. In One vs All approach, a binary SVM classifier is trained for each class with samples from that class as positive examples and others as negative, and during prediction, the test sample is passed to each model and the highest score determines the predicted class. In One vs One approach, multiple binary classifiers are trained for each pair of classes, and during prediction, each test sample  is  passed  to  all  binary  classifiers  and  a  voting  scheme  is  applied  on  the  output  to determine the final class label.  