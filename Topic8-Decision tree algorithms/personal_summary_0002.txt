topic 7 covered svm formulation and solution svm is a supervised learning alg used for classification and regression. for linearly separable data svm aims to find the best hyperplane that separates two classes with maximum margin (the distance between hyperplane and the closest datapoints from each class) for non-separable data svm uses a slack variable and regularisation parameter c to allow for misclassification for better generalisation. aim is to minimise the trade-off between maximising the margin and  minimising the classification error. kernal trick model uses the kernal trick, which maps the input data into higher-dimensional space where it becomes linearly  separable. both types of problems are convex quadratic problems statistical learning theory of svm based on the vapnik-chervonenkis dimension and the structural risk minimisation (srm) principle. as mentioned above svm aims to minimise the generalisation  error by balancing model complexity and training error the vc dimension helps quantify the model complexity while srm helps to find the optimal trade off to minimise generalisation error multi-class classification svm is inherently a binary classifier, although it can be extended to multi class classification such techniques for this are ovo where multiple binary svm classifiers are trained for each pair of classes and the final class is decided based on majority voting ovrwhere a binary svm classifier is trained for each class against the rest and the final class is decided based on the highest decision function value svr extends svm for regression problems objective is to find a function that fits the data points within an epsilon tube. similar to classification, svr can use kernal functions for non linear  regression tasks. is also a convex quadratic problem 