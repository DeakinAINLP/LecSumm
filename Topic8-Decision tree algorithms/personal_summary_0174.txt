 Dual optimization problem:      Lagrange multipliers can be used in the SVM formulation to reformulate the original primal optimization issue into its dual form. In place of using the weight vector directly, this enables us to address the issue in terms of the Lagrange multipliers.    The dual optimization issue for SVM with linearly separable data is as follows:  Formula:  Given n data points in a labelled dataset:  (x1, y1), (x2, y2), (xn, yn), where xi denotes the input characteristics and yi denotes the associated class labels (+1 or -1) for binary classification.  Finding a hyperplane with the following definition:  w x + b = 0.  where:    w is the hyperplane's normal vector   b is the bias term   and stands for the dot product.  Under the condition that every data point is accurately categorized with a margin of at least 1, the goal is to identify the best hyperplane that maximizes the margin between the classes.  Solution:    Rewrite the primary optimization issue as a LaGrange problem and introduce the Lagrange  multipliers i for each data point (xi, yi):          L (w, b, ) = yi (w xi + b) - 1 ||w||2 - 12 ||w||2 - i    where the vector of Lagrange multipliers connected to the constraints is = (1, 2,…. n).   The LaGrange must be maximized about the Lagrange multipliers in order to achieve the dual  optimization problem, subject to the following restrictions:    the greatest possible L(w, b, ) = 12 ||w||2 - i [yi (w xi + b)]    To determine the values of w and b, differentiate L(w, b, ) with respect to w and b and set the  derivatives to zero:  ∂L/∂w = 0 => w = ∑ αᵢ yᵢ xᵢ,  ∂L/∂b = 0 => ∑ αᵢ yᵢ = 0.    To get the dual optimization issue in terms of, re-insert the values of w and b into the  LaGrange: subject to i 0 for every i and i yi = 0, maximize () = i - 12 i j yi yj xi xj.     The dual objective function in this case is (), and all pairs of data points are represented by  the inner sum.    To get the ideal values of the Lagrange multipliers, solve the dual optimization problem.   The weight vector w and bias term b may be calculated after we have the ideal i:   For each support vector xi, w = i yi xi, and b = yi - w xi   By determining the sign of (w x + b), it is possible to classify a new input x.  SVM formulation and solution for linearly non-separable data:    The usual SVM formulation and solution must be changed to account for the occurrence of  misclassified points when working with linearly non-separable data.    One strategy is to include a lenient margin that permits certain misclassifications. Soft margin  SVM is what this is called.    The concept and answer for SVM with linearly non-separable data are given below:  Formula:  Given a labelled dataset with n data points, write (x1, y1), (x2, y2),..., (xn, yn) where xi is the input feature and yi is the class label (+1 or -1) for binary classification.             Find a hyperplane with the following definition: w x + b = 0.  where w is the hyperplane's normal vector, b is the bias term, and stands for the dot product.  The goal is to identify the best hyperplane, while allowing for certain misclassifications, that maximizes the gap between the classes.  Solution:    For each data point (xi, yi), add slack variables (i, 0). These variables show the degree of  misclassification or margin constraint violation.    When i = 1, 2,..., n, the optimization problem is: minimize 12 ||w||2 + C i, subject to yi (w xi + b) 1 - i, where C > 0 is a hyperparameter that regulates the trade-off between maximizing the margin and minimizing the misclassifications. It establishes the fine for incorrectly categorized points.     The goal of the objective function 12 ||w||2 + C i is to minimize misclassifications while  maximizing the margin.    By penalizing them, the term C i allows for certain misclassifications.   Utilize methods like quadratic programming or Lagrange multipliers to solve the optimization  challenge.    As in the case of linearly separable systems, this results in the dual optimization issue.   The weight vector w may be calculated by using the formula: w = i yi xi, where the total of all  support vectors is considered, and i > 0.    Any support vector xi that satisfies the equation yi (w xi + b) = 1 - i may be used to determine  the bias term b.    By determining the sign of (w x + b), it is possible to classify a new input x.  Linear regression:    A well-liked supervised learning approach for predicting continuous numeric values is linear  regression.    By applying a linear equation to the data, it models the relationship between the input  characteristics and the goal variable.  The definition of linear regression is as follows:  Given a labelled dataset with n data points, the expressions (x1, y1), (x2, y2),..., (xn, yn) indicate the input features and the associated target values for regression, respectively.            The objective: is to locate a linear equation of the type y = 0 + 1x1 + 2x2 +... + rxr.    where r is the number of input features.   x1, x2,…. xr are the coefficients (also known as weights) associated with the input features.    y is the predicted target variable.    The goal: Is to determine the coefficients' ideal values (0, 1, 2,..., r), which minimize the discrepancy between the projected values y and the actual target values yi.    Different approaches, such as the ordinary least squares (OLS) method, gradient descent, or matrix factorization techniques, can be employed to solve the linear regression issue. The ordinary least squares method, which minimizes the sum of squared discrepancies between the anticipated and actual goal values, is the most widely used technique.      When using the ordinary least squares method, the residual sum of squares (RSS), which has  the formula RSS = (yi - (0 + 1x1i + 2x2i +... + rxri)), is minimized.    By differentiating the RSS with respect to each coefficient, setting the derivatives to zero, and solving the ensuing system of equations, the optimal values of the coefficients 0, 1, 2,..., r may be discovered.  Statistical learning theory of SVM:    The generalization characteristics and risk limitations of SVM classifiers are theoretically  explained by the statistical learning theory of Support Vector Machines (SVM).    The theory is based on ideas from statistical learning theory, which is concerned with figuring out how well a learning algorithm generalizes depending on the model complexity, size of the training dataset, and other factors.  Here are some essential ideas and elements of the SVM statistical learning theory:    Empirical Risk Minimization: SVM seeks to reduce the average loss or error on the training dataset, which is the empirical risk. When it comes to SVM, the misclassified or margin- violating points are the cause of the loss.    Structural Risk Minimization: SVM aims to limit the model's complexity to prevent  overfitting as well as to minimize the empirical risk. To do this, one looks for a hyperplane with a wide margin that maximizes the distance between the classes.    Margin maximization: SVM looks for a hyperplane that minimizes the distance between the  hyperplane and the nearest data points for each class and maximizes the margin.    Support Vectors: Support vectors are the training data points that are on or near the margin.  They are essential in determining the decision boundary and the ideal hyperplane.          The Vapnik-Chervonenkis (VC) dimension is a statistical learning theory notion that describes a learning algorithm's or hypothesis space's capability. The dimensions of the input space and the margin size affect the VC dimension of SVM. It affects the generalization performance and offers a gauge of the model's complexity.    Risk Bounds: The statistical learning theory offers theoretical assurances regarding the SVM's generalization capabilities. Bounds on the anticipated risk (generalization error) may be obtained by regulating the VC dimension and the empirical risk. These constraints reveal information about how successfully.  Multi-class Classification:    We must create the N-binary classifier models for the N-class examples dataset in one-vs.-all  classification.    The number of created binary classifiers and the number of class labels contained in the  dataset must be equal.         Let’s understand with one example by taking three test features values as y1, y2, and y3,  respectively.    We passed test data to the classifier models. We got the outcome in the form of a  positive rating derived from the green class classifier with a probability score of (0.9).    Again, we got a positive rating from the Blue class with a probability score  of (0.4) along with a negative classification score from the remaining Red classifier.    Hence, based on the positive responses and decisive probability score, we can say that  our test input belongs to the green class.  One vs. One (Ovo):       In One-vs-One classification, for the N-class instances dataset, we must generate  the N* (N-1)/2 binary classifier models.    Using this classification approach, we split the primary dataset into one dataset for  each class opposite to every other class.    Taking the above example, we have a classification problem having three  types: Green, Blue, and Red (N=3).    We divide this problem into N* (N-1)/2 = 3 binary classifier problems:  Classifier 1: Green vs. Blue  Classifier 2: Green vs. Red  Classifier 3: Blue vs. Red    Each binary classifier predicts one class label. When we input the test data to the  classifier, then the model with the majority counts is concluded as a result.       