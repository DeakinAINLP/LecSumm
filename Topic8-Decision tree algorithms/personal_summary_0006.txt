SVM is a supervised learning algorithm which is mainly used for classification problems. The SVM algorithm solves convex optimisation problem by finding a hyperplane that maximizes the margin while satisfying a constraint. SVM's primal problem is to maximize margin, which is converted into an unconstrained optimization problem using Lagrange multipliers. This leads to a dual problem where the hyperplane is expressed as a combination of support vectors. The classification function also uses dot products between support vectors and input data. The dual  problem  allows  for  the  use  of  arbitrary  kernels,  leading  to  significantly  nonlinear boundaries. The primal problem has computational requirements of order n^2, while the dual requires n^3, but the dual is popular due to its flexibility with kernels. SVM is designed for linearly separable data, but what if the data is not linearly separable? In such cases, we can use the soft margin concept and allow for some misclassifications using slack  variables.  SVM  with  soft  margin  minimizes  the  sum  of  slack  variables  while  still maximizing the margin . The soft margin dual problem is used to find the solution. The dual problem  is  the  same  as  the  primal  problem,  except  with  an  upper  bound  on  the  Lagrange multipliers. Support vectors are still used in the classification function, and the use of kernels allows for nonlinear boundaries. The linear model seeks to minimize the empirical risk via the square loss function. To solve this minimization problem, we can take the derivative of the error function with respect to the  slope parameter and equate it to zero. The Moore-Penrose pseudo-inverse matrix can be used to solve for the optimal parameters. The statistical  learning theory of Support Vector  Machines (SVM) is based on  the Vapnik-Chervonenkis (VC) Dimension, which characterizes the complexity of the hypothesis class by looking  at  how  many  instances  it  can  shatter.  The  VC  dimension  of  the  optimal  linear separators has an upper bound, which is related to the margin, diameter, and dimensionality of the data. This implies that maximizing the margin will result in having a less complex model, which is why we aim for maximizing margins in SVM. The probabilistic guarantee of SVM provides an upper bound on the generalization error of the model, which can be minimized by either increasing the number of training samples or reducing the complexity of the model. 