Lesson Overview â€“ SVM Models  Support  Vector  Machines  (SVM)  is  a  supervised  machine  learning  algorithm  often  used  for classification and regression analysis.  When working with a binary classification, SVM would work by finding  the  best  hyperplane  which  separates  data  points  in  high-dimensional  space  into  different classes by maximizing the space between the two classes.  Data points which are the closest to the hyperplane are called support vectors, and these data points would govern where the hyperplane is. An optimal hyperplane should be placed in a position where the distance from the support vectors is maximised to accommodate for variances of any new data points. SVM has the ability to perform non- linear  classification,  as well as  linear,  using various kernel functions.  With a  large  enough  training dataset, SVM can achieve high classification accuracy.  With linearly non-separable data in SVM, slack variables can be introduced to handle outliers, errors, and misclassification of data points, while still maintaining  an  optimal  margin  of  space  between  the  two  classes.    VC  dimension  measures  the capacity of a statistical classification algorithm.  Using VC dimensions, decisions can be made in terms of algorithm, parameters and function for a given learning problem.  One vs All and One vs One are two multi-class classification in SVM.  In a one vs all method, each class is  being  trained  separately  to  all  other  classes.    The  class  being  considered  is  viewed  as  positive examples and all other classes are viewed as negative examples.  The predicted class is determined based on the highest output score obtained from the model.  In a One vs One method, SVM trains a binary classifier for each pair of classes.   Each test is then passed to all binary classifiers and the class with the most votes is chosen as the predicted class.  