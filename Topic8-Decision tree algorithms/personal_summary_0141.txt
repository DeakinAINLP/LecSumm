 The major topics that were covered during topic 7 includes SVM, its formulation and solution for linearly separable and non-separable data, non-linear SVM and Kernel trick, Support vector regression, Statistical learning theory of SVM, Multi-class classification in SVM, and how to use python to implement some of the most important topics mentioned above.  The primary reference source was the unit site contents along with the external video links and articles provided. The initial phase dealt with SVM formulation and solution for linearly separable data including dual optimization problem. Next I explored the same topic for linearly non-separable data including the soft margin dual problem. Then, I got a chance to learn non-linear SVM and kernel trick, using which find a line and fit the line to the data points. Another major concept covered was the Support vector regression with example. Then I gained a solid understanding of the statistical learning theory of SVM including an illustration of Vapnik-Chervonenkis (VC) Dimension. Next, I gained knowledge related to the two multi-class classification of SVM â€“ namely one vs all and one vs one. During the final phase, I acquired knowledge about implementing SVM in python using linear kernel which taught regarding one vs rest and one vs one classifications. This was followed by  the implementation of polynomial kernel and RBF kernel which in turn was followed by some python tasks to be submitted and a quiz for the topic was taken in which hundred percent was obtained. (proof attached below)      