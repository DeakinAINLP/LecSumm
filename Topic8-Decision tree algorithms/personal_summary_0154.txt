The main of the SVM is to ﬁnd a hyperplane so that the margin is maximized which ultimately helps in establishing a separation for linearly inseparable data. This can be shown in ﬁgure 1.  Figure 1: SVM hyperplane (NA, 2019)  In a dual optimization problem, the primal problem in SVM is maximizing the margin with the help of Lagrange multipliers. Lagrange multipliers is a strategy for identifying the local maxima and minima of the function that is subject to equality constraints. This duality in its operations yields the dual formulation where the data points are multiplied with the dot product which in terms of linear algebra represents the similarity between vectors. This in the case of linearly separable data.  Data  can  also  be  inseparable  due  to  noise,  therefore  a  trade-oﬀ  between  the  margin  and number  of  errors  in  identifying  the  training  instances  need  to  be  done.  The  trade-oﬀ introduces a soR margin concept where data are not linearly separable. Slack variables are added  for  misclassiﬁcation  of  outliers,  noise,  or  diﬃculty  to  classify  instances.  Thereby allowing for some data to be wrongfully classiﬁed. This trade-oﬀ between large margins and ﬁUng training data can be achieved through the parameter C. The soR margin dual problem is when the primal problem is changed with soR margins to retain the duality.  Linear  regression  helps  to  analyze  the  relationship  between  two  continuous  variables. However,  in  the  case  where  multiple  dimensions  are  introduced,  the  equation  can  be rewriXen to accommodate the d dimensions. Error can be deﬁned as the diﬀerence between predicted and true values. The linear models’ objective is to reduce the empirical risk through square loss. Structural risk minimizations help in preventing over-ﬁUng by imposing a penalty on the model complexity. Despite dimensionality, the model complexity can be minimized by maximizing the margin, resulting in a higher probability for smaller test values.  Multi-class classiﬁcation in SVM can be done as one vs all or one vs one. For one vs one the SVM algorithm trains multiple binary classiﬁers, whereas for one vs all, the SVM classiﬁer is trained with samples from the class being viewed as positive or negative.            References Mahto  KK  (1  May  2023).  “Demystifying  Maths  of  SVM  —  Part  1”,  Towards  Data  Science, accessed  1  May  2023.  hXps://towardsdatascience.com/demystifying-maths-of-svm- 13ccfe00091e      