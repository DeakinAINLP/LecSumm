(SVMs) are techniques used in machine learning for both classification and regression tasks. SVMs look for a hyperplane that divides the data points of various classes with the greatest margin when the data is linearly separable. This margin is the separation between each class's closest data points, or support vectors, and the hyperplane. The SVM formulation needs to be altered when working with linearly non-separable data in order to account for some misclassifications. The soft margin dual issue, also referred to as the soft margin SVM, is created as a result of this adjustment. The soft margin SVM adds a slack variable to permit some misclassification while still trying to achieve a strong class separation. In order to implicitly map data into a higher-dimensional feature space using support vector machines (SVMs), the kernel is utilized. This eliminates the need to manually calculate the transformed feature vectors. By identifying non-linear decision boundaries, it enables SVMs to handle non-linearly separable data effectively. A variation of Support Vector Machines (SVM) used for regression applications is Support Vector Regression (SVR). SVR is made to locate a regression function that maximises the margin around the predicted values while fitting the data. When there are more than two classes, assigning instances to one of the numerous classes is referred to as multi-class categorization. Support Vector Machines (SVMs) can now tackle multi-class problems utilizing two popular methods: One-vs-All (OvA) and One-vs-One (OvO). In the OvA method, a unique binary SVM classifier is trained against the rest of the classes for each class. A binary SVM classifier is trained for each pair of classes in the OvO technique.  