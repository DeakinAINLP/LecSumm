 SVM formulation and solution for linearly separable data: For linearly separable data, the formulation of SVM involves finding the optimal hyperplane that maximizes the margin between the classes. The solution is obtained by solving an optimization problem where the objective is to minimize ||w||Â²/2 subject to constraints that ensure correct classification and a margin of separation. The support vectors, which lie on the margin or violate the margin constraints, define the final decision boundary. The process aims to find the hyperplane that best separates the classes, allowing for perfect separation in the case of linearly separable data.  SVM formulation and solution for linearly non-separable data: For linearly non-separable data, SVM employs techniques like the kernel trick to transform the data into a higher-dimensional feature space where it becomes linearly separable. The formulation and solution process remain similar to that of linearly separable data, but with the use of a kernel function to map the input features to a higher-dimensional space. This allows for the construction of a nonlinear decision boundary that effectively separates the classes. The kernel function implicitly computes the dot product between the transformed feature vectors, eliminating the need to explicitly perform the transformation. The solution involves finding the optimal hyperplane in the transformed space, which corresponds to a nonlinear decision boundary in the original feature space.  Kernel trick and non-linear SVM: The kernel trick is used in SVM to handle non-linear data. It implicitly maps the input features to a higher-dimensional space using a kernel function. This allows SVM to construct a non-linear decision boundary without explicitly computing the transformation. Common kernel functions include linear, polynomial, Gaussian (RBF), and sigmoid kernels. By choosing an appropriate kernel, SVM can effectively handle non-linearly separable data.      