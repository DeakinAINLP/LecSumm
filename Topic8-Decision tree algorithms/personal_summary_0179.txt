 In this topic we learnt about Support Vector Machines (SVM) model. SVM models works for both linear and nonlinear solutions. SVM is considered to be a complex model. SVMs can be linear (separable or almost separable data points) or nonlinear SVM. The main concept of SVM is to discover  the  best  possible  line  or  plane  (hyperplane)  that  can  effectively  divide  data  points belonging  to  different  classes  in  a  feature  space  with  multiple  dimensions.  This  hyperplane  is chosen in a way that maximizes the distance between the hyperplane and the closest data points of each class, which is known as the margin. The margin can be defines as the distance between the support vectors. The data points that are closest to the hyperplane, known as support vectors, are of significant importance as they determine the decision boundary.  The  primal  problem  in  SVMs  is  maximising  the  margin.  This  constrained  optimization  is converted  into  an  unconstrained  optimization  problem  using  Lagrange  multipliers.  The  linear SVM (Support Vector Machine) dual optimization problem involves finding the optimal solution for a binary classification task using a linear decision boundary. The solution to the optimization problem involves finding the values of αᵢ that satisfy the constraints and maximize the Lagrangian function. In linear SVM with almost separable data points, it is acceptable to have large margins even though some of the constraints are violated and also no interference with boundary even with small  noisy  data  points  or  outliers.  Soft  margin  concept  is  used  in  this  case.  Slack  variable parameter© is introduced to bring the soft margin concept. The parameter C can be used as a way to achieve the tradeoff between large margin and fitting training data. High value of C penalize miss-classification and low C value allows more miss-classifications.  In  non-linear  SVMs,  the  features  are  transformed  to  a  higher  dimensional  space  where  data  is linearly separable to handle non-linearity. A Kernel function is used to compute dot products in a high dimensional feature space. It is said that every positive, semi-definite, symmetric function is a  kernel  function.  Some  popular  Kernel  functions  are  Linear  Kernel,  Polynomial  Kernel  with degree p, Radial basis function kernel etcetera. Support Vector Regression is a complex model which is less popular. The classifier defines a margin which looks like a tube. The data points are clear  and  fine  if  they  are  within  the  tube.  Complexity  of  the  SVM  model  can  be  decreased  by increasing the margin. Two kinds of Multi-class classification approach in SVM are One vs All a One vs One. One vs One is having more accuracy, but is computationally expensive compared to One vs All approach.   