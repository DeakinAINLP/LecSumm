This topic’s content takes us into Support Vector Machine (SVM) models and their linear and non-linear implementations. Following the theory introduction, we move into an intro on the Python implementation of it. SVM Formulation and Solution for Linearly Separable Data The first look into SVM is that of SVM models that are applied on linearly separable data. As the name suggests, linearly separable data is data in which the hyperplane can perfectly separate the classes in the data. The aim of SVM models is to find a hyperplane, or in simpler terms a dividing line, that is able to maximize the margin (the distance between the hyperplane and the nearest data points) whilst satisfying the constraint which is the equation that ensures that all training data is classified to the correct class and lies on the correct side of the hyperplane. SVM Formulation and Solution for Non-Linearly Separable Data Data is not always linearly separable. In certain datasets, the data may only be linearly separable with a very small margin and at other times, with the existence of noise, some data instances may not be linearly separable at all. For this reason and the preference to not interfere with the decision boundary, the soft margin concept is employed to find the optimal relationship between the margin and the number of errors when training instances are classified. The soft margin concept makes use of slack variables to allow for outliers, noise and difficult to classify data instances to be classified. This in turn allows some data points to lie on the incorrect side of the decision boundary. In other words, it allows for the misclassification of data points. To account for this, the previously mentioned slack variables are linked to data points and data points that are misclassified are given a slack variable value, thus the SVM with soft margin model can use a formula that accounts for the misclassified instances by looking at those with non-zero slack variable values. The model can then assign less weight to these data points when it is training to minimize how these points skew the model. Kernel Trick and Non-Linear SVM The kernel trick is a function of SVM that allows for a hyperplane to be determined for data points that are not linearly separable. This is done by mapping the original data points to a higher-dimensional feature space in which they can be linearly separated. The kernel trick doesn’t actually compute the transformed feature vectors but rather it uses the kernel function to calculate the dot products between the transformed vectors. This can help prevent the kernel trick from being too computationally expensive. There are several different kernel functions but the most commonly used ones include the linear and polynomial kernels, the Gaussian or RBF kernel and the sigmoid kernel. Without the kernel trick, SVM would not be able to handle non-linearly separable data and as such it is an extremely important function of SVM. Support Vector Regression Support Vector Regression (SVR) is a variation of SVM that is used for regression tasks. SVR extends upon SVM to solve regression problems by ways of predicting continuous numerical values as opposed to discrete class labels. SVR has the same goals as SVM in that it aims to find a hyperplane which maximizes the margin whilst keeping the number of errors within an acceptable range all whilst satisfying the constraint. SVR also employs the kernel trick to allow it to make predictions on data that contains non-linear patterns. SVR is another important concept as it allows for the strengths of SVM to be applied to regression problems as opposed to purely classification problems. Statistical Learning Theory for SVM Statistical Learning Theory (SLT) for SVM provides the theory foundation to allow us to understand the behavior and performance of SVM models. Through SLT, insights can be gained into the ability of a SVM model to generalize unseen data correctly. SLT also allows us to find the Vapnik-Chervonenkis Dimension which is the theoretical maximum margin value. Calculating the VC Dimension also provides insight into the complexity of the model and helps us find the lowest complexity model. 