Summary  Super vector machines SVM: a supervised machine learning model that uses classification algorithms two classifies given data as one of two categories. Hyperplane: a line that sits in between the two different categories. The distance between the two types of vectors and the hyperplane should be as far as possible. Support vectors: the vectors of the two categories that are closest to the hyperplane. These are vectors sit on the outskirts of each cluster. A line parallel to the hyperplane at the position of the support vector is called the support lines. Distance margin: the total distance between support lines and the hyperplane. Quadratic programming: used to optimize quadratic functions.  SVM formulation for non-separable linear data There will be scenarios when linear data cannot be completely separate by a hyperplane. This is referred to as a soft margin concept, where the training instances are not linearly separable. Slack variables are used to allow misclassification of outliers but should be kept at a minimum.  Statistical learning theory of SVM Simpler functions are preferred over complex functions to prevent overfitting. A sufficiently complex model indicates that hypothesis class used to label the data has a high accuracy. VC dimensions: The number of instances a hypothesis class can ‘shatter’ or perfectly classify each data point and still result in separatable data. In 2-dimensions, there will always be a line to shatter the labelling of 3 points.  