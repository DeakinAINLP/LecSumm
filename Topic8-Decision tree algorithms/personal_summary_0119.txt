 A. SVM formulation and solution for linearly separable data  a) Primal problem in SVM:  b)  B. SVM formulation and solution for linearly non-separable data  a) Soft margin: when we need a trade-off between the margin and the number of  errors in classifying the training instances  b) c) Soft margin dual problem: when we change the primal problem with soft  margins to dual. It remains the same except that there is an upper bound on the Lagrange multipliers  d)  C. Kernel trick and non-linear SVM  a) Linear regression formulation  b)  D. Statistical learning theory of SVM  a) Structural risk minimisation seeks to prevent over-fitting by incorporating a  b)  penalty on the model complexity If our hypothesis class is rich enough to learn any association of labels to the data, the hypothesis is sufficiently complex. To assess the level of complexity, we use the number of instances a hypothesis class can shatter (called Vapnik- Chervonenkis (VC) Dimension).  c) The Probabilistic Guarantee  The second way is to minimise h which is the complexity of the model.  d) e) Minimize the error by, First increase N which is the number of training samples f) g) By reducing the complexity of the model, you have a higher chance for smaller test values (smaller upper bound). Maximising margins will result in having a less complex model (a smaller upper bound for h).  E. Multi-class classification in SVM  a) One vs All: For each class, a binary SVM classifier is trained with samples  from that class being viewed as positive examples and samples from all other classes being viewed as negative examples.  b) One vs One: The SVM algorithm trains multiple binary classifiers, each trained  to distinguish between two classes.  