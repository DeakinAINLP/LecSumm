 The SVM (singular vector machines) main objectives:    Primal Problem: Maximizing the margin.  Hyperparameters:    C parameters: It is the value that determines the margin size (for soft/hard margins) that  determines the trade off between large margins and fitting training data.  o  High value: Misclassification is penalized heavily. o  Small value: Misclassification is more tolerated.  Kernel trick for non-linear SVM:  o  To handle non linearity, the data is converted to a higher dimension where it is linearly  separable and then separated and mapped back into the original dimension.  Kernel function: A function used to compute dot products in high dimensional feature space.  Vapnik-Chervonenkis (VC) Dimension: The number of instances a hypothesis class can shatter.  Methods of SVM:  o  One vs one: Multiple binary classifiers are created and a voting scheme is applied on each  classifier outputs to determine the final class label.  o  One vs all: For each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples.   