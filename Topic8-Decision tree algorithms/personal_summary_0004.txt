This module covered Linear models with a focus on SVM models. For data to be defines as linearly separable, you can draw any line through the data and clearly separate it into (usually) binary categories. With this line you can calculate a few key measures being the hyperplane (which is the line) and the support vectors which are the closest vectors to the line that dictate the space between the data and the hyperplane, the distance margin. In order to classify data that is not linearly separable, it needs to be manipulated in order to correctly classify the data. This can be achieved through introducing a higher dimension which can be used to separate the feature vectors on a multidimensional plane and tuned by adjusting the hyperparameters for the kernel such as C and gamma. Adjusting C will vary the trade-off between maximising the margin and minimising the classification error, whereas gammas control the width of the kernel and the smoothness of the decision boundary. Both are adjusted to get the best fir model. 