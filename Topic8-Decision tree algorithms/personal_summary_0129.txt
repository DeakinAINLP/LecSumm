This topic we focused on the Support Vector Machine (SVM) model. We  covered the basic concept of SVM and how it works including its  application for linear and non linear data.  For linearly separable data, we discussed both the primal and dual  optimization problems. Similarly, for Linearly non separable data(no  straight line that can classify the data), both primal and soft margin dual  problems were explored.  Kernel tricks for reducing the computational complexities for non linear  data were also introduced. Further, Statistical learning theory for SVM  and the use of probability guarantee of generalization error for ensuring  that the model performs well on unseen data was also covered.  Lastly, we discussed multi-class classification in SVM, including the  one-vs-one and one-vs-all approaches.  