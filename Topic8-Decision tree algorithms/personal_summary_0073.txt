Evidence of learning Prerequisite knowledge Lagrangian optimization Lagrangian optimization is a mathematical framework used to convert constrained optimization problems into unconstrained ones. In the context of SVM, it is employed to find the optimal hyperplane that maximizes the margin between classes.  More reading: Lagrange multipliers intro | Constrained optimization (article) | Khan Academy  Duality theory Duality theory is a concept in optimization that establishes a connection between two related optimization problems: the primal problem and the dual problem. The primal problem seeks to optimize an objective function subject to constraints, while the dual problem is derived from the primal problem by introducing additional variables called Lagrange multipliers.  Duality theory reveals that the optimal solution to the dual problem provides bounds on the optimal solution of the primal problem, allowing us to gain insights and information about the original problem.  Support vector machines The aim of a support vector machine (SVM) is to find a hyperplane with the maximum distance between the classes it is separating.  Mathematically:  - -  -  The hyperplane is defined as: (𝑤, 𝑏)  The margin is defined as: The margin must be maximized while satisfying this constraint: 𝑦𝑖(𝑤𝑇𝑥𝑖 + 𝑏) ≥ 1  2 ||𝑤||  In quadratic programming, this is the primal problem of SVM.  The SVM formulation solves this optimisation problem:  1 Minimising 2  2 ||𝑤||  is the same as maximizing  because that is its dual form. It is constructed by  2 ||𝑤||  introducing Lagrangian multipliers to the primal problem  If we introduce Lagrange multipliers to the SVM formulation we get this function:  Setting the derivative equal to 0 gives us the dual formulation of this function:      Soft margins – handling almost separable data point In some datasets it is possible that the data may not be linearly separable, or that they can be separated but only with a very narrow margin.  The purpose of SVM is to separate data points belonging to different classes with as wide of a margin as possible. We may have to choose between reducing the number of misclassified data points and increasing the margin of the hyperplane. This is the basic concept of soft margins.  Classification error is quantified using the ζ (lowercase zeta) symbol. Thus, the objective function to minimise becomes:    The C parameter is a weight used to control the trade-off between the hyperplane margin size and how much error the model can tolerate.  The dual problem of the soft margin problem is defined as:  Given a solution to the Lagrange multipliers (a), the hyperplane w is defined as:  All 𝑥𝑖 corresponding to non-zero 𝛼𝑖 are called support vectors.  Once we know the hyperplane (w, b), the classification function can be written as:  Model complexity and Vapnik–Chervonenkis theory If a hypothesis class can correctly separate all n samples from each other based on their class for all possible positions of the n samples, then the hypothesis class can shatter n samples. This implies that the hypothesis class has enough flexibility and expressive power to represent any possible labeling of the samples.  The number of samples that a hypothesis class can shatter is known as the Vapnik–Chervonenkis or VC dimension.       If we restricted ourselves to using only linear hyperplanes and there were exactly 2 different classes in the dataset, the VCD of the model will always be 𝑑 + 1, where d is the number of dimensions in the dataset.  More reading: Vapnik–Chervonenkis theory - Wikipedia  Proof of the theory of maximizing margins in SVM Vapnik demonstrated the theoretical rationale behind maximum margin by establishing the following findings. The VC dimension of the class of optimal linear separators is upper-bounded by h:  Where p is the margin, D is the diameter of the smallest sphere that can enclose all the training examples, and d is the dimensionality.  This implies that we can reduce the VCD of the model by increasing the size of the margin.  This describes that the test error is upper bounded by the training error + some value.  According to this, we can reduce the overall error of the model by:  Increasing N, or the number of training samples  - -  Minimizing h, that is, minimize the complexity of the model. We know from earlier that h  can be reduced by increasing the size of the margin in SVD.  Overall, this implies that increasing the margin size decreases the complexity of the model, which in turn decreases the error of the model.  Multi-class classification in SVM One vs all A different binary SVM classifier is trained for each class that exists in the dataset. Each model is trained to identify samples that belong to the chosen class and samples that don’t belong to that class.     One vs one  N models are trained on every possible 2-combination of the dataset. In total there will be  𝑁(𝑁−1) 2  models trained. Each model votes on what the sample being classified’s class is, and the majority becomes the classification for the sample.   