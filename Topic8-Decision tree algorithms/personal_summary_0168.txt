In topic 7, we were introduced to the prowess of the Support Vector Machine(SVM) algorithm and its various aspects. The SVM algorithm is inherently a supervised ML model  that  can  represent non-linear  functions  and  has  efficient  training  methods.  It rose to prominence during its initial days due to its ability to produce high accuracy results in handwritten character recognition applications.  Types of SVM  1.  Linear SVM  i. ii.  Perfectly separable data points Almost separable data points  2.  Non-linear SVM  Principle of SVM  Consider a binary classification problem with several datapoints in a 2D or 3D space. The  distance  between  the  boundary  lines  of  both  classes  is  called  margin.  SVM operates in this  margin space, trying  to obtain a decision boundary hyperplane that ensures the data points of both classes are farthest from it. This is aimed at minimising conflicts in classification or misclassification.  The distance ‘r’ is the shortest distance between data point and hyperplane, and the associated vectors are called support vectors.  In SVM, we try to solve the above optimisation problem of maximising the margin (or minimising 1/margin). We use Lagrange multipliers in order to do so.  Computational Requirements  The primal problem has computational requirements of O(d^3), and the dual problem of O(n^3), where d refers to the number of dimensions in feature space and n refers to number of instances.  Almost separable data points  In the real world scenario, we don’t have clearly separable data points as above. There is usually always  a trade off  between  margin  and  no. of  errors  in classification. For this, we need to have a ‘soft margin’ that allows for certain misclassification.        We control the degree of this misclassification with the help of the parameter ‘C’. Low value of C allows for more misclassification whereas high value of C allows for less misclassification.  Non-Linear SVM  In a case when the data points of both classes are completely intertwined, we can’t use the above methods. Hence, we introduce a third dimension place with the help of a kernel function.  A  kernel  function  is  a  function  that  is  used  to  compute  dot  products  in  a  high dimensional  feature  space.  When  kernel  functions  evaluated  on  each  pair  of  data instances, we obtain a matrix called gram matrix.  Types of Kernel Functions:  1.  Linear kernel – controlled using ‘C’ parameter 2.  Polynomial kernel – controlled using ‘C’ and ‘p’ parameter. C and p should be  decreased to avoid overfitting.  3.  Radial  basis  function  (RBF)  kernel  –  controlled  using  the  sigma  and  C  parameters. This may go onto infinite dimensions.  Non-linear SVM – Support Vector Regressor  This is similar to almost separable data points SVM case, but relatively less popular. It maybe used mainly for predicting continuous data.  We  then  briefly  learnt  about  the  Statistical  learning  theory  of  SVM  and  the  Vapnik- Chervonenkis (VC) dimension (VC dimension of a line in 2D is 3. In d-dimensional, it is d+1).  Multi-class Classification in SVM  It is implemented when there are more than 2 possible classes or set of labels for the algorithm to classify into.  2 types:  1.  One vs. All 2.  One vs. One  The One vs. One method is less commonly used, unless it is a small dataset where accuracy is the main focus. This is because it is computationally very expensive.            