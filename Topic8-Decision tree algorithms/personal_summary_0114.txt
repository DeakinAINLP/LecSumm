SVM for Linearly separable data: SVM can identify the best hyperplane that most eﬀectively separates the data points of various classes when the data is linearly separable.  The best hyperplane is the one that maximises the separation between the hyperplane and the support vectors, which are the nearest data points for each class. The input variables may be used to represent the hyperplane as a linear function as follows:  f(x) = w^T x + b  where x is the input vector, w is the weight vector, b is the bias term, and f(x) is the output of the SVM classiﬁer.  An optimisation algorithm, such as the Sequential Minimal Optimisation (SMO) algorithm, can be used to learn the weight  vector  w  and the  bias  term  b  from  the  training  data.  Under  the  condition  that  every  training  data  point  is properly identiﬁed, the objective is to minimise the weight vector's norm. The SVM may categorise new data points by determining the sign of the output f(x) once the weight vector w and bias term b have been learnt. The data point x belongs to the positive class if f(x) > 0, else it does not and is in the negative class.  SVM is a potent approach for linearly separable data in general because it can locate the best hyperplane with the highest margin, which aids in achieving strong generalisation performance.  SVM for linearly non-separable data: SVMs are generally made to function with linearly separable data, in which case the various classes may be entirely separated using a hyperplane. Many real-world datasets, however, cannot be separated linearly and must be classiﬁed using more advanced methods.  SVMs employ the idea of so  margins, which permits some misclassiﬁcations to happen to identify a hyperplane that can as eﬀectively separate the classes as feasible, to deal with this problem. With the introduction of slack variables, the so  margin technique still penalises incorrect classiﬁcations even when some points are on the wrong side of the margin or even the hyperplane. A regularisation parameter C balances the trade-oﬀ between maximising the margin and reducing the frequency of misclassiﬁcations, controlling the margin and slack variables.  Kernel trick and non-linear SVM: The  kernel  trick, which  entails  translating  the  data  into  a  higher-dimensional  feature  space  where  it  may  be  more conveniently separated by a hyperplane, is another method for handling non-separable data. The mapping between the initial feature space and the higher-dimensional space  is  speciﬁed by a kernel function. The polynomial kernel, radial  basis  function  (RBF)  kernel,  and  sigmoid  kernel  are  three  frequently  employed  kernel  functions.  The  kernel function selected, and its se(cid:427)ngs can have a signiﬁcant impact on how well the SVM performs.  SVM employs the kernel trick approach to handle non-linearly separable data. We identify a hyperplane in linear SVM that eﬃciently divides the data into two groups. Finding a linear hyperplane that can properly separate the data in non-linearly separable data, however, may not be atiainable.  To split the data using a hyperplane, the kernel method entails projecting the original data into a higher dimensional environment. However, it may be computationally costly or even impossible to directly compute the coordinates of the data points in this higher-dimensional space. As a result, the kernel method involves computing the dot products of the  data  points  in  this  higher-dimensional  space  rather  than  directly  computing  the  coordinates  in  the  higher- dimensional space.  The dot product between the mapped data points in the higher-dimensional space is deﬁned by the kernel function. The performance of the SVM can be signiﬁcantly impacted by the kernel function selection. Typical kernel operations in SVM include:  1.  Linear Kernel: K(x, y) = x*y 2.  Polynomial Kernel: K(x, y) = (ax*y + c)^d 3.  Gaussian (RBF) Kernel: K(x, y) = exp(-gamma * ||x-y||^2)  Once the kernel function is chosen, the SVM algorithm ﬁnds the hyperplane that best separates the data points in the higher-dimensional  space.  The  resulting  decision  boundary  in  the  original  space  is  non-linear  and  can  be  used  for classiﬁcation.  Support Vector Regression: A  supervised  machine  learning  approach  called  Support  Vector  Regression  (SVR)  may  be  applied  to  regression challenges. For classiﬁcation  problems,  it  is  a  variation  of  the  Support  Vector  Machine  (SVM)  technique.  Finding  a hyperplane  in  a  high-dimensional  space  that  can  roughly  simulate  the  non-linear  connection  between  the  input characteristics and the output variable is the basic goal of SVR. SVR can accommodate non-linear correlations between the input features and the output variable by utilising a non-linear kernel function, unlike linear regression, which tries to ﬁt a straight line to the data.  In SVR, the objective is to locate a hyperplane with a maximum tolerance margin surrounding the training data points. Epsilon and C serve as the factors that deﬁne this margin. The trade-oﬀ between minimising the margin and minimising the  training  error  is  controlled  by  C  and  Epsilon,  respectively.  The  SVR  algorithm  minimises  a  cost  function  that penalises  outside-the-margin  points.  The  diﬀerence  between  the  expected  and  actual  values  is  also  taken  into consideration by the cost function. An optimisation algorithm can be used to solve the quadratic programming issue that is used to construct the optimisation problem.  SVR works with both continuous and discrete output variables and may be applied to uni- and multi-variate regression applications. Numerous ﬁelds, including ﬁnance, engineering, and biology, have eﬀectively used it.  Statistical learning of SVM: Performance of the technique is theoretically supported by the SVM's (Support Vector Machine) statistical learning theory. The Vapnik-Chervonenkis (VC) dimension, a gauge of a learning algorithm's ability to ﬁt a variety of functions, is the foundation of the theory.  The major goal of the structural risk minimisation (SRM) criteria, which balances model complexity and data ﬁt, is to be minimised in the statistical learning theory of the SVM. The empirical risk and the regularisation term are combined to form the SRM criteria. While the regularisation term gauges the model's complexity, the empirical risk gauges how well the model ﬁts the training set of data.  The hyperplane that maximises the margin between the two classes of data is found by the SVM to reduce the SRM criteria. The distance between the hyperplane and the nearest data points from either class is referred to as the margin. The  SVM  aims  to  locate  the  hyperplane  that  is  most  resilient  to  noise  and  generalises  eﬀectively  to  new  data  by maximising the margin.  The generalisation error of the method, which gauges how well it can predict fresh data, is likewise given boundaries by the statistical learning theory of SVM. These constraints, which are based on the VC dimension and the margin of the hyperplane, guarantee that the SVM performs well in terms of generalisation even in the presence of noisy training data or complicated models.  Overall, the statistical learning theory of SVM oﬀers a solid theoretical basis for the algorithm's performance, assisting in understanding its advantages and disadvantages and directing its practical use.  Multi-class classiﬁcation of SVM: SVM is used in binary classiﬁcation to divide data points into two groups, denoted by the values +1 and  -1. But  by merging several binary SVMs, SVM may also be utilised for multi-class classiﬁcation. When utilising SVM for multi-class classiﬁcation, there are two primary methods:  1.  One-vs-One  (OvO)  approach:  This  technique  teaches  SVMs  to  recognise  all  conceivable  pairs  of  classes, including class 1 vs class 2, class 1 versus class 3, class 2 versus class 3, and so on. Classiﬁers are trained for N*(N-1)/2 classes. Each classiﬁer casts a vote during testing for the class that corresponds to it, and the class with the highest votes is selected as the predicted class.  2.  One-vs-All approach (OvA): This method develops a distinct binary SVM for each class, using that class as the positive class and all other classes as the negative classes. All the binary SVMs classify the data point during testing, and the projected class is then determined by the class that had the highest score.  Both strategies have merits and drawbacks. Although the OvO technique needs training several SVMs, it may handle imbalanced data betier and is computationally more costly. OvA technique has a lower computational cost because it trains just N SVMs, although class imbalance can be a concern.  Quiz Atiempt:    