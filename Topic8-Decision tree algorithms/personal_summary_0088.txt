   The SVM algorithm, which identifies the hyperplane that maximises the margin between two classes of data, is used for binary classification. SVM discovers a separating hyperplane for data that can be linearly divided into two classes by maximising the margin. The formulation of the optimisation issue is to minimise the weight vector's norm with the restriction that all samples must be on the proper side of the hyperplane.    To transfer the data into a higher-dimensional feature space where a linear  hyperplane can separate it, SVM employs a method known as the kernel trick when the data cannot be linearly separated. The kernel function does not explicitly compute the coordinates of the data before mapping it into this higher-dimensional feature space.    Another well-liked machine learning approach for regression applications is linear regression. This straightforward approach uses a linear equation to represent the relationship between the input and output variables. Finding the best-fit line that reduces the sum of squared errors between the expected and actual output values is the objective.    The structural risk minimisation principle, which seeks to reduce the predicted error throughout the whole probability distribution, serves as the foundation for the statistical learning theory of SVM. This theory offers a framework for comprehending the SVM's generalisation characteristics and the trade-off between model performance and model complexity.    There are two popular methods used to solve multi-class classification issues using  SVM: One vs All and One vs One. The classifier is trained on each class in comparison to all the other classes using the one vs. all method. The classifier is trained on each pair of classes in the one vs. one method. The class that receives the most votes throughout the pairwise classification will be chosen as the winner at the end. The choice of strategy is based on the particulars of the situation at hand, as both offer benefits and drawbacks.      