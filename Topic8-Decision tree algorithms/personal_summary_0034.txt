This topic was all about Support Vector Machine models. The following points were covered:  Support Vector Machine algorithms:  Can be used in both regression and classification problems, but primarily used for  classification.  Objective is to find a hyperplane in an N-dimensional space that can separate  classes. SVM seeks to find the hyperplane that has the maximum margin i.e., leaves the most amount of distance possible between the observations that are closest to the hyperplane and the hyperplane.  Finding the hyperplane is an optimization problem, solved using Lagrange  Multipliers that can transform the problem into a format where it can be solved analytically.  The observations that fall closest to the hyperplane are known as the ‘support  vectors’.  The primal problem in SVM is maximising the margin – the solution to the dual problem provides a lower bound to the solution for the primal problem. In practice, there is a trade-off between the margin and the number of errors in classifying the training instances. A soft margin is put in place when it is not possible to linearly separate all the training instances. The hyperparameter C can be used to achieve the desired trade-off, which speicifies how much the model should be penalised for misclassification.  Kernel Trick and Non-linear SVM  When the instances are not linearly separable, they can be projected, using the kernel trick, into a higher-dimensional space in which they can be separated.  VC Dimension  The complexity of the hypothesis class is characterised by examining how many instances it can perfectly fit for all labels). The number of instances that that hypothesis class can ‘shatter’ is known as the VC Dimension.  For example, in 2 dimensions, a single line is guaranteed to be able to shatter 3  points.  How to implement SVM in python scikit-learn library using different kinds of kernels: Linear,  Polynomial and the RBF kernel.  