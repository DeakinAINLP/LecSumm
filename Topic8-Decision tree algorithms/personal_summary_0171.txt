Support Vector Machine (SVM):    It  is  a  common  machine  learning  technique  for  classiﬁca4on  and regression tasks.  Linear SVMs: It is an SVM variant that works well when the data is linearly separable. The purpose of linear SVM is to discover the hyperplane that best splits the data into two classes.   Linear SVM op5misa5on: This problem involves iden4fying the ideal weights and bias values that minimise the objec4ve func4on while complying to a set of constraints. The dual form of the  op4misa4on  problem  is  determining  the  weights  that  maximise  the  sum  of  Lagrange mul4pliers under a new set of constraints. Linearly non-separable data: If the data is not linearly separable, linear SVM can s4ll be used by  mapping  the  data  onto  a  higher-dimensional  space  where  it  becomes  separable.  This  is accomplished through the use of a kernel func4on that evaluates the similarity of two data points in the new space. So<  margins  SVM:  It  is  applied  when  a  hyperplane  cannot  fully  separate  the  data.  In  this example, a margin of error is allowed for certain data points to be misclassiﬁed. In the dual form of the soG margin problem, slack variables are introduced to measure the amount by which  each  data  point  breaches  the  margin.  The  best  solu4on  is  obtained  by  minimising  a trade-oﬀ between the size of the margin and the number of misclassiﬁca4ons.  Non-Linear SVMs:  When the data in the input space cannot be separated linearly, nonlinear SVMs are used. The data is mapped  into  a  higher-dimensional  space  where  it  can  be  separated  using  a  method  known  as  the kernel trick by nonlinear SVMs. In order for SVM to choose the best separa4ng hyperplane, the kernel func4on calculates the similarity between pairs of data points in the new space.  i.  ii.  iii.  iv.  Kernel trick: It is an approach that makes it possible for SVM to operate eﬀec4vely in high- dimensional feature spaces without explicitly compu4ng the coordinates of the data in that space.  The  dot  product  between  the  feature  vectors  in  the  high-dimensional  space  is computed using a kernel func4on that is deﬁned to accomplish this. Support  vector  regression:  A  varia4on  of  SVM  used  for  regression  applica4ons  is  support vector  regression  (SVR).  SVR  operates  by  loca4ng  a  hyperplane  that  maximises  the  margin around it while best ﬁOng the data. In contrast to linear regression, SVR uses nonlinear kernel func4ons to manage interac4ons between the input and output variables that are not linear. Sta5s5cal  Learning  Theory  of  SVM:  By  examining  the  generalisa4on  error  of  SVM,  the sta4s4cal learning theory of SVM gives a theore4cal framework for the method. By minimising the structural risk func4onal, which represents a trade-oﬀ between the empirical risk (error on  the  training  set)  and  the  model  complexity,  the  theory  demonstrates  that  the  best separa4on hyperplane may be discovered. Vapnik-Chervonenkis (VC) dimension: It measures how well a classiﬁer can ﬁt any label that might be present in the input space. It is used to es4mate the model's complexity and choose the appropriate sample size for generalisa4on with a certain degree of assurance. Because the VC dimension of SVM with a nonlinear kernel increases with the number of support vectors, it is less eﬀec4ve for large datasets.   Concept of margin: The margin in a Support Vector Machine (SVM) is the separa4on of the nearest data points for each class from the separa4ng hyperplane. SVM seeks to iden4fy the hyperplane that maximises  the  diﬀerence  between  the  two  classes.  The  margin  is  signiﬁcant  since  it  reﬂects  the classiﬁer's capacity for generalisa4on. The classiﬁer is less likely to overﬁt to the training data and is more likely to generalise successfully to fresh, untried data if the margin is larger.  Mul5-class classiﬁca5on in SVM:  There are two widely used methods for SVM mul4-class classiﬁca4on:  i.  ii.  One vs. All (OvA): In this method, a classiﬁer is trained speciﬁcally to diﬀeren4ate one class from all others. The class that has the greatest output score from its associated classiﬁer is selected as the predicted class during inference. One vs. One (OvO): In this method, a binary classiﬁer is trained for each pair of classes. Each classiﬁer casts a vote for the class label that corresponds to it during inference, and the class that receives the most votes is selected as the predicted class. OvO costs more to compute than OvA, but in some circumstances, especially when the data is imbalanced or the classes are highly overlapping, it can be more accurate.  