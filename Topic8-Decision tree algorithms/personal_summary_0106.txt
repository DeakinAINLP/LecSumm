Support Vector Machines:  Topic – 7  Support Vector machine are generally used to classify the data by identifying the hyperparameter between the data points.  There are two method of approaches for SVM.  1.  Linear SVM 2.  Non-Linear SVM  The concept of Support vector Machine uses the idea to classify vectors based on the decision boundary that data from different classes should be as far as possible from each other.  Linear SVM:  This method of approach is useful to classify the datapoints when these can be separatable linearly, i.e., a straight line.  i.  ii.  Linearly non -separatable data: Linear SVM model approach can still be used of the even if the data is not separatable linearly, Here the data the is mapped the datapoints on higher dimension space where the data can become separatable by evaluating the similarity between two datapoints in the new space. Soft Margin SVM:  This approach is useful when the datapoints cannot be perfectly classified linearly i.e., linearly inseparable which indicate there is insufficient data to separate. In this approach the SVM model is allowed to make certain errors to the margin as wide as possible to classify other datapoints correctly and helps in overfitting in unknown data.  Non-Linear SVM:  Non-linear SVM models are used to s classify datapoints which can’t be separated linear. The Non- linear SVM algorithms makes use of Kernal tricks which makes us of plotting the data points on higher dimension feature space where the data is linearly separable.  i.  ii.  Kernal Trick: The Kernal Trick makes use of dot product logic to identify the similarities between datapoints without plotting them in higher dimension spaces. Some of the most commonly used Kernal function are Linear Kernal (used when data is linear separatable), Polynomial Kernal (applying Polynomial transformation to degree d to the dot product ), RBF – Gaussian Kernal (Measures the similarity between two datapoints using Euclidean Distance between them) and finally Sigmoid Kernel (Based on hyperbolic tangent function). Support Vector Regression:  The idea behind the SVR, is same as the SVM to separate data by finding the best fitting hyperplane which maximise the classifying the datapoints.  Multi-class Classification in SVM:  This method of approach is used to for classification problems whose task is to classify the given dataset into 2 or more categories and assumes that a given sample can’t be classified into categories into two categories at once.  i.  One-vs All:  This strategy makes use of binary classification algorithms for multi-class classification. Where the classifier is trained to split the multi-class dataset into multiple   ii.  binary classification problem. Now each binary classifier is trained against other binary classification. One vs One: In this strategy, each binary classifier is trained against each binary classifier. In the prediction phase, each binary classifier is predicted to test sample and compared using a voting scheme to determine the final class label.      