 Summary (main points)  Support Vector Machines (SVM) formulation and solution for linearly separable data  This problem is well known in the optimisation community and is called quadratic programming. Do not get confused with the word programming. It just means optimisation. The optimisation problem is often solved by constructing an equivalent problem called a dual problem.  In quadratic programming, the original optimisation problem is called the primal problem. The solution to the dual problem provides a lower bound to the solution of the primal (minimisation) problem.  Every linear programming problem can be thought of as a primal problem which can be converted to its corresponding dual problem. This means solving problem P1 (Primal) is equivalent to solving problem P2 (Dual of P1). This is because the optimal solution to P1 (that is, maximum value of the objective function in P1) is the same as that of P2 (that is, minimum value of the objective function in P2).  Machine Learning  1.  Supervised Learning  a.  Classification (most common place SVM is used)  i.  SVM is a supervised learning method that looks at data and sorts into two  categories  b.  Regression  2.  Unsupervised Learning 3.  Reinforcement Learning  Sometimes, data can be linearly separable but with a narrow margin. At other times, due to noise, some of the instances may not be linearly separable (see the figure for noisy data).  It is generally preferred not to interfere with the boundary even with small noisy data points or outliers. It is acceptable to have large margins even though some of the constraints are violated. In practice, we need a trade-off between the margin and the number of errors in classifying the training instances.    This trade-off brings us to the soft margin concept. Consider the following figure; the soft margin concept is defined when the training instances are not linearly separable. Slack variables are added to allow misclassification of outliers, noisy or difficult to classify instances. So basically we are allowing some of the data points to cross the borders and to be in the wrong side of the boundary or to be misclassified.  Soft margin concept.  Although we allow some of the training instances to be misclassified, we still want to minimise the sum of slack variables. So for those data points which their … value is non-zero, we can infer that they are misclassified, and the amount of misclassification is also presented ….  The parameter C can be used as a way to achieve the trade-off between large margins and fitting training data. For the high values of C,  we highly penalise the misclassification but for the small values of C, we allow more misclassifications. That is how SVM handles this trade-off around misclassification.  Soft margin dual problem = the soft margin dual problem is defined when we change the primal problem with soft margins to dual. It remains the same except that there is an upper bound on the Lagrange multipliers.  Linear regression formulation  In linear regression we want to find a line similar to ℎ.     The linear equation should allow us to summarise and study relationships between two  continuous (quantitative) variables.  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity. This means, it prefers simpler functions over more complex functions. So we would like choose a less complex model with a small error.  Suppose we pick n instances and assign labels of + and − to them randomly. If our hypothesis class is rich enough to learn any association of labels to the data, it is sufficiently complex. How about we      characterise the complexity of the hypothesis class by looking at how many instances it can shatter(i.e. can fit perfectly for all possible label assignments). The number of instances a hypothesis class can shatter is called its Vapnik-Chervonenkis (VC) Dimension.  An Illustration of VC Dimension  Let us assume that we are using lines (or hyperplanes) as our hypothesis class. In 2−dimension, we can find a line to shatter any labelling of 3 points. But a line may not be able to shatter some labelling of 4. Therefore, VC dimension of a line in 2−dimension is 3. (In d−dimension: d+1).  Consider the following figure. As you can see in the top image, these 3 points with any combination of labels can be separated by a line. It doesn’t matter if you change the labels of the data points.  As you can see from some of the example in the figure, we can successfully separate these points with a line. But in the bottom images you can see that we can come up with situations in which we can not use a single line to separate these data points.  Because in 2−dimension, we can always find a line to shatter any labelling of 3 points. It might not be possible to find a line to shatter any labelling of 4 points.  Where p is the margin, D  is the diameter of the smallest sphere that can enclose all of the training examples, and d is the dimensionality.  Intuitively, this implies that regardless of dimensionality d, we can minimise the model complexity (VC dimension) by maximising the margin p. Maximising margins will result in having a less complex model (a smaller upper bound for ℎ)  Probabilistic guarantees can be used to assess the accuracy and reliability of machine learning models, based on their ability to predict outcomes with a certain degree of certainty.    The upper bound on the generalisation error increases with higher complexity (higher ℎ)   The upper bound on the generalisation error reduces with larger training sets (higher N).    SVM one vs rest  The idea behind OvR (SVM one vs rest) is to train one binary classifier for each class, where the positive examples are from that class and the negative examples are from all other classes combined.  SVM one vs one  The idea behind OvO is to train one binary classifier for each pair of classes, where the positive examples are from one class, and the negative ones are from the other.  