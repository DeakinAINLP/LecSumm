 The crucial subject of Support Vector Machines (SVM), which are frequently employed in machine learning for classification and regression issues, is covered in this topic's material. The formulation and solution of SVM for linearly separable and non-separable data were explained. While allowing for some misclassification in non-separable data, SVM aims to identify the best hyperplane that maximizes the margin between two classes. The kernel technique, which transforms nonlinearly separable data into a higher-dimensional space, is covered. Support Vector Regression, which is employed to forecast continuous values, is described in detail. Also, the statistical learning theory of SVM is covered, which explains how SVM reduces the possibility of generalization mistakes. Also learned about how to use linear, polynomial, and radial basis function (RBF) kernels to create SVM in Python. The information presented this topic offers a thorough introduction to SVM and its uses in machine learning.  