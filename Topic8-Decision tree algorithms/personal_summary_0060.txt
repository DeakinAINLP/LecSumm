Topic7: SVM(Support Vector Machine) models  SVM formulation and solution for linearly separable data  -  -  SVM aims to find a hyperplane(w,b) so that the margin ‚à•  the constraint SVM formulation solves following optimization problem 1 ‚à• w ‚à•2   Minimise 2   Subject to yi(wTxi + b) ‚â• 1   ‚àÄùíæ   Quadratic programming: optimization  2 w  ‚à• is maximized while satisfying  : often solved by constructing an equivalent problem(dual problem) : the optimization problem- primal problem  -  Dual optimization problem    Primal problem in SVM which is maximizing the margin   Using Lagrange multipliers can convert a constrained optimization into an unconstrained  one : a strategy for finding the local maxima and minima of a function subject to quality constraints.  SVM formulation and solution for linearly non-separable data  -  Need trade off between margins and number of errors -  This will bring to the soft margin concept which is defined when the training instances are not linearly separable.   Slack variables are added to allow misclassification   Parameter C can be used as a way to achieve the trade-off between large margins and  fitting training data.    High value of C, misclassification is highly penalized and small C, allow more  -  misclassification Soft margin dual problem   Defined when we caught the primal problem with soft margins to dual  Kernel trick and non-linear SVM  Linear hypothesis  - -  How to find the best linear  Line minimizes the total absolute error Line minimizes the total squared error      w = (XTX)  -1  XTy  statistical learning theory of SVM  -  theoretical justification for maximum margin is shown by Vapnik, the class of optimal linear separators has VC dimension h bounded from illustration of shattering 3 points buy a line vs 4 points where this not possible  h  ‚â§  min {d, [  D2 p2]}   + 1  p ‚Äì margin D- diameter of the smallest sphere that can enclose all of the training examples d- dimensionality  : implies that regardless of d, we can minmise the model complexity by maximizing p  : if we look for a classifier with high margin, means that we have a smaller value for [  D2 p2]  -  the probabilistic Guarantee  Etest ‚â§ Etrain   +   (  h  +  h log (  2N h N  1 2  ) - log (  p ) 4 )  E train = error on training set E test = error on test set(generalization error) N = size of training set h = VC dimension in hypothesis class p = upper bound on probability that this bound fails  increase N  1. 2.  minimize h(complexity of model)  : higher chance for smaller test values(smaller upper bound) by reducing complexity of the model  Multi- class classification in SVM  -  One vs all  : a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes viewed as negative examples : in the prediction phase, the test sample is passed to each model(classifier) and the predicted class is determined based on the highest score obtained from the models  -  One vs one  : the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For N number of classes, the number of binary classifiers that will be generated  in this approach is  N x (N-1) 2  : during the prediction phase, each test sample is passed to all binary classifiers and a voting scheme is applied on the output of individual binary classifier outputs to determine the final class label  