Support Vector Machines (SVM) have linear and non-linear formulations. Linear SVM finds a hyperplane wx + b, so that the distance from the hyperplane to the nearest point in each group is maximised. The points on the hyperplane are support vectors. This is equivalent to minimising 0.5 || w ||2 subject to yi(WTxi + b) >= 1 for all yi. [inversion] Using Lagrange multipliers converts this to an unconstrained problem and avoids explicit parameterisation with constants. L(w, b, α) =  0.5 || w ||2 + Σ αi(1 - yi(WTxi + b)) It is useful in high dimensionality situations as the dual problem is O(n3) rather than the primal O(d3), such as image analysis, natural language processing and bioinformatics. It handles sparse document vectors and has a regularisation to reduce bias. If the data is not linearly separable, adding another dimension might help, using a kernel. Otherwise  the soft margin uses slack variables allows a small number of misclassifications to occur without affecting the hyperplane. Large values of C prioritise correct classification, low values of C prioritise separation of the margin. The Vapnik-Chervonenkis dimension is the number of classes able to be shattered (match perfectly) by a given hypothesis function. h < min(d, D2 / p2) + 1 where  h is Vapnik-Chervonenkis dimension,  d is dimensionality,D is diameter of smallest sphere enclosing training data,p is the margin. The probabilistic guarantee states that error in generalising from the training to test sets is minimised by decreasing h, or increasing N. Multiclass SVM includes one vs all or one vs one approaches. In one vs all, a classifier is trained for each class treating all other classes as negative examples. There are N classifiers. The classifier with the highest score for a point is used to label it.In one vs one, a classifier is trained to distinguish each pair of classes. This requires N(N-1)/2 classifiers. The most common output from the classifiers (mode) is used to label a point. The RBF kernel has an additional hyperparameter gamma, which affects how far individual samplesreach. It is the inverse of distance. 