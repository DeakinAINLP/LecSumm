This topic we will learn more in depth concepts of machine learning.In this topic our focus will be on exploring linear SVM and non-linear SVM.We learn about the formulation and solution of SVM for linearly separable data.  we also learn about quadratic programming and Lagrange multipliers, which convert the primal problem into a dual problem.It is popular because it allows the use of arbitrary kernels, which means SVM boundaries can be signiﬁcantly nonlinear.Morever, we learned about the so(cid:332) margin concept in SVM, which allows for some misclassiﬁcation of data points that are not linearly separable. Slack variables are added to the original formulation to allow for misclassiﬁcation. The parameter C is used to achieve a trade-oﬀ between large margins and ﬁ(cid:427)ng training data. We also learned about the so(cid:332) margin dual problem and the classiﬁcation function for non-linearly separable data points. Finally, we were introduced to the concept of non-linear SVMs.Furthermore, we also learn about linear regression. The goal of linear regression is to ﬁnd a line that can help to summarise and study relationships between two continuous variables. The line has two parameters: the slope of the line and the intercept of the line. The linear regression model seeks to minimizes the empirical risk via the square loss. To ﬁt the line to data points, the derivative of the error function with respect to the parameters is taken and equated to zero. The matrix multiplication of the pseudo-inverse of the feature matrix and the output values vector gives us the optimal parameters.I also read an example regarding linear regression problem on the unit content provided by our university for machine learning.The example gives more be(cid:425)er understanding of a linear regression problem where ﬁve students took a math aptitude test before starting their statistics course. The goal is to ﬁnd a linear regression equation to predict the statistics performance based on the math aptitude scores. A dummy feature is created with all values set to one, and using the method of minimizing the mean squared error, the values of slope and intercept are computed. The computed values are then used to predict the statistics grade of a student who scored 8 on the aptitude test, which is found to be 8.8.  We also explore more about the theoretical justiﬁcation for maximum margin in SVM. The VC dimension is introduced as a measure of hypothesis class complexity and the importance of maximising margins in minimising model complexity is demonstrated. Furthermore, we learn about two approaches for multi-class classiﬁcation in SVM: One vs All and One vs One. In the One vs All approach, a binary SVM classiﬁer is trained for each class with samples from that class as positive and samples from other classes as negative. In the One vs One approach, multiple binary classiﬁers are trained for each pair of classes. During the prediction phase, the test sample is passed to all classiﬁers, and a voting scheme is used to determine the ﬁnal class label.we also learn about the process of building a support vector machine (SVM) classiﬁcation model using a linear kernel in Python. The process includes data scaling, importing necessary modules, loading iris dataset, spli(cid:427)ng data into training and testing sets, and applying the one vs rest (OvR) SVM method to train one binary classiﬁer for each class.    We explore more about practical applications about SVM in Python with a polynomial kernel. SVM with a linear kernel is ﬁrst implemented to get a linear decision boundary, which is a straight line between two categories.We also learn about an example which explores the use of SVMs with an RBF kernel using two parameters C and gamma. The gamma parameter deﬁnes how far the inﬂuence of a single training example reaches and the C parameter trades oﬀ misclassiﬁcation of training examples against simplicity of the decision surface. The code example shows the implementation of the RBF kernel with gamma set at 100 and reports the training accuracy of 0.8875 and testing accuracy of 0.85.  To conclude, this topic's focus on exploring linear and non-linear SVM, we gained an in- depth understanding of the formulation and solution of SVM for linearly separable data.We learned about the so(cid:332) margin concept in SVM, which allows for misclassiﬁcation of non- linearly separable data points.We also explored the theoretical justiﬁcation for maximum margin in SVM and the importance of maximising margins in minimising model complexity. We learned about the two approaches for multi-class classiﬁcation in SVM and the process of building a support vector machine (SVM) classiﬁcation model using a linear kernel in Python.Additionally, we gained practical knowledge about SVM in Python with a polynomial kernel.Through these concepts and examples,we have developed a comprehensive understanding of SVM and its applications in real-world problems. SVM is a powerful tool for classiﬁcation problems and oﬀers a robust approach for handling complex datasets with non-linear boundaries.Overall, this topic's learning has equipped us with the essential tools and concepts needed to build and deploy SVM models in various applications.Here also i have added scrrenshot of the quiz with the document.     