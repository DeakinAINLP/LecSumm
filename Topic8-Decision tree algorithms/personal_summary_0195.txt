Support Vector Machines  Support vector machines (SVM) involve classifying data into two groups. This binary approach considers the relevant attributes of the data, and finds the best line of separation between the data points on the most relevant plane or dimension. This can be easily demonstrated when the data is linearly separable, but sometimes our data is not so considerate. Sometimes when data is not linearly separable, there is ‘noise’ that remains in the data where some of the data overlaps clusters. We need to be able to compromise between having clear margins and the number of errors having these clear margins produce. This is where the idea of ‘soft margins’ becomes helpful.  Using a soft margins adds ‘slack variables’ in and allows some of the data points to be misclassified. When using a soft margin, we want to minimise the number of these slack margins to keep our classification predictions as accurate as possible.  We need to find this line in order to identify the ideal separation point of the data, which we find by finding the w and b parameters, being the slope of the line and the y intercept respectively, and then fitting this do our data. We can then adjust the Gamma parameter to determine how much margin we add to this line to accommodate our slack variables and the C parameter to set the amount of error margin we will allow in the predictions. This will allow for the line to remain accurate without being overly affected by outlier data.          