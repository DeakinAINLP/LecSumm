Overview In topic 7, we focused on the following learning goals:    Linear SVM   Non-linear SVM  SVM Formulation and Solution for Linearly Separable Data SVM aim to ﬁnd a hyperplane (w, b), so that the margin 2/||w|| is  maximized while satisfying the constraint Therefore, the SVM formulation solves the following optimization problem:  . Remember the problem of maximizing 2/||w|| is the same as (1/2)||w||^2. We need to optimize a quadratic function in w subject to linear constraints. Also, the primal roblem in SVM is maximizing the margin (or minimizing 1/margin)  Watch:  htps://youtu.be/_YPScrckx28  htps://youtu.be/TtKF996oEl8  SVM Formulation and Solution for Linearly Non- Separable Data Basically, what if the data is no linearly separable?  It is acceptable to have large margins even though some of the constraints are violated. We need a trade-oﬀ between the margin and the number of errors in classifying the training instances.  The soti margin concept is deﬁned when the training instances are not linearly separable. Clack variables are added to allow misclassiﬁcation of outliers, noisy or diﬃcult to classify instances.  Slack variables are the ones on the wrong side.  Watch:  htps://youtu.be/5oVQBF_p6kY  Support Vector Regression Example of linear regression:  Five randomly selected students took a math aptitude test before they began their stat course. The statistics department wants to know: what linear regression equation best predicts statistics performance, based on math aptitude scores?  The following ﬁgure illustrates the data, Xi is the math aptitude scores and Yi is the statistics performance.  To solve this problem, we ﬁrst create a dummy feature which contains 1 , then append it to X. So we have:  and  Based on the minimization of error function, we know that w should  be error.  in order to have the minimum mean square  So we can ﬁnd the values of w as:  Now let’s make a prediction based on the trained model.  Prediction:  If a student made an 80 on the aptitude test, what grade would we expect him to make in stat?  Answer:  Therefore, based on the trained model, we expect the stat grade of this student to be 78.288  Statistical Learning Theory of SVM Theoretically, does maximum margin make sense?  In 2-dimension, we can ﬁnd a line to shater any labelling of 3 points. But a line may not be able to shater some labelling of 4. Therefore, VC dimension of a line in 2-dimension is 3.  Consider the following ﬁgure. As can be observed, these 3 points with any combination of labels can be separated by a line. But it’s not the case with 4 points:  Multi-Class Classiﬁcation in SVM Multiclass classiﬁcation in SVM can be done as follows:  1. One vs all 2. One vs one  One vs All For example, if we have classes A, B and C in the original dataset then three models will be trained where each of them will classify samples from  -  A vs {B, C} -  B vs {A, C} -  C vs {A, B}  Basically, for n number of classes, the number of models to be generated is n as well  One vs One For example, if we have three classes (Blue, Green, and Red), we would train three binary classiﬁers:  -  Blue vs Green -  Blue vs Red -  Green vs Red   