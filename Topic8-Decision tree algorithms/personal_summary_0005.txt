This topic we focused on SVM models:  Linear SVM   Non-linear SVM We can SVM for both classification and regression. And it works great in small and complex datasets. Also, SVM is really great with very high dimensional inputs and as it works good with high dimensional inputs, this model can be used in text mining and image analysis too.SVM formulation and solution for linearly separable data: SVM aims to find a hyperplane so that the margin 2/||w|| is maximised while satisfying the constraint.We know that the problem of maximising 2/||w|| is the same as minimising Â½*||w||^2. So, we need to optimise a quadratic function w subject to linear constraints. This problem is known in the optimisation community and is called quadratic programming. The original optimisation problem is called the primal problem. The solution to the dual problem provides a lower bound to the solution of the primal (minimisation) problem. Dual Optimisation problem: The primal problem is SVM which is maximising the margin/or minimising (1/margin). By using Lagrange multipliers we can convert a constrained optimisation into an unconstrained optimisation problem. Lagrange multiplier is a strategy for finding the local maxima and minima of a function subject to equality constraints. Nonlinear SVM: The key idea to handle non-linearity is to transform the features to a higher dimensional space where data is linearly separable. At this figure, we transformed a 2D space to a 3D space. A kernel function is a function that is used to compute dot products in a high dimensional feature space. So, what functions are kernel function? Mercer`s theorem says that every positive, semi-definite and symmetric function is a kernel function. Kernel functions when evaluated on each pair of data instances give rise to a matrix called a Gram matrix. This matrix is a positive semi-definite and symmetric matrix. Some popular kernel functions are: Linear kernel,lynomial Kernel with degree p,Radial basis function (RBF) kernel. Nonlinear SVM Support Vector Regression: At this SVR, the classifier still defines a margin. If the data points are in the tube, they are clear and fine. If the data point goes outside of the tube or margin, the point is a deviation.  So, there is a question; does maximum margin make sense? Seeking to prevent over-fitting by incorporating a penalty on the model complexity. Also the general idea is to minimise the structural risk. Where is the complexity of hypothesis function and is a penalty parameter? So, we want to choose a model with small error and less complex. Nonlinear SVM Vapnik-Chervonenkis (VC) Dimension: Suppose we pick n instances and assign labels + and - to them randomly. If our hypothesis class is rich enough to learn any association of labels to the data, it`s sufficiently complex. How about we characterize the complexity of the hypothesis class by looking at how many instances it can shatter. The number of instances a hypothesis class can shatter is called its Vapnik-Chervonenkis Dimension. In the figure below we can see the application of VC dimension of a line in 2d is 3. Nonlinear SVM Summary of VC Dimension: It implies that regardless of dimensionality, we can minimise the model complexity by maximizing the margin. If is maximized or in other words, a classifier with high margins, a smaller value for the complexity of model. Multi-class classification in SVM: 1 One vs All SVM: In this approach, for each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples. So, if we had 3 classes of 0,1 and 2 in the original dataset; then three models will be trained where each of them will classify samples from 0 vs {1,2}, 1 vs {0,2}, 2 vs {0.1}. 2 One vs One SVM: In this method, the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For example, if we have three classes 1,2 and 3; we would train three binary classifiers, 1 vs 2, 1 vs 3, 2 vs 3. For N number of classes, the number of binary classifiers that will be generated in this approach is. During the prediction phase, each test sample is passed to all binary classifiers and a voting schema is applied on the output of individual binary classifier outputs to determine the final class label. 