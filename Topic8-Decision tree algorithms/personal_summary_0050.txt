 Perfectly separable data points Almost separable data points  Nonlinear SVMs  Support Vector Machine(SVM) tries to find the best line boundary to be able to predict new data to a class.  Overall idea is that the line boundary or the decision boundary should be as far away from the data points of both classes as possible. We have border lines for each class which measures the margin. The aim is to get the best line or the maximum margin that minimizes possible conflicts and not misclassify our data. Maximize the margin which is the hyperplane.  Support Vector Machine(SVM) Concept of margin. 2:11pm  We need to find the euclidean distance from a data point to the decision boundary  The shortest distance between a point and a hyperplane is perpendicular to the plane  The distance of 𝑥𝑖 to the separating hyperplane is :       Instances that are closest to the hyperplane are called Support vectors   at 𝑟 distance from the hyperplane.  The margin is defined as the distance between the support vectors and is given as:    From those two equations, we can derive:   Perfectly separable data points  Linear SVM formulation (linearly separable data) 2:15pm  SVM is trying to find a hyperplane (w,b)   Find the maximized margin of  Satisfy the constraint of  SVM formulation therefore solves the following optimisation problem where:  We minimize the possible conflicts or problems  Subject to constraint of   This problem is well known in optimisation community and is called quadratic programming. This just means optimisation  Linear SVM Dual optimisation problem 2:18pm  Primal problem in SVM is just maximising the margin (or minimising 1/margin):    Using Lagrange multipliers  Convert a constrained optimisation into an unconstrained optimisation problem.   The Lagrange function to minimise is:     where 𝛼=[𝛼1,…,𝛼𝑛] are Lagrange multipliers. Get the minimum Lagrange The Lagrange function to minimise is:   This is the dual problem This function only uses 𝛼, x and y variables  If you carefully look at the derived problem:   𝑥𝑖 and 𝑥𝑗 as our data points are multiplied with dot product Represents the similarity of these two vectors  With the help of the Lagrange multipliers  A dual problem for  Since Lagrange function uses only 𝛼, x and y variables.  Which can Maximize the margin in SVM  So if we have found the 𝛼 values, so given a solution to 𝛼=[𝛼1,…,𝛼𝑛] the  hyperplane w is given as:  Overall, if we found the 𝛼 values, we can then find the w and the b.   There is one 𝛼𝑖 corresponding to each 𝑥𝑖.   The 𝑥𝑖 corresponding to each non-zero 𝛼𝑖 is a support vector.  Given w and b, we can write classification function as:  Which will allow us to find the hyperplane  One thing to take note is that Primal problem has computational requirements of the  order 𝑂(𝑑3), whereas the dual problem requires an order 𝑂(𝑛3).  Where 𝑑 is the dimension of feature space and 𝑛 is the number of instances in  the training set.  Dual problem is popular, as it allows the use of arbitrary Kernels  Almost separable data points  Linear SVM SVM formulation (linearly non-separable data) 2:24pm  In SVM, we have so far assumed that data is linearly separable.  What approach should we take when data can be linearly separable but with a narrow  margin?   The image above shows us that these decision boundary lines are not the best and can lead to misclassifications on new data points.     In the first image, no interference with the boundary even with small noisy data points or outliers leads to a very small margin which will lead to mis-classification. In the second image, it is acceptable to have large margins even though some of the constraints are violated.   This is called the soft margin concept, that allows small mis-classification to allow the margin to be maximized. 2:28pm   allowing some of the data points to cross the borders and to be in the wrong side of the boundary or to be mis-classified  Slack variables 𝜁𝑖 are added to allow mis-classification of outliers, noisy or  difficult to classify instances.   In practice, we need a trade-off between the margin and the number of errors in classifying the training instances.  Allow some of the training instances to be mis-classified  In order to Minimize the sum of slack variables Data points for which 𝜁𝑖≠0, which are mis-classified  SVM with soft margin uses the following formulation:    The parameter 𝐶 can be used as a way to achieve the trade-off between large margin and fitting training data.  𝐶 is a way to control how many outliers can be mis-classified to get the optimum  margin.  High values of 𝐶 means:  Highly penalize the mis-classification.  Small values of 𝐶 means:  Allows more mis-classifications. Less accurate and more bias Less complex  Linear SVM Soft margin dual problem 2:34pm   The dual problem is given as:   Given a solution to 𝛼=[𝛼1,…,𝛼𝑛] the hyperplane 𝑤 is given as:   Given w and b, we can write the classification function as:   Overall, the process is the same with the dual problem.  Linear SVMs  Perfectly separable data points means finding the maximum margin Almost separable data points means finding the soft margin where we not only find the  maximum margin, we also mis-classify the outliers to find the maximum margin.  Nonlinear SVM  Nonlinear SVM Kernel trick and non-linear SVM 2:36pm  In the image above, what happens if we can’t draw a decision boundary line when the classes are group like the bottom right image.  Therefore to handle non-linearity we:   transform the features to a higher dimensional space where data is linearly separable   The figure below illustrates a 2D space in which the data points can only be separated through a nonlinear curve.  However by transforming these data points to a 3D space, it looks that data points are  now linearly separable. This is done using the Kernel trick.   A kernel function is a function that is used to compute dot products in a high dimensional feature space  Dual problem of Nonlinear SVM   Find  such that:  Where K(xi, xj) is the Kernal function  Nonlinear SVM Kernel function 2:42pm  Kernel functions when evaluated on each pair of data instances give rise to a matrix  called a Gram matrix. This matrix (denoted as K) is a positive semi-definite and symmetric matrix.    Some popular kernel functions where:   Linear Kernel:  Optimize C  Polynomial Kernel with degree p:   Optimize C and P  Radial basis function (RBF) kernel:  Optimize C and Sigma which is 2o2 Mapping φ(x) for RBF kernel is infinite dimensional  Nonlinear SVM Summary 2:48pm  SVM fits a linear hyperplane in high dimensional space   In original space the boundaries are nonlinear  Use of the linear kernel   generates linear boundaries  Polynomial kernel with degree 3   The SVM came up with curve boundaries.  Nonlinear SVM Statistical learning theory of SVM 2:50pm  Structural risk minimization   seeks to prevent over-fitting by incorporating a penalty on the model complexity.  Also the general idea is to minimize the structural risk as:  where ℎ(𝑓) is the complexity of hypothesis function 𝑓 and 𝜆 is a penalty parameter. So we would like to choose a model with small error(Remp(f)) and less  complex(𝜆ℎ(𝑓)).  Nonlinear SVM Vapnik-Chervonenkis (VC) Dimension 2:52pm Find what is the best dimension for the solution    In the left figure:   3 points with any combination of labels can be separated by a line. It will not matter even if you change the labels of the data points.  But in the right figure  A single line cannot separate these data points.   Therefore, VC dimension of a line in 2−dimension is 3   In d−dimension: d+1   In summary, we are trying to minimize the complexity of dimensions in the model The class of optimal linear separators have VC dimension ℎ, bounded as:  Where ρ is the margin, D is the diameter of the smallest sphere that can enclose  all of the training examples, and d is the dimensionality. Intuitively, this implies that regardless of dimensionality 𝑑, we can minimize the model complexity (VC dimension) by maximizing the margin 𝜌. If 𝜌 is maximized or in other words,     a classifier with high margins using SVM   a smaller value for the complexity of model ℎ Thus is can control overfitting  Multi-class classification in SVM 2:58pm  One vs all 3:00pm     For this approach, we split the multiclass dataset where a binary SVM classifier is trained with samples from a class being viewed as positive examples and samples from the other classes being viewed as negative examples. In the image below, we see that we have classes '0', '1' and '2' in the original dataset then three models will be trained where each of them will classify samples from '0' vs {'1' ,'2'}, '1' vs {'0' ,'2'} and '2' vs {'0' ,'1'} for N number of classes the number of models that will be generated is N.  Can be disadvantageous if there are a lot of classes.  This is a common technique used as it uses binary SVM classifier  One vs One 3:04pm     For this approach, the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For example, if we have three classes (Blue, Green, and Red), we would train three binary classifiers: Blue vs Green, Blue vs Red, and Green vs Red. For N number of classes, the number of binary classifiers that will be generated in this approach is (𝑁×(𝑁−1))/2.  During the prediction phase, each test sample is passed to all binary classifiers and a voting scheme is applied on the output of individual binary classifier outputs to determine the final class label .   The most voted predicted label will be selected as the predicted final class label.  Computational expensive as it trains many binary classifiers. However, it is more accurate than One vs All SVM method.    