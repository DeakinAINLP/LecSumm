SVM Formulation and solution for linearly separable data  SVM aims to  - -  Therefore solves the following optimization problem:  o o  Need to optimize a quadratic function in w subject to linear constraints o  This problem is notorious and known as quadratic programming  ▪  The optimization problem is often solved by constructing an equivalent problem  called a duel problem  Duel optimization problem  -  The primal problem in SVM which is maximizing the margin( or minimizing 1/margin)  o o  Using Lagrange multipliers we can convert a constrained optimization into an  unconstrained optimization problem  ▪  Lagrange multipliers are a strategy for finding the local maxima and minima of a  function subjection to equality constraints  SVM Formulation and solution for linearly non-separable data  -  Sometimes data can be linearly separable, at other times some of the instances may not be  linearly separable:  -  Need a trade-off between the margin and the number of errors in classifying the training instances  -  Soft margin concept  -  Soft margin dual problem     Defined when we change the primal problem with soft margins to dual, it remains the  same except that there is an upper bound on the Lagrange multipliers  o  Kernel trick and non-linear SVM  -  Linear regression formulation  o  The linear equation should allow us to summaries and study relationships between two  continuous (quantitative) variables  o  Support vector regression  -  5 randomly selected students took a test, we want to know:  o  What linear regression equation best predicts performance based on aptitude scores   To solve the problem we first need to create a dummy feature which contains 1 and append it to the X  So we have:  Based on the minimisation of error function, we know that w should be the minimum mean square error. SO we can find the values of w as:  in order to have    Statistical learning theory of SVM  Structural  risk minimization   -  Seaks to prevent over-fitting by incorporating a penalty on the model complexity; meaning it  prefers simpler functions over more complex functions  o  Idea is to minimse the structural risk where h(f) is the complexity of the hypothesis  function f and  is a penalty parameter:  ▪  An Illustration of VC Dimension  -  -  o  P  = margin o  D = diameter of the smallest sphere that can enclose all of the training examples o  d = dimensionality  -  Regardless of dimensionality d, we can minimize the model complexity (VC dimensions) by  maximizing the margin p. if p is maximized, we look for a classifier with high margins it means  that we have a smaller value for complexity of our model h. In conclusion, maximising margins will result in havinga  less complex model (a small upper bound for h).  , therefore we have a smaller upper bound for the  -  Etrain = error on training set -  Etest = error on test set (generalisation error)   -  N = size of training set -  H = VC dimension of the hypothesis class -  P = upper bound on probability that this bound fails  -  2 ways to minimize this  o  First increase N which is the number of training samples o  Second way is to minimize h which is the complexity f the model  -  By reducing the complexity of the model, you have a higher chance for smaller test values  (smaller upper bound). This Is another way to show the importance of maximizing margins and handling the complexity of the models.  SVM in python – linear kernel   SVM in Python – RBF kernel  RBF kernel  -  Specify RBF Kernel using two parameteres Cand gammma.  o  Gamma parameter defines how far the influence of a single training example reaches,  with low values meaning ‘far’ and high values meaning close  o  Gamma parameter can be seen as the inverse of the radius of influence of samples  selected by the model as support vectors  o  The C parameter trades off misclassification of training exmaples against simplicity of  the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to sleect more samples as support vectors   