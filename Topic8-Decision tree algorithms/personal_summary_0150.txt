During this lesson Support Vector machine(SVM), Linear SVM, Non-linear SVM, Support Vector Regression, Statistical Learning Theory of SVM and Multi-Class Classification.  Support Vector Machine  Common binary Classifier. With Linear Regression, we find the best line. SVM works for linear and non-linear models. SVM is one of the complex models. SVM is a good algorithm. It works with outliers. And it works with non-linear datasets.  In linear regression, we learn the with the given data.  The question is how can we find the best line?  The concept that SVM brings is finding the best line from the set of possible lines.  We try to learn margin. Separate both classes with the highest margin. It means the line is far from each boundary. If the line goes closer to one of the borders, then the new data will be misclassified. That‚Äôs why we maximize the margin.  How can we maximize the margin?  Margin  We need to calculate the distance from each data point to the line. We use Euclidean distance to calculate the distance from each data point from each class to the hyperplane.  We need to maximize the margin. We can use minimise  1 ùëö  Get the minimum of the Lagrange multiplier. There is no w,b in the Lagrange multiplier.  Linear SVM Dual optimisation  If we can find  Œ±  then we can find w, b.  SVM works with bigger datasets. If the data set is bigger, better results are obtained.  ‚óè Linear SVMs  ‚óã Perfectly separable data points.  Classes are perfectly distributed and we have no outliers. But the data we have always have outliers.  ‚óã Almost separable data points.  ‚óè Nonlinear SVMs  Find the line that can perfectly divide these two classes. We need to allow these misclassification.  Then for the new data the model has tolerance to accept the new distribution of data.  Soft margin  - Accept the classification but also consider the maximum margin. (Almost separate data point) - Allowing some of the data points to cross the borders and to be on the wrong side of the boundary  or to be misclassified.  Add slack variable to the process, allowing misclassifications.  Increase C means we focus more on misclassification, it means fewer misclassifications.  A higher margin means a less complex model. Losing accuracy, high bias.  Decrease C - Misclassification increases, loose accuracy and is less complex.  ‚óè The parameter can be used as a way to achieve the trade-off between large margins and fitting  training data.  ‚óã High values of  ‚ñ† Highly penalize the misclassification.  ‚óè  Small values of  ‚óã Allows more misclassifications.  Adding depth (z-axis) to the data. We can see blue dots are upper and the red dots are lower.  Changing the direction of the view.  We use the training dataset to add new features.  As we increase the dimensions dot product doesn‚Äôt work anymore. We need to replace the dot product with the kernel function. Then we can add new dimensions to the feature space.  Nonlinear SVM - Kernel function  ‚óè Linear Kernel ‚óè Polynomial Kernel with degree p ‚óè Radial basis function (RBF) kernel  Nonlinear SVM - Support vector regression  - Complex model Less popular - Same as the classifier - - Talks about boundary (lower and upper boundary trying to cover data points) - When we increase the boundary model becomes complex.  Nonlinear SVM - An Illustration of VC Dimension  This comes with the concept of shattering data points. If we have 3 data points, how can we perfectly differentiate those 3 points using a line?  Multi-class classification in SVM  1. One vs all  2. One vs One  One vs all  - Heuristic method of using a binary classification algorithm. -  Splitting the multiclass dataset into binary classification problems and predictions are made on the model that is most confident.  - Use binary classification for training and during testing, we use each of those trained classifiers  for prediction for a given dataset.  Disadvantage  - Require one model to be created for each class.  One vs One  - Less commonly used. - Useful for smaller datasets. -  SVM is trained for each pair of classes in the dataset.  -  n classes needs  ùëõ(ùëõ‚àí1) 2  models.  - More accurate compared to one vs all  Disadvantage  - Computationally expensive  