 Support vector machines, or SVM is a supervised learning algorithm, which can be used in classification or regression tasks to find the optimal hyperplane in a high dimensional dataset. SVM are suitable for both linearly and non-linearly separable data. A soft margin can be introduced to data sets that are nearly linearly separable or have noise. SVM can also transform non-linearly separable data to higher dimensional spaces where separation is possible by using kernel functions.  SVMs are based on statistical learning theory principles, which help maximise the generalisation performance of a model. The Vapnik-Chervonenkis Dimension is the number of instances an algorithm can shatter or classify, and can measure the complexity of a machine learning algorithm.  Multi-class classification is an extension of binary classification, which deals with tasks where there are more than two possible classes present. SVM can be extended beyond its original form to accommodate this in two ways. The one vs all method takes the target class as true and treats all others as false, using multiple binary classifiers to distinguish one class from the remaining classes. The one vs one approach also deals with multiple binary classifiers, but instead compares pairs of classes to determine the best fit. It requires more classifiers than one vs all, however there may be cases in which classifiers perform better on particular pairs of classes.  