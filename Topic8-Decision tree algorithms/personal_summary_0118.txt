TASK 7.1  SUMMARISE THE MAIN POINTS THAT IS COVERED IN THIS TOPIC  In summary, Linear SVM is suitable for linearly separable datasets and aims to find an optimal  linear decision boundary with a maximum margin. Non-linear SVM, on the other hand,  utilizes the kernel trick to handle non-linearly separable data by mapping it into a higher-  dimensional space. Non-linear SVM provides more flexibility in capturing complex  relationships between data points but requires careful selection of kernel functions and  regularization parameters to prevent overfitting.  Linear SVM:  Linear SVM is a variant of SVM that works well when the data is linearly separable. It aims to  find a linear decision boundary that maximizes the margin between the classes. The margin is  the distance between the decision boundary and the closest data points from each class. By  maximizing the margin, the SVM achieves better generalization and robustness.  SVM formulation and solution for linearly separable data  Support Vector Machines (SVM) are a popular supervised learning algorithm used for  classification and regression tasks. In the case of linearly separable data, SVM aims to find a  hyperplane that maximizes the margin between the classes while satisfying the constraint.  The SVM formulation involves solving an optimization problem to maximize the margin or  minimize the objective function. This problem can be formulated as a quadratic programming  problem, where the goal is to optimize a quadratic function subject to linear constraints.  To solve the SVM optimization problem, a dual problem is constructed using Lagrange  multipliers. By applying Lagrange multipliers, the primal problem is converted into an  unconstrained optimization problem. The Lagrange function is derived, and by setting its  derivative to zero, the dual formulation is obtained.    SVM formulation and solution for linearly non-separable data  In the case of linearly non-separable data, the standard SVM formulation and solution need to  be modified to allow for misclassification of instances. This is known as the soft margin  concept, where slack variables are introduced to account for misclassified or difficult-to-  classify instances.  The soft margin concept adds flexibility to the SVM algorithm, allowing for a trade-off  between margin maximization and the number of misclassifications. It accepts the presence of  some misclassified instances and aims to minimize the sum of the slack variables, which  represent the degree of misclassification.  The SVM with soft margin formulation introduces a parameter, C, that controls the balance  between margin maximization and misclassification penalty. Higher values of C penalize  misclassifications more, leading to narrower margins, while lower values of C allow for more  misclassifications and wider margins.  The soft margin dual problem is derived from the modified primal problem, introducing an  upper bound on the Lagrange multipliers. The dual problem remains similar to the original  dual problem, but with the added constraint on the Lagrange multipliers.  In summary, when dealing with linearly non-separable data, SVM introduces the soft margin  concept to allow for misclassifications. The soft margin formulation modifies the optimization  problem and introduces slack variables. The soft margin dual problem is derived, and the  solution provides the support vectors and the hyperplane equation for classification. The  parameter C controls the trade-off between margin maximization and misclassification  penalty, providing flexibility in handling non-separable data.     Linear regression formulation  Linear Regression Formulation, we learned the following:  Linear hypothesis: Linear regression aims to find a line that summarizes and studies the  relationship between two continuous variables. The line is defined by the slope (β) and the  intercept (α).  Multi-dimensional formulation: Linear regression can be extended to multiple dimensions by  using vector notation. The formula involves the dot product between the feature vector and the  parameter vector.  Fitting the line to data points: The goal is to minimize the empirical risk or mean square error  (MSE) between the predicted values and the true values. This is achieved by finding the  parameters (β) that minimize the MSE.  Overall, linear regression provides a straightforward and interpretable approach to modeling  the relationship between variables. It seeks to find the best-fitting line that minimizes the error  between predicted and true values, and the solution is obtained through the normal equation  and the pseudo-inverse of the feature matrix.  Support vector regression  In the example of Linear Regression, we have the following scenario:  We have data from five randomly selected students who took a math aptitude test before  starting their statistics course. The goal is to find a linear regression  equation that can predict the statistics performance based on the math aptitude scores.  To solve this problem, we first create a dummy feature that contains 1 and append it to the  math aptitude scores. This is done to account for the intercept term in the linear regression  equation.   Next, we use the normal equation to find the optimal values of the parameters (β) that  minimize the mean square error. By substituting the given data into the equation, we obtain the  values for β.  Once we have the parameter values, we can make predictions using the trained model. For  example, if a student scored 80 on the math aptitude test, we can use the equation to predict  their statistics grade. By substituting 80 into the equation, we can calculate the predicted  statistics grade.  Statistical learning theory of SVM  The statistical learning theory of SVM provides a theoretical understanding of why maximum  margin is a desirable property for classification.  In structural risk minimization, we aim to choose a model with low complexity and small error  to prevent overfitting. The complexity of a hypothesis class can be characterized by its  Vapnik-Chervonenkis (VC) Dimension, which represents the number of instances that can be  perfectly separated by the hypothesis class.  The VC dimension of a linear separator in a specific dimension is bounded by a formula that  involves the margin, the diameter of the smallest enclosing sphere, and the dimensionality.  This implies that maximizing the margin can minimize the complexity of the model, leading to  a smaller upper bound on the VC dimension.  Maximizing the margin in SVM is justified by the theoretical results of Vapnik. By  maximizing the margin, we can achieve a less complex model, reducing the VC dimension and  preventing overfitting.  Overall, the statistical learning theory of SVM highlights the importance of maximizing  margins and controlling model complexity to achieve better generalization performance and  minimize the risk of overfitting.   Multi-class classification in SVM  In multi-class classification using SVM, there are two commonly used approaches: One vs All  and One vs One.  1.  One vs All (One vs Rest):  In this approach, a separate binary SVM classifier is trained for each class, treating samples  from that class as positive examples and samples from the other classes as negative examples.  For a dataset with classes,  binary classifiers are trained. Each classifier distinguishes between one class and the rest of the  classes.  During prediction, the test sample is passed through each binary classifier, and the class with  the highest score or confidence is selected as the predicted class.  2.  One vs One:  In this method, multiple binary SVM classifiers are trained, each trained to distinguish  between two classes.  For a dataset with classes,  binary classifiers are trained. Each classifier compares pairs of classes, resulting in classifiers.  During prediction, the test sample is evaluated by all binary classifiers, and a voting scheme  (e.g., majority voting) is applied to determine the final class label based on the outputs of  individual binary classifiers.  Both approaches have their advantages and considerations:  One vs All is computationally efficient since it requires training classifiers, but it can be biased  towards the majority class in imbalanced datasets.  One vs One requires training classifiers, which can be computationally expensive for large  datasets, but it can handle imbalanced datasets better and may lead to more accurate  predictions.     SVM in Python - Linear kernel  The code snippet demonstrates how to build a support vector machine (SVM) classification  model using a linear kernel in Python. Here's a brief summary of the steps:  1.  Data Preparation:  The code imports necessary modules such as StandardScaler for data scaling, seaborn for  visualization, train_test_split for splitting data into training and test sets, pandas for data  manipulation, numpy for numerical operations, and datasets from sklearn for loading the iris  dataset.  The iris dataset is loaded, and the features (petal length and width) and target values are  assigned to variables X and y, respectively.  The iris data is converted into a pandas dataframe for easier manipulation.  The data is split into training and test sets using train_test_split with a test size of 20% and a  random state of 0.  The StandardScaler is applied to scale the training and test data.  2.  Data Visualization:  The histograms of the original iris data and the scaled data are plotted using matplotlib  3.  SVM One vs Rest (OvR):  The code imports plot_decision_regions from mlxtend.plotting and PCA from  sklearn.decomposition for dimensionality reduction.  The training data is reduced to 2 dimensions using PCA.  An SVM classifier with a linear kernel and OvR decision function shape is trained on the  reduced training data using SVC.  The decision regions are plotted using plot_decision_regions for the reduced test data, true  labels, and the trained classifier.  4.  SVM One vs One (OvO):  Similar to the OvR approach, an SVM classifier with a linear kernel and OvO decision  function shape is trained on the reduced training data using SVC.  The decision regions are plotted using plot_decision_regions for the reduced test data, true  labels, and the trained classifier.   SVM in Python - Polynomial kernel  1.  Data Preparation:  The code imports necessary modules such as numpy for numerical operations, train_test_split  for splitting data into training and test sets, pandas for data manipulation, and datasets from  sklearn for loading the iris dataset.  The iris dataset is loaded, and only the first two columns of features are used for easy  plotting/visualization.  The samples belonging to class 1 and class 2 are selected, and the corresponding features and  labels are assigned to variables x and y, respectively.  The data is split into 80% training and 20% testing sets using train_test_split.  2.  SVM with Linear Kernel:  The code defines a function plot_estimator to plot the decision boundaries of the trained  model.  An SVM classifier with a linear kernel is trained on the training data using svm.SVC with  kernel='linear'.  The training accuracy is computed using metrics.accuracy_score.  The plot_estimator function is called to plot the decision boundaries and original training  samples.  3.  Support Vectors Visualization:  The code plots the support vectors by scattering the support_vectors_ attribute of the trained  SVM model.  4.  Effect of C parameter on SVM:  The code demonstrates the effect of the C parameter on the SVM model.  A high C value (C=1e2) is set, which corresponds to a high penalty for misclassification. The  decision boundary becomes narrow, resulting in a smaller number of support vectors.  A low C value (C=1e-2) is set, which corresponds to low regularization and a larger margin.  This leads to more support vectors.  The training accuracy and testing accuracy are computed for both cases using  metrics.accuracy_score.  5.  SVM with Polynomial Kernel:  The code fits an SVM model with a polynomial kernel and a specified degree (degree=2).  The training accuracy and testing accuracy are computed using metrics.accuracy_score.  The plot_estimator function is called to plot the decision boundaries.  6.  Support Vectors Visualization:  The code plots the support vectors for the polynomial kernel SVM.  The output of the code includes the decision boundaries, support vectors, and accuracy scores  for different SVM models with linear and polynomial kernels.  This code provides a basic understanding of using SVM with a polynomial kernel, tuning the  C parameter, and visualizing the decision boundaries and support vectors in Python.      SVM in Python - RBF kernel  In summary, using the RBF kernel in SVMs allows for handling non-linearly separable data  and provides flexibility in creating complex decision boundaries. Understanding the influence  of the gamma parameter helps in controlling the model's complexity and generalization. It is  important to strike a balance between training accuracy and testing accuracy by selecting an  appropriate gamma value for a given problem.  1.  RBF Kernel in SVM:  The RBF kernel is a popular choice for SVMs and is effective in handling non-linearly  separable datasets.  It allows SVMs to create non-linear decision boundaries by mapping the input data into a  higher-dimensional feature space.  2.  Parameters of RBF Kernel:  C Parameter: It trades off misclassification of training examples against simplicity of the  decision surface. A smaller C value makes the decision surface smoother, while a larger C  value aims at classifying all training examples correctly by allowing the model to select more  support vectors.  Gamma Parameter: It defines the reach or influence of a single training example. A low  gamma value implies a broader influence, whereas a high gamma value implies a narrower  influence. It can be seen as the inverse of the radius of influence of the support vectors.  3.  Influence of Gamma:  By varying the gamma parameter, you can observe the differences in the decision surface and  accuracy.  Higher gamma values result in more complex decision boundaries that closely fit the training  data, potentially leading to overfitting.  Lower gamma values result in smoother decision boundaries that generalize better to unseen  data.  4.  Practical Implementation:  The provided code example demonstrates how to use the RBF kernel in SVMs with different  gamma values (e.g., 10, 100, 1000) on a given dataset.  The code fits the SVM model, visualizes the decision surface, and calculates the training and  testing accuracies.  The support vectors are also plotted to visualize their distribution.  5.  Learning Outcomes:  By experimenting with different gamma values, you can observe the impact on the decision  boundary and accuracy.  Higher gamma values can lead to better training accuracy but may result in overfitting and  reduced testing accuracy.  Lower gamma values can result in smoother decision boundaries and better generalization but  may sacrifice some training accuracy.  