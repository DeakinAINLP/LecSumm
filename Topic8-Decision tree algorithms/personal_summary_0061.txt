  Formulation and solution of  SVMs for linearly separable data  This problem is well known in the optimization community and is called quadratic programming. Not to be confused with the word programming . It simply means optimization. Optimization problems are often solved by constructing equivalent problems, called dual problems.  In quadratic programming, the original optimization problem is called the principal problem. The solution of the dual problem provides a lower bound for the solution of the main  (minimized) problem.  Double op)miza)on ques)on  Recall the  main problems with SVMs that maximize (or minimize) margins.  1/margin  You can use the Lagrange multiplier to convert constrained optimization into an unconstrained optimization problem. The Lagrange multiplier is a strategy for finding the maximum and minimum of an equality-constrained function. The Lagrange function to minimize is α=[α 1,... , αn] Lagrange multiplier is:       Now that we have the function to maximize, we can zero (w)  by setting the derivative and assign the result to the Lagrange function to obtain a dual formulation (dual problem).  If you look carefully at the derived issues, you can see the following:   This is because xi  and xj data points are multiplied by the dot product. For vectors, this represents the similarity between these two vectors.  Later, we will analyze this function more precisely. Recall that for now, with the help of the Lagrange multiplier, we created a double problem of minimization.  1/2∥w∥^2 This maximizes the margin of the SVM.  Now, let's say we find α, the value, and therefore the given solution α=[α 1,...  , αn] hyperplane w is given as:  Note that solves  For a and b, the dot product should only use the training data in the form of xi^Txj.  In addition, note that the class function also uses the dot product between:  x and support vector xi.  Optimization details are not discussed here. This is because you need to have a be.er understanding  of quadratic programming. However, you should know that the main problem has a requirement for calculating the degree. 〇(d^3), whereas the dual problem requires degree 〇(n^3). where d is the  dimension of the feature space and n is the number of instances in the training set. When n>d solving duals is more expensive than the main one. The dual problem is often used because you can use any kernel. This means that SVM boundaries can be significantly nonlinear.   Formulation and solution of  SVMs for linearly inseparable data       SVMs have historically assumed that data is linearly separable. If the data is not linearly separated, what approach should I take?  In some cases, data can be separated linearly but have narrow margins. Also, due to noise, some instances may not be linearly separated (see figure for noisy data).  In general  , it is recommended that small noisy data points or outliers do not interfere with the boundary. Even if some constraints are violated, it is acceptable to have a large margin. In practice, there is a trade-off between margin and error count when classifying training instances.  This trade-off creates the concept of soft margins. Consider the following diagram.  The concept of soft margin is defined when  training instances  cannot be linearly separated. Slack variable Zeta I  It is added to allow misclassification when outliers, noisy, or instances are difficult to classify. So, basically, you're allowing some data points to cross the boundary and be on the wrong side of the boundary or misclassified.    You want to allow some training instances to be misclassified, but you still want  to minimize the sum of the slack variables. Therefore, for those data points, if the  ZetaI value is not zero, we can assume that they are misclassified, and the amount of misclassification is also expressed as:  Zeta I.  SVMs  with soft margins use the following formulation:  Parameter c  can be used as a way to achieve trade-offs between large margins and  fitting training data. For a high value of c, it  penalizes the misclassification very much, but for a smaller value of c, it allows more misclassification. This is how SVMs handle this trade-off for misclassification.  So# margin double ques2on  A soft margin dual problem  is defined as changing a main problem with soft margins to dual. It remains the same, except that there is an upper limit on the Lagrange multiplier.  The dual problem is given as follows:  As you can see, the difference is c given  solution α=[α 1,... , αn] hyperplane w is given as:      There is also one αi corresponding to each xi.  The α i  corresponding to each non-zero of  xi is called the support vector. Given w and b, we can write the classifier function as follows:  To solve again a and b, we need to use the training data only in the form of the dot product xi^Tx further, the class function only uses the dot product between them.  x and support vector-xi.   Linear regression formulation  In linear regression, we want to find a line like time. Linear equations allow you   to summarize and examine the relaIonship between two conInuous (quantitative) variables. First, define the line (see figure below).  Linear hypothesis  So how can you find a straight line? This line has two parameters.  w, the slope of the straight line,  and the interception of the b, y line. Given these two parameters, you can use them to find a straight line.  Given the points, we  can estimate the value of xi y  ^(xi).  We will call you for a quote y(xi). Thus the row predicts y^(x i) for xi.         Now, if x is not a single-dimensional value?  The problem may have multiple single functions, so the problem may be in the d dimension. In this case, we would write linear regression as:   Statistical Learning Theory of  SVMs  In theory, does the maximum margin make sense? As you will recall, structural risk minimization attempts to prevent overﬁNng by incorporaIng penalties into the complexity of the model. This means that you prefer simpler functions over more complex ones. The general idea is to minimize  structural risks. h(f) is the complexity of the hypothesis function  f  and λ are penalty parameters.        Therefore, it is necessary to choose a model with a small error and not complexity.  Let's say  you choose Assign instances and labels + and − randomly to them. If the hypothesis class is  rich enough to learn to associate labels with data, it is complex enough.  Why not characterize the complexity of a hypothetical class by looking at how many instances it can crush  (that is, does it fit all possible label assignments perfectly)? The number of instances that a hypothesis class can crush is called its Vapnik-Chervonenkis (VC) dimension.  VC dimensions illustrated  Let's assume that we are using a line (or hyperplane)  as our hypothesis class. In 2− dimensions, you can find a line to grind labeling 3 points. But the line may not be able to break down some labeling 4.  Therefore, the VC dimension of the line  is  2− the dimension is 3.  (d-dimension: d+1.  Consider the following diagram: As you can see in the image above, these three points with any combination of labels   can be separated by a line .  It's okay to change the label of a data point.  As you can see from some examples in the figure, we can successfully separate these points with a line. However, in the image below, you can see that there may be situations where these data points cannot be separated by a single line.  Because, with 2-dimension, labeling  you  can always find a line to crush 3 points. The labeling may not be able to find a line to crush 4 points.  The theoretical justification for the maximum margin is shown by Vapnik in the following results  : The class of best linear separators has a  VC dimension  and time from the top is limited as follows:     where the p margin is  D the smallest sphere diameter that can surround all training examples. d-dimension.  Intuitively, this minimizes the complexity of the model  (VC  dimension) by maximizing the margin regardless of dimension.   p if p  is maximized. That is, if you look for a classifier with a high margin , it means that the value is small. [p^2  /p^2], thus reducing the upper limit of model complexity h.  In conclusion, maximizing margins reduces model complexity (h)  This almost proves why we aim to maximize margins on SVMs.  But what is a probabilis2c guarantee?  You have only two ways to minimise this.  First increase N which is the number of training samples, basically, increase the number of samples for training, which is obvious.  The second way is to minimise ℎ which is the complexity of the model. This is really useful.  This equation states, by reducing the complexity of the model, you have a higher chance for smaller test values (smaller upper bound). So this is another way to show the importance of maximising margins and handling the complexity of the models.       Multiclass classification on  SVMs  Multiclass classification on SVMs can be performed as follows:  1 to all members  1. 2.  One-to-one  1 to all members  In this approach, a binary  SVM  classifier is trained on each class, and samples from that class are considered positive examples and samples from other classes are considered negative examples. For example, if the original dataset has  classes '0', '1', and '2', three models are trained, each with '0  ' and  Classify samples {'1','2'}. '1' vs. {'0','2'} and '2' vs.  {'0','1'} (as shown in the following figure).  In this approach,  the number of classes and  the number of models generated  are .  Here, in the prediction phase, a test sample is passed to each model (classifier) to determine the prediction class based on the highest score obtained from the model (see figure above).     One-to-one  In this method, the SVM algorithm is trained to train multiple binary classifiers, each of which  distinguishes  between two classes. For example, if you   have three classes (blue, green, and red),  train three binary classifiers: blue and green, blue and red, and green and red. For the number of classes, the number of binary classifiers generated by this approach is ×( -1)2.  During the prediction phase, each test sample is passed to all binary classifiers, and a voting scheme is applied to the output of each binary classifier to determine the final  class label (see the following figure).  2.  Provide summary of your reading list–external resources, websites, book  chapters, code libraries, etc.  3.  Reflecton the knowledge that you have gained by reading contents of this  topic with respect to machine learning.      Here are some of the key areas we'll be covering this topic:  Linear SVM § §  Nonlinear SVMs.  (1) Linear SVM  SVM, like logistic regression, is a method of finding data boundaries and classifying data.  Find a hyperplane that maximizes the distance from the hyperplane (class boundary) to the data (support vector) closest to the hyperplane of each class.  The hyperplane of the SVM is drawn as far away from each data as possible.  Therefore, the generalization ability for unknown data is high, and the accuracy is easy to improve.  The disadvantage is that as the amount of data increases, the amount of calculation increases, and it takes time to produce results.  (2) Nonlinear SVM  SVMs perform linear classification, but by using a method called kernel trick (data manipulation by a transformation formula called a kernel function), it is possible to handle data that cannot be linearly classified.  The kernel trick maps real space data to a space that can be classified by hyperplanes, and then classifies the target data in the same way as linear SVM.  