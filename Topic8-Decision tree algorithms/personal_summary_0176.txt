Machine learning (Pass Task- Topic 7)  Support Vector Machines (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is particularly effective for handling complex and high- dimensional datasets. SVMs are based on the concept of finding the best hyperplane that separates the data points belonging to different classes.  SVM is a powerful algorithm that uses kernel functions to handle data that may be separated into linear and nonlinear categories. It has seen widespread application across a number of industries, including text categorization, picture recognition, bioinformatics, and finance.  IN SVM, the basic idea is that the decision boundary should be as far away from the data of both classes as possible.  The aim is to minimize possible conflicts which may happen during classification  The best hyperplane is the one that is farthest from both classes, and this is what SVM primarily aims to achieve. This is accomplished by identifying various hyperplanes that best classify the labels, after which it selects the one that is either the furthest away from the data points or has the largest margin.  The minimum distance of the hyperplane from any instance is:  The margin is defined as the distance between the two vectors:  From the two equations we can derive:              It is generally preferred not to interfere with the boundary even with small noisy data points or outliers. It is acceptable to have large margins even though some of the constraints are violated. In practice, we need a trade-off between the margin and the number of errors in classifying the training instances.  This trade-off brings us to the soft margin concept. Consider the following figure; the soft margin concept is defined when the training instances are not linearly separable.  Soft margin concept:    allowing some of the data points to cross the borders and to be in the wrong side of the  boundary or to be mis-classified.    Slack variables are added to allow mis-classification of outliers, noisy or difficult to  classify instances.  Kernel Trick and Non-linear SVM  To handle non-linearity:    Transform the features to a higher dimensional space where data is linearly separable (see  figure below)    This figure illustrates a 2D space in which the data points can only be separated through a  nonlinear curve.        By transforming these data points to a 3D space, it looks that data points are now linearly  separable!  Kernel function is a function that is used to compute dot products in a high dimensional feature space.    SVM fits a linear hyperplane in high dimensional space: In original space the boundaries  are nonlinear    Use of the linear kernel: generates linear boundaries   Polynomial kernel with degree 3: the SVM came up with curve boundaries.  Statistical  learning theory of SVM            VC Dimension  In the left figure:3 points with any combination of labels can be separated by a line. No matter even you change the labels of the data points.  But in the right figure: a single line cannot separate these data points. Therefore, VC dimension of a line in 2−dimension is 3 (In d−dimension: d+1)  Multi-class  classification  in SVM  Multiclass classification in SVM can be done as follows:  1.   One vs all  In this approach, for each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples. For example, if we have classes '0', '1' and '2' in the original dataset then three models will be trained where each of them will classify samples from '0' vs {'1' ,'2'}, '1' vs {'0' ,'2'} and '2' vs {'0' ,'1'} (as shown in following figure). In this approach, for N number of classes the number of models that will be generated is N. In the prediction phase, the test sample is passed to each model (classifier) and the predicted class is determined based on the highest score obtained from the models      2.  One vs One  In this method, the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For example, if we have three classes (Blue, Green, and Red), we would train three binary classifiers: blue vs green, blue vs red, and green vs Red.  For N number of classes, the number of binary classifiers that will be generated in this approach is N×(N-1)2.  During the prediction phase, each test sample is passed to all binary classifiers and a voting scheme is applied on the output of individual binary classifier outputs to determine the final class label  