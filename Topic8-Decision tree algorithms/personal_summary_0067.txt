The statement "The upper bound on the generalisation error increases with higher complexity (higher h)"  suggests  that  as  the  VC  dimension  of  the  hypothesis  class  increases,  the  upper  bound  on  the generalisation error also increases. More complex models may overfit the training data, resulting in poor performance on new, unseen data. Therefore, this statement favours simpler models with lower VC dimensions over more complex ones. When choosing a model, one should consider the tradeoff between  model  complexity  and  generalisation  error  and  select  a  model  with  the  appropriate  VC dimension.  The statement "The upper bound on the generalisation error reduces with larger training sets (higher N)" suggests that as the size of the training set increases, the upper bound on the generalisation error decreases. This means that models trained on larger datasets will likely perform better on new, unseen data.  Therefore,  this  statement  favours  models  that  can  handle  larger  datasets  and  have  enough capacity to learn from them. When choosing a model, one should consider the size of the available training data and select a model that can effectively learn from it.  The two statements suggest that the ideal model should balance the VC dimension and the training data size tradeoff. A simple model with a low VC dimension may underfit the training data and perform poorly on new data. At the same time, a complex model with a high VC dimension may overfit the training data and perform poorly on new data. Additionally, the ideal model should be trained on a sufficiently large dataset to learn these patterns accurately and have appropriate VC dimensions to avoid overfitting.  Activity 7.9 – Python - Polynomial Kernel Did you read the discussion on the influence of c in SVMS? What is the most valuable information you gained? The most valuable information I gained from reading the discussion of the influence of C in SVMs with linear kernels is that C is a regularisation parameter that controls the tradeoff between maximising the  margin  and  minimising  the  training  error.  A  larger  C  means  a  higher  penalty  for  misclassifying training examples, which leads to a smaller-margin hyperplane that fits the data better. A smaller C means a lower penalty for misclassifying training examples, which leads to a larger-margin hyperplane that may generalise better. Therefore, C affects the complexity and bias-variance tradeoff of the SVM model  Activity 7.10 – SVM in Python - RBF kernel What is the purpose of using the RBF kernel? The RBF kernel is a commonly used function in various machine learning algorithms, especially in SVMs and  kernelised  clustering.  Its  main  purpose  is  to  transform  the  input  data  into  a  high-dimensional feature space, which can be more easily separated by a linear classifier or clustered. The RBF kernel is advantageous in capturing complex and non-linear relationships between data points and can handle high-dimensional data with relatively few parameters. Overall, the RBF kernel is an effective method for modelling complex data.  