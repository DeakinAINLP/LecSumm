      This module is optional for SIT307 students.  SIT720 student must complete this module.  Multi-class classification in SVM Multiclass classification in SVM can be done as follows:  One vs all One vs One One vs all In this approach,  for each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples. For example, if we have classes '0', '1' and '2' in the original dataset then three models will be trained where each of them will classify samples from '0' vs {'1' ,'2'}, '1' vs {'0' ,'2'} and '2' vs {'0' ,'1'} (as shown in following figure). In this approach, for N{"version":"1.1","math":"<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math>"} number of classes the number of models that will be generated is N{"version":"1.1","math":"<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math>"}.  Now, in the prediction phase, the test sample is passed to each model (classifier) and the predicted class is determined based on the highest score obtained from the models (see the above figure).  One vs One In this method, the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For example, if we have three classes (Blue, Green, and Red), we would train three binary classifiers: Blue vs Green, Blue vs Red, and Green vs Red.  For N{"version":"1.1","math":"<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math>"} number of classes, the  number of binary classifiers that will be generated in this approach is N×(N-1)2{"version":"1.1","math":"<math xmlns="http://www.w3.org/1998/Math/MathML"><mstyle mathsize="20px"><mfrac><mrow><mi>N</mi><mo>&#xD7;</mo><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mn>2</mn></mfrac></mstyle></math>"}. During the prediction phase, each test sample is passed to all binary classifiers and a voting scheme is applied on the output of individual binary classifier outputs to determine the final class label (see the following figure). References: http://machinelearning101.pbworks.com/f/MulticlassSVM10.1.1.110.6789.pdf Let’s look at a simple example of linear regression. Five randomly selected students took a math aptitude test before they began their statistics course. The Statistics Department wants to know: What linear regression equation best predicts statistics performance, based on math aptitude scores? The following figure illustrates the data. Xi{"version":"1.1","math":"\(X_i\)"} is the math aptitude scores and yi{"version":"1.1","math":"\(y_i\)"} is the statistics performance. Figure. Illustration of the data. For solving this problem, first we need to create a dummy feature which contains 1{"version":"1.1","math":"\(1\)"} and append it the X{"version":"1.1","math":"\(X\)"}. So we have: X=[195185180170160]{"version":"1.1","math":"X = \begin{bmatrix} 1 & 95 \\ 1 & 85 \\ 1 & 80 \\ 1 & 70 \\ 1 & 60 \\ \end{bmatrix} %"} Y=[8595706570]{"version":"1.1","math":"Y = \begin{bmatrix} 85 \\ 95 \\ 70 \\ 65 \\ 70 \\ \end{bmatrix}"} Based on the minimisation of error function, we know thatw{"version":"1.1","math":"\(\textbf{w}\)"} should be w=(XTX)−1XTy{"version":"1.1","math":"\(\textbf{w} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}\)"} in order to have the minimum mean square error. So we can find the values of w{"version":"1.1","math":"\(\textbf{w}\)"} as: w=(XTX)−1XTy=Y=[w0w1]=[26.7810.644]{"version":"1.1","math":"\(\textbf{w} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y} = Y = \begin{bmatrix} w_0 \\ w_1 \\ \end{bmatrix} = \begin{bmatrix} 26.781 \\ 0.644 \\ \end{bmatrix}\)"} Now that we have the w{"version":"1.1","math":"\(\textbf{w}\)"},  how can we actually use it? Let’s make a prediction based on the trained model. Prediction:If a student made an 80{"version":"1.1","math":"\(80\)"} on the aptitude test, what grade would we expect her to make in statistics? Answer:. Predicted Statistics Grade is computed as Statistics grade: [1   80][w0w1]=1×26.781+80×0.644=78.288{"version":"1.1","math":"\([1\ \ \ 80]\begin{bmatrix}w_0\\w_1\end{bmatrix} = 1 \times 26.781 + 80 \times 0.644 = 78.288\)"}  Therefore based on the trained model we expect the statistics grade of this student to be 78.288{"version":"1.1","math":"\(78.288\)"}. View transcript SPEAKER: In this tutorial, we're going to show you a real example of linear regression. So consider this problem. Our training data is five randomly selected students took a math aptitude test before they began their statistic course. The statistics department wants to know a linear regression equation which best predicts the statistic performance based on the math aptitude score. So what we need here is we want to map this x-i, which is the math aptitude score, to the y-i, which is the score of the statistics performance. In order to find this linear model, which maps x-i to y-i, first we need to construct the matrix x and y. As the first step, if you remember, we need to put in the value of the x into the matrix x, also add a column of dummy feature 1, also we do the same for the y-i and put this in the matrix or vector of y. Now as you remember for finding the weights in the linear regression, it was a pretty simple formula-- x transpose x, inverse, x transpose y. So all we need to do is to find this value, which is a two by one vector. As you see, if you calculate value it would be 26.781 and 0.644. This is basically the weights of the linear model we found. But what if a new student cames in and she has a score of 80 on the math aptitude test? So what should we expect for her score in the statistics? As we already have the value of the weights, all we need to do is to take the score and the math aptitude, which right now here is we assume it is 80. Also, we add this one dummy feature to that and then we multiply that to the vector of the weights. As you can see, it is 1 multiplied by 26.781 plus 80 multiplied by 0.644. And our predicted result is 78.288. View transcript We have seen several examples of model performances in complexity. We have also talked many times about performance in machine learning models. But now we would like to explain VC dimension, a quantity of complexity which captures much of what we need in machine learning models. First, suppose we pick n instances and assign labels of positive and negative to them randomly. If our hypothesis class is rich enough to learn any association of labels to the data, it's sufficiently complex. Consider this figure. There are only two cases in here, whether this positive point is in the right side or in the left side. In both cases, this separating line is rich enough to learn any association of labels to this data. So we call that it's sufficiently complex. We characterise the complexity of hypothesis class by looking at how many instances it can shatter. So this can fit perfectly for all possible label assignments. The number of instances the hypothesis class can shatter is called the VC dimension. In other words, VC dimension of a function said f is the cardinality of the largest dataset that can be shattered by f. Let's see an example. Let us assume that we are using a line or a hyperplane as our hypothesis class. If you look at this figure, you can see we can find at least one set of three points into 2D dimension, which all of whose eight possible labelling can be separated by some hyperplane. If you consider the first one, it has been separated by one line. Also, this case. Also, the case which we only have one label, which are these two cases. And also, in other cases, in all eight possible cases, a line could separate this data. But is it possible for this line to shatter labelling of four points? As we can see, it's not, because in this case or whether in this case, we can see one single line is not enough for the model to shatter the data points. And sometimes we need more than one line. So that's why we said if we see dimension of a line in 2D dimension is 3.It means that we can handle all the possible labelings of these three points into 2D dimension. That was a simple definition of VC dimension. Theoretically, does maximum margin make sense? As you will recall, structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity. This means, it prefers simpler functions over more complex functions. The general idea is to minimise the structural risk as where h(f){"version":"1.1","math":"\(h(f)\)"} is the complexity of hypothesis function f{"version":"1.1","math":"\(f\)"} and λ{"version":"1.1","math":"\(\lambda\)"} is a penalty parameter: Rstr(f)=Remp(f)+λh(f){"version":"1.1","math":"\\ R_{str} (f) = R_{emp}(f) + \lambda h(f) \\"} So we would like choose a less complex model with a small error. Suppose we pick n{"version":"1.1","math":"\(n\)"} instances and assign labels of +{"version":"1.1","math":"\(+\)"} and −{"version":"1.1","math":"\(-\)"} to them randomly. If our hypothesis class is rich enough to learn any association of labels to the data, it is sufficiently complex. How about we characterise the complexity of the hypothesis class by looking at how many instances it can shatter(i.e. can fit perfectly for all possible label assignments). The number of instances a hypothesis class can shatter is called its Vapnik-Chervonenkis (VC) Dimension. An Illustration of VC Dimension Let us assume that we are using lines (or hyperplanes) as our hypothesis class. In 2−{"version":"1.1","math":"\(2-\)"}dimension, we can find a line to shatter any labelling of 3{"version":"1.1","math":"\(3\)"} points. But a line may not be able to shatter some labelling of 4{"version":"1.1","math":"\(4\)"}. Therefore, VC dimension of a line in 2−{"version":"1.1","math":"\(2-\)"}dimension is 3{"version":"1.1","math":"\(3\)"}. (Ind−{"version":"1.1","math":"\(d-\)"}dimension: d+1{"version":"1.1","math":"\(d+1\)"}. Consider the following figure. As you can see in the top image, these 3 points with any combination of labels can be separated by a line. It doesn’t matter if you change the labels of the data points. Figure. illustration of shattering 3 points by a line vs 4 points where this not possible. As you can see from some of the example in the figure, we can successfully separate these points with a line. But in the bottom images you can see that we can come up with situations in which we can not use a single line to separate these data points. Because in 2−{"version":"1.1","math":"\(2-\)"}dimension, we can always find a line to shatter any labelling of 3{"version":"1.1","math":"\(3\)"} points. It might not be possible to find a line to shatter any labelling of 4{"version":"1.1","math":"\(4\)"} points. The theoretical justification for maximum margin is shown by Vapnik in the following results. The class of optimal linear separators has VC dimension h{"version":"1.1","math":"\(h\)"} bounded from above as:  h≤min{d,[D2ρ2]}+1{"version":"1.1","math":"\\ h \leq min\{d,\Big[\frac{D^2}{\rho^2}\Big]\} + 1 \\"} Where p{"version":"1.1","math":"\(p\)"} is the margin, D{"version":"1.1","math":"\(D\)"}  is the diameter of the smallest sphere that can enclose all of the training examples, and d{"version":"1.1","math":"\(d\)"} is the dimensionality. Intuitively, this implies that regardless of dimensionality d{"version":"1.1","math":"\(d\)"}, we can minimise the model complexity (VC dimension) by maximising the margin p{"version":"1.1","math":"\(p\)"}. If p{"version":"1.1","math":"\(p\)"} is maximised or in other words, if we look for a classifier with high margins it means that we have a smaller value for [D2ρ2]{"version":"1.1","math":"\(\Big[\frac{D^2}{\rho^2}\Big]\)"}, therefore we are going to have a smaller upper bound for the complexity of our model h{"version":"1.1","math":"\(h\)"}. To conclude, maximising margins will result in having a less complex model (a smaller upper bound for h{"version":"1.1","math":"\(h\)"})  This almost proves why we aim for maximising margins in SVM. But what is the The Probabilistic Guarantee? Etext≤Etrain+(h+h log(2Nh)−log(p4)N)12{"version":"1.1","math":"\\ E_{text} \leq E_{train} + \Bigg( \frac{h+h\ log(\frac{2N}{h}) - log(\frac{p}{4})}{N}\Bigg)^\frac{1}{2} \\"} In which: Etrain={"version":"1.1","math":"\(E_{train} =\)"} Error on training set  Etest={"version":"1.1","math":"\(E_{test} =\)"} Error on test set (generalisation error) N={"version":"1.1","math":"\(N =\)"} Size of training set h={"version":"1.1","math":"\(h =\)"} VC dimension of the hypothesis class p={"version":"1.1","math":"\(p =\)"} upper bound on probability that this bound fails Do not get too confused with the formulas. This equation states that the test error (Etest{"version":"1.1","math":"\(E_{test}\)"}) is upper bounded by the training error (Etrain{"version":"1.1","math":"\(E_{train}\)"}) +{"version":"1.1","math":"\(+\)"} some value. Definitely we would want to minimise this value to be as accurate as possible like our training model. But how can we minimise it? Look at the equation:  (h+h log(2Nh)−log(p4)N)12{"version":"1.1","math":"\\ \Bigg( \frac{h+h\ log(\frac{2N}{h}) - log(\frac{p}{4})}{N}\Bigg)^\frac{1}{2} \\"} You have only two ways to minimise this. First increase N{"version":"1.1","math":"\(N\)"} which is the number of training samples, basically, increase the number of samples for training, which is obvious. The second way is to minimise h{"version":"1.1","math":"\(h\)"} which is the complexity of the model. This is really useful. This equation states, by reducing the complexity of the model, you have a higher chance for smaller test values (smaller upper bound). So this is another way to show the importance of maximising margins and handling the complexity of the models. Activity How do the following statements influence model choice? Linear regression formulation In linear regression we want to find a line similar to h{"version":"1.1","math":"\(h\)"}.  The linear equation should allow us to summarise and study relationships between two continuous (quantitative) variables. First we are defining a line (see the figure below): y=h(x)=wx+b{"version":"1.1","math":"\(y = h(\textbf{x}) = w\textbf{x} + b\)"} Figure. Illustration of w{"version":"1.1","math":"\(w\)"} and b{"version":"1.1","math":"\(b\)"} in a linear formulation. Linear hypothesis So how can we find the line? The line has two parameters. w{"version":"1.1","math":"\(w\)"},  the slope of the line, and b{"version":"1.1","math":"\(b\)"}, the y{"version":"1.1","math":"\(y\)"} intercept of the line. If we have these two parameters, we can use them to find our straight line. If we have a point, xi{"version":"1.1","math":"\(\textbf{x}_i\)"} we can estimate the value of y^(xi){"version":"1.1","math":"\(\hat{y}(\textbf{x}_i)\)"}. We will call the estimate y(xi){"version":"1.1","math":"\(y(\textbf{x}_i)\)"}.  So the line predicts y^(xi){"version":"1.1","math":"\(\hat{y}(\textbf{x}_i)\)"} for xi{"version":"1.1","math":"\(\textbf{x}_i\)"}. Now, what if x{"version":"1.1","math":"\(x\)"} is not a single dimension value? Because we may have more than one single feature in the problem, the problem can be in d{"version":"1.1","math":"\(d\)"} dimensions. In this case we write the linear regression as: y^(xi)=w0xi0+w1xi1+w2xi2+...+wdxid{"version":"1.1","math":"\(\hat{y}(\textbf{x}_i) = w_0x_{i0} + w_1x_{i1} + w_2x_{i2} + ... + w_dx_{id}\)"}  =b+w1xi1+w2xi2+...+wdxid{"version":"1.1","math":"\(\quad \quad \ = b + w_1x_{i1} + w_2x_{i2} + ... + w_dx_{id}\)"} Where xi0{"version":"1.1","math":"\(x_{i0}\)"} is a dummy feature with the value of xi0=1{"version":"1.1","math":"\(x_{i0} = 1\)"} and also w0=b{"version":"1.1","math":"\(w_0 = b\)"}. As you can see the dimensioned formula is almost the same as the single dimension formula. The only difference is regarding the multiplication of w{"version":"1.1","math":"\(w\)"} and x{"version":"1.1","math":"\(\textbf{x}\)"} which should be handled in all d{"version":"1.1","math":"\(d\)"} dimensions in the introduced formulation. Using vector notation, we can write the above as y^i=xiTw{"version":"1.1","math":"\(\hat{y}_i = \textbf{x}_i^T w\)"} using d+1{"version":"1.1","math":"\(d+1\)"} dimensional vectors. For i=1,...,n{"version":"1.1","math":"\(i=1,...,n\)"} we have: y^1=x1Tw,for i=1{"version":"1.1","math":"\(\hat{y}_1 = \textbf{x}_{1}^{T} w, \quad for \ i= 1\)"} y^n=xnTw,for i=n{"version":"1.1","math":"\(\hat{y}_n = \textbf{x}_{n}^{T} w, \quad for \ i= n\)"} Therefore, collectively we can write:  y^=Xw{"version":"1.1","math":"\(\hat{\textbf{y}} = \textbf{X}w\)"} where y^=[y^1,...,y^n]T{"version":"1.1","math":"\(\hat{y} = [\hat{y}_1,...,\hat{y}_n]^T\)"} and w=[w0…wd]T{"version":"1.1","math":"\(w = [w_0 \ldots w_d]^T\)"}. Now lets move to the next step. How can we fit this line to data points? But before that, lets review what is the error of value prediction in regression. The difference between what we predicted and the true valueor output of that point, is considered to be the error. We show the error for data point i{"version":"1.1","math":"\(i\)"} and ei{"version":"1.1","math":"\(e_i\)"}: ei=yi−y^i{"version":"1.1","math":"\(e_i = y_i - \hat{y}_i\)"}  Obviously the linear model seeks to minimise the empirical risk R(w){"version":"1.1","math":"\(R(\textbf{w})\)"} via the square loss (yi−y^i)2{"version":"1.1","math":"\((y_i - \hat{y}_i)^2\)"} as: minw1n∑i=1n(yi−y^i)2{"version":"1.1","math":"\(\min_{w} \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)"} We can also rewrite the formula by replacing the y^i{"version":"1.1","math":"\(\hat{y}_i\)"} with xiTw{"version":"1.1","math":"\(x_i^T\textbf{w}\)"}: minw1n∑i=1n(yi−xiTw)2{"version":"1.1","math":"\(\min_{w} \frac{1}{n} \sum_{i=1}^{n} (y_i - x_i^T\textbf{w})^2\)"} The above formulation is just the mean of the square error function which has already been introduced as a proper measurement for regression problems. But how can we solve this minimisation problem? As with other optimisation problems, with a closed form function, we can take the derivative of the error function with respect to w{"version":"1.1","math":"\(w\)"} and equate it to 0{"version":"1.1","math":"\(0\)"}. Then we are able to find the w{"version":"1.1","math":"\(w\)"} which can minimise this error. You may ask yourself, why the derivative is with respect to w{"version":"1.1","math":"\(\textbf{w}\)"} and not x{"version":"1.1","math":"\(\textbf{x}\)"}? The answer is really easy, do not forget that we have the feature vectors of data points which represent  x{"version":"1.1","math":"\(\textbf{x}\)"} and we are looking for proper w{"version":"1.1","math":"\(\textbf{w}\)"} to fit the line on feature vectors. So by taking the derivative of the error function and equating it to 0{"version":"1.1","math":"\(0\)"}, we will find that: w=(XTX)−1XTy{"version":"1.1","math":"\textbf{w} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}"} Just for your knowledge, the matrix  (XTX)−1XTy{"version":"1.1","math":"\((\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}\)"}  is known as the Moore-Penrose pseudo-inverse of the matrix X{"version":"1.1","math":"\(\textbf{X}\)"} and often denoted as X†{"version":"1.1","math":"\( X^{\dagger}\)"}. This can be thought as generalisation of the notion of matrix inverse to non-square matrices. There is an example of linear regression next that may help to further explain this concept. SVM in Python - Linear kernel Now you will extend the code you started in the previous lesson to build a support vector machine classification model using a linear kernel. Let’s start with a linear kernel with the default parameters. The Sklearn documentation for SVM can be helpful to understand this process. data scaling from sklearn.preprocessing import StandardScaler import seaborn as sns from sklearn.model_selection import train_test_split # Import data and modules import pandas as pd import numpy as np from sklearn import datasets iris = datasets.load_iris() # We'll use the petal length and width only for this analysis X = iris.data y = iris.target # Place the iris data into a pandas dataframe iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0) sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) X_test_std = sc.transform(X_test) plt.figure() iris_df.hist() plt.suptitle("Before scaling") pd.DataFrame(X_train_std, columns=iris_df.columns).hist() plt.suptitle("After scaling") plt.show() Output: SVM one vs rest The idea behind OvR (SVM one vs rest) is to train one binary classifier for each class, where the positive examples are from that class and the negative examples are from all other classes combined. from mlxtend.plotting import plot_decision_regions from sklearn.decomposition import PCA pca=PCA(n_components=2) reduced_data_train=pca.fit_transform(X_train_std) reduced_data_test=pca.transform(X_test_std) classifier=SVC(kernel='linear', random_state=0, gamma=.10, C=1.0).fit(reduced_data_train,y_train) plt.figure(figsize=(10,3)) classifier=SVC(kernel="linear", random_state=0, gamma=.10, C=1.0,decision_function_shape='ovr').fit(reduced_data_train,y_train) # Plot decision boundary plot_decision_regions(reduced_data_test, y_test, clf=classifier, legend=2) plt.title(kernel) plt.legend(ncol=3) plt.axis("off") plt.show() Output: SVM one vs one The idea behind OvO is to train one binary classifier for each pair of classes, where the positive examples are from one class, and the negative ones are from the other.  from mlxtend.plotting import plot_decision_regions plt.figure(figsize=(10,3)) classifier=SVC(kernel="linear", random_state=0, gamma=.10, C=1.0,decision_function_shape='ovo').fit(reduced_data_train,y_train) # Plot decision boundary plot_decision_regions(reduced_data_test, y_test, clf=classifier, legend=2) plt.title(kernel) plt.legend(ncol=3) plt.axis("off") SVM in Python - Polynomial kernel In the previous code we implemented an SVM with a linear kernel. In this practical, you will apply a polynomial kernel. As we have seen, a Linear kernel gave us a linear decision boundary: the separation boundary is a straight line between the two categories. What do you think a polynomial kernel will produce? Regularisation with SVM In order to facilitate the visualisation, let’s consider the Iris data set Class 1 and Class 2 samples. These two types are not linearly separable , so we will see something more interesting. Here we use the in1d function in numpy to do this easily. Code example 1 import numpy as np from sklearn.model_selection import train_test_split # Import data and modules import pandas as pd import numpy as np from sklearn import datasets iris = datasets.load_iris() # We'll use the petal length and width only for this analysis X = iris.data y = iris.target X = X[:,:2] # Use only the first 2 columns. This is for easy plotting/visualisation x, y = X[np.in1d(y, [1, 2])], y[np.in1d(y, [1, 2])] #Split the data into 80% Training and 20% Testing sets Xtrain, Xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=42) print(Xtrain.shape) print(ytrain.shape) print(Xtest.shape) print(ytest.shape) Outputs : (80, 2) (80, ) (20, 2) (20, ) Code example 2 # Fit SVM using linear kernel on training data from matplotlib.colors import ListedColormap # We define a colormap with three colors, for three labels our data cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF']) def plot_estimator(estimator, X, y):     '''     This function takes a model (estimator),     '''     print()     estimator.fit(X, y)     # Determine the maximum and minimum mesh as a boundary     x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1     y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1     # Generating the points on the mesh     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),     np.linspace(y_min, y_max, 100))     # Make predictions on the grid points     Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])     # for color     Z = Z.reshape(xx.shape)     plt.figure()     plt.pcolormesh(xx, yy, Z, cmap=cmap_light)     # Original training sample     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)     plt.axis('tight')     plt.axis('off')     plt.tight_layout() # Fit SVM using linear kernel on training data from sklearn import svm from sklearn import metrics import matplotlib.pyplot as plt svc_model = svm.SVC(kernel='linear') svc_model.fit(Xtrain, ytrain) #Training/Testing Accuracy: svc_acc = metrics.accuracy_score(ytrain, svc_model.predict(Xtrain)) print("SVM Training Accuracy: {}".format(svc_acc)) plot_estimator(svc_model,x,y) Outputs : SVM Training Accuracy: 0.7125 Figure. Note the training accuracy Code example 3 # Plotting support vectors plot_estimator(svc_model,x,y) plt.scatter(svc_model.support_vectors_[:, 0], svc_model.support_vectors_[:, 1], s=100, marker='*', edgecolors='g', zorder=10) plt.show() Figure. Support vectors #Testing Accuracy: svc_acc_test = metrics.accuracy_score(ytest, svc_model.predict(Xtest)) print("SVM Testing Accuracy: {}".format(svc_acc_test)) SVM Testing Accuracy: 0.7 The effect of the C parameter on SVM C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to regularise more the estimation. It is similar to the L2 regularization parameter. A good discussion on this is available here. You are highly encouraged to read it. Lets start with a high C value. This corresponds to a high penalty for misclassification. Hence the learnt margin will be narrow, resulting in a small number of support vectors. Code example 4 svc_model = svm.SVC(kernel='linear', C=1e2) svc_model.fit(Xtrain, ytrain) plot_estimator(svc_model, Xtrain, ytrain) print("Data has a total of {} support vectors".format(svc_model.support_vectors_.shape[0])) #Training accuracy: print("Training accuracy: {}".format(metrics.accuracy_score(ytrain, svc_model.predict(Xtrain)))) print("Testing accuracy : {}".format(metrics.accuracy_score(ytest, svc_model.predict(Xtest)))) Outputs : Data has a total of 55 support vectors Training accuracy: 0.725 Testing accuracy : 0.7 Figure. Note the number of support vectors__ Code example 5 # Plotting support vectors plot_estimator(svc_model,x,y) plt.scatter(svc_model.support_vectors_[:, 0], svc_model.support_vectors_[:, 1], s=100, marker='*', edgecolors='g', zorder=10) plt.title('High C values: small number of support vectors') plt.show() Figure. Note the number of support vectors If we have a low C value, we get no regularization, a low penalty for misclassification. Hence we find a larger margin with more support vectors. Code example 6 svc_model = svm.SVC(kernel='linear', C=1e-2) svc_model.fit(Xtrain, ytrain) print("Data has a total of {} support vectors".format(svc_model.support_vectors_.shape[0])) plot_estimator(svc_model, x, y) #Training accuracy: print("Training accuracy: {}".format(metrics.accuracy_score(ytrain, svc_model.predict(Xtrain)))) print("Testing accuracy : {}".format(metrics.accuracy_score(ytest, svc_model.predict(Xtest)))) Outputs : Data has a total of 76 support vectors Training accuracy: 0.7 Testing accuracy : 0.65 Figure. How many are support vectors? Code example 7 # Plotting support vectors plot_estimator(svc_model,x,y) plt.scatter(svc_model.support_vectors_[:, 0], svc_model.support_vectors_[:, 1], s=100, marker='*', edgecolors='g', zorder=10) plt.title('Low C values: high number of support vectors') plt.show() Figure. Almost all are support vectors Notice that the decision boundary changes for the model, decreasing the testing accuracy. Also now almost all the points become support vectors. This is one of the issues with a small sample data. You can experiment by changing C and observing its effect on the decision boundary and testing accuracy. In sklearn, LinearSVC there is another implementation of Linear kernel. It has more flexibility in terms of parameters. The default usage is as: Code example 8 svc_model = svm.LinearSVC() plot_estimator(svc_model, Xtrain, ytrain) plt.title('Linear SVC Kernel') plt.show() Figure. Notice any differences SVM with Polynomial Kernel Here, you fit a polynomial kernel with varying degrees. The degree of the polynomial kernel can be specified using the “degree” parameter. You can experiment by changing this parameter. Observe the change in the decision boundary. A degree > 4 will take longer. Code example 9 svc_model = svm.SVC(kernel='poly', degree=2) svc_model.fit(Xtrain,ytrain) plot_estimator(svc_model, Xtrain, ytrain) plt.title('Polynomial kernel') #Training accuracy: print("Training accuracy: {}".format(metrics.accuracy_score(ytrain, svc_model.predict(Xtrain)))) print("Testing accuracy : {}".format(metrics.accuracy_score(ytest, svc_model.predict(Xtest)))) Outputs : Training accuracy: 0.7125 Testing accuracy : 0.7 Figure. Polynomial kernel Code example 10 #plotting support vectors plot_estimator(svc_model, x, y) plt.scatter(svc_model.support_vectors_[:, 0], svc_model.support_vectors_[:, 1], marker='*', s=100, edgecolors='g', zorder=10) plt.title('Polynomial kernel') plt.show() Activity SVM formulation and solution for linearly non-separable data In SVM, we have so far assumed that data is linearly separable. What approach should we take when data is not linearly separable? Sometimes, data can be linearly separable but with a narrow margin. At other times, due to noise, some of the instances may not be linearly separable (see the figure for noisy data). Figure. Noise in data. It is generally preferred not to interfere with the boundary even with small noisy data points or outliers. It is acceptable to have large margins even though some of the constraints are violated. In practice, we need a trade-off between the margin and the number of errors in classifying the training instances. This trade-off brings us to the soft margin concept. Consider the following figure; the soft margin concept is defined when the training instances are not linearly separable. Slack variables ζi{"version":"1.1","math":"\(\zeta_i\)"}  are added to allow misclassification of outliers, noisy or difficult to classify instances. So basically we are allowing some of the data points to cross the borders and to be in the wrong side of the boundary or to be misclassified. Figure. Soft margin concept. Although we allow some of the training instances to be misclassified, we still want to minimise the sum of slack variables. So for those data points which their ζi{"version":"1.1","math":"\(\zeta_i\)"} value is non-zero, we can infer that they are misclassified, and the amount of misclassification is also presented by ζi{"version":"1.1","math":"\(\zeta_i\)"}. SVM with soft margin uses the following formulation: Figure. __Soft margins The parameter C{"version":"1.1","math":"\(C\)"} can be used as a way to achieve the trade-off between large margins and fitting training data. For the high values of C{"version":"1.1","math":"\(C\)"},  we highly penalise the misclassification but for the small values of C{"version":"1.1","math":"\(C\)"}, we allow more misclassifications. That is how SVM handles this trade-off around misclassification. Soft margin dual problem The soft margin dual problem is defined when we change the primal problem with soft margins to dual. It remains the same except that there is an upper bound on the Lagrange multipliers. The dual problem is given as: minα∑i=1nαi−12∑i=1n∑i=1nαiαjyiyjxiTxjSubject to: ∑i=1nαiyi=0  and  0≤αi≤C  ∀i{"version":"1.1","math":"\\ \min_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n}\sum_{i=1}^{n} \alpha_i\alpha_j y_i y_j \textbf{x}_i^T \textbf{x}_j \\ \\ Subject\ to:\ \sum_{i=1}^{n} \alpha_i y_i = 0\ \ and \ \ 0 \leq \alpha_i \leq C \ \ \forall i \\"} As you can see the difference is C{"version":"1.1","math":"\(C\)"} Given a solution to α=[α1,...,αn]{"version":"1.1","math":"\(\alpha = [\alpha_1,...,\alpha_n]\)"} the hyperplane w{"version":"1.1","math":"\(\textbf{w}\)"} is given as: w=∑i=1nαiyixib=yk(1−ζk)−∑i=1nαiyixiTxk using any k such that αk>0{"version":"1.1","math":"\\ \textbf{w} = \sum_{i=1}^{n} \alpha_i y_i \textbf{x}_i \\ \\ b =y_k(1-\zeta_k) - \sum_{i=1}^{n} \alpha_i y_i \textbf{x}_i^T \textbf{x}_k\ using\ any\ k\ such\ that\ \alpha_k > 0 \\"} Once again there is one αi{"version":"1.1","math":"\(\alpha_i\)"} corresponding to each xi{"version":"1.1","math":"\(x_i\)"}. The xi{"version":"1.1","math":"\(x_i\)"} corresponding to each non-zero αi{"version":"1.1","math":"\(\alpha_i\)"} is called a support vector. Given w{"version":"1.1","math":"\(\textbf{w}\)"} and b{"version":"1.1","math":"\(b\)"}, we can write the classification function as: f(x)=wTx+b=∑i=1nαiyixiTx+b{"version":"1.1","math":"f(\textbf{x}) = \textbf{w}^T\textbf{x} + b = \sum_{i=1}^{n} \alpha_i y_i \textbf{x}_i^T \textbf{x} + b"} Once again solving for α{"version":"1.1","math":"\(\alpha\)"} and b{"version":"1.1","math":"\(b\)"}, we need to use training data only in the form of dot products xiTx{"version":"1.1","math":"\(\textbf{x}_{i}^{T}\textbf{x}\)"} Further, the classification function uses only a dot product between x{"version":"1.1","math":"\(\textbf{x}\)"} and support vectorsxi{"version":"1.1","math":"\(\textbf{x}_i\)"}. Summary In the previous step we investigated how an SVM handles perfectly separable data points. In this lesson we looked at how it handles almost separable data points. In the next lesson we are going to review non-linear SVMs. Activity SVM in Python - RBF kernel This is the final step in the series of practicals you have implemented to explore SVMs. In this practical you will be using a RBF kernel. Important: please ensure that you have implemented all the previous steps before you continue because some of the initial setup was done previously. The RBF kernel You can specify RBF Kernel using two parameters C and gamma. Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors. The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors. For this dataset, let's look at the influence of gamma. You can set gamma as 10, 100, 1000 and see the differences to the decision surface and accuracy. Code example 1 # data x,y from previous exmaple svc_model = svm.SVC(kernel='rbf', gamma=1e2) svc_model.fit(x,y) plot_estimator(svc_model, x, y) plt.title('RBF kernel') #Training accuracy: print("Training accuracy: {}".format(metrics.accuracy_score(ytrain, svc_model.predict(Xtrain)))) print("Testing accuracy : {}".format(metrics.accuracy_score(ytest, svc_model.predict(Xtest)))) Outputs : Training accuracy: 0.8875 Testing accuracy : 0.85 Figure. Results of the RBF kernel Code example 2 #plotting support vectors plot_estimator(svc_model, x, y) plt.scatter(svc_model.support_vectors_[:, 0], svc_model.support_vectors_[:, 1], s=100, marker='*', edgecolors='g', zorder=10) plt.show() Figure. Plot of the support vectors Activity What is the purpose of using the RBF kernel? Share your feedback and experiences in the discussion forum. If you would like to investigate further you will notice there are several documents attached below. Feel free to explore and extend your knowledge if you wish. SEE ALSO ANOTHER SVM EXAMPLE WITH IRIS DATASET 