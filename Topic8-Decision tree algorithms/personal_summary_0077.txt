Summary and Reflection of Topic 7:  SVM models can be used for face detection, text and hypertext categorization, classification of image and bioinformatics.  SVM is a supervised learning method that looks at data and sorts it into one of 2 categories. In other words, it separates the data in the best possible way, such that the maximum distance space between the two classes are achieved. In technical terms, we can say that this is the distance between the support vector and the hyperplane. Support vectors are the extreme points in the dataset.  SVM – need to find an optimal hyperplane. It does this by solving an optimization problem subject to certain linear constraints.  Transform a data set from 1-D (input) to 2-D (output) via a kernel.  SVM advantages – it automatically adjusts for high dimensional input space, spare document vector and regularization parameter i.e. it naturally avoids overfitting to a specific instance or bias to a high and/or low value.  In SVM – data may not be linearly separable. Sometimes data can be linearly separate but with a narrow margin.  At other times, due to noise, some of the instances may not be linearly separately.  In  practice,  we  need  a  trade-off  between  the margin and  the number  of  errors in  classifying  the  training instances. That is a trade-off being a large separation between classification (i.e. whereby we care more about the largest distance points) and perfect classification (whereby the model may not trade off well). The question to ask “How much should an SVM care about getting everything right vs. getting the things that it gest right “very” right.  Linear regression  Liner regression: Attempt to model a relationship between two variables by fitting a linear equation to the observed data.  To find the best linear trend line you can use two methods:  1)  Find the line that minimizes the total absolute error (by effectively trying different combinations  as there is no analytical solution).  2)  Find the line that minimizes the total squared error (there is an analytical solution that proves that you can exactly calculate m and b that minimizes the squared error, hence this is the preferred method).  Measure of model complexity  The number of class instances a model can ‘shatter’ (that is perfectly fit for all possible label assignments’ is called its VC dimension  2D – we can find at least one set of 3 points in 2D all whose 8 possible labelling can be separate by some hyperplane. Hence the VC of a line in 2-dimension is 3.  But a line may not be able to shatter some labelling of 4 points.           Multi-class classification in SVM can be done as follows:  1.  One vs all  For each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples.  If we have classes '0', '1' and '2' in the original dataset then three models will be trained where each of them will classify samples from '0' vs {'1' ,'2'}, '1' vs {'0' ,'2'} and '2' vs {'0' ,'1'}. In the prediction phase, the test sample is passed to each model (classifier) and the predicted class is determined based on the highest score obtained from the models.  2.  One vs One  In this method, the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes.  For  example,  if  we  have  three  classes  (Blue,  Green,  and  Red),  we  would  train  three  binary classifiers: Blue vs Green, Blue vs Red, and Green vs Red. In the prediction phase, each test sample is passed to all binary classifiers and a voting scheme is applied on the output of individual binary classifier outputs to determine the final class label (see the following figure).    The  above  summary  has  been  created  using  the  full  course  material  for  Topic  7  (including  the relevant YouTube clips)     