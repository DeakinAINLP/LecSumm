An SVM (support vector machine) is a supervised learning method, it tries to find a hyperplane so that the margin is maximised while satisfying a constraint. This solves optimisation problems.  It attempts to optimise the margin selection, this is done by analysing the support, or most extreme points and choosing the furthest possible distance from the boundary.  7.3:  SVMs can be applied to non-linearly separable data. A soft margin is introduced to allow for slack variables to cross the decision boundary, this accounts for noisy data.  It results in some points being misclassified however and does this with the addition of a variable, c. Low c indicating less strictness regarding the broadness of the margin.  The model still selects for a low volume of these slack variables however.  7.4:  To find the best linear trend line, essentially linear regression, can be done by 1minimising absolute error. This is done by trying various constants in the equation of the line.  It can also be accomplished by minimising total squared error, this can be done with an analytical solution to determine the constants in the slope equation.  7.5:  In a linear regression problem, the minimisation of error function can be applied to have the minimum mean square error. This can then be fitted to data and prediction made.  7.6:  Model complexity can be defined by the VC dimension, that is, the number of instances in a hypothesis class that can be shattered. This means fitting perfectly for all possible label assignments. Overall, maximising margins will result in a less complex model.  The probablistic guarantee dictates that the test error is ceilinged by the training error, this should be minimised. It can be done by increasing training samples or minimise complexity which indicates the value of minimising model complexity.  