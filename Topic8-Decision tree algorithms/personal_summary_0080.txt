 Introduction to Topic 7  key areas you will explore this topic are listed below:  Linear SVM - - Non-linear SVM.  1 1 1 1 5 7 9 9 10 12 12 13 13 13 13 13  7.2 SVM formulation and solution for linearly separable data  Primary used for Supervised Learning for Classification Distance Margin - The distance between the Hyperplane and support vector should be as Far as possible. Support vectors extreme points in the dataset.  7.3 SVM formulation and solution for linearly non-separable data  Summary In the previous step we investigated how an SVM handles perfectly separable data points. In this lesson we looked at how it handles almost separable data points. In the next lesson we are going to review non-linear SVMs.  7.4 Kernel trick and non-linear SVM  7.5 Support vector regression  Covered in previous topics  7.6 Statistical learning theory of SVM  https://video.deakin.edu.au/media/t/0_sy70eel3  How about we characterise the complexity of the hypothesis class by looking at how many instances it can shatter(i.e. can fit perfectly for all possible label assignments). The number of instances a hypothesis class can shatter is called its Vapnik-Chervonenkis (VC) Dimension.  The two statements provided influence the choice of model in the following ways:  The upper bound on the generalisation error increases with higher complexity (higher h): This means that as the complexity of the model increases, the risk of overfitting to the training data also increases, which can lead to poor generalisation performance. As a result, it is important to consider the complexity of the model when choosing an appropriate model for a given problem. In general, simpler models with fewer parameters may have lower complexity and may be less prone to overfitting, which can lead to better generalisation performance.  The upper bound on the generalisation error reduces with larger training sets (higher N): This means that as the size of the training set increases, the model has more examples to learn from, which can lead to improved generalisation performance. In general, having a larger training set can reduce the risk of overfitting and improve the model's ability to generalise to  new, unseen data. However, it is important to note that increasing the size of the training set may also increase the computational requirements for training the model.  Overall, when choosing a model for a given problem, it is important to consider the trade-off between complexity, training set size, and generalisation performance, and to choose a model that balances these factors appropriately.  7.7 Multi-class classification in SVM  Multi-class classification in SVM  Multiclass classification in SVM can be done as follows:  1. 2.  One vs all One vs One  One vs all  In this approach, for each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples. For example, if we have classes '0', '1' and '2' in the original dataset then three models will be trained where each of them will classify samples from '0' vs {'1' ,'2'},  '1' vs {'0' ,'2'} and '2' vs {'0' ,'1'} (as shown in following figure). In this approach, for N number of classes the number of models that will be generated is N.  Now, in the prediction phase, the test sample is passed to each model (classifier) and the predicted class is determined based on the highest score obtained from the models (see the above figure).  7.8 SVM in Python - Linear kernel  Refer to working ipyb workbook file  7.9 SVM in Python - Polynomial kernel  SVM in Python - Polynomial kernel  In the previous code we implemented an SVM with a linear kernel. In this practical, you will apply a polynomial kernel.  As we have seen, a Linear kernel gave us a linear decision boundary: the separation boundary is a straight line between the two categories. What do you think a polynomial kernel will produce?  Regularisation with SVM  In order to facilitate the visualisation, let’s consider the Iris data set Class 1 and Class 2 samples. These two types are not linearly separable , so we will see something more interesting. Here we use the in1d function in numpy to do this easily  7.9 SVM in Python - Polynomial kernel  Refer to work book code  7.10 SVM in Python - RBF kernel (Radial Basis Function)  SVM in Python - RBF kernel This is the final step in the series of practicals you have implemented to explore SVMs.  In this practical you will be using a RBF kernel.  Important: please ensure that you have implemented all the previous steps before you continue because some of the initial setup was done previously.  The RBF kernel You can specify RBF Kernel using two parameters C and gamma.  Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.  The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.  For this dataset, let's look at the influence of gamma. You can set gamma as 10, 100, 1000 and see the differences to the decision surface and accuracy.  