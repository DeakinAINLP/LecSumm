  This topic we learnt about SVM, which stands for support vector machine. They are popular within supervised machine, learning algorithms speciﬁcally for classiﬁcation and regression problems.   For linearly separable data for binary classiﬁcation. We are given a training dataset with some labels. The labels for a vector X and a corresponding label y for each example. The reason for this is to ﬁnd a line that separates the data points of the diﬀerent classes with the maximum amount of space/margin. With linear Lee, separable data, it is possible to ﬁnd a line or a hyper plane that will separate the data points of diﬀerent classes, accurately or without Misclassiﬁcations   The problem is formulated as follows:   Minimize:  ||w||^2/2   Subject to:  yi(w^T xi + b) >= 1 for all i = 1, 2, ..., n   This is known as the dual optimisation problem  W Is the weight vector, which is perpendicular to the line.  B stand for bias  xi ith example of the feature vector  Yi is class label   Without the use of miss classiﬁcations, linearly non-separable data cannot be perfectly separated as suggested by its name. This maybe due to some data points, becoming attached to other data points from other classes, which make it diﬃcult to ﬁt a straight line to separate them. Therefore, the goal is to ﬁnd a lion or a hyper plane that minimises the intermingling or misclassiﬁcations while maximising the space between them.   In essence, linearly separable data diﬀers from linearly non-separable data in terms of the possibility of a hyper plane, being perfectly ﬁt between the data points to separate the classes without any misclassiﬁcations. Clear separation is usually done by linearly separable data, while the utilisation of techniques, such as the softer margin, slack variables, and a regular Liza shin parameter for Misclassiﬁcations is used by linearly non-separable data.    