The content within this module mainly includes upon the explanation of the SVM (Support vector Machine) formulation and its solutions to separable data. The module dives deeper on explaining solutions for linearly non-separable data through the soft margin concept, and the Kernel trick for non-linear SVM utilising the linear regression formulation. The module then provides detailed examples upon linear regression through support vector regression. Furthermore, this module dives deeper onto the concepts of SVM through its explanations of the statistical learning theory of SVM, and the multi-class classifications of SVM’s through either one vs all or One vs one.  Within the completion of this module, I am able to gain an understanding of the concepts of SVM’s and how they are implemented within machine learning. I am also able to understand the multiple solutions provided within the module, based on the multiple scenarios that would be possible when doing SVM. Finally, I am able to understand on how the linear regression formulation can be utilised for the Kernal trick and non-linear SVM’s.  I am then able to utilise the python examples given to me within this module, with some added external assistance through documentation, to complete the pass tasks provided within this module. I am able to use SVM with RBF kernel to fit the “digits” data from SKlearn after training and applying PCA within the dataset, with visualisations and performance metrics provided.  Overall, the completion of this module allowed me to understand how SVM’s are implemented within python and the multiple solutions that is provided based on the possible scenarios that could occur. Through this, I am able to apply this knowledge by implementing SVM with RBF kernel in python as for the pass tasks requirements.  