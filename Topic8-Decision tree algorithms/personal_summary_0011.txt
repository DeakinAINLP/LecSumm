Summary This topic was focused on linear and non-linear support vector machines. The main of aim of SVMs is to find a hyperplane and maximise the boundaries between classes to more accurately predict the classes of data points within a dataset. The margins of the hyperplane are maximised by calculating a the largest distance between the hyperplane line and each of the nearest element of each class. As there can be non-linear data which does not necessarily work well with a linear line there is a kernel trick that can be utilised. This kernel trick allows the machine learning algorithm to bypass transforming the data and instead perform a calculation in the single dimension. There are three main kernel tricks: linear kernel, polynomial kernel and RBF kernel. Non-linear SVMs allow for some misclassification which is known as a soft margin, with C being a penalty parameter and gamma being the influence of a single training example on the rest of the training. The pass activity had us load the digits dataset from SKlearn, apply PCA for 3 components and perform an RBF Kernel SVM on the data set. It could be seen that from just three components the SVM was correctly predicting approximately 76% of the digits. This was a good demonstration to show how effective SVM can be considering the original number of components within the dataset was 64. 