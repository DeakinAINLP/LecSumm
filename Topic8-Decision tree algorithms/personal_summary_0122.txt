 Topic -7 gave the details of the basics of SVM model. Some of the important points are listed  below:  1.  Linearly separable data refers to a scenario where the classes can be perfectly separated  by a hyperplane in the input feature space for SVM [1].  2.  For linearly non-separable data, the traditional SVM formulation and solution need to  be modified to handle the overlapping or misclassified samples [1]. The standard SVM  formulation aims to find a hyperplane that maximally separates the classes, but when  the classes cannot be perfectly separated by a linear boundary, additional techniques  are employed [2].  3.  The  Kernel  Trick  is  a  powerful  technique  employed  in  Support  Vector  Machines  (SVMs)  to  address  the  challenge  of  non-linearly  separable  data.  It  involves  transforming the data into a higher-dimensional feature space using kernel functions,  without  explicitly  calculating  the  transformed  feature  vectors  [3].  By  utilizing  these  kernel  functions,  SVMs  can  capture  intricate  patterns  and  non-linear  decision  boundaries  that  cannot  be  accomplished  with  simple  linear  separation.  The  kernel  functions  compute  the  similarity  between  data  points,  enabling  SVMs  to  determine  optimal decision boundaries within the higher-dimensional space.  4.  Support  Vector  Regression  (SVR)  is  a  regression  technique  derived  from  Support  Vector Machines (SVM) that aims to model the relationship between input features and  a target variable [4]. Unlike traditional regression methods, SVR focuses on finding a  function  that  best  fits  the  data  within  a  specified  margin.  It  utilizes  the  concept  of    support vectors to identify a hyperplane that maximizes the margin while allowing for  a tolerance of errors. By minimizing the deviation between predicted and actual target  values, SVR enables robust regression modeling even in the presence of noise or non-  linear relationships.  5.  Multi-class  classification  in  SVM  involves  extending  the  binary  classification  capabilities of Support  Vector Machines (SVM) to handle scenarios with more than  two classes. There are several approaches to accomplish this. The choice of approach  depends on factors like the number of classes, computational efficiency, and the desired  trade-off between accuracy and training complexity [5].  6.  SVMs  can  utilize  various  kernels  to  handle  different  types  of  data  and  decision  boundaries [6]:  a.  Linear Kernel: The linear kernel (kernel='linear') is the simplest form of SVM.  It assumes that the data is linearly separable, meaning a straight line can be used  to  separate the classes.  The linear kernel  performs  well when the classes are  well-separated by a hyperplane in the input space.  b.  Polynomial  Kernel:  The  polynomial  kernel  (kernel='poly')  allows  SVMs  to  capture non-linear relationships between features. It maps the data into a higher-  dimensional feature space using polynomial functions. The degree parameter  determines the complexity of the polynomial transformation. The polynomial  kernel is effective for data with complex decision boundaries.  c.  RBF  Kernel:  The  Radial  Basis  Function  (RBF)  kernel  (kernel='rbf')  is  a  popular choice for SVMs. It creates a decision boundary based on the similarity  (or  distance)  between  data  points.  The  RBF  kernel  can  handle  non-linearly  separable data by mapping it into an infinite-dimensional  feature space. It  is  versatile and widely used due to its ability to capture complex patterns.    