The first topic introduced is Support Vector Machine which is another supervised machine  learning algorithm which can be used for linear and non-linear solutions and can be a complex model and  can be good for high volumes of data. With large training data, we can achieve high classification  accuracy with this, and an example is the 99% accuracy for handwritten digits recognition. In terms of  using this to find a hyperplane that can separate data points for the purpose of classification, we utilize  something known as a margin and our goal is to maximise this margin and we want to make sure that the  decision boundary is as far away from the data points from the other classes as possible. To further  elaborate, we have border planes that acts as the border of each class and the margin is the distance  between them. The reason we want to maximize this margin is to ensure we can allow for variants. For  example, if we choose a hyperplane closer to one class, if we get any new data that is part of that class  but might sit slightly further away, we won’t accurately classify that. To maximize, we can calculate the  distance (using Euclidean distance) from each data point to the line. Some maths was introduced but we  essentially need to optimise a quadratic function with linear constraints, and we need to solve a dual  optimisation problem.  The issue with what was covered so far is that it works for data with not many outliers (perfectly  separable) which is not reflective of real time data. As such, we with a formulation for linearly non-  separable data. There is a concept of soft margin where we accept misclassification while also  considering the maximum margin. We introduce something known as the slack variable and the goal is to  minimize its sum and this introduces a c constant to the formulation (we optimize c essentially). There is  essentially a trade-off here.  In terms of non-linear SVM, we can transform features to a higher dimension that would allow  for linear separation, and it does this by using the kernel functions on each pair of data instances which  result in something known as the Gram matrix. There are multiple types of kernel functions such as  linear, polynomial, and radial basis function.  In terms  of structural risk minimization to make a model with small error and be less complex  and how that can happen when we maximize the margin, we have something known as Vapnik  Chervinenkis Dimension. To my understanding, this topic in the lecture seems to be a proof of sorts that  shows that even though we are trying to maximize the margin, we are still decreasing the complexity of  the model.  We have multi-class classification in SVM as a topic. We have One vs all SVM where for each  class, we train a binary SVM classifier. In other words, let’s say we have three classes, we would then  have three binary classification problems where it would be one class against all, and we then take the  maximum score of the models for a prediction of a data point. We also have one vs one SVM and in this        case, we train each pair of classes in its own classifier where each would only contain two classes. Here, a  data point will be put through all classifiers, and we select the class based on the maximum predicted  class.   