 This topic in the Machine Learning unit addresses in more depth the SVM:  -  -  SVM aims to find a hyperplane that would separate the data while also maximizing the margin,  which is the distance between the closest points  in the  separated data.  This makes SVM a  constrained optimization problem, however, using langrage multipliers, it is converted into an  unconstrained  optimization  problem,  which  is  solved  by  taking  the  derivative  of  the  SVM  objective function and equating it to zero.  It is useful in SVM sometimes to have a trade off between the margin and the accuracy of the  separation. Noisy data might make the data not linearly separable or the margin very small.  Allowing some error, could convert the data back to being linearly separable or increase the  margin.  -  To prevent overfitting, the risk minimization function includes a penalty parameter for model  complexity, aiming for minimal error with less model complexity. In SVM, the margin is linked  with the model complexity in terms of reducing model complexity would increase the margin  value.  -  Multi class classification could be done in two ways:  o  One vs all: in this approach, a binary classifier is trained for each class in which its one  for that class and zero for the rest of the classes. Then in the prediction stage, the data  point is entered into all classifiers and the classifier with the highest score is chosen.  o  One vs One: in this approach, binary classifiers are trained to classify between two  classes. In the prediction stage, the class that was the output of most of the trained  binary classifiers is chosen as the final output.   