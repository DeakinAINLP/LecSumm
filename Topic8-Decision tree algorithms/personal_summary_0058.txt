  SVM formulation and solution for linearly separable data  SVM looks for a hyperplane that maximises the margin while still satisfies the limitation. It's important to keep in mind that the difficulty with maximising and minimising is the same.  A quadratic function in w must be optimised while being constrained by linear rules. Quadratic programming is the name given to this issue by experts in optimisation. Do not misunderstand what the word "programming" means. Just optimise, that's all. Building a dual problem, an analogous problem, is a common method for solving the optimisation problem. The primary difficulty in quadratic programming is known as the primal problem. The dual problem's resolution sets a lower bound on the resolution of the primary (minimisation) problem.  Dual Optimisation problem  The main challenge in SVM is to maximise the margin (or minimise 1/margin). We can change a situation involving confined optimisation into one involving unconstrained optimisation by using Lagrange multipliers. A method for locating a function's local maxima and minima that is subject to equality requirements is called Lagrange multipliers.  For the time being, keep in mind that we created a dual issue for minimisation with the aid of the Lagrange multipliers, which will cause the margin in SVM to be maximised. You should be aware that the computational requirements for the dual problem are O(n3), but they are O(d3) for the primal problem. Here, n denotes the size of the training set, while d designates the dimension of the feature space. When solving, the dual costs more than the primal. Because the dual problem permits the use of arbitrary Kernels, SVM bounds can be highly nonlinear, which is one of the reasons it is so well-liked.  SVM formulation and solution for linearly non-separable data  We've assumed up to this point in SVM that data can be separated linearly. What strategy should we employ when data cannot be separated linearly? Data may occasionally be linearly separable, but with little room for error. Other times, noise may prevent some of the occurrences from being linearly separable.  Even with little noisy data points or outliers, it is often preferred to avoid interfering with the boundary. Even when some of the restrictions are broken, having huge margins is allowed. To classify the training examples, we must compromise between the margin and the number of errors. The soft margin idea is  introduced to us through this trade-off. The training instances must not be linearly separable for the soft margin concept to be defined. Slack variables are included to enable the misclassification of cases that are outliers, noisy, or challenging to categorise. In essence, we are allowing certain data points to cross borders, land on the wrong side of the line, or be mislabelled.  When we convert the primal soft margin problem to a dual problem, the soft margin dual problem is established. Except for a limit on the Lagrange multipliers, nothing has changed.  Linear Regression Formulation  We seek a line that is like h in linear regression. The linear equation ought to make it possible for us to analyse and examine the connections between two continuous (quantitative) variables.  How can we locate the line then? The slope of the line and the line's intercept are two of the line's parameters. We can utilise these two parameters to find our straight line if we have them.  The error is the discrepancy between our prediction and the actual value or result at that point.  Statistical Learning Theory of SVM  By a model complexity penalty, structural risk minimisation aims to prevent over-fitting. This indicates that it favours simpler functions over complex functions. The goal is to reduce structural risk.  Let's say we randomly choose n occurrences and give them the labels + and -. If any association of labels to the data can be learned using our hypothesis class, then it is sufficiently complex. Let's try to gauge the complexity of the hypothesis class by counting the number of instances it can shatter, or perfectly suit all conceivable label assignments. A hypothesis class's Vapnik-Chervonenkis (VC) Dimension is the maximum number of instances it can shatter.  A less complex model (one with a smaller upper bound for (h)) will arise from maximising margins. You have a better probability of getting smaller test results (a smaller upper bound) if you simplify the model. This demonstrates the significance of maximising margins and managing model complexity once more.  SVM in Python  The goal of OvR (SVM one vs rest) is to train a binary classifier with positive examples from each class and negative examples from all other classes put together.  Using positive examples from one class and negative examples from the other, OvO trains a binary classifier for each pair of classes.  RBF kernel: The two parameters C and gamma can be used to specify the RBF Kernel.  The gamma parameter, with low values denoting "far" and high values denoting "close," intuitively specifies the extent to which a single training example has an impact. The inverse of the radius of impact of samples chosen by the model as support vectors, or gamma parameters, can be thought of. Misclassification of training examples is traded off against the decision surface's simplicity using the C parameter. A low C soothes the decision surface, whereas a high C seeks to correctly categorise every training example by allowing the model to choose more samples as support vectors.  