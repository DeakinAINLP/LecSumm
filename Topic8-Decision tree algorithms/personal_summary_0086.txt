SVM formulation and solution for linearly separable data  SVM solves the optimisation problem of:  As it aims to find a hyperplane (w,b) so that the margin 2/||w|| ca be maximised, maximising this is the same as minimising the above function which requires a quadratic function w that is subject to linear constraints. This problem is known as quadratic programming which can be often solved by constructing an equivalent problem called a dual problem. In quadratic programming the primal problem is the original optimisation problem and the solution to the dual problem provides the lower bound to the solution of the primal problem.  This can be tackled by using lagrange multipliers which allow us to convert a contrained optimisation into an unconstrained optimisation problem. Lagrange multipliers involve finding the local maxima and minima of a function subject to equality constrains.  Now by setting the derivative to zero and substituting the results of the Lagrange function the dual formulation (dual problem) for minimising ½ / ||w||2 is made, this will result in maximising the margin in SVM.  SVM formulation and solution for linearly non-separable data  So far in SVM we have assumed the data to be linearly separable, a different approach is required when it is not. Data can be linearly separable but only with a narrow margin, whereas other instances involve cases where it isn’t linearly separable at all.  We prefer not to change the boundary even if there are noisy points or outliers, it is acceptable to have large margins even though they may violate some constraints. We need to balance the trade-off between the margin and the number of errors when classifying the training instances. The soft margin concept is used when training instances are not linearly separable, slack variables are added to allow misclassification of outliers, noisy and difficult to classify instances, meaning that we are allowing some data to be misclassified. Even though we allow this we still want to minimise its occurrence, the parameter C is used to achieve this trade off. If C is high we penalise the misclassification but when C is small we allow more misclassifications, this is how SVM handles the trade-off.  Kernel trick and non-linear SVM  Linear regression involves finding a line similar to h and creating a linear equation that allows us to summarise the relationship between two continuous (quantitative) variables. When predicting a value y using x what do we do if the x is not a single dimension value? We have to write our linear regression in regard to d dimensions, the dimensioned formula is similar to the single dimension formula.  The error prediction value in regression is the difference between what we predicted and what the true value is at that point. The mean square error function can be used to measure this but still introduces a minimisation problem in this scenario. This is solved with a closed form function where we take the derivative of the error function with respect to w and equate it to 0 allowing us to find the w which minimises the error.   Statistical learning theory of SVM  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity, meaning that it prefers simpler functions as opposed to complex functions. We want to minimise the structural risk where h(f) is the complexity of the hypothesis function in regard to is penalty parameter. Thus, we like to choose a less complex model with less error. If we pick n instances and assigner labels or positive or negative to them randomly then if our hypothesis class is rich enough to learn any association of the labels to the data it will be complex enough. The number of instances that a hypothesis class can shatter is called the Vapnik-Chervonenkis Dimension.  When using a lines or hyperplanes for our hypothesis class, in 2-dimension we will be able to find a line to shatter any labelling of three points, but the lines may not be able to shatter some labelling of four points. Meaning that a VC dimension of a 2-dimensional line is 3. Regardless of dimensionality d we can minimise the model complexity (VC dimension) by maximising the dimension p. Meaning that if p is maximised (if we look for a classifier with a high enough margin) then we will have a smaller value and thus, smaller upper bound for the complexity of our model h. Maximising margins results in having a less complex model as the upper bound for h is smaller which is why we aim to maximised margins in SVM. The probabilistic guarantee states that the test error is upper bounded by the training error. We can minimise this value in two ways, by increasing the number of training samples (N) or minimising the complexity of the model (h).  Reflection  This topic was useful and at some points took a few rereads to actually understand since some parts are quite tricky. Mainly it was understanding the specifics of the uses of SVM and how it actually works to help optimise classification that was tricky to understand, but I think I’ve got a good grasp on it now. Similarly understanding how to approach non-separable data was difficult at first but I think I have improved my understanding thoroughly from this topic. While difficult in parts I now think I have an overall better grasp of how to approach more than just simple linear data.  