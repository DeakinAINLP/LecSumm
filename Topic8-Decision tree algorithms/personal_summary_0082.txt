Topic 7 SVM  SVM formulation linearly  ● SVM formulation and solution for linearly separable data SVM aims to find hyperplanes therefore the margin is maximized while the constraints are satisfied.  ● The SVM formulation solves the optimization problem considering the  maximization problem the same as minimising.  ● This problem is well-known in the optimization community and is called  quadratic programming.  ● In quadratic programming, the original optimization problem is called the  primal problem.  SVM formulation non linearly  ● At other times, due to noise, some of the instances may not be linearly  separable.  ● In practice, we need a trade-off between the margin and the number of errors  in classifying the training instances.  ● Consider the following ﬁgure; the soft margin concept is deﬁned when the  training instances are not linearly separable.  ● Slack variables are added to allow the misclassiﬁcation of outliers and noisy  or diﬃcult-to-classify instances.  ● Although we allow some of the training instances to be misclassiﬁed, we still  want to minimise the sum of slack variables.  Kernel trick  ● The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map data points to a higher-dimensional space without explicitly computing the transformation.  ● It allows SVMs to capture non-linear relationships in the data by transforming the data points to a higher-dimensional space where they may be more easily separable by a hyperplane.  Statistical learning theory of SVM  ● Statistical learning theory is a framework used in Support Vector Machines to  understand the generalization performance of the model.  ● SVMs aim to ﬁnd the hyperplane that maximizes the margin between different  classes while minimizing the classiﬁcation error.  ● Statistical learning theory provides theoretical guarantees on the  generalization performance of SVMs, allowing us to estimate the expected error on unseen data based on the training error and the complexity of the model.  Multi-class classification in SVM  ● Multi-class classiﬁcation in SVM is the task of classifying instances into  multiple classes using Support Vector Machines, which is originally a binary classiﬁcation algorithm.  ● One common approach for multi-class classiﬁcation in SVM is the  One-vs-Rest (OvR) or One-vs-All (OvA) strategy, where multiple binary SVM classiﬁers are trained, each one distinguishing between one class and the rest of the classes.  