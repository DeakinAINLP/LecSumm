Topic 7: SVM Models In the previous topic, we had started learning about supervised learning, and recently focusing on  linear regression and classical. We’ve looked into the logistic regression and uses PCA to reduce and train the model. Lastly, we uses performance metric to test out the accuracy of the model.  In this topic around, we will focus on something we have learned in the previous unit, which is SVM.  The objectives of this topic will be:  Linear SVM which dived into an algorithm in supervised learning  - -  Non-linear SVM  SVM  So, what is SVM? SVM or support vector machine is a deep learning algorithm used in supervised learning. Its main goal is to categorize the models for both the regression and classification models. How does it work? The solution of SVM is to find the number of linear hyperplanes in an N- dimension model, this will then used to divides the cluster dataset. The SVM formulation wants to optimize the minimum function while satisfying a constraint. We will be looking a lot into quadratic programming, as this will be the function for optimizing the solution.  Since SVM is about optimize function while satisfying constraints, by using Lagrange multiplier,  we can make it a unconstraint optimization problem. This will be used to find the local maxima and minima in a function. By adding a derivative on top of the Lagrange function, we will achieve what it called, dual optimization or dual formulation.  A problem with hyperplane is that it is a linear function, so there will be a model that will be  inseparable due to noisy data or outliers. The concept for this solution is called soft margin. Before we gets to that, we should allow outliers instead, so those values will be set as slack variable which means either out of boundary or misclassified. However, we still wants to minimize as many slack variable as possible to fit the data better. There are two options when we are setting a tradeoff of C, by setting the value C high, we will penalize the misclassification, vice versa, setting it low will allow more misclassification. The solution to soft margin, is to add the tradeoff constraint to the function, making Lagrange multiplier lower than C, it will then make the hyperplane corresponding only to certain values.  Here’s the difference between the two Lagrange function, without and with the soft margin.  Fitting non-linear SVM  So how do you fit a linear regression line/hyperplane to a datapoint. Generally, we want the  hyperplane to be in the center point of both the datapoint, to balance and fit the model, but how can we do this. The key is to find the residual or error between the datapoint to the hyperplane. The linear model purpose is to minimize the risk through the mean square loss. With the closed form function, by doing a derivative of the error respect to the slope of the function.  By setting derivative respect to w, we will set everything to 0 and calculate for w. This will be used to fit the line on the features. By having the hyperplane and the mean squared error, we can use SVM to predicts data as well.  Maximizing Margin Generally, we wants to make our model as simple as possible, which we can do by either reducing dimensionality, or minimize the structural risk. When finding a hyperplane function, we wants to maximize the margin between the two instance as much as possible, to perfectly fit the model or making the model less complex.  Here we will be looking into two different classification:  Multiclass SVM  -  One vs all -  One vs one  Just like what their name says, the functionalities work the same way. For example, if we have 3 sample data, one vs all classification compares one sample to all other sample, however, for one vs one, each sample compare with one another to create prediction. The One vs One method trains the multiple binary classifiers as it distinguishes between each of them.  