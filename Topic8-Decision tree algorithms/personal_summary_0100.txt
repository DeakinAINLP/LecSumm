Learning Summary  SVM Formulation: Its aim is finding out a hyperplane  by which the margin  can be  maximized  while  satisfying  the  constraint  .  By  the  use  of  SVM  formulation  leads to solving the optimization problem:  For solving  an optimization  problem there  needs to  be an  equivalent problem  that needs  to  be constructed  which  is  known  as  dual  problem.  Within  quadratic  programming  the  original optimization  problem  is  known  as  primal  problem.  The  dual  problem  provides  a  lower  bound solution for the solution of the primal problem.  Dual Optimization Problem: With the use of Lagrange multiplier one can be able to convert  a constrained optimization to an unconstrained optimization problem. This is a strategy for finding out the local maxima and minima for a function that is subject to equality constraints.  Data can sometimes be linearly separable with a very narrow margin. Generally it’s preferred that no such kind of interference takes place with the boundary and it’s acceptable to comprise of large margins where even some constraints if are violated it’s considered to be OK. The trade-off among margins and number of error in classifying the instances of training needs to be done in practice.  Soft-Margin Concept: When  training instances  do not  be separable  linearly, this  is known  as soft-margin concept. Where slack variables get added for allowing any sort of misclassification of outliers, difficult for  classification of instances  or noisy. Hence,  we are just  allowing some  data points for crossing borders and be in the wrong side of the boundary or be misclassified.  Although, some  training  instances  for  being  misclassified  are allowed  but  they  still  need  to  be minimized when it comes to the addition of the slack variables.  Soft Margin Dual Problem: This is defined when alterations of the primal problem takes place with the  soft margins  to  dual. It  remains to  be  the same  except there’s  an  upper bound  of  the Lagrange multipliers.  Classification in SVM: There are two types of classifications that can be done in SVM. One vs All and One vs One    One vs All: Within  this approach, for  every class there  is a binary  SVM classifier that  gets trained with samples from that  particular class  that are being  viewed as positive as well  as negative examples. When the prediction takes place, the test samples gets passed to every model and predicted class gets determined on the basis of the highest score that is  received from the models.    One vs One: Within this, the SVM algo trains several binary classifiers, that are trained  for distinguishing among two classes. At the time of prediction every test sample gets passed to all the binary classifiers where a voting scheme gets applied over the output of  the individual binary classifiers for determining the final class label.  