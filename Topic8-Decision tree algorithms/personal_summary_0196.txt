SVM (Support Vector Machine) is a powerful supervised algorithm that is good at handling high-dimensional data and aim at finding a hyperplane that helps to reduce the problem overfitting. Optimisation problem is dealt by introducing solution to dual problem to the primal problem in SVM which is margin maximisation and minimization. Lagrange multipliers are used to solve constrained optimization problems by using dual problem for minimization. These are true for linearly separable data but because of noise few features might not be linearly separable. A trade-off between the margin and quantity of incorrect classifications of training data  is  the  soft  margin  concept,  where  certain  datapoints  might  be  misclassified  because  of  crossing  the margin. The way that SVM handles this trade-off is by penalising the misclassified higher values and allowing further classification for small values. Soft margin dual margin problem is observed when conversion of primal problem to dual problem is done using soft margin. The upper bound of Lagrange multipliers remains the same. The parameters of slope of line and intercept of line are used to find the straight hypothesis line which can estimate the output function using the provided input points. For high dimension data this line equation is altered to include the multiple feature points. The multiplication of w and x in high dimensional data is to be handled by all the dimensions that are introduced. Error of value prediction which is the difference between true value and predicted value is to be minimised. Square loss equation is used to minimise the empirical risk. Ways eliminate the minimisation problem are discussed with example using minimisation error function. Less complex model with small error is preferred. Structural risk minimisation seeks to prevent the problem of data overfitting by applying penalty based on model complexity. Vapnik-Chervonesis (VC) Dimension is used to characterise the complexity of hypothesis class. VC dimension measures the ability of a hypothesis class to separate the data points. For example, one hypothesis line can be used for 2-dimensional dataset to shatter 3 points, but the same line cannot shatter 4 or more data points. Equation for VC dimension h is discussed. Less complex  model  is  obtained  by  maximising  margins  in  SVM.  Highlight  on  the  importance  of  maximising margins and dealing with model complexity is discovered. There are two ways for multi-class classification in SVM which are one vs all and one vs one. In one vs one approach while training the data target class data are labelled as positive and others are labelled negative. During testing the class with highest confidence score is assigned as predicted class. Equal number of models are generated for equal number of classes. For one vs one approach a binary classifier is trained to differentiate between two classes. Final class label is obtained by passing the test sample through each of the binary classifier and output is considered to determine the final class labels. Various python codes to run methods like grid search and other hyperparameter tuning techniques are studied.  