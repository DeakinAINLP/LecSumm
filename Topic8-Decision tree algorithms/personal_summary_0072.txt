 Key Concepts this topic: Linear SVM, Non-Linear SVM  SVM formulation and solution for linearly separable data  SVM formulation and solution for linearly separable data: In the case of linearly separable data, SVMs aim to find the hyperplane with the maximum margin from the closest data points. The optimization problem can be solved using techniques such as quadratic programming.  Screenshots below from Unit site as they have useful formulas in for future reference:      The margin is defined as the distance between the hyperplane and the closest data points from each class, and the closest points are called support vectors. The solution involves formulating an optimization problem that maximizes the margin while minimizing the misclassification error.  SVM formulation and solution for linearly non-separable data  SVM formulation and solution for linearly non-separable data: In the case of linearly non-separable data.  SVMs use a technique called "soft margin" to allow some data points to be misclassified.  The soft margin is defined as a region around the hyperplane where some misclassifications are allowed. To solve this we can use  an optimization problem that maximizes the margin while minimizing the misclassification error and the soft margin violation.     Kernel trick and non-linear SVM  The kernel trick is a powerful technique used to extend SVMs to handle non-linearly separable data. The kernel trick involves implicitly mapping the data to a higher-dimensional feature space, where it becomes linearly separable.  The kernel function defines the similarity measure between two data points in the higher- dimensional space.  We can come up with an optimization problem that maximizes the margin while minimizing the misclassification error, and the kernel function is used to implicitly map the data to the higher- dimensional space.  Support vector regression  SVMs can also be used for regression tasks, where the goal is to find the best hyperplane that fits the data within a certain margin of error. Support Vector Regression (SVR) is a variant of SVMs that involves minimizing the error while maximizing the margin.      Statistical learning theory of SVM  The statistical learning theory of SVM provides a theoretical framework for understanding the behaviour of SVMs.  The theory is based on the concept of structural risk minimization, which balances the model's complexity and the margin size.  Probabilistic Guarantee  The generalization error of SVMs can be bounded by the margin and the complexity of the model, providing a way to estimate the model's performance on new data.         Multi-class classification in SVM  SVMs can be extended to handle multi-class classification tasks using techniques such as one-vs-all and one-vs-one.  One-vs-all involves training multiple SVMs, each one for a specific class against the rest of the data.  One-vs-one involves training multiple SVMs, each one for a pair of classes. The solution involves combining the outputs of the SVMs to make the final prediction.  VM in Python - Linear kernel  Python provides several libraries for implementing SVMs. The scikit-learn library is a common choice for SVMs amongst coders.  To implement a linear kernel SVM in Python, we can use an SVC class with what is called the linear kernel. We can specify other parameters such as the regularization parameter and the class weights.  SVM in Python - Polynomial kernel  To implement a polynomial kernel SVM in Python, we can use the SVC class with the poly kernel. We can specify the degree of the polynomial using parameters such as degree in this case. We can also specify other parameters such as the regularization parameter and the class weights.  SVM in Python - RBF kernel  To implement an RBF kernel SVM in Python, for this one we can use the SVC class with the RBF kernel. WE can then use the gamma parameter to check the width of the kernel.  