 SVM aims to find a hyperplane that maximises the margin between classes while satisfying certain constraints. Quadratic programming is an optimisation problem which minimises a quadratic function subject to linear constraints. The dual problem is based on Lagrange multipliers which convert the constrained optimisation problem into an unconstrained one, which helps maximise the margin in SVM. Support vectors are data points which non-zero Lagrange multipliers, where the classification function is written using the dot product of the input and support vector.  SVM for linearly non-separable data involves soft margins to deal with instance which are not perfectly separable. Slack variables allow for some misclassification of outliers or noising data points to help balance the trade-off between margin size and classification error. The soft margin dual problem is like the regular dual problem however has an additional upper bound constraint on the Lagrange multipliers.  Statistical Learning Theory of SVM is based on structural risk minimisation which aims to prevent over-fitting by including a penalty on model complexity. The Vapnik-Chervonenkis (VC) Dimension measures the complexity of a hypothesis class by counting of the number of instances it can shatter. The Probabilistic Guarantee equation highlights that the test error is upper bounded by the training error, which can be minimised by increasing the number of training samples or reducing model complexity.  SVM supports multi-class classification through the following:    One vs all – A separate binary SVM classifier is trained for each class, treating samples from the target class as positive examples and from other classes as negative samples.    One vs One – Trains multiple binary classifiers, each designed to distinguish between specific classes. A voting is applied during the prediction to determine the final class label based on the individual classifier outputs.  Reading List    Cloud Deakin reading materials.   Sklearn documentation 1.4. Support Vector Machines — scikit-learn 1.2.2 documentation   Hyper-parameter Tuning: A Comprehensive Guide on Hyperparameter Tuning and its  Techniques (analyticsvidhya.com)  