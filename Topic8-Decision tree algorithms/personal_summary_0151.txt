   Summary:  o  Support Vector Machine (SVM):  ▪  Linear SVMs:    Perfectly  separable  data  points:  SVMs  are  supervised  machine learning algorithms that are utilised for classification or regression applications.  Linear  SVMs  determine  the  best  decision  boundary (hyperplane) for distinguishing distinct classes in data points that are completely separable. They maximise the class margin, which is the  distance  between  the  decision  border  and  the  nearest  data points  (support  vectors)  from  each  class.  To  identify  the  ideal hyperplane,  convex  optimisation  methods  such  as  Sequential Minimal Optimisation or gradient descent are utilised. Linear SVMs are  useful  for  completely  separable  data  points  because  they improve generalisation and decrease overfitting.    Almost separable data points: Linear SVMs may be employed with soft  margin  classification  for  virtually  separable  data  points, allowing  some  misclassifications  to  reach  a  more  generalizable decision  boundary.  To  balance  maximising  the  margin  and minimising  classification  mistakes,  the  approach  employs  slack variables  and  a  regularisation  parameter  'C'.  Cross-validation  can be  used  to  discover  the  best  'C'  value  for  model  generalisation. Linear  SVMs  with  soft  margin  classification  may  handle  nearly separable data points while still doing well in generalisation.  ▪  Nonlinear SVM:    Support  vector  regression:  Nonlinear  Support  Vector  Regression (SVR)  is  a  regression  job  that  extends  the  SVM  technique  by predicting  continuous  values  while  capturing  complicated  input data connections. It maximises the margin around the regression function,  with  the  parameter  ε controlling  the  error  tolerance. Nonlinear  SVR  maps  input  characteristics  to  higher-dimensional spaces  using  kernel  functions  such  as  polynomial,  radial  basis function (RBF), and sigmoid kernels. The choice of kernel function and hyperparameters has a significant impact on performance, and cross-validation techniques are employed to optimise.    Statistical  Learning  Theory  of  SVM:  The  cornerstone  of  Support Vector Machines (SVMs) is Statistical Learning Theory (SLT), which provides  insights  into  the  learning  process  and  generalisation abilities  in  machine  learning.  Empirical  risk  minimisation  (ERM), structural  risk  minimisation  (SRM),  and  the  VC  dimension  are  all important  SLT  ideas.  ERM  minimises  average  loss  over  training data,  whereas  SRM  combines  empirical  risk  minimisation  with model  complexity  for  improved  generalisation.  SVMs  strive  to   minimise the VC dimension, which assesses a model's capacity or complexity. Nonlinear SVMs manage complicated data connections using  kernel  functions,  with  SRM  and  VC  dimension  principles assuring robustness and generalisation ability.  ▪  Multi-class classification in SVM:    One vs all: The One-vs-All (OvA) technique is a popular way to apply SVMs  to multi-class  classification  problems by  dividing  them  into numerous  binary  classification  jobs.  K  unique  SVM  classifiers  are trained for each of the K classes, each discriminating one class from the others. New instances are categorised by running them through all  classifiers  and  allocating  them  to  the  class  with  the  greatest is  simple  to  construct  and  allows  for decision  value.  OvA parallelized training, but for many classes it can be computationally costly, resulting in biased classifiers and inferior performance.   One vs One: By training an SVM classifier for each pair of classes, the  One-vs-One  (OvO)  technique  extends  SVMs  to  multi-class classification problems. K(K-1)/2 binary classifiers are trained for K classes. New instances are categorised by running them through all classifiers and assigning them to the class with the greatest votes. OvO provides benefits such as speedier training on smaller subsets and resistance to class imbalance. However, with more classes, it necessitates an increasing number of classifiers, which may result in ties, needing tie-breaking procedures.    Reading list: Lecture Slides, Lecture Recordings, Learning Contents.   My reflections: SVMs are supervised machine learning algorithms that may be used to perform  classification  and  regression  problems.  Linear  SVMs  handle  completely separable data points by maximising the margin between classes, whereas soft margin classification is utilised for data points that are almost separable. Nonlinear SVMs, like Support  Vector  Regression,  employ  kernel  functions  to  capture  complicated  data connections. Statistical Learning Theory directs the learning process in order to achieve resilience and generalisation. SVMs may do multi-class classification utilising One-vs- All or One-vs-One techniques. One-vs-All trains classifiers for each class individually, whereas One-vs-One trains classifiers for each pair of classes. Both techniques offer benefits and drawbacks in terms of computing costs, bias, and performance.   