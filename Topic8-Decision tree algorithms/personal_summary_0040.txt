Learning Report  Topic 7  Learning summary  -  SVM formulation and solution for linearly separable data  SVM Error = Margin Error + Classification Error. The higher the margin, the lower would-  o  be margin error, and vice versa. In quadratic programming, the original optimisation problem is called the primal problem. The solution to the dual problem provides a lower bound to the solution of the primal (minimisation) problem.  Dual optimisation problem  the primal problem in SVM which is maximising the margin  ▪ Using Lagrange multipliers we can convert a constrained optimisation into an unconstrained optimisation problem. Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints.  -  SVM formulation and solution for linearly non-separable data  Sometimes, data can be linearly separable but with a narrow margin. At other times, due  o  to noise, some of the instances may not be linearly separable It is generally preferred not to interfere with the boundary even with small noisy data points or outliers. It is acceptable to have large margins even though some of the constraints are violated. In practice, we need a trade-off between the margin and the number of errors in classifying the training instances.  This trade-off brings us to the soft margin concept. Consider the following figure; the soft margin concept is defined when the training instances are not linearly separable. Slack variables       are added to allow misclassification of outliers, noisy or difficult to classify instances. So basically, we are allowing some of the data points to cross the borders and to be in the wrong side of the boundary or to be misclassified.   Soft margin dual problem: The soft margin dual problem is defined when we change the primal problem with soft margins to dual. It remains the same except that there is an upper bound on the Lagrange multipliers.  -  Statistical learning theory of SVM  Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity. This means, it prefers simpler functions over more complex functions. The general idea is to minimise the structural risk as where h(f) is the complexity of hypothesis function f and λ is a penalty parameter:  The number of instances a hypothesis class can shatter is called its Vapnik-Chervonenkis  (VC) Dimension.  The Vapnik-Chervonenkis dimension, more commonly known as the VC dimension, is a model capacity measurement used in statistics and machine learning. It is termed informally as a measure of a model's capacity. It is used frequently to guide the model selection process while developing machine learning applications.  -  Multi-class classification in SVM  Multiclass classification in SVM can be done as follows: One vs all: One-against-all classification, in which there is one binary SVM for each class  to separate members of that class from members of other classes. Pairwise classification, in which there is one binary SVM for each pair of classes to separate members of one class from members of the other.  One vs One: One-vs-One (OvO for short) is another heuristic method for using binary  o  classification algorithms for multi-class classification. In this method, the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For example, if we have three classes (Blue, Green, and Red), we will train three binary classifiers: Blue vs Green, Blue vs Red, and Green vs Red.         