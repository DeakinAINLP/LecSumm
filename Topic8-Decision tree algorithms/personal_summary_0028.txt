Linear SVM  SVM stands for Support Vector Machine  The intuitive concept that underpines an SVM is to find an optimal vector that separates classes of  data:  https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf  The location of the vector is found so that it is equidistant from a support vector for each class.  The support vector for each class is a vector that describes the 'boundary' of that class.  As seen in the picture above, the optimal vector to separate the classes is the one that gives the  largest margin between support vectors.  Finding the SVM  Finding the SVM is an optimisation problem.  This is done using a quadratic programming approach, which converts the problem to a dual  optimisation problem.  Approach should be selected based on the dimensionality of the input data.  In practice, dual optimisation approach is often used as it allows for the 'kernel trick' described below.  Soft Margin  Sometimes a linear vector separating data cannot be identified.  If this occurs due to outliers or noise in the data, then it is possible to ignore these values.  This forms a trade-off between margin size and number of classification errors for the training data.  This trade off can be called a soft-margin (below), which uses slack variables ( ) to allow outlier  Non-Linear SVM  When the data is such that it cannot be linearly separated even with soft margins then non-linear  support vector machines are required.  This approach applies a transform function to the data before performing a linear separation.  This can take the form of extending the data into an additional dimension, or of remapping the data to a  different coordinate space.  An example of this is to remap data where one class is surrounded by another to polar coordinates.  This allows the data to be separated based in the dimsension that captures distance from centre. 