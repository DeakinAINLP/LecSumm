Evidence of Learning  Module 7: SVM Models  Module Learning Outcomes -  I certify that I have successfully learned and understood the following topics:  1.  SVM formulation and solution for linearly separable data. 2.  SVM formulation and solution for non-linearly separable data. 3.  Kernel trick and non-linear SVM. 4.  Statistical learning theory of SVM.  Summary and reflection –  The  pages  below  contain  the  handwritten  summary  referring  to  the  given  learning resources. It includes all the important points of the topic 7 module.  In this module, we first learnt about the SVM formulation and solution for linearly separable data.  In  that  section,  we  learnt  optimisation  problems,  quadratic  programming,  primal problem,  and  dual  optimisation  problems.  Next,  we  moved  on  to  the  next  part,  SVM formulation  and  solution  for  linearly  non-separable  data.    In  that  section,  we  specifically discussed  noise  in  data,  SVM  with  soft  margin,  and  soft  margin  dual  problems.  Then  we studied linear regression formulation. We went through linear hypothesis and error of value prediction  in  regression.  Then  we  walked  through  an  example  of  linear  regression  which helped  us  a  lot  in  understanding  it  properly  as  it  was  given  in  detail.  That  section  even contained  a  video  that  refreshed  our  knowledge  in  previous  sections  of  this  unit  as  well. Then  we  learnt  about  the  statistical  learning  theory  of  SVM.  In  that  section,  we  discussed maximum margin, structural risk, illustrations of VC dimension, and probabilistic guarantee.  When we started topic 7, I was not familiar with any of these concepts at all. So, it was very interesting and exciting to learn about a new side of machine learning which affects every and all aspects of it. And the practical part done with Python was a little bit confusing at first but  was  able  to  understand  all  the  necessary  commands  and  ways  of  implementation  by referring to both given resources and online resources.  Activity Sheet for Topic 7 –  How do the following statements influence model choice?    The  upper  bound  on  the  generalization  error  increases  with  higher  complexity  (higher h)    The upper bound on the generalization error reduces with larger training sets (higher  N).  The statements provided have implications for model choice by highlighting the relationship between  model  complexity,  training  set  size,  and  generalization  error.  Let's  break  down each statement:    "The  upper  bound  on  the  generalization  error  increases  with  higher  complexity  (higher h)."  This statement suggests that as the complexity of a model increases (e.g., more parameters, greater  flexibility),  the  upper  bound  on  the  generalization  error  also  increases.  In  other words,  highly  complex  models  may  be  more  prone  to  overfitting,  fitting  the  training  data extremely  well  but  failing  to  generalize  to  unseen  data.  This  insight  implies  that  when considering model choice, it is essential to strike a balance between model complexity and the risk of overfitting. Generally, simpler models with lower complexity may be preferred to mitigate the potential for overfitting and improve generalization performance.    "The  upper  bound  on  the  generalization  error  reduces  with  larger  training  sets  (higher N)."  This  statement  indicates  that  increasing  the  size  of  the  training  set  can  help  reduce  the upper  bound  on  the  generalization  error.  When  the  training  set  is  larger,  the  model  has access  to  more  diverse  examples,  which  can  improve  its  ability  to  learn  patterns  and generalize well to unseen data. By using more training data, the model can better capture the  underlying  distribution  of  the  data,  leading  to  improved  generalization  performance. Consequently,  a  larger  training  set  is  advantageous  when  selecting  a  model,  as  it  can potentially result in lower generalization error.  Overall,  these  statements  emphasize  the  trade-off  between  model  complexity  and generalization  error  and  highlight  the  positive  impact  of  larger  training  sets  on  reducing generalization error. Therefore, when choosing a model, one should consider the complexity of the model and the availability of a sufficient amount of diverse training data to strike an optimal balance between the two factors.  