   The idea behind SVM is to find a hyperplane that separates the data into two classes, maximizing the margin between them. The margin is defined as the distance between the hyperplane and the closest points from each class. The points closest to the hyperplane are called support vectors.    Sometimes, data can be linearly separable but with a narrow margin. At other times, due to  noise, some of the instances may not be linearly separable. The soft margin concept is defined when the training instances are not linearly separable and allowing some of the data points to cross the borders and to be in the wrong side of the boundary or to be misclassified.  Linear regression example   Multi-class classification in SVM  o  One vs all  for each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples. For example, if we have classes '0', '1' and '2' in the original dataset then three models will be trained where each of them will classify samples from '0' vs {'1' ,'2'}, '1' vs {'0' ,'2'} and '2' vs {'0' ,'1'}  o  One vs one  The SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For example, if we have three classes (Blue, Green, and Red), we would train three binary classifiers: Blue vs Green, Blue vs Red, and Green vs Red  