This topic the unit covered Support Vector Machines (SVM) their application in linear and non-linear situations, statistical learning theory, multi-class classification and implementation using Python.  Support Vector Machines are a complex classification algorithm that can be used for both linear and non-linear data sets. The algorithm finds a hyperplane that separates two categories of data. When finding the hyperplane, the algorithm maximizes the distance between the nearest point of each category. This is simple when the hyperplane that separates the two classes is linear, however, when the dataset cannot be separated by a linear hyperplane the dataset needs to be transformed into a higher- dimensional space. The transformation is done using a kernel function such as Radial Basis Function (RBF), polynomial or sigmoid kernel.  Statistical learning theory was the next topic covered by the unit. Statistical learning theory of SVM is a method for understanding and optimizing the performance of SVM, based on structural risk minimization and empirical risk minimization. Structural risk is the error rate that the model has on unseen data, while the empirical error is the error on training data. As we have seen previously the minimization of empirical error can lead to overfitting which leads to a higher structural risk. Statistical learning theory includes a term that penalizes the model to avoid overfitting and assist in finding an optimal solution.  Despite being a binary classification algorithm there are two methods of applying SVM to multi-class classification problems. The first method, One-vs-One, compares each class to one other class; this is repeated until all classes have been compared. The models for each of these classes is then used to place a new feature, with the class that has the most classifications being the class it is assigned. The second method, One-vs-All, compares each dataset to an aggregation of all other datasets. From all of these models the class with the highest score is where the new data is categorized. The One-vs-One method is a more accurate method; however, it has a larger computational cost.  