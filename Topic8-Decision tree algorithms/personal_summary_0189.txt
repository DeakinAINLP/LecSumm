SIT720 Machine Learning Task 7.1  Screenshot of topic 7 quiz result:  Learning Summary Support Vector Machines (SVMs) are the focus of SIT720 Machine Learning Topic 7 because they are a powerful and versatile machine learning model capable of performing linear or nonlinear classiﬁcation, regression, and even outlier detection. The following are the main concepts and activities covered in this context:  1.  Understanding SVMs: SVMs seek the best hyperplane that divides data points of  diﬀerent classes by maximising the margin between them. The margin is the distance between the data points that are closest to the hyperplane (support vectors).  2.  The function of slack variables and the C parameter: Slack variables provide some  ﬂexibility in class separation when dealing with noisy or non-linearly separable data. The C parameter governs the trade-oﬀ between the maximisation of margin and minimisation of classiﬁcation error. Higher C values produce a smaller margin with be(cid:425)er classiﬁcation, whereas lower C values produce a larger margin with some misclassiﬁcation.  3.  SVMs with linear and polynomial kernels: In this context, we practised ﬁtting SVMs with linear and polynomial kernels using Python's Scikit-learn library. We looked at     how changing the degree of the polynomial kernel aﬀected the decision boundary and the accuracy of the model.  4.  Using the RBF kernel to implement SVMs: We also looked into the Radial Basis  Function (RBF) kernel, which is useful for non-linearly separable data. The eﬀects of the gamma parameter, which controls the inﬂuence of individual training examples on the decision boundary, were investigated.  5.  K-fold cross-validation was discussed as a technique for evaluating the performance of an SVM model, in which the dataset is divided into K subsets and the model is trained and tested K times using diﬀerent combinations of these subsets.  6.  We used Python's Scikit-learn library to implement SVMs, plotting decision  boundaries and support vectors for various kernel and hyperparameter settings. To evaluate the models, we calculated training and testing accuracies.  7.  Discussion of C's impact on SVMs: We investigated how the C parameter aﬀects the decision boundary and testing accuracy, and how it aids in balancing the trade-oﬀ between margin maximisation and classiﬁcation error minimisation.  