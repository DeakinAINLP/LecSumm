In this topic I have learned about various SVM models in machine learning. I have come across SVM formulation and solutions for linear separable and linear non-separable data. I have understood the concept of dual optimization problem for linear separable data which is mainly used for finding the local maxima and minima of a function. I have practiced the soft margin concept and soft margin dual problem. In the mid of topic 7 I have also understood the concept of kernel trick and non-linear SVM which also includes linear regression formulation, linear hypothesis etc. SVM seeks to identify the hyperplane that optimizes the margin between the two classes in a binary classification task. The margin is the separation between the nearest data points from each class and the hyperplane. Finding the hyperplane with the margin maximization yields higher generalization performance. SVM has a wide range of uses, including bioinformatics, text classification, and picture classification. By utilizing kernel functions, it has the benefit of being able to handle data that can be separated linearly and non-linearly. Machine learning frequently employs the One-vs-One and One-vs-All techniques for multi-class classification problems. For each pair of classes in  a binary classifier is trained, yielding (n*(n-1))/2 classifiers for n classes. Only the data points from the two classes being distinguished between are used to train the classifiers. A single classifier is trained in OvA to separate each class from every other class. As a consequence, n classifiers are created, each of which is trained using all the data points from its own class as well as a portion of the data points from the other classes. The decision between the two procedures depends on elements like the size of the dataset and each has advantages and downsides.  I have also attempted the quiz and the result is attached below.       