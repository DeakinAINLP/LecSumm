In this topic, we are mainly focus on two areas,  Linear SVM  Non-linear SVM  SVM stands for Support Vector Machine, which is a popular algorithm used for classification and regression tasks in machine learning. The basic idea behind SVM is to find a hyperplane that separates two classes of data points in the best possible way. The SVM formulation involves defining a decision boundary in the feature space that separates the data points into their respective classes. In SVM, we have so far assumed that data is linearly separable.  Sometimes, data can be linearly separable but with a narrow margin. At other times, due to noise, some of the instances may not be linearly separable. The distance between the hyperplane and the closest data points from either class is called the margin. The optimal hyperplane is the one that maximizes this margin.  Multiclass classification in SVM can be done as follows:  One vs all One vs One  One vs all - for each class, a binary SVM classifier is trained with samples from that class being viewed as positive examples and samples from the other classes being viewed as negative examples. For example, if we have classes '0', '1' and '2' in the original dataset then three models will be trained where each of them will classify samples from '0' vs {'1' ,'2'}, '1' vs {'0' ,'2'} and '2' vs {'0' ,'1'}       One vs One - the SVM algorithm trains multiple binary classifiers, each trained to distinguish between two classes. For example, if we have three classes (Blue, Green, and Red), we would train three binary classifiers: Blue vs Green, Blue vs Red, and Green vs Red.  Then we learn how to do this using python      