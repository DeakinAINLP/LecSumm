This topic, we have learned about linear and non-linear methods for classification  models. In this topic, we talked about Support Vector Machines (or SVMs). And SVM aims to find a hyperplane among all the instances in a dataset, such that the margin is maximised while satisfying a particular constraint. Therefore, it solves an optimization problem, which is also Call a quadratic optimization This is often solved by constructing an equivalent problem called a dual problem, which provides a lower bound to the solution of the original optimization problem (also called a primal problem). We have also discovered that to convert a constrained optimization into an unconstrained counterpart, we can use a mathematical concept called LaGrange multipliers. The LaGrange function minimise the weights used as LaGrange multipliers, hence maximises the margins between the hyperplane in the SVM.  This topic, we have also assumed our data to be non-linearly separable and formulated a suitable solution for it as well. This can likely be because of small noisy data points or outliers. It is acceptable to have large margins even though some of the constraints are violated. In practice, we need a trade-off between the margin and the number of errors in classifying in the training instances. This brought us to the concept of soft margin.  This is represented by â€˜Slack variablesâ€™, which are added to allow misclassification of outliers or simply noisy instances. It must also be kept in mind that we also aim to minimise this sum of these slack variables. Another variable which can be used in SVM algorithms is the parameter C. This parameter is used as a way of achieving A trade-off between large margins and the fitting of training data. The higher the value of C the greater the penalty for classification, and vice versa.  For understanding the formulation of the nonlinear SVM, we recapped some of the  formulation concepts for linear regression as well. This included the Moore-Penrose pseudo inverse of the Matrix. Likewise, we have the support vector regression formulation explain with an example of linear regression counterpart.  We also game about the statistical learning theory of SVM. Under this section these  were some of the concepts we have learned. â€˜Structural risk minimizationâ€™ is a concept which seeks to prevent overfitting, by incorporating a penalty on this model complexity. It prefers simpler functions over more complex functions. We aim to choose a less complex model with a small error. We also learned about the â€˜Vapnik-Chervonenkisâ€™ dimension, which refers to the number of instances a hypothesis class can shatter.  Finally, we have also learned about multiclass classification in SVMs. This can be done  by either one of two methods: â€˜One vs Allâ€™ and â€˜One vs Oneâ€™. In â€˜One vs Allâ€™, ğ‘ Number of models will be generated for ğ‘ number of classes. In the prediction phase however, the test sample is passed to each of the model and the predicted class is determined based on the high score obtained from the models. In â€˜One vs Oneâ€™, The SVM algorithm trains multiple binary classifiers, each train to distinguish between two classes.  For ğ‘ number of classes, the number of binary classifiers generated will be ğ‘ âˆ— (ğ‘ âˆ’ 1)/2 . In the prediction phase however, each test sample is passed to all binary classifiers and devoting scheme is applied on the output of individual binary classifiers, to determine the final class label.   