Support Vector Machines or SVMs are used for classiﬁcation and regression analysis and their aim is to ﬁnd a hyperplane that maximises the margin between two classes while satisfying a constraint. The primal problem is formulated as a quadratic optimization problem subject to linear constraints  At times, we can have data that is linearly separable, but only by a narrow margin, and at other times, instances can be linearly non-separable due to noisy data. In general, it is preferable to avoid inﬂuencing the margin, even when there are a few minor noisy data points or outliers. Large margins are acceptable even though some of the restrictions are broken as long as there is a trade-oﬀ between the margin and the number of errors in classifying the training instances. The soti-margin concept introduces a slack variable that allows some data points to be on the wrong side of the margin or even misclassiﬁed. The trade-oﬀ between maximizing the margin and minimizing the slack variable is controlled by a regularization parameter called "C", which determines the penalty for misclassiﬁcation. A higher value of C results in a harder margin, where misclassiﬁcation is penalized more severely, while a lower value of C will result in a sotier margin, where some degree of misclassiﬁcation is allowed.  Our goal in Linear Regression is to ﬁnd a linear equation that will allow us to study the relationships between two continuous variables. In order to ﬁnd the line, we need the slope of the line as well as its y-intercept. However, in some cases, the data point may be in multiple dimensions, and we need to change up the equation so that the slope and the data point are both being handled in all dimensions.  Structural risk minimisation is a method used to prevent overﬁting by incorporating a penalty on the model complexity. The idea is to choose a less complex model with a small error to minimize structural risk. To measure the complexity of the hypothesis function, we use the Vapnik- Chervonenkis (VC) dimension which characterizes the maximum number of instances that can be perfectly ﬁted for all possible label assignments. A rich hypothesis class that can learn any association of labels to the data is considered suﬃciently complex. When using hyperplanes as our hypothesis class, in 1-dimensional space, we can always ﬁnd a line to shater any labelling of two points, however, in higher dimensions, a line may not be able to shater some labelling’s of two points, which is why the VC dimension of a line is d + 1. From the Vapnik theory, we see that we can minimise the complexity of a model by maximising the margin.  There are two approaches for multiclass classiﬁcation in SVM: One vs. All or One vs. One. For the One vs. All approach, binary SVM classiﬁers use a separate SVM classiﬁer is trained for each class. The classiﬁer is trained to recognize positive examples from the current class and negative examples from all other classes. In the One vs. One method, the algorithm trains multiple binary classiﬁers, training each of them to distinguish between two classes.    