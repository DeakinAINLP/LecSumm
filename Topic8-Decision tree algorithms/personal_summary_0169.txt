  Support Vector Machine (SVM) Support Vector Machine (SVM) is a supervised machine learning model that uses classification algorithms  for  two-group  classification  problems.  They  are  based  on  statistical  learning frameworks or VC theory proposed by Vapnik and Chervonenkis. SVMs are widely used in classification objectives and are highly preferred by many as they produce significant accuracy with less computation power. The  objective  of  the  support  vector  machine  algorithm  is  to  find  a  hyperplane  in  an  N- dimensional space(N — the number of features) that distinctly classifies the data points.  To separate the two classes of data points, there are many possible hyperplanes that could be chosen.  Our  objective  is  to  find  a  plane  that  has  the  maximum  margin,  i.e  the  maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.  Kernel Kernel Trick utilizes existing features, applies some transformations, and creates new features. Those new features are the key for SVM to find the nonlinear decision boundary. In Sklearn — svm.SVC(), we can choose ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable as our kernel/transformation.    polynomial kernel  polynomial kernel as a transformer/processor to generate new features by applying the polynomial combination of all the existing features.    Radial Basis Function kernel  Radial Basis Function kernel as a transformer/processor to generate new features by measuring the distance between all other dots to a specific dot/dots — centers. The most popular/basic RBF kernel is the Gaussian Radial Basis Function:   Multiclass Classification using SVM In its most simple type SVM are applied on binary classification, dividing data points either in 1 or 0. For multiclass classification, the same principle is utilized. The multiclass problem is broken down to multiple binary classification cases, which is also called one-vs-one. In scikit- learn one-vs-one is not default and needs to be selected explicitly (as can be seen further down in the code). One-vs-rest is set as default. It basically divides the data points in class x and rest. Consecutively a certain class is distinguished from all other classes. The number of classifiers necessary for one-vs-one multiclass classification can be retrieved with the following formula (with n being the number of classes):  In  the  one-vs-one  approach,  each  classiﬁer  separates  points  of  two  diﬀerent  classes  and comprising all one-vs-one classiﬁers leads to a mul:class classiﬁer.  