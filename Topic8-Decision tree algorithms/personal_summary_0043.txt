In this Topic 7 we have basically and majorly learned about the SVM formulation and solution for linear saperation data and non-linear data. SVM aims to find a hyperplane so that the margin is maximised while satisfying the constraint. Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high dimensional spaces. In SVM, we have so far assumed that data is linearly separable. Sometimes, data can be linearly separable but with a narrow margin. At other times, due to noise, some of the instances may not be linearly separable. it totally depends upon the data type either the data is noisy or not. We also had the concept of kernal trick. The kernel approach eliminates the requirement for explicit mapping in order for linear learning algorithms to learn a nonlinear function or decision boundary. Certain functions can be described as an inner product in another space for all and in the input space. Also we had a overview of Statistical learning theory of SVM and Multi-class classification in SVM. We practiced our theratical learning by implementing it into python coding. By implementing our theratical concept into python code we got a better understanding of the working of the code and SVM. To get more understanding of this topic i did some research on internet related to our ULO. In that youtube, wikipedia and and google scholars were a great help. For coding expects Deakin unit site materal was a great help and for more understanding sklearn and stack overflow had all the understanding materaial i required to finish my assignement and polish my concept. Futur to test my theratical understanding i took this topicâ€™s quiz on deakin unit site, which result is attacked as bellow.   