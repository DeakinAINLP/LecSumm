 This  topic  I  have  learned  about  two  different  Machine  learning  models,  which  are  Support Vector Machines or shortly known as SVM. SVM draws the hyperplane between the data and separates  them  to  predict  the  future  input.  The  math  behind  SVM  model  is  that  it  finds  the hyperplane which maximizes the distance between two clusters.  What if the data is not linearly separable?  One solution is to trade-off some error to accurately find  the  right  hyperplane,  this  error  is  called  as  soft  margin,  and  it  is  also  the  subject  of  how accurately we want the model to be, if this error is nearly zero, then there is highly likely that model will underfit  and vise  versa.  While programming  this  value  is  called  C  and  needs to be hyper-tuned.  SVM can be used to classify more than two categories, in such cases, the one vs all or one vs one approach is being used. Then for the new input data, it  is to check whether  it belongs to specific category depending on model score.  The advantages of SVM is that it can be performed on large dimensions data, and it can handle non-linearly separable data, however, it can be computationally challenging for the larger data.  Later,  I  learned  how  to  apply  an  SVM  model  in  Python  programming  language.  And  how  to evaluate the model.      