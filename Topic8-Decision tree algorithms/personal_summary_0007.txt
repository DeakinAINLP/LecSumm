SVM models SVM aims to find a hyperplane (w,b) so that the margin 2/w is satisfying its constraint. The SVM formula solves the optimisation problem of maximising 2/w and minimising ½ || w ||^2. To solve quadratic programming/optimisation we need to construct an equivalent problem called a dual problem. The original problem is the primal; problem and the solution the dual problem provides is a lower bound to the solution of the primal (minimisation problem). Using Lagrange multiplier, we can convert a constrained optimisation into an unconstrained optimisation problem. Lagrange is a strategy to find the local maxima and minima of a function. Once you have the function to minimise, where a are Lagrange multiplier, you then have a function to maximise, which can be done by setting the derivative to zero and subbing the results in the function which results in the dual problem.  So far in SVM we have assumed that data is linearly separable. Sometimes it can be linearly separable with a narrow function and sometimes it cannot be due to noisy data. It can be acceptable to have large margins even though constraints may be violated as in practice we need a trade-off between the number of errors and the margin. This trade-off results in the soft margin concept. Slack variables are added to allow misclassification of outliers and noisy instances. Allowing some points to be on the wrong side of the boundary. We still want to minimise the sum of the slack variables so for data which the slack value is nonzero we assume this to be misclassified and use a formula to move the parameter C so that the trade off between the margins and fitting the training data. We highly penalise high values of C but for small values we allow more misclassifications. In linear regression we want to find a line similar to h. The linear equation should allow us to summarise and study the relationship between 2 variables. A line is defined as y = h(x) = wx + b. If we have 2 parameters we can find the line by finding the slope w and b the y intercept. Then we can use an x value to estimate the value of y. If we have multiple dimensions the equation is almost the same except we handle the multiplication in all d dimensions. The error of the value predicated in regression compared to the true value is shown as ei = yi – y(predicted), and the goal is to minimise the total error of the line. Structural risk minimisation seeks to prevent over-fitting by incorporating a penalty on the model complexity. It prefers simpler functions to more complex functions. Where h(f) is the complexity of the hypothesis function f and lambda is a penalty parameter. So we would choose a less complex with a small error. If we pick n instances and assign labels of – and + to them randomly, then if the hypothesis is reach enough to learn any association of labels to the data it is sufficiently complex. To characterise the complexity of the hypothesis class we look at how many instances it can shatter, fit perfectly for all possible label assignments. This number it can shatter is called its Vpnik-Chervonekis Dimension (VC). In a 2-D we can find a line to shatter any labelling of 3 points, but a line may not be able to shatter 4 or more. Therefore VC dimension of a line 2D is 3. Regardless of dimensionality d, we can minimise the model complexity (VC) by maximising the margin p. If the margin is maximised we look for a classifier with high margins which means we have a smaller value for D2/p2, therefore, we have a smaller upper bound for the complexity of our model h. 