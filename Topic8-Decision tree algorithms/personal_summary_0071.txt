 Main Points  Support Vector Machine(SVM) can represent non-linear functions and they have the  efficient training algorithm.  Linear SVM: perfectly separable data points; almost separable data points  SVM concept of margin: the basic idea is that the decision boundary should be as far  away from the data of both classes as possible. it aims to minimize possible conflicts  which may happen during classification.  SVM formulation solves the optimization problem, which need to optimize the  quadratic function in subject to linear constraints.  Linear SVM dual optimization problem:  the primal problem has computational requirements of the order, whereas the dual  problem requires an order.  Linear SVM formulation(linear non-separable data):  acceptable to have large margins even though some of the constraints are violated.  In practice, it needs a trade-off between the margin and the number of errors in  classifying the training instances.  Non-linear SVM Kernel trick and non-linear SVM:  To handle non-linearity, transform the features to a higher dimensional space where  data is linearly separable. A kernel function is used to compute dot products in a high  dimensional feature space. Gram matrix evaluated on each pair of data instances  give rise to a matrix, it is a positive semi-definite and symmetric matrix.  Support Vector Regression(SVR) defines the margin if the data points are in the tube,  if the data point goes outside of the tube or margin.  Multi-class classification in SVM:  One vs All:In this approach, for each class, a binary SVM classifier is trained with  samples from that class being viewed as positive examples and samples from the  other classes being viewed as negative examples  One vs One: the SVM algorithm trains multiple binary classifiers, each trained to  distinguish between two classes  Summary of Reading  Based on the reading of resources, the advantages of SVM include the different  aspects as following:  It works well with the clear margin of separation;  It is effective in high dimensional spaces;  It is effective in the classes where the amount of dimensions is greater than the  number of samples;  It uses the subset of the training set in the decision function.  As the same time, there are also limits of SVM:  It does not perform well when there is a large data set as the required training time  is higher;  It is also weakness when the data set has more noise, such as the target classes are  overlapping;  SVM does not directly provide probability estimates, there are calculated using the  expensive five-fold-cross-validation. It is consisted in the SVC method of the Python  scikit-learn library.  Self-reflection  According to the learning of SVM, it is a possible problem to decide which kernel will  work efficiently with the real datasets. Generally, it depends on the problems that  we are solving about, it the data is linearly separable, without a second thought, go  for the linear kernel.  In another aspect, when we need to select other kernel functions, the advantages  are listed specifically as following:  Gaussian kernel tend to give good results when there is no additional information  regarding data that is not available;  RBF kernel is a kind of Gaussian kernel which projects the high dimensional data and  then searches a linear separation for it;  Polynomial kernel give good results for problems where all the training data is  normalized.   