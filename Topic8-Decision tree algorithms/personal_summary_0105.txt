 Main learning points:  Support Vector Machines:    A threshold between two clusters can be chosen to distinguish the two clusters as two  categories.  o  This threshold takes the form of line, plane, hyperplane depending on the  dimensions.  o  This division is inherently binary, but it can be extended to classify multiple classes.    The distance between the threshold and cluster (its point with the shortest distance) is  called the margin.  o  The model tries to maximise this margin.    Certain points near the edges of the clusters, support vectors, define the margin.  o  All points that are not support vectors do NOT contribute to the margin.     If the threshold disallows misclassifications, it is a hard margin. If the threshold allows for misclassifications, it is a soft margin.  Parameters:    C:  o  Larger C value narrows the margin (penalises misclassifications) o  Smaller C value allows more misclassifications.    o  Optimum C is found by cross-validation  Kernel trick is when SVM maps non-linearly-separable data to higher dimensional space where they can be linearly separated.    RBF: maps to infinite dimensional space  If the gamma parameter is too high: overfitting. If the gamma parameter is too low: generalisation.  o o o  e-gamma(a-b)^2 o  (a-b)^2 is the squared distance between two points o  Gamma scales this squared distance and thus the influence of distance  ▪  Gamma is determined by cross-validation    Polynomial kernel:  o  Computes relationships between pairs of observations o  (a*b + r)d  ▪  a and b are two different observations ▪ r determines coefficient of polynomial ▪  d is polynomial degree o  Cross-validation determines r and d  