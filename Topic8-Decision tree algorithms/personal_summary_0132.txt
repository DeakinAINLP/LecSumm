  Ans: SVM formulation solves the optimization problems via quadratic programming.  ➢  The Lagrangian dual problem is obtained by forming a Lagrangian of a non-negative multiplier to  add constraints in order to solve the functions for primal variables.  ➢  Sometimes due to noise the data may not be linearly separable, pulls in soft margin, where soft  margin concept is defined when training instances are not linearly separable.  ➢  Kernel Trick is widely used in Support Vector Machines (SVM) model to bridge linearity and non- linearity. It converts non-linear lower dimension space to a higher dimension space thereby we can get a linear classification.  ➢  Statistical learning theory aims to minimize the expected errors of predictions. ➢  SVMs are based in statistical learning theory that can be applied in both linear and nonlinear  data.  ➢  The multiclass problem is broken down to multiple binary classification cases, which is also  called one-vs-one.  ➢  One-vs-rest is set as default. It basically divides the data points in class x and rest. ➢  Sample codes & examples for usage of SVM in python with Linear kernel which gives us a linear  decision boundary a straight line between two different data points.  ➢  SVM with polynomial kernel is a kind of SVM kernel that uses a polynomial function to map the data into a higher-dimensional space. It does this by taking the dot product of the data points in the original space and the polynomial function in the new space.  ➢  The Radial Basis Function (RBF) kernel is one of the most powerful, useful, and popular kernels in  the Support Vector Machine (SVM) family.  