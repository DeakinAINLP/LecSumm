 This topic we discussed the Support Vector Machine (SVM). The SVM can come in two forms, linear and non-linear. SVM can be used for both Classification and Regression problems.    Linear SVM – Linear SVM finds a line or hyperplane that best separates the data points of different classes in the feature space.    Non-Linear SVM - Non-linear SVM finds a non-linear decision boundary that best separates the data points of different classes. This is useful when the data cannot be separated by a straight line or hyperplane in the original feature space.    Classification Problems – In classification problems SVM aims to in the simplest case (2D)  draw a line that separates two classes. The line has  ‘tube’ which is +/- epsilon. The aim is to increase epsilon until the line is furthers from both groups, i.e. the middle. As the dimensions increase the line becomes a plane, and then a hyper plane. As the number of classes increase we can use a One-vs-One approach where each pair of classes has their own classifying plane, or One-vs-Many where each class has a hyperplane classifier that separates it from the rest of the classes.    Regression Problems – In the case of linear regression the SVM aims to find a function  (line/plane/hyperplane) that describes linearly dependent data, much like Linear Regression.  