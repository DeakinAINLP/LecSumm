SVM formulation and solution for linearly separable data  SVM algorithmâ€™s goal is to determine the best possible boundary separation between the different classes in a machine learning model. It separates these classes by finding the maximum possible value margin between the decision boundary and the nearest data points from each class to the boundary. The SVM algorithm can be used to manage non-linear separation between data classes as it can be transferred to high-dimensional feature spaces, making it powerful in its ability to handle high dimensional data and cope with any noise.  Soft margins are used to allow for the misclassification of noisy data points and difficult data plotting instances with the introduction of slack variables. The sum of all slack variables is reduced to minimize the number of misclassifications. The C Parameter is used to manage the swap between larger margins and the fitting of training data, with higher values of C penalizing misclassification more. The dual problem for soft margins is similar to the primal problem with an upper bound on the Lagrange multipliers.  Statistical learning theory of SVM  How do the following statements influence model choice?  The upper bound on the generalisation error increases with higher complexity (higher H)  This statement is suggesting that when the complexity of the model increases, the upper boundary of the generalization error increases as well, meaning that the more complex models have a higher likelihood of overfitting the training data. Overfitting will obviously lead to an inaccurate capture of new data. The model should therefore be a simple as possible whilst still having enough complexity to recognize the overall patterns  The upper bound on the generalisation error reduces with larger training sets (higher N).  This statement depicts the idea that having a greater amount of training data will reduce the generalization error for complex models. Larger training sets reduces the upper bound generalization error. It puts importance on aiming to collect more training data rather than focusing on model complexity. 