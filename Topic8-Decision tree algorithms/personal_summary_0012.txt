SVM formulation and solution for linearly separable data Support Vector Machines (SVM) is a method used to find a hyperplane that maximizes the margin between two classes while satisfying a constraint. The optimization problem is solved using quadratic programming and a dual problem is constructed using Lagrange multipliers. The dual problem provides a lower bound to the solution of the primal problem. The classification function uses dot products between vectors and support vectors. The dual problem allows the use of arbitrary kernels, which makes SVM boundaries significantly nonlinear. The primal problem has computational requirements of the order O(dn^2) while the dual problem requires O(n^3). SVM formulation and solution for linearly non-separable data When data is not linearly separable, we can use the soft margin concept in SVM to allow some misclassifications by adding slack variables. The objective is to minimize the sum of slack variables while still maximizing the margin. The trade-off between margin and misclassification is controlled by a parameter called C. The soft margin formulation leads to a new dual problem with an upper bound on Lagrange multipliers. The solution provides a hyperplane and a classification function similar to the linearly separable case. Linear regression formulation Linear regression formulation goal is to find a line that summarizes and studies relationships between two continuous variables. The linear hypothesis involves finding the line parameters, including the slope and y-intercept. The linear regression equation can be extended to multiple dimensions using vector notation. The error of value prediction in regression is defined as the difference between predicted and true values. The goal is to minimize the empirical risk by minimizing the mean of the square error function. The optimal solution can be found by taking the derivative of the error function with respect to the slope parameter and equating it to zero. The matrix in this process is known as the Moore-Penrose pseudo-inverse of the matrix and is denoted as A+. Statistical learning theory of SVM The Statistical Learning Theory of Support Vector Machines (SVM) aims to prevent over-fitting by minimizing the structural risk, which incorporates a penalty on the complexity of the model. The complexity of the hypothesis class is characterized by its Vapnik-Chervonenkis (VC) Dimension, which is the number of instances a hypothesis class can shatter. The class of optimal linear separators has VC dimension bounded from above by a formula that includes the margin, the diameter of the smallest sphere enclosing all training examples, and the dimensionality. Maximizing the margin in SVM results in a less complex model, which minimizes the upper bound for the complexity of the model. The Probabilistic Guarantee provides an upper bound on the test error, which is upper bounded by the training error and a value that depends on the size of the training set and the complexity of the model. The complexity of the model can be minimized by reducing its VC dimension, which can be achieved by maximizing the margin. 