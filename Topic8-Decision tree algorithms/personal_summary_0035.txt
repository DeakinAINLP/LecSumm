The key points covered in topic seven are:  1.  SVM for linearly separable data – SVM uses hinge loss and L2 regularisation.  With hinge  loss, training samples that are predicted correctly beyond a defined margin of error falls into the zero-loss region and will not contribute to the model fit. SVM maximises the margin for linearly separable data.  2.  SVM for non-linearly separable data/ almost linearly separable data: concept of margins,  soft margin, number of errors, slack variables. Only samples that are in the margin and are misclassified are considered support vectors.  3.  Kernel trick and non-linear SVM: kernel SVMs are fast as the fit time depends only on the  number of support vectors, not the total number of training samples.  4.  Support Vector for Regression: hard to scale for large datasets 5.  Relationship between margin and model complexity 6.  Multiclass classification in SVM – One vs One and One vs All  