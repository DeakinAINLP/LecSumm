A support vector machine (SVM) is a supervised learning algorithm used for many classification and regression problems.  The objective of the SVM algorithm is to find a hyperplane that separates data points of one class from those of another class. A hyperplane is created that “marginally” separates classes under consideration. This “hard margin” sits parallel to the hyperplane and has no interior data points. An algorithm works well in finding a hyperplane for linearly separable problems, in others which are not linearly separable the algorithm maximizes a “soft margin” allowing for a small number of errors in classification.  Linear SVM  A linear SVM is used to address classification and regression analysis problems using linearly separable data. This is where a dataset can be segregated into categories or classes with the help of a single straight line.  Non-Linear SVM  Classification of data that is unable to be separated into distinct categories or classes through the use of a straight line is performed via Non-Linear SVM (or kernals). This is where features are included into higher dimensions rather than relying on smaller dimensional space. These newly added features help fit a hyperplane that more readily separates categories or classes. Kernel SVMs are typically used to handle optimization problems that have multiple variables.  SVM Kernel Functions  The function of kernel is to transform input data into a required form. Different SVM algorithms use different types of kernel functions; such as linear, nonlinear, polynomial, radial basis function (RBF) and sigmoid functions.  Binary Classifiers for Multi-Class Classification  Classification is a predictive modelling problem that involves assigning a class label to an example.  Binary classification is where the algorithm trains multiple binary classifiers, each trained to make a distinction between two classes.  Multi-class classification on the other hand for is those tasks where examples are assigned one of more than two classes. This is where a classifier is trained with samples from its own class being viewed as positive examples and samples from all the other classes being viewed as negative.     