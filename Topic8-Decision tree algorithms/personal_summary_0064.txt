Topic 7 – SVM Models  Formulation and solution for linearly separable data What is it?    Finding the hyperplane (w, b), so margin 2/||w|| is maximised.    While satisfying the constraint   This known as quadratic optimisation, which can be solved by creating an equivalent problem  called dual problem (originally derived from the primal problem)    Dual problem discovers a solution for the lower bound of primal problem.  Dual optimisation problem    Maximising margin (or minimising 1/margin)   By implementing LaGrange multipliers to convert a constrained optimisation, it can create a  unconstrainted optimisation problem   This will create a function to maximise    From here, dual function can be created by setting a derivative to 0, and substituting the results  in LaGrange function.    A classification function can be written given w and b, where each ai corresponds to an xi  Formulation and solution for linearly non-separable data What is it?   Some cases data can have noise, which cannot be separated by a linear equation.    Soft margin; the need to trade-off between the margin and the number of errors in classifying the training instances [1]. The following is created based on non-separable variables in the dataset, which initialises Slack variables.   Dual problem (soft margin)    Converting primal to dual problem via a soft margin’s conversion   The original dual problem is given, however, C is given to a = [ai to an] with w as a given  hyperplane    A classification function can be created in the form of  Kernal trick and non-linear SVM[2]    Finding a linear which minimises the total absolute error of y = mx + b   Find m and b using Minimum Absolute Error  The MSE equation, where ^yi  can be replaced with x Tiw:    Take the derivate and make it equal to 0; where function will equate to    Which is the Moorse-Penrose pseudo-inverse of the matrix X.  Statistical learning theory of SVM An illustration of VC dimension   Probabilistic Guarantee  