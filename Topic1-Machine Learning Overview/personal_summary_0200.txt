Topic 1 Introduction to Machine Learning  Summarise the main points that were covered in this topic. Definition of Machine Learning Samuel A. defined machine learning in 1959 as the "Field of study that gives computers the ability to learn without being explicitly programmed." The fundamental idea behind machine learning is to enable computers to learn from data and automate  decision-making processes, without requiring explicit instructions  for every possible outcome.  Samuel's definition of machine learning lays out the groundwork for an artificial intelligence (AI) system that can analyse data, recognise patterns, and make predictions based on past experiences.  Machine learning involves developing algorithms and statistical models that allow computers to improve their performance on specific tasks over time. This is achieved through the use of training data, which is used to teach the computer how to recognise patterns and make predictions. As the computer receives more data, it becomes better at recognising complex patterns and making accurate predictions.  Samuel  A.'s  also  highlighted  the  importance  of  feedback  mechanisms,  which  allow  machines  to  continuously learn and improve. By using feedback to adjust their algorithms, machine learning systems are able to refine their  predictions  and  automate  more  complex  decision-making  processes.  Over  the  years,  researchers  have developed numerous machine learning techniques, including deep learning, neural networks, and decision trees, all of which aim to create more sophisticated AI systems.  Types of Machine Learning 1.  Supervised Learning: This type of machine learning involves using labelled data to train a model to make predictions or classify new data. The goal is to teach the algorithm to accurately identify patterns and relationships in the data so that it can make accurate predictions on new, unseen data.  Supervised learning involves training a model to predict outputs based on a given set of inputs, using labelled data. The formula for supervised learning can be written as:  y = f(x)  Where y is the predicted output, x is the input data, and f is the function or model that maps inputs to outputs. The goal of supervised learning is to find the best possible model f, which can accurately predict the output y for new, unseen input data.  In supervised learning the data available is split into training (most data, usually 70%) and evaluation data (30%). As we know the right answer (available in the training data) we can select manipulate and refine the features to train the model. The more often answer is the correct, the closer you are to having a useful algorithm.  This  is  achieved  by  minimising  the  error  between  the  predicted  output  and  the  actual  output,  using  various optimisation techniques.  Classification Supervised learning classification problems can be solved with linear or non-linear models. Linear models use a linear function to separate or classify the data, while non-linear models use more complex functions such as polynomials or neural networks to separate the data.  1   Linear models are faster to train and less complex, making them a good option for smaller datasets with fewer features. Non-linear models are more complex, and better for datasets with many features or more complex relationships between the predictors and outcome variables.  Examples of linear models include logistic regression and linear support vector machines (SVMs), while examples of non-linear models include decision trees, random forests, and artificial neural networks.  Regression Regression problems are a type of machine learning problem where the goal is to predict a continuous numerical value for a given set of input features.  Some examples of regression problems include predicting the price of a house based on its characteristics or predicting  a  person's  annual  income  based  on  their  education  and  job  history.  There  are  many  different algorithms and techniques that can be used to solve regression problems, including linear regression, decision trees, and neural networks.  2.  Unsupervised Learning: Unsupervised  learning,  also  known  as ,  uses  machine  learning  algorithms  to analyse and cluster unlabelled datasets. These algorithms discover hidden patterns or data groupings without unsupervised  machine  learning the need for human intervention. Its ability to discover similarities and differences in information make it the ideal  solution  for  exploratory  data  analysis,  cross-selling  strategies,  customer  segmentation,  and  image recognition (IBM, 2023).  2     K-means clustering is a common clustering  method where data points are assigned into K  groups, where K represents the number of  clusters based on the distance from each  group’s centroid. The data points closest to a  given centroid will be clustered under the  same category. A larger K value will be  indicative of smaller groupings with more  granularity whereas a smaller K value will  have larger groupings and less granularity.  K-means clustering is commonly used in  market segmentation, document clustering,  image segmentation, and image compression.  Overlapping clusters differs from exclusive  clustering in that it allows data points to  belong to multiple clusters with separate  degrees of membership. “Soft” or fuzzy k-  means clustering is an example of overlapping  clustering.    Hierarchical clustering    Agglomerative clustering is considered a  “bottoms-up approach.” Its data points are  isolated as separate groupings initially, and  then they are merged together iteratively  on the basis of similarity until one cluster  has been achieved.  Euclidean distance is the most common  metric used to calculate these distances;  however, other metrics, such as  Manhattan distance, are also cited in  clustering literature.    Divisive clustering it takes a “top-down”  approach. In this case, a single data cluster  is divided based on the differences  between data points.  Common unsupervised learning approaches Unsupervised learning models are utilized for three main tasks— clustering, association, and dimensionality reduction. Below we’ll define each learning method and highlight common algorithms and approaches to conduct them effectively.  Clustering Clustering is a data mining technique which groups unlabelled data based on their similarities or differences. Clustering algorithms are used  to  process  raw,  unclassified  data  objects  into  groups represented by structures or patterns in the information.  Association Rules An association rule is a rule-based method for finding relationships between  variables in  a  given  dataset.  These  methods  are frequently used for market basket analysis, allowing companies to better  understand  relationships  between  different  products. Understanding  consumption  habits  of  customers  enables businesses to  develop  better  cross-selling  strategies  and recommendation engines.  Dimensionality reduction While more data yields more accurate results, it can also impact the performance of machine learning algorithms (e.g., overfitting) and it can also make it difficult to visualize datasets. Dimensionality reduction  is  a  technique  used  when  the  number  of  features,  or dimensions, in a given dataset is too high. It reduces the number of data inputs to a manageable size while also preserving the integrity of the dataset as much as possible.  Applications of unsupervised learning   News  Sections: Google  News  uses  unsupervised  learning  to categorize  articles  on  the  same  story  from  various  online  news  outlets. For example, the results of a presidential election could be  categorized under their label for “US” news.   Computer  vision: Unsupervised  learning  algorithms  are  used  for visual perception tasks, such as object recognition.    Medical  imaging: Unsupervised  machine  learning  provides  essential  features  to  medical  imaging  devices, such  as  image  detection,  classification  and  segmentation,  used  in  radiology  and  pathology  to  diagnose  patients quickly and accurately.    Anomaly detection: Unsupervised learning models can comb through large amounts of data and discover atypical data points within a dataset. These anomalies can raise awareness around faulty equipment, human  error, or breaches in security.    Customer  personas: Defining  customer  personas  makes  it  easier  to  understand  common  traits  and business clients' purchasing habits. Unsupervised learning allows businesses to build better buyer persona  profiles, enabling organizations to align their product messaging more appropriately.  3     Recommendation Engines: Using past purchase behaviour data, unsupervised learning can help to discover data trends that can be used to develop more effective cross-selling strategies. This is used to make relevant  add-on recommendations to customers during the checkout process for online retailers.  Challenges of unsupervised learning While  unsupervised  learning  has  many  benefits,  some  challenges  can  occur  when  it  allows  machine  learning models to execute without any human intervention. Some of these challenges can include:        Computational complexity due to a high volume of training data  Longer training times  Higher risk of inaccurate results  Human intervention to validate output variables  Lack of transparency into the basis on which data was clustered  Reinforcement machine learning Reinforcement  machine  learning  is  a  machine learning  model  that  is  similar  to  supervised  learning,  but  the algorithm isn’t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes will be reinforced to develop the best recommendation or policy for a given problem.  Steps in Machine Learning 1.  Data Collection: The first step in machine learning is collecting data that will be used to train the model. This data  can come from various sources such as databases, sensors, and data feeds.  2.  Data Cleaning and Preparation: Once the data has been collected, it needs to be cleaned and prepared to ensure that it is suitable for use in training the model. This can involve tasks such as removing duplicates, filling in missing  values, and transforming the data into a suitable format.  3.  Model Selection and Building: There are many different machine learning algorithms available, and selecting the right one for the task at hand is critical. Factors such as the type of data, the size of the dataset, and the desired  output will influence the choice of algorithm. Once selected the model must be built to suit the selected dataset.  4   Feature  Engineering:  The  features  of  the  data  need  to  be  extracted  and  transformed  into  a  format  that  the  machine  learning algorithm can use. This process involves creating new  features and selecting the most relevant ones for the model.  4.  Training  the  Model:  The  selected  machine  learning  algorithm is trained on the prepared data using a variety of  Model Selection In an ideal situation, we would split the data into training, validation, and test sets, then fit candidate models on the training set, evaluate and select them on the validation set, and report the performance of the  final  model  on  the  test  set.  In  many  applications,  however,  the supply of data for training and testing will be limited, and in order to build  good  models,  we  wish  to  use  as  much  of  the  available  data  as possible for training. However, if the validation set is small, it will give a noisy estimate of predictive performance. (Bishop, 2006)  Instead, there are two main classes of techniques to approximate the ideal case of model selection; they are:  techniques  such  as  supervised  or  unsupervised  learning.  Probabilistic Measures  During the training process, the model learns to recognise  patterns in the data and make predictions based on those  patterns.  5.  Testing and Evaluation: Once the model has been trained, it needs to be evaluated to determine how well it performs.  This involves testing the model on a set of data that it has  not seen before and comparing the predicted results with  the actual results.  Model Evaluation  Two  commonly  used  metrics  for  evaluating  a  supervised  learning model's performance are accuracy and error.  Accuracy  measures  the  proportion  of  correctly  classified  instances out of how many total instances there are.  Error,  on  the  other  hand,  measures  the  proportion  of  Probabilistic  measures involve  analytically  scoring  a  candidate  model using both its performance on the training dataset and the complexity of the model.  It is known that training error is optimistically biased, and therefore is not  a  good  basis  for  choosing  a  model.  The  performance  can  be penalized based on how optimistic the training error is believed to be. This is typically achieved using algorithm-specific methods, often linear, that penalize the score based on the complexity of the model.  Historically  various  ‘information  criteria’  have  been  proposed  that attempt to correct for the bias of maximum likelihood by the addition of a  penalty  term  to  compensate  for  the  over-fitting  of  more  complex models. (Bishop, 2006)  A model with fewer parameters is less complex, and because of this, is preferred because it is likely to generalize better on average.  Four commonly used probabilistic model selection measures include:       Akaike Information Criterion (AIC). Bayesian Information Criterion (BIC). Minimum Description Length (MDL). Structural Risk Minimization (SRM).  Probabilistic  measures  are  appropriate  when  using  simpler  linear models like linear regression or logistic regression where the calculating of  model  complexity  penalty  (e.g.,  in  sample  bias)  is  known  and tractable.  incorrectly classified instances out of how many total instances  Resampling Methods  there are.  Accuracy  and  error  may  not  always  be  the  best  ways  to  evaluate  a  model's  performance, depending  on  the problem  domain and specific goals of the model. Other metrics, such as  precision, recall, F1 score, AUC-ROC, and others, may be more  appropriate in certain contexts.  6.  Improving  and  Tuning:  Machine  learning  models  have many hyper parameters that can be adjusted to optimise  their  performance.  Fine-tuning  these  parameters  is  an  essential  step  in  achieving  the  best  possible  accuracy  for  the model.  7.  Deployment:  Finally,  once  the  model  has  been  trained, in  a  evaluated,  and  optimised,  it  can  be  deployed  Resampling methods seek to estimate the performance of a model (or more  precisely,  the  model  development  process)  on  out-of-sample data.  This is achieved by splitting the training dataset into sub train and test sets, fitting a model on the sub train set, and evaluating it on the test set. This process may then be repeated multiple times and the mean performance across each trial is reported.  It is a type of Monte Carlo estimate of model performance on out-of- sample  data,  although  each  trial  is  not  strictly  independent  as depending  on  the  resampling  method  chosen,  the  same  data  may appear multiple times in different training datasets, or test datasets.  Three common resampling model selection methods include:      Random train/test splits. Cross-Validation (k-fold, LOOCV, etc.). Bootstrap.  Most  of  the  time  probabilistic  measures  (described  in  the  previous section) are not available, therefore resampling methods are used.  By far the most popular is the cross-validation family of methods that includes many subtypes.  production  environment,  where  it  can  be  used  to  make  An example is the widely used k-fold cross-validation that splits the training dataset into k folds where each example appears in a test set only once.  predictions on new data.  Types of Data 1.  Numeric  data:  Continuous  or  discrete  numerical  values, such as age, height, weight, temperature, and stock prices.  Another is the leave one out (LOOCV) where the test set is comprised of a single sample and each sample is given an opportunity to be the test set, requiring N (the number of samples in the training set) models to be constructed and evaluated.  5    2.  Categorical data: Categorical variables that represent a particular class or category, such as gender, race, status,  and country.  Image data: Visual data, such as photographs, videos, and scanned documents.  3.  Text data: Unstructured text data, such as emails, social media posts, and news articles. 4. 5.  Time series data:  Data is collected over time, such as stock prices, weather data, and sensor data. 6.  Audio data: Sound recordings, such as voice commands, music, and noise signals. 7.  Geospatial data: Location-based data, such as GPS coordinates, maps, and satellite images.  Mathematics and Machine Learning Vector In data analytics, a vector is a mathematical representation of a quantity that has both magnitude and direction. It  is  a  set  of  numbers  or  values  that  are  arranged  in  a  specific  order  and  can  be  used  to  represent  various attributes  or  properties  of  a  dataset.  Vectors  are  commonly  used  in  data  analysis  and  machine  learning algorithms to represent features, variables, or observations.  For example, in a dataset that contains information about customers' age, gender, income, and purchase history, each customer's data can be represented as a vector with four elements that correspond to these attributes. This vector can then be used to perform various analyses or predictive modelling techniques.  Vectors can also be used to measure the similarity or distance between different data points in a dataset. For instance, in a clustering analysis, vectors can be used to group similar data points into clusters based on their proximity in the feature space. Similarly, in a classification problem, vectors can be used to classify new data points into predefined categories based on their similarity to the training data.  Three main operations in Vectors are:    Transposed- Means interchanging a column vector to a row vector   Addition- Adding two vectors of the same length. Add every element  components and adding the results. The aim is to create a scalar value- one value.  Inner  Product-  We  have  both  the  column  and  the  row  vector  and  we  are  multiplying  the  corresponding  The magnitude of a vector is the length of the vector, represented by a non-negative scalar value. To find the magnitude of a vector, you can use the Pythagorean theorem.  Suppose  you  have  a  vector  represented  by  the  coordinates  (x,y,z),  then  the  magnitude  of  the  vector  can  be calculated as:  |v| = sqrt(x^2 + y^2 + z^2)  For example, if you have a vector (3,4,5), the magnitude of the vector would be:  |v| = sqrt(3^2 + 4^2 + 5^2) = sqrt(50) = 7.07  Therefore, the magnitude of the vector is approximately 7.07.  The 2-norm of a vector is the same as the magnitude or Euclidean norm of the vector. It is denoted by ||v||2 and is calculated as the square root of the sum of squares of the vector components.  Suppose you have a vector represented by the coordinates (x1, x2, ..., xn), then the 2-norm of the vector can be calculated as:  ||v||2 = sqrt(x1^2 + x2^2 + ... + xn^2)  For example, if you have a vector (3, -4, 5), the 2-norm of the vector would be:  6   ||v||2 = sqrt(3^2 + (-4)^2 + 5^2) = sqrt(50) = 7.07  Therefore, the 2-norm of the vector is approximately 7.07.  The p-norm of a vector is a generalization of the Euclidean norm (2-norm) and is defined as:  ||v||p = (|x1|^p + |x2|^p + ... + |xn|^p)^(1/p)  where p is a positive real number.  When p=2, the p-norm reduces to the Euclidean norm. When p=1, the p-norm is also known as the Manhattan norm, and when p=infinity, it is called the supremum norm or the max norm.  For example, if you have a vector (3, -4, 5), the 3-norm of the vector would be:  ||v||3 = (|3|^3 + |-4|^3 + |5|^3)^(1/3) = (27 + 64 + 125)^(1/3) = 6.16  Therefore, the 3-norm of the vector is approximately 6.16.  Distance  between  vectors/Vector  similarity  is  a  measure  of  how  similar  two  vectors  are  in  terms  of  their direction and/or magnitude. There are several ways to measure vector similarity, and the choice of similarity measure depends on the specific application and the type of vectors being compared. Here are a few common similarity measures:  1.  Cosine similarity: This measures the cosine of the angle between two vectors and is defined as the dot product of the vectors divided by the product of their magnitudes. Cosine similarity ranges from -1 (opposite directions) to 1  (same direction).  2.  Euclidean distance: This measures the distance between two vectors in n-dimensional space and is defined as the  square root of the sum of the squared differences between the corresponding components of the vectors.  Matrix Types of matrices  Square  matrix:  A  square  matrix  is  a  matrix  in  which the  number  of  rows  is  equal  to  the  number  of columns. For example, a 3x3 matrix is a square matrix.  Symmetric matrix: A symmetric matrix is a square 1. matrix in which the element a_ij is equal to a_ji for all i  and  j.  This  means  that  the  matrix  is  equal  to  its  transpose.  Diagonal  matrix:  A  diagonal  matrix  is  a  square 2. matrix in which all the non-diagonal elements are zero.  The diagonal elements can be any value, including zero.  3.  Identity matrix: An identity matrix is a special type of diagonal matrix in which all the diagonal elements are equal to one. It is denoted by I and has the property that when multiplied by any other matrix, it results in the same  matrix.  7   Matrix operations Addition and Subtraction:  the  To  add  or  subtract  two  matrices,  they have same  dimensions.  For example, if we have matrices A and B, sum  (A  +  B)  or  difference  (A  -  B)  is calculated  by  adding  or  subtracting corresponding elements.  1.  Multiplication:  Scalar  multiplication involves  multiplying  a  matrix  by  (a  single  number).  To  perform  must  their  a  scalar  scalar  multiplication, you simply multiply every element of the matrix by the scalar. For example, if A is a matrix and  k is a scalar, then kA is the result of multiplying every element of A by k.  Elementwise  multiplication  involves  multiplying  two  matrices  element  by  element.  To  perform elementwise  multiplication,  the  matrices  must  have  the  same  dimensions.  For  example,  if  A  and  B  are  matrices of the same dimensions, then A*B is the result of multiplying the corresponding elements of A and  B.  Matrix to matrix multiplication involves multiplying two matrices using a specific rule. To perform matrix to matrix multiplication, the number of columns in the first matrix must be equal to the number of  rows in the second matrix.  Inverse Matrix - Matrix A is called as inverse of matrix B, if and only if BA=AB=I.   Since AB=BA, both A and B need to be a square matrix. If A is inverse of B, we denote it as:  =  − 1  Inverse of a matrix A exists only if it’s determinant is nonzero.  columns.  𝐵𝐵  𝐴𝐴  Transpose: The transpose of a matrix A, denoted as A^T, is obtained by interchanging the rows and  Vector Space Model A  vector  space  contains  a  collection  of  objects  called  vectors  which  are  numerical  representations  of  words, sentence, and even documents. While a simple vector like map coordinates only has two dimensions, those used in natural language processing can have thousands.  The vector space model is an algebraic model that represents objects (like text) as vectors. This makes it easy to determine  the  similarity  between  words  or  the  relevance  between  a  search  query  and  document.  Cosine similarity is often used to determine similarity between vectors.  8    The vector space model uses linear algebra with non-binary term weights. This means the continuous degree of similarity  between  two  objects,  like  a  query  and  documents,  can  be  calculated  allowing  for  partial  matching (MarketMuse, 2023).  The vector space model is based on two main assumptions:      The relevance of a document to a query can be measured by the similarity between the vectors representing  the document and the query.  The importance of a term in a document is proportional to the frequency of the term in the document and  inversely proportional to the frequency of the term in the corpus.  To calculate the similarity between two vectors in the vector space model, several measures can be used, such as cosine similarity, Euclidean distance, or Jaccard similarity. These measures are used to rank the documents in order of relevance to the query.  The vector space model has many applications in information retrieval and text mining, such as search engines, document  classification,  and  clustering.  It  is  a  powerful  tool  for  representing  and  comparing  text  documents based on their content.  