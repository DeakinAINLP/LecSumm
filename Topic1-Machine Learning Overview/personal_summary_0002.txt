Machine learning allows computers to learn without being explicitly programmed. In supervised learning the training process involves exposure to data labelled with the expected outcome, and used to alter the policy generating the response so that the error is reduced. Some applications are cutting edge hardware such as A Eye for detecting the full light spectrum and labelling colours, and neuromorphic computer chips to mimic brain neurons. Board games such as Chess, Checkers, Go and Catan. Voice recognition and individual voice characteristic identification.Digit and handwriting recognition. Using machine learning for image segmentation or lidar interpretation for self driving cars is another example. Data acquisition can be the hardest part of machine learning. A lot of data is required and it must be cleaned to remove errors, outliers, excessive noise and missing results to avoid these affecting the outcomes. It should also be converted into an appropriate representation for analysis and stored appropriately (considering confidentiality, availability and integrity) Analysis can involve identifying relationships and correlations in the data, exploratory data analysis to identify its features and predictive machine learning. Analytics involves evaluating the performance Supervised learning uses paired input and output, and finds the target function mapping between them. This can be used for classification or regression. Target Function f: X→Y (this is the actual relationship) Examples: (xi, yi) (divided into training and testing, or more advanced models like k-fold cross validation which reduce bias) Hypothesis g: X → Y (this is estimated relationship) x is the attributes (input) and y is the output (eg. labels for classification or real numbers for regression) Unsupervised learning finds clusters between inputs (without having labelled outputs). KNN is one technique. This may allow it to learn from a massive amount of data, as the preparation and labelling step which limits supervised learning data sets is not required. Reinforcement learning makes decisions based on inputs and rewards them based on the (delayed) outcome. Early examples are back propagation of neural networks. Model evaluation involves estimating the performance of the model on unseen data. Selecting the best model can be done by evaluating several models, comparing their performance. Avoiding overfitting is important, as if the model learns the details in the training data too precisely it will not be able to generalise with unseen data.  This might be more common when training on small datasets, or if cross validation does not occur. Vector operations – transpose (switches orientation along diagonal), add or subtract (element wise), inner product, magnitude (2-norm) Distance between vectors Manhattan (distance based on sum of differences along each axis – L1 norm) Euclidean – (sqrt of sum of squared distance – L2 norm) Minkowski distance – special cases are order 1 (Manhattan), 2 (Euclidean) or inf (Chebyshev) Cosine distance (1 - angular difference between vectors) = 1 – ( (A.B) / ||A|| ||B|| ) Jaccard distance – similarity between sets = | A n B| / | A U B| Matrix algebra Addition and subtraction are element wise, as are scalar multiplication and division Transposition flips on diagonal. A symmetric matrix is the same as its transpose A = A ^ T Matrix multiplication across rows and down columns, and is not commutative (order matters, except for the special case of a square matrix) A diagonal matrix only has values along the diagonal and is square. Identity has 1 along the diagonal. A matrix is inverse if AB = BA = I, then A = B ^ -1 A matrix is orthogonal if its transpose equals its inverse A ^ T = A ^ -1, which occurs for identity and rotation matrices. A determinant for 2x2 is ad – bc, and for higher dimensions uses a pattern Only an invertable matrix has a non-zero value Eigenvalues are the scaling applied by a rotation to eigenvectors, which are vectors which are not rotated off their span. They obey Ax = λx. An eigenbasis is a basis in which every vector is an eigenvector. They are found by setting the determinant of A-Iλ and using row reduction to find the factors for each column, which are the eigenvalues. Substituting these into the original equation and solving finds the eigenvalues. 