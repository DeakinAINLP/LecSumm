1.4  Machine learning: a form of science where we train computers to learn and adapt to data by improving its performance over time. Eg: recognising patterns or making decisions. It is utilised in several areas such as filtering of spam texts, recommendation systems, self-driving vehicles and medical diagnosis. It has the main purpose of helping us humans to evolve to the next stage by automating mundane tasks, solving complex problems and improving our quality of life.  Another definition of ML that I found very useful is a field of study that involves developing algorithms and models that can learn from data, identify patterns, and make predictions or decisions without being explicitly programmed. In other words, it is a way of teaching computers to learn from experience and improve their performance over time, without being explicitly programmed for each specific task.  1.5  Precision Agriculture: Machine learning can analyse data from soil sensors, drones, and satellites to help farmers make better decisions about crop management, such as optimal planting times, fertilizer application, and pest control.  Mental Health Monitoring: Machine learning can also be used to analyse speech patterns, facial expressions, or text data to detect early signs of mental health issues such as depression or anxiety, potentially leading to timely interventions.  1.6:  Data Manipulation:  - Collect Data: Gather relevant data for the problem you want to solve.  - Prepare Data: Clean, pre-process, and transform the data to remove inconsistencies, missing values, and irrelevant information.  -  Feature Engineering: Create new features or modify existing ones to better represent the problem.  -  Split Data: Divide the data into training, validation, and testing sets.  Analytics:  -  Select a Model: Choose an appropriate machine learning algorithm based on the problem type, data characteristics, and desired outcomes.  -  Train the Model: Feed the training data into the chosen algorithm and adjust the model's parameters to minimize the error between the model's predictions and the actual target values.  -  Tune the Model: Use the validation set to fine-tune the model's parameters and optimize its performance using techniques like cross-validation, grid search, or random search.  Evaluation/Visualization:   -  Evaluate the Model: Assess the model's performance using the testing set and appropriate evaluation metrics, such as accuracy, precision, recall, or F1 score.  -  Interpret the Results: Understand and communicate the model's predictions, considering any limitations, biases, or uncertainties.  -  Visualize the Results: Use visual tools like plots, charts, or graphs to help better understand the model's performance, feature importance, or relationships between variables.  -  Deploy the Model: Integrate the trained model into a real-world application and monitor its performance.  1.7  A real-life example of machine learning that uses supervised learning is image classification. Image classification involves training a machine learning model to identify different objects or patterns in images, such as cats, dogs, cars, and buildings. The model is trained using a labelled dataset of images, where each image is identified with the correct object or pattern that it contains. The supervised learning algorithm then learns to associate certain features or patterns in the images with their corresponding labels, allowing it to accurately classify new, unseen images. This technology is used in a wide range of applications, such as facial recognition, self-driving cars, and medical imaging. For example, in the medical field, supervised learning algorithms can be trained to identify certain types of tumours or other anomalies in medical images, assisting doctors in making accurate diagnoses.  1.8  An example of unsupervised learning is clustering, which involves grouping similar objects or data points together based on their characteristics or features. One application of clustering is customer segmentation in marketing, where customers are grouped together based on their shared characteristics and preferences.  For example, a company might use unsupervised learning to segment its customers based on their purchasing behaviour, demographic information, or website browsing activity. The machine learning algorithm would analyse the data and group customers into different segments or clusters based on their similarities, without any pre-existing labels or categories.  This is an example of unsupervised learning because there are no pre-defined categories or labels for the algorithm to use. The algorithm is tasked with finding patterns and similarities in the data on its own, without any explicit guidance or feedback from a human. In contrast, supervised learning algorithms rely on labelled data to learn how to make predictions or classifications. In this case, there are no labels or pre-defined categories for the algorithm to use, so it must use unsupervised learning techniques to find patterns and group similar data points together.   1.9  Reinforcement learning is used in a wide range of applications beyond games. One example is robotics, where reinforcement learning is used to train robots to perform complex tasks and learn from their mistakes in real-time.  For example, a robot might be trained using reinforcement learning to navigate through a maze or to grasp and move objects in a cluttered environment. The robot would receive feedback in the form of rewards or penalties based on its actions, and it would adjust its behaviour over time to maximize its rewards and minimize its penalties.  Reinforcement learning is well-suited to robotics because it allows the robot to learn from experience and improve its behaviour over time, without the need for explicit programming for each specific task or scenario. This is important because robots often operate in complex and unpredictable environments where it is difficult to anticipate all possible scenarios in advance.  1.10  Model evaluation in machine learning is the process of measuring the performance of a trained model on new, unseen data. The goal of model evaluation is to determine how well the model generalizes to new data and how accurate its predictions are.  Training Dataset: A set of labelled data used to train the model by adjusting its parameters and learning from the data.  Validation Dataset: A set of labelled data used to evaluate the performance of the model during training and tune its parameters to avoid overfitting.  Test Dataset: A set of labelled data used to evaluate the final performance of the model after training and validation, and to get an accurate estimate of how well it will perform on new, unseen data in the real world.  The main differences between these datasets are their purpose and usage. The training dataset is used to train the model, the validation dataset is used to tune the model and avoid overfitting, and the test dataset is used to evaluate the final performance of the model on new, unseen data. By splitting the data in this way, we can ensure that our models are accurate and effective at making predictions in the real world.  1.13  We have 3 vectors as follows:  A = 1,5,2,9  B = 5,8,2,9  C = 3,0,2,6  Q: Compute the relation between Distance(A,B) and Distance(A,C) using multiple distance measures outlined in the linked article.   1.14  Q: Discuss what I learnt about Eigenvalues and Eigenvalues    Eigenvectors: An eigenvector of a square matrix is a non-zero vector that, when multiplied by the matrix, yields a scalar multiple of the original vector. In other words, if A is a square matrix and v is an eigenvector of A with eigenvalue λ, then Av = λv. Eigenvectors are often used to identify the principal components or directions of variation in a dataset.    Eigenvalues: An eigenvalue of a square matrix is a scalar that represents the amount by  which an eigenvector of the matrix is scaled when multiplied by the matrix. In other words, if A is a square matrix and v is an eigenvector of A with eigenvalue λ, then Av = λv. Eigenvalues are often used to measure the amount of variation in a dataset that is captured by the corresponding eigenvectors.  Q: Programmatically modify an image (3Blue1Brown Chapter 9)  I have decided to use DALL.E which is an artificial intelligence program developed by OpenAI that can generate images from textual descriptions. Here is an example:  To modify this image, I can upload this auto generated one back to DALL.E which will then produce slight variations of it. Here is an example of that:   1.15:  The vector space model is a commonly used method to represent a set of documents as vectors in information retrieval operations. This method is used in a variety of applications, ranging from simple query problems to real search engines. In this report, we will describe the steps involved in using the vector space model to compare the similarity between two documents.  Vocabulary:  Before representing the documents as vectors, we need to create a vocabulary of unique words from both documents. This vocabulary consists of the following terms:   document   vector space model   information retrieval   query   search engines   text data representation   feature vectors   Euclidean distance    closeness  Text Data Representation:  After creating the vocabulary, we create a feature vector for each document by counting the occurrences of each word in the vocabulary. For example, consider the following two documents:  Document 1: "Scientists discover new species of dinosaur in South America”.  Document 2: "New dinosaur species found in South America by scientists”.  We can represent these documents as feature vectors using the vocabulary we created earlier:  Feature vector for Document 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]  Feature vector for Document 2: [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]  Calculating Euclidean Distance:  To compare the similarity between these documents, we can calculate the Euclidean distance between their feature vectors. The Euclidean distance is a measure of the distance between two points in n-dimensional space. In our case, the feature vectors represent points in a 10-dimensional space. The smaller the distance, the more similar the documents are:   Based on a Euclidean distance of 1.732, we can conclude that the two documents are similar as they share a majority of the same words in their respective feature vectors.  1.22:  Scipy is a package in Python that provides functions for scientific computing and numerical analysis. It is used in fields such as physics, engineering, and data science where numerical computations are required. Scipy is helpful for solving complex problems, such as optimization and linear algebra, and has a wide range of applications in areas such as image analysis, machine learning, simulation, and data visualization. Scipy can be used with other Python packages, such as NumPy, Pandas, and Matplotlib, to create powerful and flexible data analysis workflows.  