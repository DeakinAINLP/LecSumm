 Learning Summary KNN and its variants has a useful technique where weights can be assigned for the contribution of data point neighbors this leads to nearer neighbors contributing more to the average instead of the distant ones. This gets useful for both regression as well as classification. Within this, the basic concept is labelling the test data point the same as nearest neighbor (NN).  KNN Theory: For getting majority of decisions on the basis of close training points one needs to perform average mean for continuous cases as well as find mode of class labels in discrete format. Another concept which partially relates to KNN which is known as Voronoi Diagram, a diagram that partitions the plane into regions on the basis of distance points in a specific subset of the plane.  It can be seen from the diagram above, it’s based on the closest neighbors, which is the same concept as KNN. The data is clearly not linearly separable as well which results in complex boundaries and decision rules. One should remember that decision surface gets formed on the basis of training examples.  Shepard’s Method: This is a method where different weights are to the neighbors on the basis of their distance from their test points. For instance, weight might be inversely squared to its distances. Meaning the higher the distance of the neighbor, the lower its weight is. Hence, all training points might influence a particular instance. This is known as Shepard’s Method.  Finding Best K value: When a K value is small, the restraining of region for a given prediction takes place where we are forcing the classifier to be get more focused on close neighbors and regions. Where are asking the classifier of not caring regarding its fairly distant points resulting in low bias and high variance.  High K values have smooth decision boundaries meaning low variance but increased bias this means that high K value asks for more information even from distant training points.  Decision Trees: It’s a map comprising of possible results of a series of related choices. It can be used for weighing possible actions against one another on the basis of their costs, probabilities and benefits. It starts with a single root node that has branches of potential outcomes.  Regression Trees: Decision Trees which use regression  model are known as regression trees.  On  an alternative basis we can fit a classification model, which makes them as a classification tree. On a usual basis extremely simple models like majority (classification) or mean (regression) are used generally.  Classification Trees: They  are quite  similar to regression  trees which  are used for  the prediction  of qualitative response instead of a quantitative response. Within a classification tree each test instance gets   assigned to majority class (model) in the region where it belongs, this action can be considered as being like a data point that is voting for itself within the region resulting in selecting the majority. Within the classification setting the sum of square error gets replaced with classification error rate as a criteria for making binary splits.  Gini and Entropy: The gini is the most common measurement that is used for inequality. This is defined  represent the fraction of training instance within the j-th as: region which is from k-th class which is a measure of total variance across the K classes. As low inequality distribution in terms of votes takes place. Gini index therefore gets considered as a measure of node purity.  Where  On the contrary, Entropy has the same definition as that of Gini but a different formula, which is as follows:  Decision Tree Algorithms: The most popular algorithms are:  1.  ID3  (Iterative  Dichotomiser  3)  uses  Entropy:  This  algorithm  generates  decision  trees  from datasets. A simple yet effective algorithm. Where first the entropy is calculated for every  feature that uses the dataset S. They are split into subsets with the use of feature for which entropy is minimum. The less the entropy value meaning a good choice of selection of attribute or feature that will be gained for more information. In the second step, the decision tree node is made which comprises the feature, and in the third step recursing the subsets using the remaining features.  2.  C4.5 (Successor of ID3) more advanced version of ID3; uses entropy 3.  CART (Classification and Regression Tree)  uses Gini Impurity  Tree Depth: If a tree is built which very deep the partitioning of feature space is taking place into small regions. If the tree is very deep one should expect low chances of visiting many training points within that sub-regions, meaning all estimations within the region are not good due to high variance.  Pruning: A technique which reduces the decision tree’s size by removing sections of tree which provides little power for classifying instances. There are two ways of Pruning trees:  1.  Pre-Pruning: Within this process one decides within the building process when nodes are to be stopped being added. With the use of this method the elimination of unnecessary complexity of the model, which could be problematic as sometimes attributed individually don ’t contribute much to a decision but if combined it can have a significant impact.  2.  Post-Pruning: Within this process it waits until the full decision tree get built and prunes the attributes by sub tree replacement. It goes for sub trees and checks whether they can be replaces with a single node or feature while incurring only a small amount of change within the entropy. If this is the case the tree is trimmed, if not the sub tree is kept as it might have useful information.     Pros and Cons of Decision Trees:  Pros: 1. Easy to understand, as the representation of rules takes place  2.  Capable of modelling non-linear functions.  3. Ability of handling categorical variables.  Cons: 1. Sensitive to small changes within the data.  2. Overfitting might be easy, when deep decision trees are built and might also have a high variance model.  3. Trees might not be very competitive when accuracy is concerned as some of the other regression and classification techniques like neural networks or SVM.  Impact of Distance Metrics on KNN Performance: KNN effectiveness can be significantly impacted by the selection of  distance metrics.  Euclidean, cosine  similarity and  Manhattan Distance  are some  of  the distance metrics that are used on a frequent basis when working with KNN. It is said that cosine similarity is a better option than Euclidean distance when working with High-dimensional data.  Feature Importance of using Decision Tree: Feature importance can be used for the identification of most relevant features for classification and feature selection for the improvement of model performance.  