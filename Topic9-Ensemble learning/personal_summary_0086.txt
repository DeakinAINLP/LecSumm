   A common classification approach called K-nearest neighbours (KNN) chooses the class of a new data point based on the consensus of its k nearest neighbours in the feature space. There are two types of KNN: weighted KNN, which gives neighbours weights depending on how close they are, and distance-weighted KNN, which gives neighbours weights inversely proportionate to how far apart they are. KNN is a non- parametric method, which means it makes no assumptions about the distribution of the underlying data. Although it is simple to comprehend and put into practise, it can be computationally costly for big datasets. It is important to pick the right value for k since it affects the bias-variance tradeoff of the model.    Similar examples are likely to have similar class labels, according to the K-nearest  neighbours (KNN) method. The class labels of a new data point's k closest neighbours in the feature space are taken into account by the algorithm when categorising it. The value of k determines the bias and variance of the model as well as the decision boundary of the algorithm. A smaller number of k results in a more complicated decision boundary, which may overfit the data yet captures local trends. A smoother decision border results from a higher number of k, although it may miss local patterns. The ideal k value is selected with the use of cross-validation.    With each area denoting a class label, decision trees are supervised learning models that divide the feature space into regions based on the input data. Internal nodes stand in for feature tests, while leaf nodes represent class labels in the tree structure. Decision trees can handle category and numerical data and are simple to comprehend. Regression and multi-class classification issues can also be handled by them. When building decision trees, the optimal characteristics to split on are chosen based on metrics like Gini impurity or entropy. Pruning techniques are employed to reduce model complexity since decision trees are prone to overfitting.    Decision tree variants known as regression trees are used to forecast continuous numerical values rather than class labels. In a regression tree, each leaf node represents a projected numerical value, and each interior node represents a feature test. Regression trees strive to reduce the variance of the predicted values within each leaf node by recursively partitioning the feature space depending on the input data. The optimal feature to divide on is chosen using splitting criteria like the sum of squared errors or the variance reduction. Techniques for pruning can be used to reduce overfitting and boost generalisation.    Decision tree methods assess a node's purity using the Gini impurity and entropy  measurements. Entropy represents the degree of disorder or uncertainty in a node's class distribution, whereas Gini impurity reflects the likelihood of erroneously categorising a randomly selected element from a node. Where a node comprises mostly one class, lower Gini impurity or entropy values imply more purity. When building decision trees, these metrics are used to choose the optimal feature to split on. Both entropy and Gini impurity are often used, however entropy can be computed more quickly.      An information gain-based decision tree method called the ID3 algorithm chooses the optimum feature for splitting. By maximising the difference between parent node impurity and child node impurity, the tree is constructed iteratively. However, ID3 favours characteristics that have a variety of consequences. To avoid overfitting, the tree's depth is constrained using the tree depth method. When the maximum depth is reached or no further impurity improvement is made, it stops dividing nodes. Pruning strategies like cost complexity pruning assist to simplify the tree while preserving accuracy. Controlling the model complexity is essential to prevent overfitting.    The level of intricacy and adaptability of a machine learning model is referred to as model complexity. Decision tree depth, the number of hidden layers in neural networks, and the quantity of features all have an impact on it. A complicated model with several parameters is readily prone to overfitting the training data, which captures noise and results in poor generalisation to new data. A decision tree is pruned to remove pointless branches or nodes as a method of reducing model complexity. Pruning stops overfitting and makes the model more adaptable to fresh data.       