Non Linear Models (KNN and DT)  K-nearest neighbors (KNN) algorithm and its variants, highlighting its usefulness in both classification and regression tasks. The algorithm assigns weights to neighboring data points, with closer neighbors having a greater influence on the average. By considering the majority label of the K-nearest neighbors, the KNN algorithm effectively classifies test data points based on their proximity to existing training points.  The best number of neighbors (K) in K-nearest neighbors (KNN) refers to finding the optimal value for K. Selecting a small K makes the algorithm sensitive to noise and outliers, leading to overfitting. A large K smoothens decision boundaries, potentially causing misclassification. Determining the best K can be done through techniques like cross-validation or grid search, striking a balance between overfitting and underfitting for effective generalization to new data.  Decision trees are supervised learning models that represent a flowchart-like structure, where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or a decision. They are primarily used for classification tasks but can also be used for regression.  Regression trees, on the other hand, are a specific type of decision tree used for regression problems. Instead of predicting class labels, regression trees predict continuous numeric values. The structure of a regression tree is similar to a decision tree, with the internal nodes representing attribute tests and the leaf nodes representing predicted numeric values.  Classification trees employ a tree-like structure where internal nodes represent attribute tests, branches represent possible outcomes of the tests, and leaf nodes correspond to class labels. Gini impurity and entropy are commonly used criteria to measure the impurity or disorder of a node in a classification tree. These metrics guide the tree-building process by helping determine the attribute and split point that minimize impurity and maximize the separation of classes within each node. Gini impurity is a measure of how often a randomly chosen element in a node would be incorrectly labeled if it were randomly labeled according to the distribution of classes in the node. It ranges from 0 to 1, with 0 indicating pure nodes where all elements belong to the same class and 1 indicating equally distributed classes. Entropy, on the other hand, is a measure of the node's impurity based on information theory. It quantifies the average amount of information needed to classify an element from the node correctly.  Similar to Gini impurity, entropy ranges from 0 to 1, with 0 indicating pure nodes and 1 indicating maximum impurity.  During the tree construction process, the algorithm evaluates different attributes and split points and calculates the Gini impurity or entropy for each potential split. It selects the attribute and split point that minimize impurity or maximize the information gain, which is the difference between the impurity of the parent node and the weighted impurities of the child nodes resulting from the split.  The ID3 (Iterative Dichotomiser 3) algorithm is a popular decision tree learning algorithm used for classification tasks. It was developed by Ross Quinlan and is based on the concept of information gain. The ID3 algorithm follows a top-down, greedy approach to construct a decision tree. It starts with the entire training dataset and recursively splits the data based on the attribute that provides the highest information gain. Information gain measures the reduction in entropy or the increase in information purity achieved by splitting the data on a particular attribute. At each iteration, the ID3 algorithm selects the attribute that maximizes the information gain and creates a new internal node for that attribute. The process continues until all instances in a node belong to the same class, or there are no more attributes left to split on. In the latter case, the algorithm assigns the most common class of the remaining instances to the leaf node.  Pre-pruning and post-pruning are methods employed to combat overfitting in decision trees. Pre-pruning stops tree growth based on conditions such as maximum depth, minimum sample size, or minimum information gain. Post-pruning involves building the full tree and then selectively removing or merging nodes based on validation set performance. These techniques enhance generalization and prevent overfitting in decision trees.  Advantages of using decision trees:  ● Interpretability: Decision trees offer a clear and intuitive representation, making them easily  understandable.  ● Handling Mixed Data: Decision trees can handle both categorical and numerical features,  accommodating mixed data types.  ● Robustness to Outliers: Decision trees are robust to outliers as they partition the feature space  based on relative order rather than relying on mean or variance.  Disadvantages of using decision trees:  ● Overfitting: Decision trees tend to overfit, especially with complex or deep structures, requiring  techniques like pruning to mitigate this.  ● Instability: Small changes in the training data can lead to different tree structures, affecting  reproducibility.  ● Difficulty in Capturing Complex Relationships: Decision trees may struggle with capturing  complex relationships involving multiple variables or non-rectangular decision boundaries.  