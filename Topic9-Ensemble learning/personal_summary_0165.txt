In  topic  8,  we  primarily  focused  on  the  k-Nearest  Neighbor  and  Decision  Tree Machine Learning (ML) algorithms, their working principles and the various aspects associated with them.  K-Nearest Neighbor Algorithm  It works on the principle of distance to the nearest surrounding neighbors of the test data point. For example, consider a data point in the test dataset called ‘x’ that can be classified as either 0 or 1. If the data point closest to x is classified as 1, then x would also be classified as 1, else it would be classified as 0.  The above process is true when k value is 1. When k value is varied to 3, the test point  ‘x’  checks  the  closest  3  neighbors  from  it.  Among  these  3,  suppose  2  are classified as 1 and 1 is classified as 0, then the point ‘x’ will be classified as 1 since majority of closest neighbors are classified as 1. We repeat this process iteratively to predict all test data points.  Some of the distance being considered are:  1.  Euclidean distance 2.  City block distance 3.  Cosine distance  In general, we are assigning weights to the neighbors based on their distance from the test point. This means the higher the distance of the neighbor, the lower its weight. All training points may influence a particular instance. This method is also known as Shepard’s method.  We must also be cognizant of the misclassification tendency in K-NN algorithm. We choose the optimal value of k that provides the least misclassification error.  Decision Tree Algorithm     In  this  ML  algorithm,  the  parent  node  is  the  original  dataset.  Thereafter,  we recursively  divide  and  sub-divide  into  child  nodes  based  on  features  with  the highest separation capability. This process continues until certain criteria is met like the max_depth parameter or the min. Samples parameter.  Decision trees can be of two types:  1.  Regressor Trees – for regression problems 2.  Classification Trees – for classification problems  We  were also  introduced  to  the  concepts  of  Gini  and  Entropy,  and  learnt about other Decision tree algorithms such as ID3, C4.5 and CART algorithms.  Inherently,  Decision  Trees  tend  to  overfit  the  data  and  hence  may  lead  to  high variance values. To avoid this, we may perform pruning which refers to removing sections of the Decision Tree that contribute very little to the classification process. We  also  learnt  about  pre-pruning(forward  pruning)  and  post-pruning(backward pruning).  Lastly, we learnt about the various advantages and disadvantages of Decision Tree algorithms and were given pointers about advance topics in this domain of K-NN and DT.          