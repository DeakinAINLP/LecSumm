Nonlinear Models (KNN and DT)  K-Nearest Neighbour (KNN) It operates on the principle of calcula;ng the distance between data points and selec;ng the "K" nearest data points to determine the class or value of a new, previously unseen data point.  Overview:    Non-parametric: Doesn't make any assump;ons about the underlying data distribution. Instance-based learning: Memorizes the training dataset instead of learning a discrimina;ve func;on. Lazy learning: No explicit training phase; computa;on happens during the predic;on phase.    Distance metric: Uses a distance metric (e.g., Euclidean, ManhaTan, or Minkowski) to calculate the similarity between data points.    K value: User-deﬁned parameter to determine the number of nearest neighbours considered for classiﬁca;on or regression.    Sensi;ve to feature scaling: Requires feature scaling to prevent features with larger ranges from domina;ng distance calcula;ons.  Ac4vity Can we use other distance metrics in KNN? Yes you can, for example, Euclidean, ManhaTan, Minkowski, Hamming, Cosine similarity, Jaccard  Best number of neighbours No universally op;mal number of neighbours (k) for KNN, as the best value depends on the speciﬁc problem and dataset. However, there are some guidelines and methods to help you ﬁnd a suitable k value:  1.  Rule of thumb: A common rule of thumb is to choose k as the square root of the number of instances in the dataset. However, this is a rough es;ma;on and may not be the best choice for every situa;on.  2.  Cross-valida;on: To ﬁnd the best k value, you can perform k-fold cross-valida;on. In this process, you split the dataset into k equal parts, train the model on k-1 parts, and validate it on the remaining part. You repeat this process for diﬀerent k values and choose the one that yields the lowest error or the highest accuracy.  3.  Trade-oﬀ between bias and variance: As the value of k increases, the decision boundary becomes smoother and less ﬂexible, reducing the variance but increasing the bias. Conversely, a smaller k value results in a more ﬂexible decision boundary, increasing the variance but reducing the bias. Ideally, you want to ﬁnd a k value that balances the bias-variance trade-oﬀ to minimize the total error.  4.  Odd or even k: If you are working on a binary classiﬁca;on problem, it is generally recommended to choose an odd k value to avoid ;es (i.e., when an equal number of neighbours belong to both classes). However, if you have a mul;-class problem or have implemented a ;e-breaking strategy, you can consider even k values as well.  5.  Complexity and computa;onal cost: Smaller k values can result in a more complex model, which may be computa;onally expensive, especially for large datasets. On the other hand, larger k values may lead to a simpler model, which can be computa;onally more eﬃcient. Decision Tree Common supervised learning method for classiﬁca;on and regression tasks. They work by recursively splicng the dataset based on feature values, eventually crea;ng a tree-like structure with decision nodes and leaf nodes represen;ng class labels or target values.    Hierarchical structure: Decision Trees create a tree-like structure with internal nodes  represen;ng decisions and leaf nodes represen;ng class labels or target    Feature selec;on: Decision Trees use measures like Gini Impurity, Informa;on Gain, or Mean Squared Error to select the best feature to split the data.    Handling categorical and con;nuous features: Decision Trees can work with both categorical and con;nuous input features.   Interpretability: Decision Trees are easy to understand and visualize, making them more interpretable compared to other machine learning models.    Pruning: Overﬁcng can be addressed by pruning the tree, which involves removing some branches to generalize the model beTer.    Greedy algorithm: Decision Trees use a greedy algorithm, which means they make locally op;mal choices at each step, but this may not always lead to a globally op;mal tree.    Ensemble methods: Decision Trees are ofen used as base learners in ensemble methods like Random Forest and Gradient Boos;ng Machines to improve performance and reduce overﬁcng. 