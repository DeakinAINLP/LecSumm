 This topic introduced two new Supervised Learning algorithms – K nearest neighbours and Decision Trees.  K Nearest Neighbours (KNN)  The K Nearest Neighbours algorithm can be used for both regression and classification problems. The algorithm works by using the position or classification of a selected K number of neighbouring data points that are closest to the testing point to determine the result. In a classification example – if using the 5 nearest neighbours, with 2 being apples and 3 being bananas, the test data point would be assigned the classification of the majority of the neighbours – in this case a banana. For regression problems, the mean value of the K nearest neighbours becomes the predicted new data value.  Decision Trees  Decision Trees are another supervised learning technique which can be used for both regression and classification problems. A Decision Tree represents all the outcomes of a series of decisions.  Regression Trees predict a value for a data point by dividing the data down into segments by splitting on a threshold value and then using a calculation based on the data in that segment – eg by using the mean of the other data points which are in the same division of data. Splitting of data continues until a stop criteria is met – for example a maximum depth of the tree.  Classification Trees also divide data into segments in a tree branch like structure, but are instead used to provide a classification rather than a predicted value. To do this, the majority classification of the segment of data is assigned to a newly predicted data point.  A very basic example could be animals, divided down into mammals and reptiles, then the mammals could be divided further into trunk or no trunk – with the trunk category esulting in a classification of Elephant.  