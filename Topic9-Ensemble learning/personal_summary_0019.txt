KNN algorithms and its variants A useful classification and regression tool that labels the test data points to the nearest neighbor. To calculate which neighborhood the data point belongs to, a weight is assigned to each neighborhood. The weight is determined by the distance from the test point.  Best number of Ks The k in ‘k-nn’ represents the number of neighborhoods, for example 3-NN indicates that there are three neighborhoods. Picking the right number of neighborhoods is essential for accurate labelling. A small k value can mean low bias and high variance whereas a high k value can mean a high bias and a low variance. Cross validation to evaluate the model with different k values is the best way to deduce the most suitable k value.  Decision tree Used to map out possible outcomes of a consecutive related choices. Starts with a single root that branches down to different choices/outcomes. Three popular algorithms:  1.  ID3: generates a decision tree from a dataset. 2.  C4.5 3.  CART  Decision tree advantages:    Easy to understand as each node represents a choice and the result.   Can be used to monitor nonlinear functions.   Can map categorical variables.  Decision tree disadvantages:    Sensitive to changes in data. The addition of a few data points can alter the rules set.   Easily overfitted.   May not be as accurate as other models such as SVM.  Regression tree A regression tree is a decision tree that uses a regression model. The features space is divided into distinct non overlapping regions. When a data point falls in a certain region, it is assumed that it is part of that classifier. Classification tree: Used to predict qualitative responses.  Tree Depth The deeper the tree, the more feature spaces which can lead to high variance. A tree being too shallow can lead to high bias. Similar to what we have learnt the last few topics, it is important to find the best suited depth.  Model complexity and pruning Pruning: to reduce the size of the decision tree by removing sections of the tree that is a low contributing factor when classifying. There are two types of pruning:  1.  Pre-pruning: decide how many nodes are required based off the level of entropy reduction when selecting different features. The splitting of nodes can cease when the entropy reduction is insignificant.  2.  Post pruning: Prunes attributes by subtree replacement once the decision tree is fully constructed. The idea behind subtree replacement is to check each subtree and check if replacing it with a single leaf node will have a minimal impact on entropy. 