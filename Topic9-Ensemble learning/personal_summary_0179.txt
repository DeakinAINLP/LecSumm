Introduction  During topic eight we covered the following algorithms:   K-Nearest Neighbour (KNN)   Decision Tree (DT)  The K-Nearest Neighbour (KNN) algorithm and its variants  K-nearest neighbours (KNN): a supervised learning non parametric1 classifier algorithm, that predicts the groupings within data based on the proximity of the individual data points. The KNN algorithm can be used in both regression and classification problems, but it is most commonly used in a classification context.  The KNN algorithm works by identifying the nearest neighbours of an input data point, and then using those neighbours to assign a class label to the input data point.  1 An algorithm that not make any assumptions about the underlying form or distribution of the data it is operating on:  it chooses rather to learn patterns directly from the data itself (‘Nonparametric statistics’ 2022).  The distance to the nearest neighbours of the input data point thus needs calculation. The most popular distance measure is Euclidean2, however different distance measures can be used.  Of interest is that KNN is lazy: in that it doesn’t explicitly build a model during a training phase: rather it performs calculations at prediction time, meaning that KNN simply consumes and stores the entire dataset at training time for use when making predictions.  The value of K  The K value in the algorithm name denotes the number of neighbours will be checked to determine the classification of the input data point. If K = 1, then the first nearest point found will be used. Defining the correct value of K to use can be tricky, as it will affect the degree of ‘fit’ that will be achieved. If K is low, high variance and low bias might be found, with increased noise sensitivity. Conversely, if K is high, then low variance and high bias might be found.  A larger value of K will include more neighbours in the classification process.  Having an odd value for K will avoid having ties in the classification: cross-validation can help find the best value for K.  The Theory of KNN This is how the KNN algorithm works:  1. Training: The training dataset is ingested.  2. Distance Calculation: When a prediction is needed for a new unseen input data point, KNN calculates the distance between that new data point and all the instances in the training data set, using the selected distance metric.  3. K Nearest Neighbours: Based on the distance calculations, the K nearest neighbours to the  input data point are selected.  4. Classification: If a classification problem, KNN will use majority3 voting to give the input  data point a class label.  5. Average: If a regression problem, KNN will calculate the average of the K nearest neighbours, and then use that average as the predicted value of the input data point.  It is common to see decision boundaries visualised with Voronoi diagrams. A Voronoi diagram is one in which a plane is divided into regions based on proximity to a given set of points, termed “seeds”.  2 The length of a line directly drawn between the two points. 3 Although the term “majority” is used, it’s actually “plurality”: especially if there are more than two categories  found in the K-neighbours.  Selecting K: the number of neighbours  As is already mentioned, (the value of cross validation can help in evaluating the effectiveness of different K values. Here we would partition our data into a training and a test set. Once trained, we would then use the test set to evaluate the performance of training with different values of K.  K   ), choosing the correct value for K can be tricky. However,  Decision trees  Like KNN, a decision tree is a supervised learning non parametric method that can be used for both classification and regression problems. A decision tree is made up of a series of nodes and branches. By starting at the root node and travelling down the branches of the tree, you will arrive at a prediction.  Decision tree for a cocktail party: source (Magee 1964)  From the above diagram we can see that a decision tree is a flow chart like structure, with each internal node representing a decision based on a feature, each branch a feature value, and each leaf node a the final prediction.  To construct a decision tree we:  1. Select the basis feature: evaluate the features in the training dataset to see which one is the  most informative. We select that one as the basis for the tree.  2. Splitting: Then we split the dataset into subsets based on our selected features values. Each  subset will be either a specific branch or child node of the decision tree.  3. Recursive splitting: The splitting is repeated amongst the subsets until stop conditions are met. Those conditions might be tree depth, number of instances in the node or a chosen metric threshold.  4. Leaf nodes and the predictions: When the splitting is finally ended, the terminal nodes of the resultant tree represents either a predicted class label (classification) or class value (regression)  Decision trees have several advantages, including being interpretable, the ability to handle both numerical and categorical features, and robustness to outliers. But they can also be prone to over fitting, especially if the tree becomes too deep or complex. Techniques like pruning and regularisation can be applied to mitigate over-fitting and improve generalisation performance.  Classification trees  Classification And Regression Trees (CART) are a type of decision tree algorithm that can be used for both classification and regression tasks. CART trees use a recursive binary splitting approach, with each internal node representing a binary decision based on a feature, and each leaf node providing the final prediction (Hoare 2017). Here are the key steps when constructing a CART tree:  1. Binary splitting: Binary splits are done at each internal node. This means that each node will have at two child nodes, based on a threshold or condition of the selected feature.  2. Splitting criteria: CART trees use different splitting criteria dependent on what they are being used for. For classification, the Gini index or entropy are commonly used. For regression, the means squared error or mean absolute error are commonly used as the splitting criteria.  3. Recursive splitting: The splitting is repeated amongst the subsets until stop conditions are met. Those conditions might be tree depth, number of instances in the node or a chosen metric threshold. This creates a hierarchical structure.  4. Leaf nodes and the predictions: When the splitting is finally ended, the terminal nodes of the resultant tree represents either a predicted class label (classification) or class value (regression). Prediction (classification) is typically done via a “majority” vote. For regression the average value of the instances within the node are used.  CART trees have the same advantages and disadvantages as Decision trees, with the additional advantage that they can be used for both numerical and categorical features.  Decision Tree Algorithms  Decision trees can be based on a variety of algorithms. Listed on Wikipedia(‘Decision tree learning’ 2023) are:  ID3 (Iterative Dichotomiser 3)   C4.5 (successor to ID3)   CART   Chi-square automatic interaction detection (CHAID)   MARS   Conditional Inference Trees  Iterative Dichotomiser 3 (ID3)  An ID3 tree is primarily used for classification. The tree is built by recursively partitioning the data based on entropy. The key steps when constructing an ID3 tree are:  1. Entropy calculation: Calculate the entropy of every feature on the input data set.  2. Splitting criteria: Then we split the input data set into subsets using the feature for which  the entropy is at a minimum.  3. Node creation: Add a node based on that attribute.  4. Recurse: Repeat the process of splitting and creating nodes until the tree is created.  As can be seen from the construction steps, ID3 is a top down greedy algorithm that is making locally optimal decisions without considering what might be the global best solution.  Tree depth  If the decision tree is deep4 then the training data has been partitioned into a large number of regions. Hence the variance is likely to be high.  Variance refers to the the models sensitivity to fluctuations in the data. A model with high variance may have matched to closely to the training data, and be unable to generalise to new data.  If the decision tree is shallow, then the training data has been partitioned into a small number of regions. Hence the bias is likely to be high.  Bias refers to the error introduced by the models assumptions. It captures the models ability to under-fit, or overlook patterns in the data. A model with high bias is likely to make faulty predictions.  To find the right depth for a decision tree, again, cross validation will be of use.  4 Depth is a measure of how many splits a tree must follow before coming to a prediction.  Tree pruning  If the decision tree is too deep, we can prune it to try and reduce the depth. We do this by identifying and removing the sections of the tree that are not classifying well. There are two types of pruning:      Pre-pruning: we prune the tree as we build it.  Post-pruning: we build the complete tree, then walk it and rework sections of it.  Page 6 of 8  Martin Paulo  SIT720 – Machine Learning  ID: 223587421  Decision Trees: the advantages and disadvantages  Advantages:   Good at capturing interactions between features in the data.   The data is grouped in ways that are easy to understand.   Can be easily be visualised by us humans.   There is no need to transform features.   Can easily handle categorical data.  Disadvantages:   They fail to deal with linear relationships   They are sensitive to changes in the data, and hence unstable.   They are not smooth. By example, if we built a decision tree that used house area to get  house valuations, and enter a 100 square meters – we get given a valuation of, say $1.2M. But we notice that we left out a bathroom of 8 square meters, and re-enter the area as 108 square meters: and the valuation suddenly jumps to $1.5M. This is not intuitive.(Molnar 2022)   They are not as accurate as other techniques.   The deeper a tree becomes, the more difficult it is for a human to understand it.  