Topic 8 summary In this topic mainly talk about KNN. 1.  KNN algorithm and its variants: The k-nearest neighbors (KNN) algorithm is a simple, non-parametric, supervised learning method used for classification and regression. The main idea is to predict the target class (or value) based on  the  majority  vote  of  the  K  nearest  neighbors.  Variants  of  KNN  include  weighted  KNN  and adaptive KNN.  2.  Theory of KNN: KNN (k-Nearest Neighbors) is a non-parametric, lazy learning algorithm used for classification and regression. It predicts the output based on the k closest training examples in the feature space. Variants include weighted KNN, where closer points have more influence, and adaptive KNN, where the value of k is adjusted depending on local point density.  3.  Best number of neighbors (K): The optimal value of K is usually determined through cross-validation, considering the trade-off between overfitting (low K) and underfitting (high K). A commonly used heuristic is to set K to the square root of the total number of instances.  4.  Decision trees: Decision  trees  are  hierarchical  structures  used  for  classification  and  regression  tasks.  They recursively  split  the  input  space  into  regions,  based  on  the  feature  values,  to  form  a  tree-like structure. The leaf nodes represent predictions.  5.  Regression trees: Regression trees are a type of decision tree used for predicting continuous-valued outputs. The predicted  value  for  a  given  input  is  the  average  target  value  of  the  instances  within  the corresponding leaf node.  6.  Classification trees: Classification trees predict categorical target values by selecting the majority class of instances in the same leaf node.  7.  Decision tree algorithms: Common algorithms to construct decision trees include ID3, C4.5, and CART. They use different splitting  criteria  (e.g.,  information  gain,  Gini  impurity)  and  vary  in  handling  missing  data  and pruning strategies.  8.  Model complexity and pruning: Overly complex trees can lead to overfitting, while overly simple trees can result in underfitting. Pruning techniques, such as reduced error pruning and cost-complexity pruning, help control the complexity and improve generalization.  9.  Decision trees advantages and disadvantages:  Advantages include interpretability, handling of missing data, and ability to handle both continuous and categorical features. Disadvantages encompass susceptibility to overfitting, instability due to small changes in data, and inability to model complex relationships. Disadvantages:  prone  to  overfitting,  sensitive  to  data  changes,  and  may  struggle  with  XOR-like problems or linearly inseparable data.  10.  Impact of distance metrics on KNN performance: Different  distance  metrics  can  yield  different  KNN  model  performances,  depending  on  the underlying data distribution and feature scales. It's essential to choose an appropriate metric based on the problem domain and characteristics of the data.  11.  Feature importance using Decision Trees: Decision  trees  can  be  used  to  measure  feature  importance  by  calculating  the  sum  of  the improvement in splitting criteria (e.g., information gain) for each feature across all nodes in the tree. This can help identify the most relevant features for a specific problem.  