KNN algorithm It simply classifies objects based on feature similarity: Find k individuals in our data “most similar” to the new individual in terms of predictor values. The algorithm determines the class or value of an unseen example by finding the k nearest neighbors to that example in the training dataset and taking a majority vote (for classification) or averaging (for regression) of their labels or values. This method is commonly used in pattern recognition and data mining.  Pros    Simple and easy to understand.   No explicit training phase, making it  fast for new instances and adaptable to incremental learning.  Cons   Computational complexity increases  with larger datasets.    Sensitivity to feature scaling, requiring  normalization.    Non-parametric, suitable for various    Performance get worse in high-  data distributions.  dimensional spaces.    Flexible, handling different types of    Choosing the optimal value of k can be  data and multiclass problems effectively.  challenging.    Sensitive to imbalanced datasets, potentially leading to biased predictions.  Decision tree algorithm Three main parts of the tree: Root node, intermediate node, and Leaf node. There are two measures help to determine the splitting criteria for creating branches in the tree: entropy and Gini. Entropy is used to measure the information gain when a dataset is split based on a particular feature while Gini impurity is used to measure the impurity reduction when a dataset is split based on a specific feature. Three popular algorithms for decision trees: ID3 (use Entropy), C4.5 (advanced version of ID3 and use Entropy), and CART (use Gini impurity)    Can handle nonlinear functions   Can handle categorical features   Simple to understand and to interpret    Prone to overfitting easily, specially with deep trees or small datasets.   Sensitive to small changes in the  training data.    Requires preprocessing for handling  missing values and outliers.  