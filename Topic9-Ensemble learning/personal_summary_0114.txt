 Topic 8 - Nonlinear models (KNN and DT) - Evernote  Topic 8 - Nonlinear models (KNN and DT)  KNN algorithm and its variants.  The basic idea is to label the test data point the same as the nearest neighbour (NN).  Voronoi diagram : It is a partitioning of a plane into regions based on distance to points in a specific  subset of the plane.  Distance - weighted nearest neighbour algorithm : In this we assign weights to the neighbours based on  their distance from the test point. This means the higher the distance of the neighbour, the lower its  weight. All training points may influence a particular instance. This method is also known as Shepard's  method.  Shepard's method - We assign different weights to distances of data points based on their distance from  the test point. Higher the distance of the neighbour, lower its weight.  Best number of neighbours (K)  Small value of K result in a low bias and high variance.  Higher values of K will have smoother decision boundaries which means lower variance but increased bias.  Hyper - parameter K can be computed by using cross validation for every possible number K and evaluating the model based on the training and test data you have partitioned.  Decision trees Its a map of the possible outcomes of a series of related choices. It can be used to weigh possible actions  against one another based on their costs, benefits and probabilities.  Regression trees  Decision trees that use a regression model are called regression trees.  Classification trees Decision trees that can be used for classification are called classification trees.  It is used to predict a qualitative response. Here we assign each test instance to the majority class of the training instances in the region where it belongs.  Certainty of Distribution It shows how certain it is that a classifier sits inside a region. A CoD value close to 1 means all of the  training points inside a region are voting for a certain class label.  The classifier in this case is certain about the prediction. When CoD is 0.5, it means we cannot trust the votes because there is a high classification error rate.  Decision tree algorithms  ID3 (Iterative Dichotomiser 3) C4.5 (Successor of ID3)  https://www.evernote.com/client/web?login=true#?n=2b506e6c-98c8-977f-634c-c44e8afde1e3&  1/3   (  )  Topic 8 - Nonlinear models (KNN and DT) - Evernote  CART (Classification and Regression Tree)  ID3 Algorithm.  Its used to generate a decision tree from a dataset.  Calculate the entropy of every feature using the dataset. Split the dataset into subsets using the  feature for which the entropy is minimum.  Make a decision tree node containing the feature  Recurse on subsets using remaining features.  Tree depth  A very deep tree would cause high variance and a very shallow tree will result in high bias.  Pruning  It is a technique that reduces the size of decision trees by removing sections of tree that provide little  power to classify instances.  Pre-pruning  In this method we decide during the building process when to stop adding nodes. This  eliminates an unnecessary complexity on the model. This would be problematic in situations where attributes individually do not contribute much to a decision but combined, may have a significant impact.  Post-pruning  In this method we wait until the full decision tree has been built and then check whether a subtree can be replaced by a single node or feature while incurring only a small amount of change in Entropy. If yes, trim the tree if not keep the subtree.  Decision trees - Advantages :  Easy to understand Capable of modelling nonlinear functions Can handle categorical variables  Decision trees - Disadvantages :  Sensitive to small changes in the data. May overfit easily Only axis-aligned splits Trees may not be as competitive in terms of accuracy as some of the other regression and  classification techniques such as SVM or neural networks.  Impact of distance metrics on KNN performance. KNN can be significantly impacted by the distance metric that is selected like Euclidean distance, Manhattan distance, and cosine similarity. For example Cosine similarity maybe a better option than Euclidean distance for high-dimensional data.  https://www.evernote.com/client/web?login=true#?n=2b506e6c-98c8-977f-634c-c44e8afde1e3&  2/3   Topic 8 - Nonlinear models (KNN and DT) - Evernote  https://www.evernote.com/client/web?login=true#?n=2b506e6c-98c8-977f-634c-c44e8afde1e3&  3/3   