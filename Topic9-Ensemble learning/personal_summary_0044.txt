 The topic 8 content starts by mentioning the concepts which were covered in the recent topics’ content relating to Support Vector Machines (SVM) and informs us of the topics which will be explored this topic, these include the K nearest neighbour (KNN) algorithm and the Decision tree (DT) algorithm.  Then the content explores the topic of the KNN algorithm and its variants, explaining the fundamental concepts behind the KNN algorithm including how weights can be assigned to the contribution of neighbouring data points so that the neighbours which are nearer have a greater effect on the average than further distant ones, which is useful for both regression and classification problems. The basic concept is to label test data points the same as their nearest neighbour (NN), K in this case represents the number of nearest neighbours and can be adjusted as desired.  The content then explores the theory of KNN in-depth, touching on the concept of Voronoi diagrams and the definition of the distance-weighted nearest algorithm. The content then explains how weights can be used based on their distance from the data points, the further the distance the lower the weights. This technique is known as the Shepards method, which is further explained in a video, another video goes into more depth on Voronoi diagrams.  The content then shifts to the topic of the best number of neighbours (K) and how to select the optimal number of K. In simple terms, K controls the shape of the decision boundary separating clusters. Smaller values of K restrain the data to a particular region of a given prediction, forcing the classifier to focus more on closer neighbours and regions. Whereas higher values of K result in smoother decision boundaries, increasing the bias while decreasing the variance.  The concepts of cross-validation are covered in detail, which is the best method available to evaluate and calibrate the hyper-parameters to be the most optimal for the model. This approach tests a range of K values and then evaluates the performance of the models, selecting the best model with the highest accuracy score.  The content then shifts to the topic of Decision Trees, explaining the basic ideas and logic behind the method. A decision tree is simply a map of the possible outcomes resulting from a series of connected choices. Decision trees can help compare the possible decisions against each other based on the cost, benefits, and probabilities. Decisions Tres typically start at a singular root node, which branches into the possible outcomes. A video explains these concepts in further detail.  Then the content moves into the area of Regression trees, which are decision trees that use a regression model. Alternatively, it is also possible to fir a classification model, which is known as a classification tree. The method’s operation procedure is then covered in detail, including a mathematical explanation. These concepts are further discussed in a couple of videos.  The content then shifts to the topic of Classification trees, Classification and Regression Trees (CART) refer to decision tree algorithms that are useful for classification and regression predictive modelling problems. The ideas and mathematical concepts of classification trees including the Gini index and Entropy lines and explored in detail, using charts and graphs to illustrate the ideas.  The content then moves to the area of Decision tree algorithms where the various popular approaches are listed and explained. The ID3 algorithm is explained with a background and history as well as its practical application. The area of Tree depth is explored including why its important to understand and manage the depth to achieve optimal results from the model. Cross-validation can be useful to achieve this through fine-tuning the depth hyper-parameter.  The content then shifts to the topic of model complexity and pruning, which involves pruning the size of a decision tree to reduce the tree size by removing branches that do not provide fruitful results. The tree pruning process is covered in detail, including pre-pruning and post-pruning practices and their effect on the Entropy value.  Then the content explored the area of decision tree advantages and disadvantages covering a list of pros and cons for each, some advantages are being easy to understand and can handle categorical data as well as modelling linear functions. Some disadvantages however are that it is sensitive to small changes in the data, and it can be easily overfit, they may also not be as competitive compared to SVM when it comes to accuracy for some regression and classification problems.  The content then moves to some advanced topics where the impact of distance metrics on KNN performance is described as well as the feature importance of using decision trees, feature importance is useful to identify relevant features for classification which ultimately helps to improve the model’s performance.  The content then explains how to implement this concept in Python code, within detail instructions and explanations of the code and how it works. Covering implementing and evaluating the performance of the KNN model to the Decision tree model implementation and evaluation.  