A decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities. A decision tree typically starts with a single root node, which branches into possible outcomes. In the video you will see an example of how a decision tree can be used to predict commute time. Figure: illustrating the sample decision tree from the video. View transcript SPEAKER: In this tutorial, we're going to see an example of decision tree about prediction of commute time. Decision trees are a type of supervised machine learning where the data is continuously displayed according to a certain parameter. The decision tree can be explained by two entities, namely decision nodes and leaves. The leaves are decisions or final outcomes, and the decision nodes are where the data is displayed. Consider this example. Imagine you want to go to work at a certain time. If you leave your house at 6:00 PM and there no cars still on the road, you will get to work in a short commute time, unless you will get there in a long commute time. But if you leave at 4:00 PM, you will definitely get there in a long commute time. The last option says, if you leave at 5:00 PM and there are no accidents on the road, you will be there in some medium commute time, unless it would take you a long time to get there. So this was a simple example of decision tree and how can we interpret a decision tree. In the next sections, you're going to learn how to construct a decision tree. Activity Classification trees Classification and Regression Trees (CART) is a term introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modeling problems. It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response. For a classification tree, we assign each test instance to the majority class (mode) of the training instances in the region where it belongs. You can consider this action as a being like a data point voting itself into a region which results in selecting the majority. In the classification setting, we replace the sum of square error by the classification error rate as a criterion for making the binary splits. The classification error rate E{"version":"1.1","math":"\(E\)"} is defined as the fraction of the training instances in that region that do not belong to the most common class. Where p^jk{"version":"1.1","math":"\(\hat{p}_{jk}\)"} represents the proportion (fraction) of training instances in the j−{"version":"1.1","math":"\(j-\)"} region that are from k−{"version":"1.1","math":"\(k-\)"}th class: E=1−maxkp^jk{"version":"1.1","math":"\(E = 1 - \max_{k} \hat{p}_{jk}\)"} Basically, Certainty of Distribution (COD) shows how certain it is that a classifier sits inside a region. Let CoD=maxkp^jk{"version":"1.1","math":"\text{Let }CoD = \max_{k} \hat{p}_{jk}"} If CoD{"version":"1.1","math":"\(CoD\)"} is close to 1{"version":"1.1","math":"\(1\)"}, it means almost all of the training points inside a region are voting for a certain class label. So the classifier in this case is certain about the decision. On the other hand, when CoD{"version":"1.1","math":"\(CoD\)"} is 0.5{"version":"1.1","math":"\(0.5\)"} it means we can not trust the votes because there is a high classification error rate (E{"version":"1.1","math":"\(E\)"}). But one of the problems of classification error is that it’s less sensitive for tree-growing. Gini and Entropy In practice people would prefer to use the Gini index and Entropy. The Gini index is the most commonly used measurement of inequality. For example in economics, the Gini index represents the income or wealth distribution of residents in a country. The Gini index is defined as  G=∑k=1Kp^jk(1−p^jk){"version":"1.1","math":"\(G = \sum_{k=1}^{K} \hat{p}_{jk} (1 - \hat{p}_{jk})\)"} Again p^jk{"version":"1.1","math":"\(\hat{p}_{jk}\)"} represents the proportion (fraction) of training instances in thej−{"version":"1.1","math":"\(j-\)"}th region that are from k−{"version":"1.1","math":"\(k-\)"}th class. Its is a measure of total variance across the K{"version":"1.1","math":"\(K\)"} classes. It takes a small value if p^jk≈0,1{"version":"1.1","math":"\(\hat{p}_{jk} \approx 0,1\)"} for all k{"version":"1.1","math":"\(k\)"} (why?). Because it faces with a low inequality distribution in the votes. Gini index is therefore considered a measure of node purity. Consider the following figure, let say the Gini index and Entropy lines are representing the probability of selecting a particular class for a data point. For example the fraction of training points in a region which are voting for class label 0{"version":"1.1","math":"\(0\)"} for a point. As you can sea in the figure, in the corners we have small values of Gini Index and mis-classification, but in the middle area which both classes are representing the same chance for classification, we have high error and it shows the most equality peak in the middle which results in high mis-classification error. If you check the Gini index, it is not linear but mis-classification is linear in form. It seems Entropy is smoother and better in this case. Figure. Illustration of Gini index, Entropy vs. Classification Error. As an alternative to Gini index, Entropy, is defined as: D=−∑k=1Kp^jklogp^jk{"version":"1.1","math":"D = -\sum_{k=1}^{K} \hat{p}_{jk} log \hat{p}_{jk}"} Again we have the same concept in this formula too. Let us see another example on the Gini Index. Consider a two class problem with following splits: Figure: Gini index example As you can see, the value of Gini index is 0{"version":"1.1","math":"\(0\)"} when facing with high inequalities such as the first case which C1=0{"version":"1.1","math":"\(C1=0\)"} and C2=6{"version":"1.1","math":"\(C2=6\)"} and its values gets bigger when we are dealing with equally distributed cases such C1=3{"version":"1.1","math":"\(C1 = 3\)"} and C2=3{"version":"1.1","math":"\(C2 = 3\)"} which results in 0.5{"version":"1.1","math":"\(0.5\)"} for Gini index. Activity Can you think of some real-world usages of the Gini Index in economics?  SIT307 and SIT720 student must complete the following topic.   Impact of distance metrics on KNN performance KNN is a well known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data. Please use the following link for further explanation.   References:  Prasath, V. B., et al. "Distance and Similarity Measures Effect on the Performance of K-Nearest Neighbor Classifier--A Review." arXiv preprint arXiv:1708.04321 (2017).  Feature importance of using Decision Trees (DT)  Decision Trees are a popular machine learning algorithm that uses feature selection to determine the most important classification features. DT operates by recursively segmenting the data into subsets based on the most informative features until a stopping criterion is reached. A criterion such as information gain or the Gini index is used to determine which feature is the most informative. At each node of the tree, the feature with the highest score is chosen as the splitting criterion. The significance of each feature can be determined by considering how much it contributes to overall improvement in the criterion. The greater the contribution, the more significant the feature. Feature importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model.  References: Best number of neighbours (K) How do you pick the variable K{"version":"1.1","math":"\(K\)"} which holds the number of neighbours? How important is selecting the right K{"version":"1.1","math":"\(K\)"}? You can think of K{"version":"1.1","math":"\(K\)"} as controlling the shape of the decision boundary we talked about earlier. For small values of K{"version":"1.1","math":"\(K\)"}, we are restraining the region of a given prediction and forcing our classifier to be more focused on the close regions and neighbours. We are asking the classifier not to care about fairly distant points. This will result in a low bias and high variance. Higher values of K{"version":"1.1","math":"\(K\)"} will have smoother decision boundaries which means lower variance but increased bias. So basically, higher values of K{"version":"1.1","math":"\(K\)"} means asking for more and more information even from distant training points. Like most of machine learning problems, finding hyper-parameters such as K{"version":"1.1","math":"\(K\)"} is not really straightforward. Finding the best answer is not always possible. But as a simple and handy method, you can use Cross-validation (see topic, Model Selection) to partition your data into test and training samples and evaluate your model with different ranges of K{"version":"1.1","math":"\(K\)"} values. For example you can consider the number of neighbours to be K=1,..,Kmax{"version":"1.1","math":"\(K={1,..,K_{max}}\)"}. Now perform the Cross-validation for every possible number of K=1,..,Kmax{"version":"1.1","math":"\(K={1,..,K_{max}}\)"} and evaluate the model based on the training and test data you have partitioned. You can use the misclassification error i.e. the number of misclassifications, as a measurement of performance in your models. Finally, by exploring different values of $K$ and their corresponding misclassification error, we can decide which K=1,..,Kmax{"version":"1.1","math":"\(K={1,..,K_{max}}\)"} has the best performance based on our partitioned data. Decision tree algorithms There are variety of algorithms for decision trees. Here are three of the more popular ones: ID3 (Iterative Dichotomiser 3)uses Entropy. C4.5 (Successor of ID3) slightly more advanced version of ID3 and also uses Entropy. CART (Classification and Regression Tree)uses Gini impurity. We will look at ID3 in detail. The ID3 Algorithm The algorithm was developed by Ross Quinlan in 1975 (He is an Australian who graduated from University of Sydney). It’s used to generate a decision tree from a dataset. Although this method is simple, it is an effective machine learning algorithm. The basic algorithm is as follows: Calculate the entropy of every feature using the data set S{"version":"1.1","math":"\(S\)"}. Split the set S{"version":"1.1","math":"\(S\)"} into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information. Make a decision tree node containing that feature. Recurse on subsets using remaining features. Tree depth If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance. On the other hand, when the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances however you may have other problems such as bias. You will have a high bias in shallow decision trees. It means your decision making process is too naive. If you need to revise these concepts they are described in topics 7 and 8. You need to create a decision tree of the right depth, to find the sweet spot in terms of depth. You can achieve that by performing the cross validation and other evaluation methods you have learned in topics 5 and 6. Remember, you need to find or tune the proper hyperparameter which is the depth of the tree. Decision trees : advantages and disadvantages What are the unique features of decision trees? What problems are best suited for their use? Advantages Very easy to understand, as they represent rules. Capable of modelling nonlinear functions. Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a Euclidean distance between two vectors having weather as a variable.) Disadvantages Sensitive to small changes in the data. If you add few data points or change some small values, your rules can be changed! May overfit easily. As we have said before, by building deep decision trees you are at high risk of overfitting and a high variance model. Only axis-aligned splits. Normal decision trees split the space along each features independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree. Trees may not be as competitive in terms of accuracy as some of the other regression and classification techniques such as SVM or neural networks. Activity Decision trees in Python Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item’s target value. It is one of the predictive modelling approaches used in statistics, data mining and machine learning. In this practical, we will use the sklearn library and the popular Titanic dataset that predicts whether a passenger survived. We use a cleaned up and reduced version of the original dataset. As the wikipedia page on Decision tree learning defines it: Tree models where the target variable can take a finite set of values are called classification trees. In these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Please download the Titanic_cleaned_data.csv dataset, rename and store in a suitable location. Lets start by importing the libraries Code example #1 from sklearn import tree from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np import pandas as pd import matplotlib.pyplot as plt Our cleaned up dataset is in the data directory: Code example #2 titanic_df = pd.read_csv("data/Titanic_cleaned_data.csv") titanic_df.head() Which displays data such as:  In this data, the features are: Survived: 0=died, 1=survived (response variable) Pclass: 1=first class, 2=second class, 3=third class Sex: 0=female, 1=male Age: numeric value Embarked: C or Q or S - binary values for port of embarking. Code example #3 We now split our data into features (X) and response (y) # define X and y feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S'] X = titanic_df[feature_cols] y = titanic_df.Survived Code example #4 Lets split the data into 70% training and 30% testing. The random_state argument takes in an integer value. This is for repeatability of results. Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42) We use the DecisionTreeClassifier in sklearn to fit a decision tree to our data. We specify the maximum depth of the tree to be constructed as 5, initially. Code example #5 # fit a classification tree with max_depth=5 from sklearn.tree import DecisionTreeClassifier treeclf = DecisionTreeClassifier(max_depth=5, random_state=1) # Fit our training data treeclf.fit(Xtrain, ytrain) Output: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,            max_features=None, max_leaf_nodes=None,            min_impurity_decrease=0.0, min_impurity_split=None,            min_samples_leaf=1, min_samples_split=2,            min_weight_fraction_leaf=0.0, presort=False, random_state=1,            splitter='best') Lets look at our training and testing accuracy: Code example #6 print("Training accuracy: {}".format(accuracy_score(ytrain, treeclf.predict(Xtrain)))) print("Testing accuracy : {}".format(accuracy_score(ytest, treeclf.predict(Xtest)))) Output: Training accuracy: 0.852327447833 Testing accuracy : 0.772388059701 Impact of tree depth on accuracy Lets do a 10-fold cross validation to find the average accuracy. We will use the cross_validation library from sklearn to help us with this. Code example #7 # use crossvalidation to get avg accuracy from sklearn.model_selection import cross_val_score scores = cross_val_score(treeclf, Xtrain, ytrain, cv=10, scoring='accuracy') print("Accuracy for each fold: {}".format(scores)) print("Mean Accuracy: {}".format(np.mean(scores))) Output: Accuracy for each fold: [0.75 0.77777778 0.87096774 0.87096774 0.79032258 0.75806452 >0.79032258 0.72580645 0.79032258 0.88709677] Mean Accuracy: 0.801164874552 To plot effect of tree-size on accuracy, we build 10 decision trees each with corresponding max_depth = 1,2,3...,10. We do a 10-fold cross-validation for each tree, and get the mean accuracy. Code example #8 from sklearn.model_selection import validation_curve # depth takes values from 1 to 10 max_depth_range = range(1, 11) # do 10-fold cross-validation for each value in max_depth_range and return the accuracy scores.  train_scores, valid_scores = validation_curve( treeclf, Xtrain, ytrain, param_name="max_depth", param_range=max_depth_range,     cv=10, scoring="accuracy") #Size of train_scores will be: length of parameter (max_depth_range) X number of folds print(train_scores.shape) Output:    (10, 10) Code example #9 # Mean accuracy score for each value of max-depth mean_train_score = np.mean(train_scores, axis=1) mean_val_score   = np.mean(valid_scores, axis=1) plt.plot(max_depth_range, mean_train_score, color="blue", linewidth=1.5, label="Training") plt.plot(max_depth_range, mean_val_score, color="red", linewidth=1.5, label="Validation") plt.legend(loc="upper left") plt.xlabel("Tree-depth") plt.ylabel("Model Accuracy") plt.title("Accuracy comparison of training/validation set") Output:    Text(0.5,1,'Accuracy comparison of training/validation set') FIgure: plot that results from using the overfit approach From the plot, we can see that as the depth of the tree increases, the decision tree starts to overfit. The best value of max_depth is 3. Code example #10 treeclf = DecisionTreeClassifier(max_depth=3) treeclf.fit(Xtrain,ytrain) print("Training accuracy: {}".format(accuracy_score(ytrain, treeclf.predict(Xtrain)))) print("Testing accuracy : {}".format(accuracy_score(ytest, treeclf.predict(Xtest)))) Output:    Training accuracy: 0.829855537721    Testing accuracy : 0.805970149254 To visualize the decision tree, you can use the code available in scikit learn documentation for decision tree. However, you will have to install graphviz, pydot and pyparsing modules using the “pip install” command. Reference Wikipedia contributors 2018, Decision tree learning. In Wikipedia, The Free Encyclopedia, viewed 6 September 2018. A useful technique can be to assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones. This is useful for both classification and regression. The basic idea is to label the test data point is the same as the nearest neighbour (NN). The following figure illustrates this concept. If a black circle as a test point falls into a region in which the closest point is a black ellipse with class label of 1{"version":"1.1","math":"\(1\)"},  based on the nearest neighbour, we are going to label this new sample as class 1{"version":"1.1","math":"\(1\)"}. Figure KNN intuition. But also K{"version":"1.1","math":"\(K\)"} in KNN can vary. Lets say someone would like to check K{"version":"1.1","math":"\(K\)"} nearest neighbours (KNN) of the test point in order to make a decision. You can label a test instance to the same as the majority label of the K-nearest neighbours. The figure below is an example a of 3−{"version":"1.1","math":"\(3-\)"}NN classification. KNN in Python The K Nearest Neighbour (KNN) algorithm as one of the most interesting and powerful machine learning methods. In this practical, you will apply them to classification problems that are possibly non-linearly separable. We begin by importing the necessary libraries. Code example 1 import numpy as np import matplotlib.pyplot as plt For this practical, we will use the simple but extremely popular Iris data set. This dataset contains 50 samples of iris flower varieties (Iris setosa, Iris virginica and Iris versicolor). Each row contains four features: the length and the width of the sepals and petals in centimetres. This data is commonly use to build classification models that take these four measurements as input and predict the species of Iris flower (setosa/virginica/versicolor). You can read more about the dataset here: Wikipedia entry sklearn.datasets page for Iris The Sklearn package contains a few toy datasets that can be used to quickly test your classification/regression model. Code example 2 We can import the iris dataset from this package using the following code: # Import the Iris data set  from sklearn import datasets iris = datasets.load_iris() # divide this data into features and labels X = iris.data y = iris.target print ("X is of type: {}".format(type(X))) print ("y is of type: {}".format(type(y))) # How does our data look #print first 5 rows of X print ("First 5 rows of our data: {}".format(X[:5,:])) #print the unique labels in y print ("unique labels: {}".format(np.unique(y))) Outputs : X is of type: <class 'numpy.ndarray'> y is of type: <class 'numpy.ndarray'> First 5 rows of our data: [[5.1 3.5 1.4 0.2] [4.9 3.  1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5.  3.6 1.4 0.2]] unique labels: [0 1 2] Code example 3 For the purpose of easy visualisation using 2 dimensional plots, we drop the last 2 columns of X. X = X[:,:2] # Use only the first 2 columns. This is for easy plotting/visualisation #print first 5 rows of X print ("First 5 rows of our data: {}".format(X[:5,:])) Outputs : First 5 rows of our data: [[5.1 3.5] [4.9 3. ] [4.7 3.2] [4.6 3.1] [5. 3.6]] Let’s split our data into 80% training and 20% testing. We will train the model using training data and visualise the decision boundaries. We will then test the model performance (in this case, let’s just look at accuracy) using our testing data. When using train_test_split, if you want the dataset to be partitioned in the same way each time you run the cell, you have to set a “random_state” variable. This is for repeatability. In other words, specifying the random state allows you to get the same training and testing set each time. Code example 4 from sklearn.model_selection import train_test_split #Split the data into 80% Training and 20% Testing sets Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size=0.2, random_state=42) print (Xtrain.shape) print (ytrain.shape) print (Xtest.shape) print (ytest.shape) Xtrain[:5,:] # first 5 rows of training data Outputs : (120, 2) (120,) (30, 2) (30,) Out[8]: array([[4.6, 3.6],       [5.7, 4.4],       [6.7, 3.1],       [4.8, 3.4],       [4.4, 3.2]]) Advanced Visualisation (This step is optional) If you are particularly interested advance visualisation you will enjoy this. Now we need to define a function to plot the true data points and the calculated decision boundaries of a given classifier model (also called an estimator). You don’t need to understand the implementation of this step. Please note that this visualisation is for: Two dimensional data Data having 3 labels or less. The function argument takes a training classifier, data matrix as a numpy array, label vector as numpy array. You should include this function in your code since we are going to use this for plotting later. Code example 5 from matplotlib.colors import ListedColormap # We define a colormap with three colors, for three labels our data cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF']) def plot_estimator(estimator, X, y):     '''     This function takes a model (estimator),      '''     estimator.fit(X, y)     # Determine the maximum and minimum mesh as a boundary     x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1     y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1     # Generating the points on the mesh     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),                          np.linspace(y_min, y_max, 100))     # Make predictions on the grid points     Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])     # for color     Z = Z.reshape(xx.shape)     plt.figure()     plt.pcolormesh(xx, yy, Z, cmap=cmap_light)     # Original training sample     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)     plt.axis('tight')     plt.axis('off')     plt.tight_layout()     plt.show() k-Nearest Neighbor Classifier We will now use a KNN classifier. We specify the number of nearest neighbours as input. Code example 6 from sklearn.neighbors import KNeighborsClassifier from sklearn import metrics # Build a kNN using 5 neighbor nodes knn_model = KNeighborsClassifier(n_neighbors=5) #Fit the model using our training data knn_model.fit(Xtrain, ytrain) # Training Accuracy: knn_acc = metrics.accuracy_score(ytrain, knn_model.predict(Xtrain)) print ("KNN Training Accuracy: {}".format(knn_acc)) # Visualize the decision bounday. The points represent the true data.  plot_estimator(knn_model, Xtrain, ytrain) Outputs : KNN Training Accuracy: 0.8333333333333334 Figure. True data In this plot, the points represent the true data, the decision boundary is plotted as a background colour. We can immediately see that one class is linearly separable from the rest. Lets now look at the testing accuracy: Code example 7 #Testing Accuracy: knn_acc_test = metrics.accuracy_score(ytest, knn_model.predict(Xtest)) print ("KNN Testing Accuracy: {}".format(knn_acc_test)) Model complexity and pruning Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances. The tree-building process that we described in previous steps may produce good predictions on the training set, but it’s likely to overfit the data, leading to poor generalization performance. A tree that has a large number of regions may have only few data points per region resulting in high variance. On the other hand, having a small number of regions may result in high bias. One possible alternative is to grow a large tree, and then prune it backin order to obtain a subtree. Generally there are several ways of pruning trees: Pre-pruning (forward pruning) Post-pruning (backward pruning) Pre-pruning In pre-pruning, we decide during the building process when to stop adding nodes (eg. by looking at entropy). Let’s say we are splitting nodes by checking the amount of entropy reduction when we select different features. We can stop splitting nodes when the entropy reduction is not significant. By using this method we are eliminating an unnecessary complexity on the model. However, this may be problematic. Sometimes attributes individually do not contribute much to a decision, but combined, they may have a significant impact. Let’s discuss another model which can handle this problem. Post-pruning Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement. Consider the selected subtree (in red) in the figure below. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error. Figure. Post-pruning. Source (Sayad 2018) Check which subtree removal introduces the smallest error and replace it with a single leaf node. As the figure shows, you wait until the full decision tree has been built, then go for subtrees and check whether you can replace it with a single node or feature, while incurring only a small amount of change in Entropy. If yes, trim the tree. If not, you should keep that subtree because it probably has useful information. Activity Can you identify any relationships between the need for pruning and over-fitting? After partitioning the feature space, we can fit a simple model in each sub-region R1,R2,…{"version":"1.1","math":"\(R_1, R_2, \dots\)"}. What does this look like in practice? Let’s start by looking at the principles of how this works. It may be helpful to read the text here before reviewing the video above for a practical example of how regression trees can be used. Decision trees that use a regression model are called regression trees. We can alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used. Let’s start with the procedure: We divide the feature space, i.e., the set of possible values for x1,…,xd{"version":"1.1","math":"\(x_1,\dots ,x_d\)"} into J{"version":"1.1","math":"\(J\)"} distinct and non-overlapping regions R1,…,RJ{"version":"1.1","math":"\(R_1,\dots,R_J\)"}. For every instance that falls into region Rj{"version":"1.1","math":"\(R_j\)"} we make the same prediction, which is simply the mean (or mode) of response values for the training observations in Rj{"version":"1.1","math":"\(R_j\)"}.  How do we perform these actions? The overall goal of regression trees is to find regions R1,R2,…,RJ{"version":"1.1","math":"\(R_1,R_2,\dots,R_J\)"} that minimize the training error: ∑j=1J∑i∈Rj(yi−y^Rj)2{"version":"1.1","math":"\sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2"} where y^Rj{"version":"1.1","math":"\(\hat{y}_{R_j}\)"} is the mean of the target values of the training instances in the j−{"version":"1.1","math":"\(j-\)"}th region. Let’s say we have J{"version":"1.1","math":"\(J\)"} regions. The formula states that for every point in the region Rj{"version":"1.1","math":"\(R_j\)"} find the difference value of the prediction and true output yi−y^Rj{"version":"1.1","math":"\(y_i - \hat{y}_{R_j}\)"}.  Obviously we would like to minimize this problem so that we have regions that result in less error. Once again we have an optimisation problem to solve. How do we find the solution? Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into J{"version":"1.1","math":"\(J\)"} regions. How big would the space of possibilities become in this case? For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. Rather than using a brute-force solution, we would like to work in a heuristic way. Solution Let us now explain the way this heuristic method works: We first select a feature xj{"version":"1.1","math":"\(x_j\)"} and a threshold s{"version":"1.1","math":"\(s\)"} such that splitting the feature space into the regions  {x|xj≤s}{"version":"1.1","math":"\(\{x \vert x_j \leq s \}\)"} and {x|xj>s}{"version":"1.1","math":"\(\{x \vert x_j \gt s \}\)"} leads to the best possible reduction in training error. So we are not going into the joint space of all features, but we work on a independent feature form such as xj{"version":"1.1","math":"\(x_j\)"} with a threshold s{"version":"1.1","math":"\(s\)"}.  Next, we repeat the process, looking for the best feature and the best threshold in order to split the data further to minimize the error in each of the resulting regions. However, this time, instead of splitting the entire feature space, we only split one of the two previously identified regions. The splitting process continues until a stopping criterion is reached. For example, we may continue until no region contains more than five instances or the nodes are getting too pure or sparse. Prediction We predict the response for a given test instance using the mean (or mode) of the training instances in the region where the test observation falls. View transcript SPEAKER: In this tutorial, we're going to show a very simple example of regression tree. Regression tree may be considered as a variant of decision trees designed to approximate real valued functions instead of being used for classification methods. Consider this dataset. This x is years. It's indicating the number of years the player has had experience of playing. As for this one, this is the number of hits the player had in the last year of his career. Also, the data points itself are colour coded. It means the red colours are indicating high salary. And the blue colours are showing low salaries. Now we can construct a decision tree on this dataset, which could be something like this. It says if years of experience is less than 4.5 years, return a value of 5.11. Then when we insert this value in this equation, and we find the value of salary. Remember this value is just a simple mean of the points, or the data points, which are partitioned into this region. What if the years of experience is more than 4.5 years? Now we have to check the number of the hits. If it is more than 117.5, you're going to insert this value in this equation, which is 6.74, which the amount of money is more than $800,000. But if the number of the hits are less than this value, we are going to return 6 as the value to be inserted in this equation. So as you can see, we partitioned the data into three regions. The first region, R1, is when the years of experience is less than 4.5 years. And this is this region. So if you take any new point in this region, you are going to return the mean value of these data points as the salary of this new point. The second region is when the years of experience is more than 4.5, but the number of the hits are less than 117.5, which is this region, R2. But the last region are the players who has more than 4.5 years of experience. Also, they have hits more than 117.5. And so this is the last region. So this was a simple example of regression tree. Remember, we can use more complicated things rather than just having a mean of these values in each region, like having a regression line, or even median of the points, or more complex algorithms. Activity Watch this interesting video on Regression Trees. Theory of KNN But lets have a close look into KNN. Assume an arbitrary data point is represented as: x=[x1,x2,...,xd]∈ Rd{"version":"1.1","math":"\\ \textbf{x} = [x_1,x_2,...,x_d] \in \ R^d \\"}  Recall the Euclidean distance between data points: dist(xi,xj)=∑r=1d(xir−xjr)2{"version":"1.1","math":"\(\\ dist(\textbf{x}_i,\textbf{x}_j) = \sqrt{\sum_{r=1}^{d} (x_{ir} - x_{jr})^2} \\\)"} For finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format. To summarise: Continuous valued target function:Mean value of the k{"version":"1.1","math":"\(k\)"} nearest training examples Discrete class label:                       Mode of the class labels of the k{"version":"1.1","math":"\(k\)"} nearest training examples There is another concept partially related to KNN which is called a  Voronoi Diagram. In mathematics, a Voronoi diagram is partitioning of a plane into regions based on distance to points in a specific subset of the plane. Have a look at the following figure. Figure. Voronoi diagram. As a Voronoi diagram, you can see it is based on closest neighbours; the same concept as KNN. Also clearly the data is not linearly separable and it results in complex boundaries and decision rules. Remember that the decision surface is formed by the training examples. Imagine a scenario in which we are performing 10−NN{"version":"1.1","math":"\(10-NN\)"} (10 Nearest Neighbours), and the only real close points are  points. The other 6{"version":"1.1","math":"\(6\)"} neighbours we are going to analyse are not really related to this point. The final result actually could be misleading! But, what if we assign a different weights to distances of data points? So the question here is what is a  distance-weighted nearest neighbour algorithm? Basically we can assign weights to the neighbours based on their distancef rom the test point. For example, weight may be inverse square of the distances. This means the higher the distance of the neighbour, the lower its weight. All training points may influence a particular instance. This method is also known as Shepard’s method. View transcript SPEAKER: In this tutorial, we're going to explain to you about the k-nearest neighbour classification. As the first step, we have an arbitrary data point, which is represented as vector x, which has d dimensions, x-1 to x-d. If you remember, the Euclidean distance between two data points, which is defined something like this, in k-nearest neighbour classification when a new point comes in, you're going to find the labels of the k nearest or closest neighbours to this data point. In case we're dealing with discrete class labels, if you're going to use them mode of the class labels of k-nearest training examples. For example, in here we have three data points from class red and we have one from class blue. So the final decision for the label of this point is red. On the other hand, when we're dealing with continuous value target functions you're going to need to return the mean value of the k-nearest training examples as the final value for our test point. Also, there is another way of using KNN, or k-nearest neighbour, which is called distance-weighted nearest neighbour algorithm. So in this method, we are assigned weights to the neighbours based on their distance from the test point. For example, weight may be inverse square of the distances, which means the higher the distance, the lower is weight. Consider this image. We have one test point, and we have three close data from the red class, and we have six in here, which is twice of these points, but they are in class blue. So what do you think you should return as a class label of this data point? In ordinary KNN, or k-nearest neighbour, we're going to return blue. Of course, if k, the number of neighbours, is 10 or 9 because we have 9 neighbours right now. But this method, which is also called Shepard's method, believes that we should return red because as we said, the higher the distance, the lower is weight. So the influence of these points are much lower than these points. So we're going to return the class level red for this data point. So all training points may influence a particular instance, but in this method based on the distance, we're going to put some weights on these data points. So this model is also known as Shepard's method. It has been proved that this method can be more accurate than KNN in some test cases. Activity The following video explores Voronoi diagrams further: 