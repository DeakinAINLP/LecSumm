K-Nearest Neighbor is based on the time-tested and reliable Supervised Learning  K-Nearest Neighbors (K-NN) is an algorithm that takes into account all of the data  The K-Nearest Neighbors (K-NN) algorithm is used to categorise fresh instances of  Student ID: s222502507 K-Nearest Neighbor(KNN) Algorithm for Machine Learning  approach, making it one of the most straightforward machine learning algorithms.  data based on their anticipated similarity to material that has already been categorised.  that is currently available in order to decide how a new data point should be categorised. This indicates that the K-NN approach may be used to successfully categorise new data as it becomes available, and that it can do so without sacrificing accuracy.  is where it is most frequently put to use.  the data it is processing before doing so.  immediately utilise the training set; however, it does save the dataset and utilise it for classification in the future.  During the training phase, the KNN algorithm merely stores the dataset, and when it is presented with fresh data, it allocates that data to a category that is the most appropriate for it.  K-Nearest Neighbors is a technique that can be used for regression, but classification  Because it is a non-parametric method, K-NN does not make any assumptions about  It is referred to as a "lazy learner algorithm" due to the fact that it does not  Decision trees Because of its many real-world similarities, the tree has had an impact on several branches of machine learning, including classification and regression. A decision tree is a useful tool in  decision  analysis  because  it  provides  a  graphical  and  textual  representation  of  the decision-making  process.  The  decision-making  process  is  tree-like,  as  suggested  by  the name. Although this essay will focus on its use in machine learning, it is a frequent data mining tool for determining how to proceed towards a goal. Survival prediction using the Titanic dataset as an illustration. Model below makes use of sex, age, and sibsp as its three features/attributes/columns from the dataset.  Classification and Regression Tree algorithm The CART (Classification and Regression Tree) algorithm is used to create the Decision Tree. Metrics like Gini impurity and information Gain are used to determine the optimal partition at each node. So that we may build a decision tree. The CART algorithm can be broken down into the following steps:  1.  The entire training dataset represents the first node in the tree. 2.  Use the characteristics of the dataset to pinpoint the sources of data contamination.  The Gini index and the entropy are useful for classifying impure samples, whereas the Mean Absolute Error, the Friedman's MSE, and the Half Poisson Deviation are useful for assessing regression accuracy.  3.  Then, it chooses the characteristic that yields the best information gain or impurity  reduction during data splitting.  4.  Separate the dataset into two halves (left and right) for each value of the selected  feature, one where the feature has that value and one where it does not. Each subgroup you get from the split should be as uncontaminated as feasible with respect to the dependent variable.  5.  Find out how impure each subset is based on the variable of interest. 6.  Iterate through steps 2â€“5 until a halting condition is met for each subset individually. The halting condition could be something like the depth of the tree, the number of samples needed to produce a split, or a certain level of impurity.  7.  For classification tasks, label the majority class, and for regression tasks, assign the  mean value to each leaf node.  Advantages:  1.  When compared to alternative methodologies, the pre-processing of data using decision  trees requires significantly less effort.  2.  It is possible to design a decision tree without first normalising the underlying data. 3.  Before utilising a decision tree, scaling the data is not required to be done. 4.  The  presence  or  absence  of  missing  values  in  the  data  does  NOT  have  a  substantial  impact on the process of generating a decision tree.  5.  Due to the fact that it is so easy to grasp and implement, a decision tree model may be  utilised by both the technical teams and the stakeholders.  Disadvantages:  1.  When even a small change in the data results in a significant reorganisation of the  decision tree's structure, an unstable state may be the result.  2.  Calculations using a decision tree can occasionally reach extraordinary levels of depth  and complexity when compared to those produced by other algorithms.  3.  In most cases, more time is required to train a model that is based on a decision tree. 4.  The training of a decision tree is an expensive activity because of the amount of time  and work it requires.  5.  It is challenging to apply regression to continuous data and to make predictions using  the Decision Tree technique.  