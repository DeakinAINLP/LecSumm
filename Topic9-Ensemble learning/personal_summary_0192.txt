KNN Algorithm  The KNN algorithm (K Nearest Neighbour) is a model which calculates the distances between datapoints to classify and group datapoints. A useful method when using the KNN algorithm is to assign weights to these distances, so that a closer distance is more weighted and therefore more valuable than a far distance. The model should then label each data point as the same as it’s nearest neighbouring datapoint. K, the number of clusters the model will separate into also determines the accuracy as too many clusters will overfit the model and too few will underfit. Model complexity also comes in when the data is not linearly separable as demonstrated in the below Voronoi diagram.  Assigning weights to the distances in scenarios such as this is also known as Shepard’s method.  The K variable can be thought to control the shape of the decision boundary. Higher values of K will have smoother decision boundaries meaning lower variance and higher bias. Find K is not a straight forward thing to do and may not even have a ‘best’ option as is the case with find hyper parameters for many machine learning models. We can use the ‘misclassification error’ when testing different K values to determine the best fit for the model and current dataset.  Decision Trees  Decision trees model the data as a flow chart with branches which map the possible outcomes of a series of related choices. They usually start with a root node and branch into the possible outcomes. Decision trees can be used to weigh the cost, benefits and probabilities of a decision against another.  Decision trees which use a regression model are called ‘regression trees’ and trees using a classification model are called ‘classification trees’. Pruning is a technique which reduces the size of these decision trees by removing paths that provide little classification power. This is to reduce the model complexity and minimise the likelihood of the model overfitting the data.  