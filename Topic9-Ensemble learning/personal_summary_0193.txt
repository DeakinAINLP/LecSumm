The  supervised  learning  algorithms  of  KNN  and  decision  tree  are  discussed  in  machine  learning.  KNN compares and classifies new data points in a training set by comparing them to the nearest neighbours. The technique of assigning weights to data points and labelling test point same as the nearest available neighbour is used by KNN for both classification and regression. Like KNN, the Voronoi diagram is built on nearest neighbours.  The  nearest  neighbour  method  known  as  the  distance-weighted  nearest  neighbour  distributes weights to neighbours based on how far they are from the test point, such as the inverse square of distances. Any  training  point  may  have  an  impact  on  a  specific  occurrence.  Weight  and  distance  are  inversely proportional that is higher the distance of neighbour lower will be its weight and vice-versa. This method is known as the Shepardâ€™s method. Selecting the value of k is a crucial step since k controls the shape of the decision boundary. By restricting the value of k to smaller numbers the classifier is being set to not care about distant points and this can produce low bias and high variance. While higher value of k will result in lower variance and increased bias because of smoother decision boundary. Cross-validation is used to split dataset into test and training datasets. In this case the number of miscalculations is one measure of performance of model. The best value for k can thus be obtained by exploring different k values for different misclassification errors.  Decision trees is one of the widely used supervised learning techniques. Decision trees are used to compare different alternative solutions that branch out of a node and compare them. In Decision tree there are nodes which branches out into further sets which later becomes node once the split is also branched out. This creates a tree like model with nodes containing possible outcomes. The leaf nodes represent the outcome or prediction of model. Decision trees can be used for both classification and regression tasks. Decision trees that are used for classification tasks are called classification tress and the decision trees used for regression tasks are called regression trees. In the process of forming regression tree initially the feature space is divided into distinct non-overlapping regions based on the set of possible input values. For each region same prediction is made which  is  the  mean  or  mode  of  response  values  for  training  observations.  Regression  trees  are  aimed  to minimise  the  training  error  which  raises  the  optimisation  problem  for  which  recursive  binary  splitting approach  is  used  rather  that  brute-force  solution.  In  general,  heuristic  methods  offer  useful  approaches  to handling complex issues where exact answers are impractical. They provide useful and effective methods for locating approximations that can frequently achieve the desired results. In heuristic method feature selection is done and a threshold is set to split the features space into regions to minimize training error. This process is repeated till the stopping criteria where no region has more than five nodes have been reached. Mean or mode of the training set where test observation falls is used to forecast the response for a specific test instance.  Classification And Regression Trees (CART) is a decision tree algorithm used to predict qualitative responses. For classification tree mode is assigned to each test instance, selecting the majority from region. Sum of square error is replaced by Classification error rate in classification setting. The percentage of training instances in that  region  that  do  not  belong  to  the  most  prevalent  class  is  known  as  the  classification  error  rate,  while Certainty of Distribution (COD) measures the certainty of a classifier residing inside a region. The closer the value of COD to 1 indicates that more data points are inside the region. The problem with classification error is  that  it  is  not  as  sensitive  to  the  length  of  tree.  Gini  index  and  entropy  are  the  measures  of  impurity  or inequality. Gini index is the measure of impurity in a decision tree, that is the extent to which classes are mixed in a node. Gini index zero represents a pure node and while the value one represents complete impurity. The quality of split is affected by value of Gini index. Entropy is also a measure of impurity and indicates the diversity of classes within a node. The aim is to maximise information gain and evaluate the quality of splits. ID3 (Iterative Dichotomiser 3), C4.5 (Successor of ID3), CART (Classification and Regression Tree) are the popular  algorithms  used  for  decision  trees  which  uses  Entropy,  and  Gini  impurity.  The  most  effective algorithm which is the ID3 algorithm where the dataset is split into subsets with minimum entropy to gain more information. It is crucial to build a decision tree with the right depth is important to avoid high variance and high bias for which cross validation and other techniques can be used. Reducing the size of tree to result in more informative model by removing sections from tree is called pruning. The tree-building process can  generate  accurate  predictions,  but  it  can  suffer  from  overfitting  and  underfitting.  To  address  these  issues, pruning  is  performed  to  reduce  variance  and  improve  generalization.  Pruning  involves  removing  certain branches or nodes from the tree to obtain a smaller, more generalized subtree. This approach allows us to capture complex relationships in the data and then selectively simplify the model to improve generalization performance. Pre- pruning (forward pruning) and post-pruning (backward pruning) are the pruning methods that can be used. Pre-pruning is a method for building decision trees in which the growth of the tree is stopped before  it  reaches  its  maximum  size depending  on  predetermined  stopping  conditions,  such  as  a maximum depth, a minimum number of samples in a node, or a threshold for impurity reduction. Pre-pruning reduces the complexity of the tree, creating a simpler and more universal model, which helps minimise overfitting. In decision tree algorithms, post-pruning, often referred to as subtree replacement or tree trimming, is a technique that involves eliminating branches or nodes from a fully grown tree to improve its generalisation performance on unseen data. It entails assessing the tree's performance on a validation set and choosing trimming nodes that  do  not  materially  increase  the  model's  overall  accuracy  or  predictive  power.  Various  advantages  and disadvantages  of  Decision  tree  are  discussed.  Advantages  includes  easy  understanding  and  handling, scalability,  Non-linear  relationship,  and  categorical  data  handling  capability.  Few  discussed  disadvantages include  prone  to  over-fitting,  sensitive  to  changes.  KNN  effectiveness  is  affected  by  the  type  of  distance metrics  that  is  used  for  analysis. The  distance  metrics  of  Euclidean  distance,  Manhattan  distance,  Cosine distance, city block and Chebyshev distance. It can be said that for high-dimensional data Cosine similarity is more effective than Euclidean distance. Feature importance to select the best features for getting improved performance of model is discussed. Python application of above discussed methods is done.  QUESTION 1: It can be observed that the accuracy and overall performance has improved when knn approach is  used  when  compared  to  SVM  techniques  used  in  task  7  which  produces  accuracy  of  86%  while  knn produces 99.25% testing accuracy.  QUESTION 2:  It can be observed than 70-30% split technique produces better performance when compared to 50-50% technique.  QUESTION 3: It can be observed that the model performance has improved in the order city block distance, Cosine distance, Euclidean distance. Euclidean distance having the highest accuracy same as Question 1 when knn with value five is used. And city block reporting the least accuracy and cosine having better accuracy than cityblock but Euclidean distance metrics reporting higher accuracy than cosine distance metrics. The below images are performance outcomes in order of Euclidean distance, Cityblock, Cosine, Question 1 outputs.   