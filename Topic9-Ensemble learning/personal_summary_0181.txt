Topic 8 Notes  K nearest neighbor (KNN) algorithm assigns weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.  The K in KNN check K nearest neighbours (KNN) of the test point in order to make a decision.  To the majority of decisions based on the close training points, the following rules apply:    For continuous valued target function: use the mean value of the k nearest training examples   For discrete class label: Mode of the class labels of the k nearest training examples   For distance-weighted nearest neighbor algorithm (Shepardâ€™s method)  - Assign weights to the neighbours based on their distance from the test point.  - Weight may be inverse square of the distances (1/D2) - Higher the distance of the neighbour, lower its weight.  K controls the shape of the decision boundary. To find optimum K, use Cross-validation to partition your data into test and training samples and evaluate your model with different ranges of K values (K = 1,2,3, .. ,Kmax).  Small value of K means: 1) forces classifier to be more focused on the close regions and neighbours and 2) can result in a low bias and high variance  Higher values of K means: 1) asking for more information from distant training points, 2) smoother decision boundaries, 3) lower variance but increases bias  DECISION TREE  A decision tree is a map of the possible outcomes of a series of related choice and can be used to weigh possible actions against one another based on their costs, benefits and probabilities.  Decision trees that use a regression model are called regression trees. If a classification model is used then it is called classification trees. Classification and Regression Trees (CART) refers to decision tree algorithms that can be used for classification or regression predictive modeling problems.  GINI INDEX and ENTROPY  The Gini index is the most commonly used measurement of inequality. For example in economics, the Gini index represents the income or wealth distribution of residents in a country.  There are variety of algorithms for decision trees:  ID3 (Iterative Dichotomiser 3) uses Entropy.     C4.5 (Successor of ID3), more advanced version of ID3 and uses Entropy.   CART (Classification and Regression Tree), uses Gini impurity.  In a very deep tree, the feature space is partitioned into small regions. Therefore, there are low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, in a shallow tree, the training data points can be inferred that they do not have high variances however it may have other problems such as high bias. It can means that the decision making process is too naive.  Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances. Ways of pruning a tree:    Pre-pruning (forward pruning)  Pruning is decided during the building process when to stop adding nodes (eg. by looking at  entropy). In case of Entropy, check the amount of entropy reduction by selecting different features and stop splitting when the entropy reduction is not significant. Note: Sometimes attributes individually do not contribute much to a decision, but combined, they may have a significant impact.    Post-pruning (backward pruning)  Pruning is done after the full decision tree has been built then prune the attributes by replacing an entire subtree with a single region or node that introduce the smallest error.  Advantages of decision tree:    Very easy to understand, as they represent rules.   Capable of modelling nonlinear functions.   Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a  Euclidean distance between two vectors having weather as a variable.)  Disadvantages of decision tree:    Sensitive to small changes in the data.   High risk of overfitting and a high variance model.   Only axis-aligned splits. Normal decision trees split the space along each features independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.    Trees may not be as competitive in terms of accuracy as some of the other regression and  classification techniques such as SVM or neural networks.  KNN classifies new data points according to their closeness to the closest neighbours using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data.  