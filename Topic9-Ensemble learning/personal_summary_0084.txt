KNN algorithm and its variants  Weights can be assigned to the contribution of data point neighbours allowing for the closer neighbours to contribute more to the average than the more distant neighbours, this can be used for both classification and regression.  In a basic sense it can be achieved by labelling a test data point as the same as its nearest neighbour. Since K in KNN can vary sometimes the test point has multiple nearest neighbour of different types, in this situation the test point is assigned to the whichever class of nearest neighbour there is more of.  Theory of KNN  The majority of decisions based on close training points require the average or the in continuous cases and require the mode of the class labels in discrete format. Therefore, Continuous valued target function requires the mean value of the k nearest training examples while discrete class label requires the mode of the class labels of the nearest training examples.  When performing a check for nearest neighbours some of the neighbours that are not as strictly close the others are not related to the point being analysed which could result in an inaccurate result. This can be overcome by assigning weights to the neighbours based on their distance from the test point, an example of this distance could be calculated through inverse square of distances. Therefore, the higher the distance of the neighbour the lower its weight, all training points may influence a specific instance, this method is the Shepards method.  Best number of neighbours  K controls the shape of the decision boundary, when K is small, we are restricting the region of the prediction and making our classifier be more focused on the close regions and neighbours. Asking the classifier to not care about fairly distant points will result in a low bias and high variance. Increasing K will result in smoother decision boundaries which also means lower variance but increased bias, higher K means asking for more and more information even from distant training points.  Finding hyper-parameters like K isn’t straightforward and the best answer is not always possible. Cross validation can be used to partition data into test and training samples so that the model can be evaluated using different ranges of K values. Cross validation is performed for every possible value of K and the model is evaluated based on the training and test data, performance can be measured using misclassification error. From there based on the different misclassification error results on each K we can decide which K has the best performance based on our data. When testing K performance, the max amount of K can be based on your training data points, there is not rule of thumb.  Decision Trees  A decision tree is a map of the possible outcomes of a series of related choices, we can use them when weighing possible actions against one another based on cost, benefits and probabilities. These trees usually start at a single root node which will then branch out into all the potential outcomes of potential choices and results based on said choices.  Regression Trees  After partitioning the feature space, a simple model can be fitted to each of the partitioned regions. Regression trees are decision trees that use a regression model and can alternatively fit a classification model to instead be a classification tree. This process involves as mentioned dividing the features   space into J distinct and non-overlapping regions. For every instance that falls into a region we make the same prediction which is the mean (or mode) of response values for the training observation. The overall goal of regression trees is to find regions that minimise the training error. The formula for this states that if we have J regions for every point on Region J we find the difference between the prediction and true output, we want to minimise this problem so that regions have less error. To optimise this we have to take a top-down greedy approach called recursive binary splitting instead of a brute force solution we have to work heuristically.  Classification Trees  Classification trees are similar to regression trees except they predict a qualitative response instead of a quantitative response. We assign each test instance to the mode of the training instances in the region where it belongs. In a classification setting the sum of square error is replaced by the classification error rate when making binary splits. The classification error refers to the fraction of the training instances in that region which do not belong to the most common class. Certainty of Distribution shows how certain it is that a classifier sits inside a region, a CoD close to 1 means that almost all the training points in a region are choosing a certain class label meaning the classifier is more certain of the decision, a CoD of 0.5 would indicate the choice cannot be trusted as there is a high classification error rate. The Gini index and Entropy are a common measurement of inequality.  Decision Tree algorithms  Three popular decision tree algorithms are ID3 (Iterative Dichotomiser 3) which uses Entropy, C4.5 (ID3 successor) which also used Entropy and CART (Classification and Regression Tree) which uses Gini impurity.  A very deep tree means that the feature space is partitioned into small regions, and we should except a lower chance of visiting many training points in that sub-region. Thus, all the estimation in that region are not good due to the high variance. Whereas, if we have big regions and a shallow tree then the data points will not have high variance but other problems such as bias may occur. A shallow decision tree will have high bias meaning that decision making process is too naïve. Finding the correct depth of a decision tree can be achieved using evaluation methods such as cross-validation.  Model Complexity and Pruning  Pruning is a technique for reducing the size of decisions trees be removing sections that don’t provide much power for classifying instances. This may be necessary since the tree-building process mentioned earlier is likely to overfit the data causing poor performance. Pre-pruning we determine during the building process when to stop adding nodes (by looking at entropy). If we are splitting nodes and checking the entropy amount we can stop when the reduction in entropy is not significant although this can be problematic as sometimes individual attributes do not contribute much to a decision but combined they may be significant. Post-pruning waits until the tree is fully built and then prunes the attributes using subtree replacement. This involves replacing a subtree with a single region or node, we check which subtree removal introduces the least error and replace it with a single leaf node.  Decision Trees Advantages and Disadvantages  Advantages  -  Easy to understand. -  Can model nonlinear functions. -  Can handle categorical values.  Disadvantages  -  Sensitive to small changes in data. -  May overfit easily. -  Only axis-aligned splits. -  Not as strong in terms of accuracy as other regression and classification techniques.  Reflection  This module was quite useful as I feel it built well upon previous module content while also introducing new concepts. The theory of KNN and determining K was easy to understand as I felt it well built upon previous knowledge and increased my confidence in using and understanding KNN. Prior to this module I didn’t really know about regression and classification trees and how they can be used and optimised along with their differences to the regression and classification techniques we have been using.  