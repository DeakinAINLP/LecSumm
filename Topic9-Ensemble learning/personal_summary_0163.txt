Key Learning Points    KNN algorithm and its variants A useful technique can be to assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.  This is useful for both classification and regression.     Theory of KNN    Decision trees A decision tree is a map of the possible outcomes of a series of related choices.  Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.      A decision tree typically starts with a single root node, which branches into possible outcomes. In the video you will see an example of how a decision tree can be used to predict commute time.    Regression trees  Decision trees that use a regression model are called regression trees. We can alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used.    Classification trees  It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response. For a classification tree, we assign each test instance to the majority class (mode) of the training instances in the region where it belongs. You can consider this action as being like a data point voting itself into a region which results in selecting the majority.    Model complexity and pruning  Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances.  The tree-building process that we described in previous steps may produce good predictions on the training set, but it’s likely to overfit the data, leading to poor generalization performance.    A tree that has a large number of regions may have only a few data points  per region resulting in high variance.    On the other hand, having a small number of regions may result in high  bias.    One possible alternative is to grow a large tree, and then prune it back in  order to obtain a subtree.  Generally, there are several ways of pruning trees:    Pre-pruning (forward pruning)   Post-pruning (backward pruning)   External resources and other learning materials The following links are the reference for the external learning materials and study points    United States of Voronoi   https://youtu.be/w4MnOA14pYs   Effects of Distance Measure Choice on KNN Classifier Performance - A  Review (arxiv.org)   05-fsel.dvi (psu.edu)   sklearn.datasets.load_iris — scikit-learn 1.2.2 documentation   Decision tree learning - Wikipedia   scikit-learn: machine learning in Python — scikit-learn 1.2.2 documentation   1.10. Decision Trees — scikit-learn 1.2.2 documentation   python - How can we find the optimum K value in K-Nearest Neighbor? -  Stack Overflow  Reflect on knowledge gained from the unit Overall, a few new classification methods and algorithms were covered in topic 8. it was also good to learn about the statistics and the concept of the different models as well. Most of my learning reflection is covered in the above section and below is one extra point that I captured in my sturdy notes regarding the depth of the tree in the Classification trees.  Tree depth If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, when the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances however you may have other problems such as bias. You will have a high bias in shallow decision trees. It means your decision-making process is too naive. We need to create a decision tree of the right depth, to find the sweet spot in terms of depth. You can achieve that by performing the cross validation and other evaluation methods  Knowledge Quiz      