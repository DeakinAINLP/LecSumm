KNN algorithm and its variants:  The K-nearest neighbors (KNN) algorithm is a simple and intuitive method for classification or regression. Given a labeled dataset, it calculates the distance between an unseen data point and all other points in the training set. It then selects the K closest neighbors based on the distance metric (such as Euclidean distance) and assigns the majority class label (for classification) or averages the target values (for regression) among those neighbors as the prediction. KNN's effectiveness depends on the choice of K and the distance metric, and it assumes that similar data points have similar labels or target values.  It uses a technique where it adds weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.  Voronoi diagram:  The Voronoi diagram is closely related to the K-nearest neighbors (KNN) algorithm and is often used as a spatial data structure to efficiently implement KNN search.  A Voronoi diagram is a partitioning of a space into regions based on the proximity to a set of points called the Voronoi seeds or generators. Each region in the Voronoi diagram corresponds to the set of points that are closer to a particular generator than any other generator in the space.  In the context of the KNN algorithm, the Voronoi diagram can be used to determine the K nearest neighbors of a given query point efficiently. The generators in the Voronoi diagram represent the training instances or data points, and each Voronoi cell represents the region of influence or proximity of a particular data point.  Best number of neighbours (K):  Choosing the appropriate value for the parameter K in K-nearest neighbors (KNN) is an important decision that can impact the performance of the algorithm. The selection of K depends on several factors and can be determined through various approaches. Here are a few common methods for choosing the value of K:  ✓  Cross-validation: Cross-validation is a widely used technique for model evaluation. It involves splitting the training data into multiple subsets (folds), training the model on some folds, and evaluating its performance on the remaining fold. By trying different values of K and measuring the performance (e.g., accuracy or mean squared error) on the validation folds, you can select the value of K that yields the best performance.  ✓  Grid search: Grid search involves specifying a range of possible values for K and evaluating the performance of the KNN algorithm for each value using a suitable performance metric. The value of K that yields the best performance is then selected.  ✓  Domain knowledge: Depending on the specific problem and domain, you might have prior knowledge or insights that can guide the selection of K. For example, if you know that the problem is highly complex or noisy, you might choose a larger value of K to reduce the influence of individual instances.   Decision trees  A decision tree is a map of the possible outcomes of a series of related choices.  Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities. A decision tree typically starts with a single root node, which branches into possible outcomes.  Regression trees:  Regression trees, also known as decision trees for regression, are predictive models used for solving regression problems. They are a type of supervised learning algorithm that partitions the feature space into regions, and each region is associated with a predicted output value.  Here's a high-level overview of how regression trees work:  ✓  Tree construction: The algorithm starts with the entire dataset and selects a feature to split the data based on a criterion that maximizes the homogeneity or purity of the resulting subsets. The most commonly used criteria are mean squared error (MSE) and mean absolute error (MAE). The dataset is then divided into two subsets based on the chosen feature and split point.  ✓  Recursive splitting: The process of splitting is repeated recursively for each resulting subset, creating a tree-like structure. The splitting continues until a stopping condition is met, such as reaching a maximum depth, reaching a minimum number of instances in a leaf node, or reaching a minimum improvement in the error metric.  ✓  Prediction: Once the tree is constructed, prediction is done by traversing the tree based on  the feature values of the instance being predicted. Starting from the root node, the instance is guided down the tree by following the appropriate branch based on the feature values. The prediction is made by taking the average (for MSE) or median (for MAE) of the output values of the instances in the leaf node reached by the instance.  However, regression trees can be prone to overfitting if the tree grows too deep and captures noise or specific instances in the training data. To mitigate this, techniques like pruning and setting appropriate stopping criteria are used.  Classification trees  Classification trees, like regression trees, are supervised learning algorithms used for solving classification problems. They partition the feature space into regions and assign class labels to each region. The criteria used for splitting in classification trees include Gini impurity and entropy.  Here's a brief explanation of Gini impurity and entropy as criteria for splitting in classification trees:  Gini impurity: Gini impurity measures the probability of misclassifying a randomly chosen instance in a region. It quantifies the impurity or disorder of a region. A Gini impurity of 0 indicates that all instances in the region belong to the same class (pure region), while a Gini impurity of 0.5 indicates an equal distribution of instances across all classes (maximum impurity).  Entropy: Entropy is another measure of impurity or disorder in a region. It calculates the expected amount of information needed to classify a randomly chosen instance in the region. Entropy ranges from 0 (pure region) to 1 (maximum impurity). A region with low entropy has instances   predominantly belonging to a single class, while a region with high entropy has instances distributed across multiple classes.  During the tree construction process, the classification tree algorithm aims to find the splits that minimize the impurity or maximize the information gain. The information gain is the reduction in impurity achieved by splitting the data based on a particular feature and split point.  The decision on which split to choose is based on evaluating the impurity or entropy before and after the split. The split that results in the greatest reduction in impurity or the highest information gain is selected.  Both Gini impurity and entropy are widely used as criteria for splitting in classification trees. Gini impurity is computationally more efficient since it only requires calculating class probabilities, while entropy involves calculating logarithmic functions. In practice, both criteria often lead to similar results, and the choice between them may depend on personal preference or specific requirements of the problem.  Tree depth:  If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, when the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances however you may have other problems such as bias. You will have a high bias in shallow decision trees. It means your decision making process is too naive.  Pruning is a technique used to reduce overfitting in decision trees by removing unnecessary branches. Pre-pruning involves stopping the tree construction early based on certain conditions, such as reaching a maximum depth or minimum number of instances in a leaf node. This prevents the tree from becoming too complex and overfitting the training data. Post-pruning, also known as backward pruning, constructs the full tree first and then prunes it by removing nodes that do not improve the tree's performance on a validation set. Pruning helps improve the generalization ability of decision trees and prevents them from memorizing noise or specific instances in the training data.  Decision trees : advantages and disadvantages:  Advantages:  ✓  Very easy to understand, as they represent rules. ✓  Capable of modelling nonlinear functions. ✓  Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a  Euclidean distance between two vectors having weather as a variable.)  Disadvantages:  ✓  May overfit easily. As we have said before, by building deep decision trees you are at high  risk of overfitting and a high variance model.  ✓  Only axis-aligned splits. Normal decision trees split the space along each features  independently. If we need to make a more complex decision tree model, we can consider joint probabilities or more complicated scenarios while modelling the tree.  ✓  Trees may not be as competitive in terms of accuracy as some of the other regression and  classification techniques such as SVM or neural networks.     