Summary KNN algorithm and its variants The  k-nearest  neighbours  algorithm,  also  known  as  KNN  or  k-NN,  is  a  non-parametric,  supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an  individual  data  point.  While  it  can  be  used  for  either  regression  or  classification  problems,  it  is typically used as a classification algorithm, working off the assumption that similar points can be found near one another (IBM, 2023).  The following figure illustrates this concept. If a black circle as a test point falls into a region in which the closest point is a black ellipse with class label of 1, based on the nearest neighbour, we are going to label this new sample as class 1.  But also, K in KNN can vary. Let’s say someone would like to check K nearest neighbours (KNN) of the test point in order to make a decision. You can label a test instance to the same as the majority label of the K-nearest neighbours. The figure below is an example a of 3-NN classification.  1 KNN  3KNN  Classification vs Regression For classification problems, a class label is assigned on the basis of a majority vote—i.e. the label that is most frequently represented around a given data point is used. While this is technically considered “plurality voting”, the term, “majority vote” is more commonly used in literature. The distinction between these terminologies is that “majority voting” technically requires a majority of greater than 50%, which primarily works when there are only two categories. When you have multiple classes—e.g. four categories, you don’t necessarily need 50% of the vote to make a conclusion about a class; you could assign a class label with a vote of greater than 25%.  Regression problems use a similar concept as classification problem, but in this case, the average the k nearest neighbours is taken to make a prediction about a classification. The main distinction here is that classification is used for discrete values, whereas regression is used with continuous ones. However, before a classification can be made, the distance must be defined. Euclidean distance is most commonly used.  For finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format.     Distance Metrics To recap, the goal of the k-nearest neighbor algorithm is to identify the nearest neighbors of a given query point, so that we can assign a class label to that point. In order to do this, KNN has a few requirements:  Determine your distance metrics. In order to determine which data points are closest to a given query point, the distance between the query point and the other data points will need to be calculated. These distance metrics help to form decision boundaries, which partitions query points into different regions. You commonly will see decision boundaries visualized with Voronoi diagrams.  As a Voronoi diagram, you can see it is based on closest neighbours; the same concept as KNN. Also, the data is not linearly separable, and it results in complex boundaries and decision rules. Remember that the decision surface is formed by the training examples.  Basically, we can assign weights to the neighbours based on their distance from the test point. For example, weight may be inverse square of the distances.  This means the higher the distance of the neighbour, the lower its weight. All training points may influence a particular instance. This method is also known as Shepard’s method.  Some distance examples are:  Euclidean distance (p=2): This is the most commonly used distance measure, and it is limited to real- valued vectors. Using the below formula, it measures a straight line between the query point and the other point being measured.  Manhattan distance (p=1): This is also another popular distance metric, which measures the absolute value between two points. It is also referred to as taxicab distance or city block distance as it is commonly visualized with a grid, illustrating how one might navigate from one address to another via city streets.  Minkowski distance: This distance measure is the generalized form of Euclidean and Manhattan distance metrics. The parameter, p, allows for the creation of other distance metrics. Euclidean distance is represented by this formula when p is equal to two, and Manhattan distance is denoted with p equal to one.  Hamming distance: This technique is used typically used with Boolean or string vectors, identifying the points where the vectors do not match. As a result, it has also been referred to as the overlap metric.   Defining k The k value in the k-NN algorithm defines how many neighbours will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbour. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.  Figure. Illustrates a sample plot of Number of Neighbours (K) and misclassification error. As you can see the minimum value of the error occurs when K=7.  Impact of distance metrics on KNN performance KNN is a well known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data. Please use the following link for further explanation.  Decision Tree A decision tree is a map of the possible outcomes of a series of related choices.  Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.  A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes (IBM, 2023).   As you can see from the diagram above, a decision tree starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset. As an example, let’s imagine that you were trying to assess whether or not you should go surf, you may use the following decision rules to make a choice:  his type of flowchart structure also creates an easy to digest representation of decision-making, allowing different groups across an organization to better understand why a decision was made.  Decision tree learning employs a divide and conquer strategy by conducting a greedy search to identify the optimal split points within a tree. This process of splitting is then repeated in a top- down, recursive manner until all, or the majority of records have been classified under specific class labels.  Whether or not all data points are classified as homogenous sets is largely dependent on the complexity of the decision tree. Smaller trees are more easily able to attain pure leaf nodes—i.e. data points in a single class. However, as a tree grows in size, it becomes increasingly difficult to maintain this purity, and it usually results in too little data falling within a given subtree. When this occurs, it is known as data fragmentation, and it can often lead to overfitting. As a result, decision trees have preference for small trees, decision trees should add complexity only if necessary, as the simplest explanation is often the best.    To reduce complexity and prevent overfitting, pruning is usually employed; this is a process, which removes branches that split on features with low importance. The model’s fit can then be evaluated through the process of cross-validation. Another way that decision trees can maintain their accuracy is by forming an ensemble via a random forest algorithm; this classifier predicts more accurate results, particularly when the individual trees are uncorrelated with each other.  Types of Decision Trees - ID3: Ross Quinlan is credited within the development of ID3, which is shorthand for “Iterative Dichotomiser 3.” This algorithm leverages entropy and information gain as metrics to evaluate candidate splits. Some of Quinlan’s research on this algorithm from 1986 can be found here (PDF, 1.4 MB) (link resides outside of ibm.com).  - C4.5: This algorithm is considered a later iteration of ID3, which was also developed by Quinlan. It can use information gain or gain ratios to evaluate split points within the decision trees.  - CART: The term, CART, is an abbreviation for “classification and regression trees” and was introduced by Leo Breiman. This algorithm typically utilizes Gini impurity to identify the ideal attribute to split on. Gini impurity measures how often a randomly chosen attribute is misclassified. When evaluating using Gini impurity, a lower value is more ideal.  Regression trees Decision trees that use a regression model are called regression trees.   Entropy values can fall between 0 and 1. If all samples in data set, S, belong to one class, then entropy will equal zero. If half of the samples are classified as one class and the other half are in another class, entropy will be at its highest at 1. In order to select the best feature to split on and find the optimal decision tree, the attribute with the smallest amount of entropy should be used. Information gain represents the difference in entropy before and after a split on a given attribute. The attribute with the highest information gain will produce the best split as it’s doing the best job at classifying the training data according to its target classification.  Pruning Pre-pruning In pre-pruning, we decide during the building process when to stop adding nodes (eg. by looking at entropy).  Let’s say we are splitting nodes by checking the amount of entropy reduction when we select different features. We can stop splitting nodes when the entropy reduction is not significant. By using this method we are eliminating an unnecessary complexity on the model. However, this may be problematic.  Sometimes attributes individually do not contribute much to a decision, but combined, they may have a significant impact.  Post-pruning Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement. Consider the selected subtree (in red) in the figure below. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error.    Check which subtree removal introduces the smallest error and replace it with a single leaf node.  As the figure shows, you wait until the full decision tree has been built, then go for subtrees and check whether you can replace it with a single node or feature, while incurring only a small amount of change in Entropy.  If yes, trim the tree. If not, you should keep that subtree because it probably has useful information.  Advantages and disadvantages of Decision Trees While decision trees can be used in a variety of use cases, other algorithms typically outperform decision tree algorithms. That said, decision trees are particularly useful for data mining and knowledge discovery tasks. Let’s explore the key benefits and challenges of utilizing decision trees more below:  Advantages - Easy to interpret: The Boolean logic and visual representations of decision trees make them easier to understand and consume. The hierarchical nature of a decision tree also makes it easy to see which attributes are most important, which isn’t always clear with other algorithms, like neural networks.  - Little to no data preparation required: Decision trees have a number of characteristics, which make it more flexible than other classifiers. It can handle various data types—i.e. discrete or continuous values, and continuous values can be converted into categorical values through the use of thresholds. Additionally, it can also handle values with missing values, which can be problematic for other classifiers, like Naïve Bayes.  - More flexible: Decision trees can be leveraged for both classification and regression tasks, making it more flexible than some other algorithms. It’s also insensitive to underlying relationships between attributes; this means that if two variables are highly correlated, the algorithm will only choose one of the features to split on.  Disadvantages - Prone to overfitting: Complex decision trees tend to overfit and do not generalize well to new data. This scenario can be avoided through the processes of pre-pruning or post-pruning. Pre- pruning halts tree growth when there is insufficient data while post-pruning removes subtrees with inadequate data after tree construction.  - High variance estimators: Small variations within data can produce a very different decision tree. Bagging, or the averaging of estimates, can be a method of reducing variance of decision trees. However, this approach is limited as it can lead to highly correlated predictors.  - More costly: Given that decision trees take a greedy search approach during construction, they can be more expensive to train compared to other algorithms.  Feature importance of using Decision Trees (DT) Decision Trees are a popular machine learning algorithm that uses feature selection to determine th e most important classification features. DT operates by recursively segmenting the data into subset s based on the most informative features until a stopping criterion is reached. A criterion such as inf ormation gain or the Gini index is used to determine which feature is the most informative. At each node of the tree, the feature with the highest score is chosen as the splitting criterion. The significan ce of each feature can be determined by considering how much it contributes to overall improveme nt in the criterion. The greater the contribution, the more significant the feature. Feature  importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model.  Reading List IBM, 2023. Decision Trees. [Online] Available at: https://www.ibm.com/topics/decision- trees?mhsrc=ibmsearch_a&mhq=decision%20trees [Accessed 20 May 2023].  IBM, 2023. K-Nearest Neighbors Algorithm. [Online] Available at: https://www.ibm.com/topics/knn#:~:text=The%20k%2Dnearest%20neighbors%20algorithm%2C%20 also%20known%20as%20KNN%20or,of%20an%20individual%20data%20point. [Accessed 20 May 2023].  Keboola, 2020. The Ultimate Guide to Decision Trees for Machine Learning. [Online] Available at: https://www.keboola.com/blog/decision-trees-machine-learning [Accessed 21 mAY 2023].  Singh, A., 2023. KNN algorithm: Introduction to K-Nearest Neighbors Algorithm for Regression. [Online] Available at: https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction- regression-python/ [Accessed 21 May 2023].  Reflection KNN    KNN is sensitive to outliers and missing values and hence we first need to impute the missing  values and get rid of the outliers before applying the KNN algorithm.    Choosing the right measure of distance is important.   Choosing the right value of K is critical   Using the elbow method to determine the k value or the grid search technique to find the  best k value. (Singh, 2023)  Decision Trees    Feature importance can be used to identify the most relevant features for classification and  for feature selection to improve the performance of the model.    How to avoid overfitting  o  Minimum samples for leaf split. Determine the minimum number of data points  which need to be present at leaf nodes.  o  Setting maximum depth allows you to determine how shallow or deep a tree can get. Shallower trees will be less accurate but will generalize better, while deeper trees will be more accurate on the training set, but have generalization issues with new data.  o  Post- Pruning preserves a high tree performance, while lowering the complexity  (number of branches and splits) of the model.  o  Ensemble methods combine multiple trees into an ensemble algorithm. The  ensemble uses tree ‘voting’ as a mechanism to determine the true answer. A special case of ensemble methods is the random forest, one of the most successful and widely used algorithms in artificial intelligence.  o  Feature selection or dimensionality reduction. When data is sparse, decision trees overfit. To avoid overfitting on sparse data, either select a subset of features or reduce the dimensionality of your sparse dataset with appropriate algorithms (e.g. PCA).  o  Boosted decision trees correct the overfitting by using the standard machine  learning method of boosting. Build shallow decision trees (e.g. three levels deep) and with each iteration, build a new decision tree onto the data partition that had the worst splitting metric. This way, you’ll improve the overall performance of your trees by avoiding overfitting.    To mitigate decision trees’ bias towards predicting the dominant class, make sure to adjust class imbalance before fitting your model. There are three approaches for tackling class imbalance in the pre-processing stage (or data cleaning stage):  o  Down sample the majority class. o  Up sample the minority class. o  Collect more data for the minority class. (Keboola, 2020)     