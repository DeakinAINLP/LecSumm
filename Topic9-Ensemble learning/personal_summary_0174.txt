This topic we are mainly focus on KNN algorithm and its variants. The k-nearest neighbors (KNN) algorithm is a popular supervised machine learning algorithm used for classification and regression tasks. It is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying data distribution. In the KNN algorithm, the "k" refers to the number of nearest neighbors to consider.  A decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weigh possible actions against one another based on their costs, benefits, and probabilities. A decision tree typically starts with a single root node, which branches into possible outcomes.  Decision trees that use a regression model are called regression trees. We can alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used.  Classification and Regression Trees (CART) is a term introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modeling problems.  It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response. For a classification tree, we assign each test instance to the majority class (mode) of the training instances in the region where it belongs. You can consider this action as a being like a data point voting itself into a region which results in selecting the majority.  The tree-building process that we described in previous steps may produce good predictions on the training set, but it’s likely to overfit the data, leading to poor generalization performance.    A tree that has a large number of regions may have only few data points per region resulting in  high variance.    On the other hand, having a small number of regions may result in high bias.   One possible alternative is to grow a large tree, and then prune it backin order to obtain a  subtree.  Generally, there are several ways of pruning trees:    Pre-pruning (forward pruning)   Post-pruning (backward pruning)  Decision trees : advantages and disadvantages  What are the unique features of decision trees? What problems are best suited for their use?  Advantages    Very easy to understand, as they represent rules.   Capable of modelling nonlinear functions.   Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a  Euclidean distance between two vectors having weather as a variable.)  Disadvantages    Sensitive to small changes in the data. If you add few data points or change some small values,  your rules can be changed!    May overfit easily. As we have said before, by building deep decision trees you are at high risk of  overfitting and a high variance model.    Only axis-aligned splits. Normal decision trees split the space along each features independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.    Trees may not be as competitive in terms of accuracy as some of the other regression and  classification techniques such as SVM or neural networks.     