This topic we learnt about KNN and Decision trees. KNN is a classifier that works by assigning a class to data points based on the closeness of K nearest neighbouring data points. KNN is a useful classification algorithm as it does not have a dedicated training phase and can handle nonlinear decision boundaries easily. However, KNN can be computationally expensive and can struggle with imbalance class problems as new points tend to be dominated by the majority class.  Decision trees are a way of mapping out the possible outcomes based on previous decisions or states. Decision trees are useful as they can be used to represent nonlinear functions and can handle categorical variables easily .Although, a disadvantage of decision trees is that they are very sensitive to small changes in data. Due to the fact that at each node there is a rule that determines where a value should be placed in terms of the leaves of the tree. If another data point is then added to the set after the tree has been constructed, then the rules will have to be changed depending on what that data point is.  Reflection: I knew what KNN was but I didnâ€™t have a very good understanding of it. This topic's content has been great at making me actually understand how KNN works and getting a deeper understanding of it. The only exposure to decision trees I had previously had were trees that worked with discrete or categorical values only. So it was interesting to see them being used with continuous values. I can definitely see the use of KNN in data science when it comes to classification, especially object recognition.   