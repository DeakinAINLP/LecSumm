 A. KNN algorithm and its variants:  a) assign weights to neighboring data points so the nearer neighbors contribute  more to the average than more distant ones; b) Useful in both classification and regression; c) Nearest neighbour (NN)  B. Theory of KNN  a) To find the majority of decisions based on the close training points b) Continuous valued target function: Mean value of the k nearest training  examples  c) Discrete class label: Mode of the class labels of the k nearest training  d)  examples In mathematics, a Voronoi diagram is partitioning of a plane into regions based on distance to points in a specific subset of the plane.  e) Shepard’s method: we can assign weights to the neighbours based on their  distance from the test point: higher distance, lower weight.  C. Best number of neighbors (K)  a) K controls the shape of the decision boundary b) A small K: low bias and high variance (more focused on the close regions and  neighbors)  c) A large K: lower variance but increased bias (asking for more information) d) Can use cross-validation to partition your data into test and training samples  and evaluate your model with different ranges of k values.  D. Decision trees  a) A decision tree is a map of the possible outcomes of a series of related  choices.  b) Typically starts with a single root node, which branches into possible  outcomes.  E. Regression trees  a) Decision trees that use a regression model are called regression trees b) Procedure: We divide the feature space, i.e., the set of possible values for  x1...xd into J distinct and non-overlapping regions R1...Rj. For every instance that falls into region Rj, we make the same prediction, which is simply the mean (or mode) of response values for the training observations in Rj.  c) The overall goal of regression trees is to find regions R1...Rj that minimize the  training error (sum of square error).  d) To find the optimal solution, we take a top-down, greedy approach that is  known as recursive binary splitting.  e) We first select a feature xj and a threshold s such that splitting the feature  space into the regions and leads to the best possible reduction in training error. So we are not going into the joint space of all features, but we work on a independent feature form such as xj and a threshold s.  f) Next, we repeat the process, looking for the best feature and the best  threshold in order to split the data further to minimize the error in each of the resulting regions.  g) However, this time, instead of splitting the entire feature space, we only split  one of the two previously identified regions.  h) The splitting process continues until a stopping criterion is reached. For  example, we may continue until no region contains more than five instances or the nodes are getting too pure or sparse.  F. Classification trees  a)  It’s similar to regression trees and predicts a qualitative response rather than a quantitative response.  b) For a classification tree, we assign each test instance to the majority class  c)  d)  (mode) of the training instances in the region where it belongs. In the classification setting, we use classification error rate as a criterion for making the binary splits. If Certainty of Distribution (COD) is close to 1, it means almost all of the training points inside a region are voting for a certain class label. So the classifier in this case is certain about the decision. On the other hand, when CoD is 0.5, it means we can not trust the votes.  e) Gini index: measure of total variance across K classes, a measure of node  purity, and inequality.  G. Decision tree algorithms  ID3 (Iterative Dichotomiser 3) uses entropy  a) b) Calculate the entropy of every feature using the data set S. Split the set S into  subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for the attribute or feature and it will gain more information.  c) Make a decision tree node containing that feature. d) Recurse on subsets using remaining features. e)  If you build a very deep tree, you are basically partitioning the feature space into small regions -- high variance  f) When the regions are very big and you have a shallow tree, you may have a  high bias.  H. Model complexity and pruning  a) Pruning is a technique that reduces the size of decision trees by removing  sections of tree that provide little power to classify instances. As a tree cannot be too large or too small, so one possible alternative is to grow a large tree, and then prune it back in order to obtain a subtree.  b) Pre-pruning: we decide during the building process when to stop adding  nodes (eg. by looking at entropy).  c) Post-pruning: it waits until the full decision tree has been built and then  prunes the attributes by subtree Replacement. Check which subtree removal introduces the smallest error and replace it with a single leaf node.  I. Decision trees : advantages and disadvantages  a) Advantages: easy to understand, can handle non-linear functions, handle  categorical variables.  b) Disadvantages: Sensitive to small changes in the data, overfit easily, Only axis-  aligned splits, not the best options when comparing accuracy.  J. Advanced topics:  a) The effectiveness of KNN can be significantly impacted by the distance metric  that is selected.  b) Decision trees can select features.  