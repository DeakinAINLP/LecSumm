   Summary: o  KNN:  ▪  Algorithm & Variants:    The  K-Nearest  Neighbours  (KNN)  technique  is  an  instance-based learning  method  that  may  be  used  for  classification  as  well  as regression.  It  generates  a  new  observation  based  on  the  most common class or the average of the K-most similar examples in a reference  database.  The  KNN  algorithm comprises  initialising  the data,  deciding  on  a  value  for  K  (the  number  of  neighbours), computing  the  distance  between  the  test  and  training  data,  and sorting  these  distances  to  choose  the  first  K  entries.  The programme then gives labels to the K labels based on their mean (for regression) or mode (for classification). Weighted KNN assigns more weight to closer points; Radius-based neighbour learning considers all neighbours within a certain radius; KD  Tree  KNN  and  Ball  Tree  KNN,  both  of  which  use  tree  data structures to organise points in space; and Locally Sensitive Hashing (LSH),  an  approximate  algorithm  that  speeds  up  the  process  for large  datasets.  The  version  chosen is  determined  by  the characteristics  of  the  issue,  such  as  dataset  size,  dimensionality, type, and available computer resources.    Best  number  of  neighbours  (K):  Choosing  the  right  number  of neighbours (K) in the K-Nearest Neighbours (KNN) method is critical since it has a big impact on model performance. Cross-validation, the  square  root  approach,  and  using  an  odd  K  for  binary classification are all strategies for determining K. Cross-validation entails partitioning the dataset into k subgroups, testing on one and training  on  the  others,  and  repeating  this  procedure  k  times. According to the square root approach, K should be set equal to the square  root  of  the  total  number  of  data  points.  An  odd  K  can prevent ties in binary classification. To avoid overfitting (when K is too small) or underfitting (when K is too high), the ideal K should be established through validation or cross-validation. o  Decision  Tree  (DT):  A  decision  tree  is  a  flexible  and  interpretable  model  for classification  and  regression  problems  that  can  handle  both  category  and numerical  data.  It  employs  a  tree-like  structure  in  which  each  internal  node represents a characteristic or attribute, each branch represents a decision rule, and each leaf node represents a result. The decision process is initiated by the root node at the top of the tree.  ▪  Regression/classification trees: Decision trees are divided into two types: regression  trees  and  classification  trees,  which  are  used  to  predict numerical and categorical variables, respectively.   The dataset is divided into smaller groups via regression trees, and the average  value  of  the  dependent  variable  is  obtained  for  each  group. Selecting  a  split  point  in  your  features,  computing  the  average  of  the dependent variable for data points on each side of the split, calculating the total error for each area, and picking the split point with the least total error  are  the  stages  in  creating  a  regression  tree.  This  operation  is continued until a halting condition is met. Classification  trees  function  similarly,  except  instead  of  computing averages, they employ metrics such as Gini impurity or entropy to assess how well a split divides the classes. The split point with the least amount of impurity or entropy is selected. Both forms of trees seek to produce pure nodes, in which all data points in a node belong to the same class or have identical values. However, if decision  trees  are  not  pruned  or  controlled  in  some  manner,  for  as  by specifying a maximum depth for the tree, they might overfit the training data.  ▪  Decision  tree  algorithms:  In  supervised  learning,  numerous  techniques are available for generating decision trees. These algorithms are critical in determining how to partition data and give values to leaf nodes. The Iterative Dichotomiser 3, or ID3, invented by Ross Quinlan is one such approach. The ID3 algorithm, which is commonly used for classification problems,  decides  the  separation  points  using Information  Gain. However, it has limitations in that it does not handle numeric properties or missing data. As a result, data pre-processing may be required while utilising this approach. The C4.5 algorithm, also invented by Ross Quinlan, is an extension of the ID3. C4.5 improves on ID3 in various ways, including the ability to handle both  continuous  and  categorical  data,  as  well  as  missing  values.  It  also enables for the trimming of trees after they have been planted in order to minimise overfitting. To counteract the tendency towards qualities with multiple  outcomes,  the  algorithm  employs  Gain  Ratio,  a  variation  of Information Gain. Another  prominent  option  is  the  Classification  and  Regression  Trees (CART)  method.  It  may  be  used  for  classification  as  well  as  regression applications. It utilises the Gini Index as a criterion for choosing splits in classification, and the reduction in variance in regression. CART, unlike ID3 and C4.5, constructs binary trees with only two outgoing edges for each node. The fundamental advantage of CART is its simplicity and versatility. The  Chi-squared  Automatic  Interaction  Detection  (CHAID)  method determines the optimal splits using the Chi-squared test. It can generate more  than  two  branches  from  a  single  node  and  perform  multi-class categorisation, which makes it popular in market research.    The  Multivariate  Adaptive  Regression  Splines  (MARS)  technique  is  a flexible regression approach that creates a model in the form of a decision tree. Although it is not a standard decision tree method, it may capture non-linear  connections  between  features  and  the  target  variable  and manage feature interactions. Finally, Random Forests should be mentioned. Despite the fact that they are essentially an ensemble approach that combines numerous decision trees, they are extensively employed owing to their efficacy. Each of these algorithms has its own set of advantages and disadvantages. As  a  result,  the  characteristics  of  the  situation  should  dictate  the algorithm selection.  ▪  Model  complexity  &  pruning:  In  machine  learning,  the  complexity  of  a decision tree is often related to its depth, with deeper trees being more complicated.  While  complicated  models  may  detect  subtle  patterns  in training data, they are vulnerable to overfitting, which occurs when they get so tuned to the training data that they perform badly on fresh data. Pruning is a method used to combat overfitting. Pruning minimises the size  of  a  tree  by  removing  portions  that  contribute  little  to  identifying instances, lowering model complexity and boosting prediction accuracy. Pruning is done in two stages: pre-pruning and post-pruning. Pre-trimming, also known as early halting, is the practise of pruning the decision tree as it is being built. This can include tactics such as restricting the tree's maximum depth, raising the minimum necessary occurrences per leaf, or establishing a minimum required impurity drop at each split. Post-trimming, on the other hand, entails forming the entire tree before pruning it. To analyse the consequences of trimming nodes from the tree, this  approach  employs  a  validation  set.  Although  post-pruning  is  more precise than pre-pruning, it is also more computationally demanding. While both pre- and post-pruning are useful in preventing overfitting, the ideal method will vary depending on the problem and dataset. It's also worth  mentioning  that,  while  decision  trees  are  powerful  and  easy  to understand  models,  they  don't  necessarily  have  the  highest  prediction accuracy. As a result, they are frequently combined with other models in ensemble approaches such as Random Forest or Gradient Boosting.    Reading list: Lecture Slides, Lecture Recordings, Learning Contents.   My reflections: The K-Nearest Neighbours (KNN) methodology is a type of instance- based learning method that may be used for classification and regression applications. It generates new observations by using the most common class or the average of the K-most similar cases in the dataset. Weighted KNN, radius-based neighbour learning, KD Tree KNN, Ball Tree KNN, and Locally Sensitive Hashing (LSH) are all KNN versions that are suited for diverse data properties and processing resources. Choosing  the  right  number  of  neighbours  (K)  in  KNN  is  critical  since  it  has  a  big influence  on  model  performance.  Cross-validation,  the  square  root  method,  and    employing an odd K for binary classification are all methods for calculating K. Cross- validation entails splitting the dataset and testing on different subsets, whereas the square  root  method  recommends  setting  K  equal  to  the  square  root  of  the  total number of data points. In binary classification, using an odd K helps to avoid ties. Another adaptable and interpretable paradigm for classification and regression is the decision tree. It employs a tree-like structure, with each internal node representing a feature, each branch representing a decision rule, and each leaf representing a result. Regression trees and classification trees are two types of decision trees that are used for numerical and categorical data, respectively. ID3,  C4.5,  CART,  CHAID,  MARS,  and  Random  Forests  are  some  of  the  decision  tree algorithms available. These algorithms govern how data is partitioned and values are assigned  to  leaf  nodes.  Each  has  advantages  and  disadvantages,  therefore  the  best method depends on the task at hand. The depth of a decision tree frequently corresponds with its complexity. While more complicated  models  may  catch  subtle  patterns,  they  run  the  danger  of  overfitting, which  occurs  when  the  model  becomes  highly  sensitive  to  the  training  data  and performs poorly on fresh data. Pruning is a method of reducing overfitting by removing portions  of  the  tree  that  have  minimal  classification  power.  Pruning  can  be  done before or after the tree is built. While decision trees are strong and simple to grasp, they may not always be the most accurate predictors. As a result, they are frequently utilised in ensemble approaches such as Random Forest or Gradient Boosting in conjunction with other models.  