Introduction to Topic 8  Some of the key areas you will explore this topic are listed below:  - K nearest neighbour (KNN) algorithm - Decision tree (DT)  1 1 1 1 5 7 9 9 10 12 12 13 13 14 14 14 14  8.2 KNN algorithm and its variants  KNN algorithm and its variants  A useful technique can be to assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.  This is useful for both classification and regression.  The basic idea is to label the test data point as the same as the nearest neighbour (NN). The following figure illustrates this concept. If a black circle as a test point falls into a region in which the closest point is a black ellipse with class label of 1, based on the nearest neighbour, we are going to label this new sample as class 1.  Figure KNN intuition.  But also K in KNN can vary. Let's say someone would like to check K nearest neighbours (KNN) of the test point in order to make a decision. You can label a test instance the same as the majority label of the K-nearest neighbours. The figure below is an example of 3− NN classification.  Figure 3−NN classification.  As you can see in the figure, a new test point falls into the scope of two training points from class 0 and one from class 1. Obviously you will trust the 0 class due to majority and label the point as 0  8.3 Theory of KNN  Voronoi diagrams are maps that divide a space into different regions based on the closest points. Imagine you have a bunch of dots on a piece of paper, and you draw lines between each dot and its nearest neighbours. The areas enclosed by these lines are the regions of the Voronoi diagram. Each region represents the space that is closer to a specific dot than any other dot. Voronoi diagrams are useful for things like finding the nearest neighbour, locating facilities, or understanding spatial relationships.  Voronoi diagrams and Delaunay triangulation are ways to divide a group of points on a plane.  Voronoi Diagrams: Imagine you have a bunch of dots on a piece of paper. To create a Voronoi diagram, you draw lines between each dot and its nearest neighbours. The regions enclosed by these lines are the Voronoi cells. Each cell represents the area closest to a specific dot.  Delaunay Triangulation: It's like connecting the dots with triangles in a smart way. In a Delaunay triangulation, you draw triangles between the points, making sure that no point is inside the circle formed by any triangle. This creates a network of triangles that cover the entire set of points.  Voronoi diagrams and Delaunay triangulation are closely related. The lines in the Voronoi diagram correspond to the edges of the triangles in the Delaunay triangulation.  These methods have various applications, like finding the closest point, analysing spatial relationships, or creating computer graphics. They help us understand and work with groups of points in a simple and efficient way.  8.4 Best number of neighbours (K)  8.5 Decision trees  A decision tree is a map of the possible outcomes of a series of related choices.  Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.  A decision tree typically starts with a single root node, which branches into possible outcomes. In the video you will see an example of how a decision tree can be used to predict commute time.  8.6 Regression trees  8.7 Classification trees  The Gini Index is used in economics to measure income or wealth inequality. It helps analyse and compare inequality within populations and across countries. The index is applied to assessing poverty levels, evaluating policies, studying economic growth, and understanding consumer behaviour. It provides insights into income distribution and assists policymakers in addressing inequality issues.  Images sourced from google images.  https://www.google.com/url?sa=i&url=https%3A%2F%2Fwol.iza.org%2Farticles%2Fmeasuring-i ncome-inequality%2Flong&psig=AOvVaw0bEdl9N1KDpa38fJ9ysH7r&ust=1683510007075000& source=images&cd=vfe&ved=0CBIQjhxqFwoTCPimvtGJ4v4CFQAAAAAdAAAAABA-  8.8 Decision tree algorithms  Decision tree algorithms There are a variety of algorithms for decision trees.  Here are three of the more popular ones:  ID3 (Iterative Dichotomiser 3) uses Entropy. C4.5 (Successor of ID3) slightly more advanced version of ID3 and also uses Entropy. CART (Classification and Regression Tree) uses Gini impurity. We will look at ID3 in detail.  The ID3 Algorithm The algorithm was developed by Ross Quinlan in 1975 (He is an Australian who graduated from University of Sydney). It’s used to generate a decision tree from a dataset. Although this method is simple, it is an effective machine learning algorithm. The basic algorithm is as follows:  Calculate the entropy of every feature using the data set . Split the set into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information. Make a decision tree node containing that feature. Recurse on subsets using remaining features. Tree depth If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, when the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances, however you may have other problems such as bias. You will have a high bias in shallow decision trees. It means your decision making process is too naive. If you need to revise these concepts they are described in topics 7 and 8.  You need to create a decision tree of the right depth, to find the sweet spot in terms of depth. You can achieve that by performing the cross validation and other evaluation methods you have learned in topics 5 and 6.  Remember, you need to find or tune the proper hyperparameter which is the depth of the tree.  In the context of decision trees, entropy is a measure of impurity or disorder within a set of data. It is used to determine the best attribute to split the data when constructing a decision tree.  TUT: https://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf  8.9 Model complexity and pruning  Model complexity and pruning Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances.  The tree-building process that we described in previous steps may produce good predictions on the training set, but it’s likely to overfit the data, leading to poor generalisation performance.  A tree that has a large number of regions may have only few data points per region resulting in high variance. On the other hand, having a small number of regions may result in high bias. One possible alternative is to grow a large tree, and then prune it back in order to obtain a subtree. Generally there are several ways of pruning trees:  Pre-pruning (forward pruning) Post-pruning (backward pruning) Pre-pruning In pre-pruning, we decide during the building process when to stop adding nodes (eg. by looking at entropy).  Let’s say we are splitting nodes by checking the amount of entropy reduction when we select different features. We can stop splitting nodes when the entropy reduction is not significant. By using this method we are eliminating an unnecessary complexity on the model. However, this may be problematic.  Sometimes attributes individually do not contribute much to a decision, but combined, they may have a significant impact.  Let’s discuss another model which can handle this problem.  Post-pruning Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement. Consider the selected subtree (in red) in the figure below. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error.  Figure. Post-pruning. Source (Sayad 2018) Check which subtree removal introduces the smallest error and replace it with a single leaf node.  As the figure shows, you wait until the full decision tree has been built, then go for subtrees and check whether you can replace it with a single node or feature, while incurring only a small amount of change in Entropy.  If yes, trim the tree. If not, you should keep that subtree because it probably has useful information.  Overfitting occurs when a model becomes overly complex and captures noise or irrelevant patterns from the training data, leading to poor generalisation. Pruning is the process of simplifying a decision tree by removing unnecessary nodes and branches. It addresses overfitting by reducing complexity and focusing on the most important features and decision rules. Pruning helps strike a balance between capturing patterns and avoiding overfitting, resulting in more robust and generalised models.  8.10 Decision trees advantages and disadvantages  What are the unique features of decision trees? What problems are best suited for their use?  Advantages  ● Very easy to understand, as they represent rules. ● Capable of modelling nonlinear functions. ● Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot  compute a Euclidean distance between two vectors having weather as a variable.)  Disadvantages  ● Sensitive to small changes in the data. If you add a few data points or change some  small values, your rules can be changed!  ● May overfit easily. As we have said before, by building deep decision trees you are at  high risk of overfitting and a high variance model.  ● Only axis-aligned splits. Normal decision trees split the space along each feature  independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.  ● Trees may not be as competitive in terms of accuracy as some of the other regression  and classification techniques such as SVM or neural networks.  8.11 Advance topics  Impact of distance metrics on KNN performance  KNN is a well known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data. Please use the following link for further explanation.  References:  1. Prasath, V. B., et al. "Distance and Similarity Measures Effect on the  Performance of K-Nearest Neighbor Classifier--A Review." arXiv preprint arXiv:1708.04321 (2017).  Feature importance of using Decision Trees (DT)  Decision Trees are a popular machine learning algorithm that uses feature selection to determine the most important classification features. DT operates by recursively segmenting the data into subsets based on the most informative features until a stopping criterion is reached. A criterion such as information gain or the Gini index is used to determine which feature is the most informative. At each node of the tree, the feature with the highest score is chosen as the splitting criterion. The significance of each feature can be determined by considering how much it contributes to an overall improvement in the criterion. The greater the contribution, the more significant the feature. Feature importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model.  