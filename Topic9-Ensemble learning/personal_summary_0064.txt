This topic we learnt about the non-linear models like KNN ( K-Nearest Neighbors ) Algorithm and DT ( Decision Tree ). These are commonly used in ML for classification and regression tasks. Here’s a brief explanation of each model: (i) KNN is a non-parametric and instance-based learning algo. It’s a simple yet powerful classification algo, where the class of a new data point is predicted based on the majority class of its k nearest neighbors in the feature space. It can handle non-linear decision boundaries and is suitable for both binary and multi-class classification tasks. (ii) DT is a flowchart-like structure where each internal node represents a feature, each branch represents a decision rule, and each leaf node represents the outcome or class label. DTs recursively partition the feature space based on attribute values to make decisions. They are capable of modeling complex non-linear relationships between features and target variables. Both KNN and DT have their strengths and weaknesses. KNN is simple to understand and implement, while DTs offer interpretability. KNN can be sensitive to outliers and requires a large amount of memory to store training instances, whereas DTs can handle high-dimensional data but may suffer from overfitting .The choice between the two depends on the nature of the problem, dataset size, interpretability requirements, and the trade-off between simplicity and performance.   