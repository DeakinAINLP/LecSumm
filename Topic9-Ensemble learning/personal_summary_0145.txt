This topic we learned many topics including KNN algorithm and also I did this topic acivity task and learned more about training of the model. If I talk about learning of this topic Then i started my learning with knowing more about K-nearest neighbor. It works by ﬁnding the k nearest neighbors to a given data point and classifying or predicting the target variables based on the neighbours’ values. By this alogorithem which involves calculating distances between data points and determining the majority class or average value amoung neighbours, we can improve our model accuracy as I did in his topic task.  This topic we also learned abou decision tree algorithem, also learned about how the decsion tree is used to predict commute time. A(cid:332)er learning decison tree i learned how they could be used for regression problems. We basically use regression trees which are the type of decision tree used for regression problems.Our main goal for this tree is to divide the feature space into distinct and non overlapping regions. A(cid:332)er that for each region, a simple model is ﬁtted such as taking the mean of the response values for the training observation in that region.The process of building a regression tree, involves ﬁnding the regions that minimize the training error. This is done by selecting features and thresholds to split the feature space in a way that reduces the error the most. we say it as recursive binary splitting.I also earned about the decision tree how to use it with python and also to evaluate KNN using the python, and continued my learning including tree depth concept. I also learned about model complexity and pruning which are the techniques to improve the generalisation performance of DT by reducing its size and complexity. Pruning we talked about removing sections of a decision tree that contribute little to the classiﬁcation of instances. We do to avoid the overﬁtting,where the tree performs well on the training set but fails to generalize to new data. We learned also types of pruning like pre-pruning and post-pruning. Also came to know about what the positives and negatives are of using a decision tree. By the end of the topic I learned also about Distance metrics, such as Euclidean distance, Manhattan distance, and cosine similarity,which play a crucial role in determining the similarity or dissimilarity between data points.Also came to know that diﬀerent distance metrics capture diﬀerent notions of similarity and can have varying eﬀects on the performance of the KNN algorithm.  To sum up, this topic was ﬁlled with valuable insights into KNN, decision trees, model complexity, pruning, and distance metrics as i gained practical knowledge by implementing these concepts in Python and deepened my understanding of how these concepts contribute to improving machine learning models. I also added my this topic quiz task with this document ﬁle, and completed this topic programming task and got to know much about practical implementation of knn and splitting techniques.  