SIT720 Task 8.1P  Learning summary report  This topic provided an in-depth coverage of the KNN algorithm and its variants including Decision Trees. We started by exploring the KNN algorithm and the theory behind it.  We looked at ways that we can identify the best number of K neighbours for the algorithm.  We then dive deeper into decision tress. We explored node and ways of splitting them.  We also explored regression trees and how to use them for predictions. We then looked at classification trees and how we can utilise Gini Index and Entropy as a measure of inequality and in order to select a particular class for a data point.  We explored decision tree algorithms such as ID3 and looked at pre- and post-pruning techniques in order to reduce the depth of a tree.  The second part of the topic focused on the use of the Python programming language and how it can be used in order to perform classification using the KNN algorithm and how to implement decision trees using the SKLearn library.  External sources  I referenced and used the following resources during this topic in order to enhance my understanding of the KNN and decision trees algorithms.  Datacamp.com – K-Nearest Neighbors (KNN) Classification with scikit-learn https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit- learn          Datacamp.com – Decision Tree Classification in Python– https://www.datacamp.com/tutorial/decision-tree-classification-python  Datacamp.com - Machine Learning with Tree-Based Models in Python https://app.datacamp.com/learn/courses/machine-learning-with-tree-based- models-in-python  Kaggle, notebooks on decision trees https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial  https://www.kaggle.com/code/satishgunjal/tutorial-decision-tree/notebook  The SKLearn website and reference docs on performing KNN classification and the decision tree classifier.  Reflection  This topic provided me with an opportunity to learn more about using the KNN algorithm for classification and it was a great build on previous knowledge learned on nearest neighbours.  I learned more about decision trees, branching and pruning mechanisms and how to manage the model complexity.  I learned the advantages and disadvantages of using each of the above algorithms and the impact of the hyperparameters on the models, especially the distance metric for KNN and the feature selection in decision trees.  I also enjoyed diving deeper into the Python and scikit learn, I particularly enjoyed looking at various open-source notebooks and tutorials on performing classification with KNN and decision trees and of course measuring the performance of the models.       Quiz screenshot     