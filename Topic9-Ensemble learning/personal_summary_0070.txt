Task8.1P Nonlinear Models  Main Points  KNN Algorithms & Variants  For both classification and regression, a useful technique is to assign weights based  on the neighbors. The nearer neighbors contribute more to the average than the  distant ones. The basic idea is to label the test data point same as the nearest  neighbor.  Distance-weighted nearest neighbor algorithm  Assign weights to the neighbors based on the distance from the test point  Weight may be inverse square of the distances  Best numbers of neighbors(K)  K controls the shape of the decision boundary  Small values of K restrains the region of the given prediction  Forces classifier to be more focused on the close regions and neighbors, this will  result in a low bias and high variance  High values of K  Asking for more information from distant training points  Smoother decision boundaries  Finding the best K  There is no rule of thumb in selecting K since it depends on the desired rate of  exploration for K.  Cross-validation to partition the data into test and training samples  Evaluate model with different ranges of K values  Decision Trees  It is a map of the possible outcomes of the series of related options.  It weighs possible actions against one another  Typically starts with the single node  Formulation of Regression Trees  The overall goal of regression trees is to find regions that minimize the training error  It is computationally infeasible to consider every possible partition of the feature  space into regions.  Prediction using DT  Using the mean or mode of training instances in the region where the test  observation falls  Classification Trees  Replace the sum of square error by the classification error rate as the criterion for  making the binary splits.  The classification error rate is defined as the fraction of the training instances that  region that do not belong to the most common class.  Decision Tree Algorithms  calculate the entropy of the feature using the data set  Split the set into subsets using the feature for which entropy is minimum  Make the decision tree node containing the feature  Re-curse on subsets using remaining features  