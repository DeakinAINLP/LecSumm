 K Nearest Neighbor (KNN) Algorithm  KNN Theory  The KNN Algorithm assigns weights of data points based on the nearest neighboring data points to that point. This technique can be applied to classification and regression. The number of neighbors considered in classifying a data point is determined by K. For discrete data the mode must be used to decide between close data points, whereas in a continuous dataset the average must be used.    Continuous value target function: Mean value of the k nearest training examples.   Discrete class label: Mode of the class labels of the k nearest training examples.  Voroni Diagram  A Voroni diagram partitions a plane into regions based on the distance to points in subsets of the surface formed by training examples. Like KNN, the Voroni diagram is based on closest neighbors. This works well for data that is not linearly separable as it is able to predict complex boundaries and decision rules.  Variations of KNN  One example of variations of K Nearest Neighbours includes weighting the distance between points as well as using K. This is known as a distance-weighted nearest neighbor algorithm. Other distance metrics can also be used in the same manner.  Best Number of Neighbors  It’s important to select the right number of neighbors (K), as K influences the shape of the decision boundary, and therefore also influences bias and variance in the model. A high K value has a smoother decision boundary and results in a lower variance, but an increased bias. Classification of points with a high K value is highly sensitive to even distant surrounding points. Small K values reduce the number of neighbors the model pays attention to. This means it will have lower bias and a high variance.  Decision Tree (DT)  Decision Tree Algorithms  Decision trees map out the possible outcomes of a series of decisions. These can be used to weigh possible choices against each other based on their likelihoods, costs, and benefits. Typically, the decision tree has a single root node, which branches off into possible outcomes.  Some examples of decision tree algorithms include:    -  ID3 (Iterative Dichotomiser 3) uses Entropy to develop a decision tree from a dataset. It calculates the entropy for each datapoint to guide the structure of the tree.  -  C4.5 (Successor of ID3) slightly more advanced version of ID3 and also uses Entropy. -  CART (Classification and Regression Tree) uses Gini impurity.  The depth of a tree is the number of levels down or a partition of the feature space. The depth determines the granularity of the decisions. “If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.”  Regression Trees  Regression trees are decision trees that use a regression model. Regression trees fit a simple model in each sub-region after the feature space has been partitioned.  Pruning to reduce Model Complexity  Pruning is the practice of removing sections of a decision tree where the subtrees are redundant or yield little improvement to the accuracy of the model.  