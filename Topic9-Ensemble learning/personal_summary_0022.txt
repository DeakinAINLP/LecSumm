Topic 8: Nonlinear models (KNN and DT) In the previous topic, we studied about SVM models, a supervised algorithm that uses hyperplane to cluster dataset. We’ve used SVM on linear and non-linear dataset on python. Here we will focus on expanding our knowledge towards supervised learning algorithm  In this topic around, we will look into kNN and Decision Tree. Although we have seen kNN  before in unsupervised learning when clustering dataset, it is originally used for supervised learning.  This topic objective will be:  KNN (K Nearest Neighbour) Algorithm  - -  Decision Tree  K Nearest Neighbor  From the previous modules related about KNN, the concept behind this algorithm is that it will pick a centroid and uses euclidean distance to select which datapoint belong to a cluster. This algorithm is good for both a classification and regression problem. It is very important that we selects the most optimal number of “K” neighbors to shows the best result. We saw that in unsupervised learning we used method likes the K Elbow method to determine the best number of clusters. By selecting a low K, it will result in low bias and high variance, because we don’t care much about distant point. On the other hand, higher K means increasing bias and lowering variance, which means we are caring more about distant points.  So how can we find the optimal value for K? If we were not to use any method like the K Elbow,  we can perform an iteration of selecting a range of K, then perform cross-validation and evaluate the best K value.  Decision Tree  So KNN decides which one is their neighbor by calculating their weights using distance. How about Decision Tree? This algorithm weighs the data point based on cost, benefits, and probabilities. Now we will go into regression and classification tree. The procedure of these decision trees goes like this: we have set of possible value x, into different non-overlapping regions R1 … RJ where J is the distinct. Every time we’ve got a prediction, we compare it with the actual value just like any other learning algorithm to better optimize it. Regarding the tree depth in a decision tree, if it’s a deep tree, it means that the tree will be high in variance due to searching and splitting features into smaller region. However, if it’s a shallow tree, it means that we have a larger size of region in each splits, showcasing greater bias and lower variance just like in KNN.  Regression Tree To do a regression tree, we can perform a top-down approach or a heuristic method on a  quantitative dataset. We split the tree by using the mean or sum of square error.  1.  We first select the possible value of x with a threshold on s by splitting all the features region.  But these values will work as an independent feature.  2.  We iterate the first step to minimize the error to optimize the prediction value by finding the  best x and s value.  3.  We split the identified region only instead of the entire tree 4.  We stop when the tree gets too pure or too sparse.  We then proceed to predict the value using the mean of the training instance.  Classification Tree  Unlike regression tree, we will perform the heuristic approach on qualitative dataset instead.  Here we will selects the node that most likely result in the majority. In classification tree, we will use the classification error to split the tree instead. We uses Certain of Distribution to determine the split and see if it belongs in that region. A common algorithm that we can use for binary splitting in classification tree is Gini and Entropy. This method can be used to determine the variance of the classes, where Gini ranges between 0 to 0.5 and Entropy from 0 to 1. The Gini and Entropy represents the inequality or the node purity of the dataset.  Limitation and Solution We know that by using recursive splitting, if the tree is large and deep, it will comes a lot of  overlapping regions and outcomes. Splitting all of them will take a very long time. The solution to this is having a memory or database that stores these regions then proceed to using pruning. So, what is pruning? Pruning is a method to stop a recursion of splitting from happening once it discover an overlapping node. This will reduce the size of the decision tree due to that section of tree being less impactful to classifying nodes.  There are two types of pruning, pre-pruning and post-pruning. Pre-pruning decides you when to stop adding more nodes when splitting, and we can do this by looking at the entropy line. If the entropy is not significant anymore, we should stop adding nodes to stop it from building a deeper tree. However, by stop adding too much nodes, it might results in less impactful decision tree, to fix this we will use post-pruning.  Post-pruning waits until the tree are built. We can then look at the tree and find which subtree has the least impact and remove it by replacing it with a single leaf node. This will reduce the depth of the decision tree.  Advantages and Disadvantages  Very easy to understand  Advantage  Modelling non-linear function Handles categorical variables  Disadvantage Sensitive to small changes, might make a change to the tree depending on the changes. Easily overfitted Only Axis-Aligned Splits, which means splitting only one independent feature at a time. Lower accuracy compared to other SL algorithm.  