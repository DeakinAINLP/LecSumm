KNN Algorithm ( K-Nearest Neighbour )  ● Technique used for classification as well as regression tasks. The idea is to find the data point with the similar / same characteristic and put them into one class ● Take the case of the three nearest neighbours (3-NN), for example. Consider the labels / values of the three closest neighbours in a situation and make a decision based on the majority. If most of the 3NN belongs to class 0, we can label the new test point as class 0 as well  Theory of KNN  ● Classification: We count the number of neighbours in each class and select the one with the greatest number of neighbours. It is the same like asking your neighbours what they think and then following the majority opinion  ● Regression: Calculate the mean / average of the target value of the neighbour ● By providing weights to closer neighbours base on distance it can also increase the  accuracy of the predictions  Best number of neighbour  ● Choosing a suitable K value is important for accurate KNN performance ● Cross Validation - You can evaluate the efficiency of the KNN model with multiple  different values of K by using cross-validation techniques, such as k-fold cross-validation.  ● You may determine how well the model applies to new data by dividing your data  into training and validation sets and testing with various values of K. This allows you to evaluate several K values' performance and determine the one that provides the top / best overall performance  Regression Tree  ● Similar to the decision tree but instead of predicting category / classification  outcome it predicts continuous numerical values ● It made prediction based on set of input features ● It creates a hierarchical structure that allows us to trace a path from the root node to  the proper leaf node for the purpose to calculate the values  ● Useful for creating accurate predictions and analysing the variables affecting  numerical results in a variety of areas, such as real estate, finance, and predictive analytics  Classification Tree  ● It made prediction about categorical outcome ● Each step of the tree can determines what class / category that the datapoint  belongs to similar to a flowchart  ● It divides / splits the data into several categories depending on features, and they  direct us to the appropriate class or category  ● Starting at the root node, you follow the path down the tree depending on the values  of the features of the data point to create a prediction. The predicted class or category that the data point belongs to can be found at the leaf node  ● Helpful at capturing the complex relationship between the features and target  variables  Decision Tree  ● Just like a map, decision tree helps guide you through a series of decision ● Helpful for classifying data, developing predictions, and clearly learning the variables  affecting outcomes  Decision Tree Algorithm  1. Calculate each feature's entropy in the dataset. The impurity or randomness in the data is measured by entropy. Features that offer more valuable information for decision-making are those with lower entropy levels  2. Based on the feature with the lowest entropy, divide the dataset into subsets. This means selecting the characteristic that provides the most insight into the desired variable.  3. Make a decision tree node that represents the chosen feature. 4. On each subgroup, recursively repeat steps 1-3 using the remaining features. This procedure is repeated until the data is correctly classified by a decision tree.  Tree depth: Number of levels / split in the decision tree  ● A Deep Tree means more splits and smaller regions in the feature space ● Tree too deep = not enough training data point in each region = unreliable prediction  because high variance  ● A Shallow Tree with lesser splits can have larger regions + potential higher bias ● A shallow tree may oversimplify the decision-making process and neglect to identify  significant data changes.  ● Using strategies like cross-validation and evaluating techniques can help to choose  the right tree depth  Model complexity  ● Finding the right amount of complexity is important because an overly complex tree may overfit the data and an underlying simple tree might miss important patterns.  Pruning  ● Used in order to made decision tree less complex ● Help balance out the accuracy and the complexity ● It removes part of the tree that doesn't contributes much in the accurate  classification  ● 2 approach: Pre-Pruning and Post-Pruning  Pre-Pruning  ● Involves making choices about when to stop adding nodes to the tree based on  certain circumstances  ● For example, an increase in classification accuracy is not significantly better, it might lead to stopping the node from splitting. This can reduce the risk of the tree turning out too complicated.  Post-Pruning  ● Creates the entire decision tree first before replacing / removing part of the tree that  does not have a lot of classification power  ● This works by evaluating how removing part of the tree will affect the overall  accuracy and If removing a part of the tree doesn't significantly reduce accuracy, it can be replaced with a simpler node or leaf.  Advantage - Decision Tree ● Easy to understand ● Nonlinear modelling: It can capture complex relationships between variables and is  capable of representing nonlinear functions  ● Good at handling categorical variables  Disadvantage - Decision Tree  ● Sensitive to changing: The tree structure will change even with a small modification ● Overfitting: When deep decision trees are built, there is a risk of overfitting, because  the model is very complicated and closely matches the training set of data  ● Axis-aligned splits: Traditional decision trees perform splits along individual features independently, which may limit their ability to capture more complex relationships that involve joint probabilities or interactions between multiple features  Impact of distance metrics on KNN performance  ● KNN classifies new data points based on how close they are to nearest neighbours in  the training data  ● The amount of similarity between data points can be determined using a variety of distance measures, such as Manhattan distance, Euclidean distance, and cosine similarity.   