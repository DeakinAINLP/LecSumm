KNN is an algorithm that makes predictions based on the closest neighbours to a target data point. In the case of continuous target values, the average of the nearest training examples is taken as the prediction while for discrete class labels, the mode of the class labels of the nearest training examples is used. A useful technique for both classification and regression is to assign weights to the contribution if data point neighbors so the closer neighbors contribute more to the average of the data the neighbors that are further away. Another concept which is partially related to KNN is the Voronoi Diagram, which divides a plane into regions based on the proximity to a set of points.  The number of neighbors(K) we select controls the shape of the decision boundary, for a smaller number of neighbors, we restrict the region and force the classifier to focus on the closer regions and not to worry about the further ones. A smaller K will result in low bias and high variance whereas a higher K will result in low variance and increased bias. Finding the best and most appropriate for K is not always easy and sometimes not possible, however, a simple method to finding K is to use Cross- validation to partition the data into test and training samples and then evaluate the model with different values of K.  A decision tree is a map of possible outcomes from a series of related choices and are used to weigh the actions against one another based on their pros and cons. It begins with a root node and branches out into possible outcomes. Decision trees that use a regression model are called regression trees and trees that use a classification model are called classification trees. To fit the models into the regions we first divide the set of possible values into distinct and non-overlapping regions and then for every instance that falls into the region, we make the same prediction. Because of the exponential growth in possibilities, it is infeasible to find an optimal partition of the feature space into a specific number of regions. Which is why a heuristic method called recursive binary splitting is used. The algorithm first selects a feature and a threshold that will be used to split the feature space into two regions and then repeats the process for each resulting region, this step is done separately for each region and is done recursively until a stopping criterion is reached.  Classification and Regression Tress (CART) refers to decision tree algorithms which can be used for both classification and regression predictive modeling problems. They are similar to regression trees; however, they are used to predict a qualitative response instead of a quantitative response. For classification trees, each test instance is assigned to the mode of the region where it belongs, and the sum of square error is replaced by the classification error rate as a criterion for making the splits.  Other than CARTR, we have a few other algorithms for decision trees, for example ID3 and C4.5. ID3 is used to generate a decision tree from a dataset. The basic steps for the algorithm are to first calculate the entropy of every feature in the dataset and split the set into subsets with the feature with the minimum entropy. Next, we create a decision tree node based on that feature and recursively apply the algorithm to each subset.  Pruning is a technique used to make the decision tree smaller by removing the sections that provide little power to classify. Two ways to prune a tree are pre-pruning and post-pruning. For pre-pruning, we look at the entropy and decide when to stop adding nodes. Post-pruning, however, waits until the decision tree has been built and then prunes the attributes by subtree replacement. Decision trees are very easy to understand, are capable of modelling non-linear functions and can handle categorical variables, however, they are sensitive to small changes in the data, are easily overfitted, are only axis-aligned and are not as competitive in terms of accuracy compared to other regression and classification techniques.      