Topic 8 Summary  1.1  KNN algorithm and its variants KNN algorithms perform classification or regression tasks by assigning weights to the contributions of the data points neighbors.  1.2  Theory of KNN The higher the distance of the neighbor, the lower its weight. All training points may influence a particular instance.  1.3  Best number of neighbors Higher  values  of  K  will  have  smoother  decision  boundaries  which  means  lower  variance  but increased bias. So basically, higher values of K means asking for more and more information even from distant training points.  1.4  Decision trees A decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.  1.5  Regression trees Decision trees that use a regression model are called regression trees.  1.6  Classification trees It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response.  1.7  Decision tree algorithms 1.7.1Calculate the entropy of every feature using the data set S. Split the set S into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information. 1.7.2Make a decision tree node containing that feature. 1.7.3Recurse on subsets using remaining features.  1.8  Model complexity and pruning Reduces the size of decision trees by removing sections of tree that provide little power to classify instances.  1.9  Decision trees’ advantages and disadvantages  1.10 KNN classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Decision Trees uses feature selection to determine the most important classification features. DT operates by recursively segmenting the data into subsets based on the most informative features until a stopping criterion is reached.  