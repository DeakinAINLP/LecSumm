Topic 8 Nonlinear models KNN algorithm  ● ANN algorithm and its variants are a useful technique to assign weights to the contributions of data points neighbours so that closer neighbours contribute more to the average than farther neighbours.  ● The basic idea is to label the test data points with their nearest neighbours  (NN).  Theory of KNN  ● Assume an arbitrary data point is represented as Recall the Euclidean  distance between data points: For ﬁnding the majority of decisions based on the close training points.  ● Perform average or mean in continuous cases and you need to ﬁnd the mode  of the class labels in discrete format.  Best number of neighbours  ● with higher values means asking for more and more information even from  distant training points.  ● A simple and handy method, you can use Cross-validation to partition your data into test and training samples and evaluate your model with different ranges of values.  ● Now perform the Cross-validation for every possible number of and evaluate  the model based on the training and test data you have partitioned.  Regression trees  ● Regression trees After partitioning the feature space, we can ﬁt a simple  model in each sub-region.  ● The overall goal of regression trees is to ﬁnd regions that minimize the  training error: where is the mean of the target values of the training instances in the region.  ● Obviously, we would like to minimize this problem so that we have regions  that result in fewer errors.  Classification trees  ● Classiﬁcation trees are decision tree algorithms used for predicting qualitative  responses.  ● They assign test instances to the majority class of training instances in the  region they belong to.  ● Different criteria, such as classiﬁcation error rate, Gini index, and entropy, are  used to measure impurity or inequality in the distribution of classes.  ● Gini index and entropy are commonly used as they provide a measure of node  purity and are smoother compared to classiﬁcation error rate.  Decision tree algorithms  ● There are several popular decision tree algorithms, including ID3, C4.5, and  CART.  ● ID3 (Iterative Dichotomiser 3) and C4.5 are similar algorithms that use entropy  as the criterion for splitting the data into subsets.  