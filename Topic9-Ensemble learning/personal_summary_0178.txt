   KNN – K Nearest Neighbours is a clustering method that classifies a new data point based on the majority of a class of k nearest neighbours. For example if k = 10 and a new data point has 7 neighbours in class A and 3 neighbours in class B then you would classify that new data point as class A.    Shepard’s Method is similar except that it applies weights to the proximity of a neighbour. If there are 3 within a small distance but the other 7 are far away (often using the inverse square distance) then it might be in the class of the close 3.    When determining what is an appropriate number for k we can use cross validation and calculate the Misclassification Error, finding the elbow can find an appropriate K value.   Decision Tree – A decision tree is a classification (or can also be used for regression) model that is comprised of decisions and leaves. The regression model iterates through conditions and results in leaves that are numerical (within a range, to increase the fidelity of the regression model you can increase the number of leaves). The classification Decision tree is slightly simpler as the outcomes are categorical (binary, classes, etc). Data splits are performed using either Gini Index or Entropy. They are impurity measures that determine what the best split is based on information gain of the split.  