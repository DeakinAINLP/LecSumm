 This topic we learnt about K Nearest Neighbours and Decision Tress and how to tune the hyperparameters for these two techniques.  K Nearest Neighbours (KNN)  1.  Assign weights to the contribution of data points so that the nearer neighbours  contribute mode to the average than the more distant ones  2.  Can be used for both Classiﬁcation and Regression 3.  We assign weights to the neighbours based on their distance from the test point.  Weight can be inverse square of the distances, which means further away the point, lesser is the weight.  4.  Shepard's method - higher the distance of the neighbour, lower is its weight. 5.  Choosing the correct value of K  a.  The value of K controls the shape of the decision boundary b.  For small values of K, we will have low bias and high variance c.  For big values of K, we will have lower variance but increased bias d.  This is then an optimisation problem to ﬁnd the correct value of K e.  We can use cross-validation together with misclassiﬁcation rate to ﬁnd the  correct value of K for a given dataset  Decision Tree(DT)  1.  This is a map of the possible outcomes of a series of related choices 2.  Decision tree starts at one node and branches out cased on costs, beneﬁts and  probabilities.  3.  Decision trees which uses regression model are known as Regression Trees  a.  Here we can consider sum of squares error  4.  Decision trees which uses classiﬁcation model are known as ClassiﬁcaIon Trees  a.  Here we can consider misclassiﬁcation error rate  5.  Decision Tree algorithms -  a.  ID3 (Iterative Dichotomiser 3) - uses Entropy b.  C4.5 (successor of ID3) - uses Entropy c.  CART (Classiﬁcation and Regression Tree) - uses Gini impurity  6.  ID3 steps -  a.  Calculate the entropy of every feature using the dataset. b.  Split the set into subsets using the feature for which entropy is minimum c.  Lesser values of Entropy means that by selecting that feature, we will gain  more information  d.  Make a decision tree containing that feature e.  Repeat for remaining features  7.  Tree depth -  a.  If the tree is very deep, then we will have high variance b.  If a tree is shallow, then we will have high bias c.  This is an optimisation problem to ﬁnd the proper height of the tree  8.  Pruning      a.  This is a technique which reduces the size of the decision tree by removing  sections of the tree that provide li[le power to classify instances  i.  ii.  A tree which has a large number of regions may have only a few data points per region, resulting in high variance A tree which has a small number of regions, may result in high bias  b.  Pre-pruning (forward pruning)  i.  We decide during the building process when to stop adding nodes  c.  Post-pruning (backward pruning)  i.  Wait till the entire tree has been built and then go to each subtree and see if removing it will result in a small amount of change for Entropy. If yes, prune the subtree, else keep it.  Advantages of DT  1.  Easy to understand 2.  Can model non-linear functions 3.  Can handle categorical variables  Disadvantages of DT  1.  Sensitive to small changes in the data 2.  Can overﬁt easily 3.  Only axis-aligned splits for normal cases. For complex cases, we need to compute  joint probabilities  4.  Accuracy for DT might not be as good as other techniques like SVM or Neural  Networks  Impact of distance metric for KNN  1.  KNN is impacted by the distance metric we select 2.  Cosine similarity is a be[er option compared to Euclidean distance for high-  dimensional data  Feature importance of using DT  1.  Decision Trees uses feature selection to determine the most important classiﬁcation  features.  2.  DT operates by recursively segmenting the data into subsets based on the most  informative features until a stopping criterion is reached.  3.  A criterion such as information gain or the Gini index is used to determine which  feature is the most informative.  4.  At each node of the tree, the feature with the highest score is chosen as the spli‘ng criterion. The signiﬁcance of each feature can be determined by considering how much it contributes to overall improvement in the criterion. 5.  The greater the contribution, the more signiﬁcant the feature. 6.  Feature importance can be used to identify the most relevant features for  classiﬁcation and for feature selection to improve the performance of the model.    