K-Nearest Neighbor(KNN) Algorithm for Machine Learning    K-Nearest  Neighbour  is  one  of  the  simplest  Machine  Learning  algorithms  based  on  Supervised Learning technique.    K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.   K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.    K-NN algorithm can be used for Regression as well as for Classification but mostly it  is used for the Classification problems.    K-NN is a non-parametric algorithm, which means it does not make any assumption    on underlying data. It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.    KNN algorithm at the training phase just stores the dataset and when it gets new data,  then it classifies that data into a category that is much similar to the new data.  Decision trees A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification and regression. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. Though a commonly used tool in data mining for deriving a strategy to reach a particular goal, its also widely used in machine learning, which will be the main focus of this article. example that uses titanic data set for predicting whether a passenger will survive or not. Below model uses 3 features/attributes/columns from the data set, namely sex, age and sibsp (number of spouses or children along).    Classification and Regression Tree algorithm To build the Decision Tree CART (Classification and Regression Tree) algorithm is used. It works by selecting the best split at each node based on metrics like Gini impurity or information Gain. In order to create a decision tree. Here are the basic steps of the CART algorithm:  1.  The root node of the tree is supposed to be the complete training dataset. 2.  Determine the impurity of the data based on each feature present in the dataset. Impurity can  be  measured  using  metrics  like  the  Gini  index  or  entropy  for  classification  and Mean squared error, Mean Absolute Error, friedman_mse, or Half Poisson deviance for regression.  3.  Then  selects  the  feature  that  results  in  the  highest  information  gain  or  impurity  reduction when splitting the data.  4.  For each possible value of the selected feature, split the dataset into two subsets (left and right), one where the feature takes on that value, and another where it does not. The split should be designed to create subsets that are as pure as possible with respect to the target variable.  5.  Based on the target variable, determine the impurity of each resulting subset. 6.  For  each  subset,  repeat  steps  2â€“5  iteratively  until  a  stopping  condition  is  met.  For example, the stopping condition could be a maximum tree depth, a minimum number of samples required to make a split or a minimum impurity threshold.  7.  Assign the majority class label for classification tasks or the mean value for regression  tasks for each terminal node (leaf node) in the tree.  Advantages:  1.  Compared to other algorithms decision trees requires less effort for data preparation  during pre-processing.  2.  A decision tree does not require normalization of data. 3.  A decision tree does not require scaling of data as well. 4.  Missing values in the data also do NOT affect the process of building a decision tree to  any considerable extent.  5.  A Decision tree model is very intuitive and easy to explain to technical teams as well  as stakeholders.  Disadvantage:  1.  A small change in the data can cause a large change in the structure of the decision tree  causing instability.  2.  For a Decision tree sometimes calculation can go far more complex compared to other  algorithms.  3.  Decision tree often involves higher time to train the model. 4.  Decision tree training is relatively expensive as the complexity and time has taken are  more.  5.  The  Decision  Tree  algorithm  is  inadequate  for  applying  regression  and  predicting  continuous values.  