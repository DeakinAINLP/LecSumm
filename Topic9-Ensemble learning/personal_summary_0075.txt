Topic 8 summary  This topic covered the nonlinear models including K nearest neighbors and decision tree by exploring k-nearest neighbors’ algorithms a non-parametric method used for classification and regression and exploring the decision tree algorithm mostly used in classification problems and advanced topics. Lastly, python programming related to DT and KNN.  The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.  KNN algorithms can assign weights to the contribution of data point neighbors, so the nearer neighbor contributes more to the averages than more distant ones. This is useful for both classification and regression. The basic idea is to label the test data point as the same as the nearest neighbour (NN). The following figure illustrates this concept. If a black circle as a test point fall into a region in which the closest point is a black ellipse with class label of 1, based on the nearest neighbour, we are going to label this new sample as class 1. But also K in KNN can vary, we can label a test instance the same as the majority label of the K-nearest neighbors.  The k value in the k-NN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.  A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes. It can be used to weigh possible actions against one another based on their costs, benefits and probabilities. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences  Classification tree analysis is when the predicted outcome is the class (discrete) to which the data belongs. Classification and Regression Trees (CART) is a term introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modeling    problems. It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response. For a classification tree, we assign each test instance to the majority class (mode) of the training instances in the region where it belongs. Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient's length of stay in a hospital).  There are three decision tree algorithms,  ▪  ID3 (Iterative Dichotomiser 3) uses Entropy.  ▪  C4.5 (Successor of ID3)  slightly more advanced version of ID3 and also uses Entropy.  ▪  CART (Classification and Regression Tree)  uses Gini impurity.  Pruning is a technique that reduce the size of decision trees by removing sections of tree tat provide little power to classify instances. There are generally several ways of pruning trees, one is pre- pruning (forward pruning), another is post-pruning (backward pruning).        