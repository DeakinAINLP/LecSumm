KNN algorithm and its variants  -  Useful technique that can assign weights to the contributions of data point neighbors so the nearer  neighbors can contribute more to the average than more distant ones  -  Useful for both classification and regression -  Idea is to label the test data point is the same as the nearest neighbor (NN)  o  -  K in KNN can vary, e.g., if someone check K nearest neighbor of the test point in order to make a decision, you can label a test instance to the same as the majority label of the K-nearest neighbors  o  Theory of KNN  -  Assume an arbitrary data point is represented as:  o  -  Recall Euclidean distance:   For finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format  o  Continuous valued target function:  ▪  Mean value of the k nearest training examples  o  Discrete class label:  ▪  Mode of the class labels of the k nearest training examples  8.1P  -  -  Best number of neighbors (K)   K ~ controlling the shape of the decision boundary  For small values of k; we are restraining the region of a given prediction and forcing our classifier to be more focused on the close regions and neighbors.  e.g., do not care about fairly distant points, this will result in low bias and high variance  Higher values of k will have smoother decision boundaries with means lower variance but increased bias. So basically, higher values of k means asking for more information even from distant training points  Decision trees  -  Map of possible outcomes of a series of related choices -  Used to weigh possible actions against one another based on their costs, benefits and  probabilities  -  Regression trees  -  Decision trees that use a regression model, if we fit a classification model they are called  classification trees Procedure:  -  o  1; divide the feature space I.e., the set of possible values into j distinct and non-  overlapping regions    o  2; for every instance that falls into region R, we make the same prediction, which is  simply the mean of response values for the training observations in R -  Computationally infeasible to consider every possible partition of the feature space into J  regions, for this reason we take a top-down approach that is known as recursive binary splitting, rather than using a brute-force solution we would like to work in a heuristic way Solution:  -  o  1. we first select a feature and a threshold such that splitting the feature space into  regions leads to the best possible reduction in training eror  o  2. next we repeat the process looking for the best feature and the best threshold in order to split the data further to minimze the error in ecah of the resulting regions o  3. instead of splitting the entire feature space we only split one of the two previously  identified regions  o  4. the splitting process continues until a stopping criterion is reached, e.g., continue  until no region contains more than 5 instances of or the nodes are getting too pure or sparce.  Prediction   Classification trees  -  Similar to regression except its used to predict a qualitative response rather than a quantitative response  -  Assign each test instance to the majority class of the training instances in the region where it  -  belongs In the classification setting, we replace the sum of square error by the classification error rate as a criterion for making the binary splits, the classification error rate E is defined as the fraction of the training instances in that region that do not belong to the most common class   Gini and Entropy  -  Most commonly used measurement of inequality, e.g., in economics Gini index represents the  income of wealth distribution in a country;  o  -  -  Decision tree algorithms   -  ID3 (Interative Dichotomiser 3)  o  Uses Entropy  -  C4.5 (Successor of ID3)  o  Slightly more advanced version of ID3 and also uses Entropy  -  CART (classification and Regression tree)  o  Uses Gini impurity  The ID3 algorithm  -  Used to generate a decision tree from a dataset  o  1. calculate the entropy of every feature using the data set S. split the set S into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information  o  Make a decision tree node containing that feature o  Recurse on subsets using remaining features  Tree Depth  -  If you build a very deep tree, you are basically partitioning the feature space into small regions, if the tree is very deep, we should expect low changes of visiting many training points in that sub-region. This means all estimations in that region are not good because of the high variance -  When the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances however you may have other problems such as bias, you will have high bias in shallow decision trees meaning your decision making process is too naïve To create a decision tree with the correct amount of depth, you find the sweet spot by performing the cross validation and other evaluation methods. You need to find or tune the proper hyperparameter which is the depth of the tree.   Model complexity and pruning  -  -  Prunung is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances Process described before may produce good predictions on training set but it is likely to overfit the data, leading to poor generalization performance  o  A tree that has a large number of regions may have only few data points per region  resulting in high variance  o  On the other hand, having a small number of regions may result in high bias o  One possible alternative is to grow a large tree, and then prune it back in order to obtain  a subtreee  -  Ways of pruning trees:  o  Pre-pruning (forward pruning) o  Post-pruning (backward pruning)  Pre-pruning  In pre-pruning we decide during the building process when to stop adding nodes  - -  We can stop splitting nodes when the entropy reduction is not significant, we eliminate  unnecessary complexity on the model  Post-pruning  -  Waits until full decision tree is built then prunes attributes by subtree replacement  -  Decision trees advantages and disadvantages  -  Advantages  o  Very easy to understand o  Capable of modelling nonlinear functions o  Can handle categorical variables  -  Disadvantages  o  Sensitive to small changes in data o  May overfit easily o  Only axis-aligned splits   o  Trees may not be as competitive in terms of accuracy as some other regression and  classification techniques  Impact of distance metrics on KNN performance  -  -  KNN is a well known learning method that classifies new data points according to their closeness to the closest neighbors in the training set using distance measures, effectiveness of KNN can be impacted by the distance metric that is selected Euclidean distance, Manhattan distance and cosine similarity are commonly used in KNN  Feature importance of using Decision Trees  -  Decision trees are a popular machine learning algorithm that uses feature selection to  determine the most important classification features  -  DT operates by recursively segmenting the data into subsets based on the most informative  -  features until a stopping criterion is reached Feature importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model  KNN in python  