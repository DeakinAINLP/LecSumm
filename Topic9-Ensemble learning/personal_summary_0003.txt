K Nearest Neighbours (KNN) considers the K nearest training points when labelling the test point. This can be used in classification using majority labelling (mode), or regression using the mean. A Voronoi digram draws decision boundaries based on distance from seed points. The Voronoi diagram can be interconverted to a Delaunay triangulation drawing lines between the seeds which share adjacent edges in the diagram. The distance weighted nearest neighbours used with Shepardâ€™s method is a similar concept, weighted by the inverse Euclidean distance from training points, so that all points may contribute to evaluation, but with less impact from those further away. Low values of K prevent distant points from influencing the output, while large values consider more points which may be further away. This hyperparameter may be search for using cross fold validation for minimum classification error. Low K has less complexity, giving low bias, high variance and blocky shapes. High K has more complexity, giving higher bias, low variance and smoother changes between classes. Euclidean distance is not the only distance metric. Others can be used such as Manhattan, cosine distance. Decision trees map choices to potential outcomes. Binary trees are used to reduce the search space. At each iteration, the best splitting point for each region is decided to minimise the total MSE of the two subregions. The split which adds the least error overall is selected. This continues until the maximum number of splits is reached, or there are not enough points in any region to split further. These ensure the model does not become overfit as the fit would otherwise improve with each split until each region contains a single point, Regression trees divide the space into multiple regions, and evaluate make a prediction for each point in each space. Because it is infeasible to find every possible partition, iterative splitting approaches are used to reduce the space. Classification and Regression Trees (CART) use decision trees for classification or regression problems. Classifiers use the classification error rate to split regions, or other methods such as Gini or Entropy which are more sensitive for tree growing. Decision tree algorithm (ID3) uses entropy for each feature and split the feature with the minimum entropy. This provides the most information gain. A decision tree is produced using that split, and the process repeats. Tree depth is a hyperparameter, as growing the tree always improves fit, but risks overfitting. Model complexity can be managed by pre (stop during building) or post pruning (combining regions). This removes areas which provide little contribution to classification. Decision trees provide explanatory power, allowing them to determine features of most importance. They model non-linear functions, and can use categorical variables. However they are very sensitive to data changes, easily overfit, All splits are axis aligned, and may not be as accurate as other models. 