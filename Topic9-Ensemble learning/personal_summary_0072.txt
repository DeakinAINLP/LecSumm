Evidence of learning Prerequisite knowledge Entropy Entropy in machine learning is a measure of impurity or disorder in a set of data.  Entropy is calculated based on the distribution of class labels in the target variable.  If a set of data is perfectly homogeneous (all instances belong to the same class), the entropy is low (close to zero). On the other hand, if the data is evenly distributed across multiple classes, the entropy is high (closer to one).  K-nearest-neighbours (KNN) KNN works by classifying an unlabelled point with the most common label from its K nearest neighbours.  This method is less useful in cases where a majority of the K nearest values are very far away relative to the data point being classified.  It can be useful to assign different weights to the contributions of data points in classification based on their relative distance to the data point being classified. This is useful for both regression and classification.   Voronoi diagrams  In a Voronoi diagram, the points used to created the diagram are called ‚Äòseeds‚Äô and the space assigned to each seed are called Voronoi cells.  The edges in a Voronoi diagram shows the boundaries where a point in Euclidean space is closest to a seed.  In the context of KNN, a Voronoi diagram shows the decision boundary of a 1-NN algorithm.  Distance-weighted nearest neighbour algorithm (Shepard‚Äôs method) In this method points that are closer to the point being classified are given a greater weight compared to weights that are far away.  All points may influence the data point being classified, or we could choose only the K nearest data points.  Choosing the best value of K The value of K controls the shape of the decision boundary.  A low value of K makes the classifier focus on close regions and neighbours, and not on distance points. This results in low bias and high variance.  A high value of K allows the classifier to use distant points. This results in smoother decision boundaries as well as high bias and low variance.    We can use cross-validation to find the best value of K by performing cross validation for every possible value of K and selecting the K value with the lowest average misclassification error.  The impact of different distance measurements on KNN Effects of Distance Measure Choice on KNN Classifier Performance - A Review (arxiv.org)  Summary:  -  -  -  ‚ÄúThe performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. For example we found that Hassanat distance performed the best when applied on most datasets comparing to the other tested distances.‚Äù ‚ÄúThere was no optimal distance metric that can be used for all types of datasets, as the results show that each dataset favors a specific distance metric‚Äù ‚ÄúSome distances are less affected by the added noise comparing to other distances, for example we found that Hassanat distance performed the best when applied on most datasets under different levels of heavy noise.‚Äù  Decision trees A decision tree is used to map all the possible outcomes of a series of related choices. They are used to weigh possible actions against each other based on their costs, benefits, and probabilities.  Regression trees A regression tree is a decision tree that uses regression models.  The steps to create a decision tree are as follows:  1.  Partition the feature space into J regions, ùëÖùêΩ 2.  For each instance in each region, make its predicted value be the average expected value of  all instances in the region.  The overall goal of regression trees is to find a partition ùëÖ1 ‚Ä¶  ùëÖùêΩ that minimizes training error:  The problem with this is that attempting every possible partition is computationally infeasible. The total number of partitions can be found with the partition function.  Recursive binary splitting Recursive binary splitting is a greedy approach that involves recursively splitting the feature set in two until a stopping criterion is reached. At each stage, this method splits the set in a way that minimizes the overall training error described above.  The steps for recursive binary splitting are:  1.  Select a feature x such that splitting the feature space into the regions {ùë•|ùë• ‚â§ ùë†} and {ùë•|ùë• >  ùë†} leads to the best possible reduction in training error.  2.  Recursively repeat the process on one of the regions until a stopping criterion is reached. For example, until a region contains no more than a certain number of instances, or until the nodes of the regression tree are getting too pure or too sparse.  Classification trees Classification trees are used to predict a qualitative response instead of a quantitative one.  Differences compared to regression trees:  -  Each instance is assigned to the majority class (mode) in each region instead of the average expected value as in regression trees.   -  The classification error rate is used instead of the sum of the squared error as the criterion for making binary splits  o  The classification error rate E is defined as the fraction of instances in a region that  do not belong to the majority class.  Topics for further reading: Certainty of distribution, Gini index, entropy  Decision tree algorithms Popular decision tree algorithms:  -  ID3 (Iterative Dichotomiser 3)  o  Uses entropy  -  C4.5 (successor to ID3)  o  Slightly more advanced version of ID3 and also uses entropy  -  CART (Classification And Regression Tree)  o  Uses Gini impurity  ID3 algorithm:  1.  Calculate the entropy of every feature in the data set. 2.  Split the data set using the feature with the minimum entropy. 3.  Make a decision tree containing that feature 4.  Recurse on subsets using the remaining features  Tree depth In a very deep tree, the feature space is partitioned into small regions so there is a low chance of visiting many training points in the sub-region. This means the estimations in the sub-region won‚Äôt be good because of high variance.  In a shallow tree, the feature space is partitioned into larger regions. While there is a good chance of visiting a lot of training points in the sub-regions, it is also likely that there will be high bias.  We need to select the tree depth that gives the best results. Therefore, we can consider the tree depth to be a hyperparameter that needs to be tuned.  Pruning decision trees Pruning is a technique to reduce the size of decision trees by removing sections of the tree that aren‚Äôt performing well.  There are several ways to prune decision trees:  Pre-pruning (forward pruning):  - -  -  In pre-pruning we decide when to stop adding nodes during the building process For example, we could choose to stop splitting the data set when the entropy reduction is no longer significant. The problem with this approach is it can be somewhat like a greedy algorithm. This is because sometimes attributes individually do not contribute much to a decision but combined they have a significant impact.  Post-pruning (backward pruning):  - -  -  In post-pruning the algorithm waits until the decision tree is fully built before pruning To prune the tree, attributes are replaced using sub-tree replacement. Sub-trees are replaced with a single leaf node. The sub-tree that incurs the smallest amount of entropy when removed is chosen as the sub-tree to prune.  Advantages and disadvantages of decision trees Advantages:  -  Very easy to understand, as they represent rules -  Capable of modelling non-linear functions -  Can handle categorical values  Disadvantages:  Sensitive to small changes in data  - -  May overfit easily (by building deep decision trees) -  Only capable of axis-aligned splits -  May not be as competitive in terms of accuracy compared to other regression and  classification techniques such as SVM or neural networks      