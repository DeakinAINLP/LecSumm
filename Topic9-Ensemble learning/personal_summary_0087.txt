Summary: Through my study of Topic 8 I learnt by reading the content, watching the videos, taking notes and working through the activities. Evidence can be seen below.  Activity 8.2:  1.  KNN algorithm and its variants  Activity 8.3:  1.  Theory of KNN  Activity 8.4:  1.  Best number of neighbours (K)  Activity 8.5:  1.  Decision trees.  Activity 8.6:  1.  Regression trees.  Activity 8.7:  1.  Classification trees  Classification and Regression Trees (CART) 2.  Gini and Entropy  Activity 8.8: Decision tree algorithms.  1.  ID3 (Iterative Dichotomiser 3) uses Entropy. 2.  C4.5 (Successor of ID3) slightly more advanced version of ID3 and also uses Entropy. 3.  CART (Classification and Regression Tree) uses Gini impurity.  The ID3 Algorithm Tree depth, tune the proper hyperparameter which is the depth of the tree.  Notes from presentation: Entropy = pi is the probability of class i Compute it as the proportion of class i in the set. Entropy comes from information theory. The higher the entropy the more the information content. ∑− i pi 2 pi  Activity 8.9: complexity and pruning. Pre and post pruning.  Activity 8.10: Decision trees: Advantages    Very easy to understand, as they represent rules.   Capable of modelling nonlinear functions.   Can handle categorical variables  Disadvantages    Sensitive to small changes in the data.   May overfit easily.    Only axis-aligned splits.   Trees may not be as competitive in terms of accuracy  Activity 8.11: Impact of distance metrics on KNN performance.  Activity 8.12: KNN in Python:  Activity 8.13: Decision trees in Python:  