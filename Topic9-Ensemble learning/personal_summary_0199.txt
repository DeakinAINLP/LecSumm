   K nearest neighbours algorithm  o  Used for both Classification and Regression o  Assigning weights to data point neighbours allows them to contribute more to the  average Improves scope of test and training data  o o  Veroni Diagram  ▪  Similar to KNN as it is related to nearest Neighbour scheme ▪  Formed by training examples to provide complex boundaries and decision  rules.  ▪  Each point is given its own polygon where it is closest to its generating point  than other ones.  ▪  Similar to country maps showing regions and capital cities o  Aim of generating K is to reach a point with LOW Bias and HIGH Variance o  Higher K requires more information. Which reduces accuracy in testing data points.    Decision Tree   Represent rules   Capable of running nonlinear Modelling functions  o  Weighing Options o  Regression trees = decision trees using regression model o  Can use CART to predict modelling problems of classification and regression  ▪  CART predict Qualitative responses ▪  Also Decision tree algorithms include; 1DS and C4.5    Deep Tree = High Variance   Shallow tree = High Bias   Forward Pruning (pre)  o  Avoid high variance or high bias by removing parts of the decision tree that provide  little contribution in classification  o  Stop adding nodes o  * May not work when individually, attributes do not contribute. But combines can  have significant impact on classification    Backward Pruning (post)  o  Check if a node can be replaced after decision tree has bene built.  