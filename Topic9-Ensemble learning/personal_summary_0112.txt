KNN algorithm and its variants: Popular  supervised  machine  learning  algorithm  K-Nearest  Neighbours  (KNN)  is  used  for  both  classiﬁcation  and regression issues. In KNN, a data point's "k" closest neighbours in the feature space decide its output value. The choice of "k" is set by the user and is a hyperparameter that must be tweaked.  There are various KNN variations, including the following:  1.  Weighted KNN: In this variation, the output value of a data point is determined by the distance between the data point and its "k" neighbours, which serves as a weight. The weight of nearby neighbours is greater than that of distant neighbours.  2.  The weights given to each neighbour in the distance-weighted KNN variation are determined by the distance between the data point and the neighbour. The weight of nearby neighbours is greater than that of distant neighbours.  3.  Edited KNN: This variation modiﬁes the training data by deleting the noisy or inconsistent data points, which  may result in a more accurate classiﬁcation.  4.  Condensed KNN: In this type, just the most useful data points are chosen from the training data subset. Faster  categorisation and decreased memory utilisation may result from this.  5.  KNN with Locality-Sensitive Hashing (LSH): This variation uses LSH to translate the high-dimensional feature  space to the low-dimensional space, which can help KNN run more eﬃciently on huge datasets.  6.  KNN with  Kernel  Density Estimation (KDE):  In this variation,  the  probability  density  function of  the  feature space is estimated using a kernel density estimator. When the data points are not evenly distributed, this can assist to increase the KNN's accuracy.  Best number of neighbours (K): As  the  performance  of  the  model  is  aﬀected,  choosing  the  best  value  for  "k"  in  the  K-Nearest  Neighbours  (KNN) algorithm is a crucial hyperparameter tuning task. The choice of "k" relies on the speciﬁc situation and the qualities of the data.The model may overﬁt if "k" is too small because it may be overly sensitive to noise and outliers in the data. On the other hand, the model could oversimplify the issue and result in underﬁting if "k" is too big.  The performance of the model is assessed using a diﬀerent validation set or using cross-validation as one method of ﬁguring out the optimal value of "k" by using a grid search or cross-validation on the training data. In this procedure, many values of "k" are tested, and the one that produces the best accuracy or lowest error rate on the validation set is chosen.  Noting that there is no one-size-ﬁts-all approach to selecting the best "k" value, it may take some trial and error to identify the ideal value. The ultimate number should be decided depending on the trade-oﬀ between bias and variance as well as the unique features of the dataset, with "k" values between 3 and 10 serving as a decent starting point in general.  Decision Trees: A supervised machine learning approach known as a decision tree is utilised for classiﬁcation and regression issues. They can handle both numerical and categorical data since they are a non-parametric model, which means they don't make any assumptions about the distribution of the underlying data.  Decision  trees work  by  recursively  dividing  the  feature  space  into  a  number of  distinct  areas  according  to  a  set  of predetermined criteria. The decision criteria are based on the input characteristics, and each division corresponds to a leaf node in the tree.  The  algorithm  chooses  a  feature  and  a  threshold  that  divides  the  data  into  two  subsets  with  the  least  amount  of impurity  or  the  most  amount  of  information  gain.  The  information  gain  measures  how  much  the  split  lowers  the uncertainty surrounding the class labels, while the impurity measures how mixed the class labels are in a subset.  Once a split has been created, the algorithm repeats the procedure for each subset until a stopping requirement, such as reaching a maximum depth or a minimum amount of samples per leaf, is satisﬁed. The output value at each leaf  node is generally either the mean or median value for regression problems or the modal or majority class of the training samples that fall into that area for classiﬁcation issues.  The interpretability, simplicity, and ability to manage non-linear interactions between the input and output variables are only a few beneﬁts of decision trees. They can, however, also be vulnerable to overﬁting, particularly if the tree is allowed to grow too deeply or if there are a lot of noisy or pointless input characteristics. Several  approaches,  including  tree  pruning,  ensemble  methods,  and  regularisation  techniques  like  ridge  or  lasso regression, can be utilised to address these problems. Regression Trees: Regression trees are a subset of decision trees that are employed in supervised learning situations when the objective is to predict a continuous numerical value, such as the cost of a home or the local weather.  Regression trees work by recursively dividing the input space into several distinct, non-overlapping areas, each of which corresponds to a leaf node in the tree. The input feature values, and matching output values provide the basis for the partitioning.  The  method  chooses  a  feature  and  a  threshold  that  divides  the  data  into  two  subsets  with  the  least variance or mean squared error (MSE), whichever is lower.  Once  a  split  has  been  made,  the  algorithm  repeats  the  process  for  each  subset  until  a  stopping  criterion,  such  as reaching a maximum depth or a minimum number of samples per leaf, is satisﬁed. The output value at each leaf node is ordinarily the mean or median value of the training samples falling within that area.  The interpretability, simplicity, and ability to handle non-linear correlations between the input and output variables are only  a  few beneﬁts of regression trees. However, they  can also  be prone to overﬁting,  especially  if  the  tree  is allowed to grow too deep or if there are numerous noisy or irrelevant input characteristics.  There are several strategies that may be used to address these problems, including regularisation methods like ridge or lasso regression, ensemble methods, and tree pruning.  Classiﬁcation Trees: Classiﬁcation Trees are a sort of decision tree used for supervised learning in which the aim is to predict a categorical target variable or class label. Classiﬁcation trees work by recursively dividing the feature space into several distinct, non-overlapping areas according to a set of decision criteria, with each region serving as a leaf node in the tree.  The  algorithm  chooses  a  feature  and  a  threshold  that  divides  the  data  into  two  subsets  with  the  least  amount  of impurity  or  the  most  amount  of  information  gain.  The  information  gain  measures  how  much  the  split  lowers  the uncertainty surrounding the class labels, whereas the impurity measures how mixed the class labels are in a subset.  Once a split has been created, the algorithm repeats the procedure for each subset until a stopping requirement, such as reaching a maximum depth or a minimum number of samples per leaf, is satisﬁed. The output value at each leaf node is generally the majority class or mode of the training samples that fall into that area.  Classiﬁcation  The  interpretability,  simplicity,  and  ability  to  manage  non-linear  interactions  between  the  input  and output variables are only a few beneﬁts of using trees. They can, however, also be vulnerable to overﬁting, particularly if the tree is allowed to grow too deeply or if there are a lot of noisy or pointless input characteristics.  Several  approaches,  including  tree  pruning,  ensemble  methods,  and  regularisation  techniques  like  ridge  or  lasso regression, can be utilised to address these problems. Furthermore, a number of algorithms, like Random Forests and Gradient Boosted Trees, are well-liked ensemble techniques that construct several decision trees and integrate their predictions to increase model accuracy.  Model complexity and pruning: A  machine  learning  model's  complexity  is  measured  by  how  many  features  or  parameters  it  contains.  A  more complicated model can capture more complex relationships in the data, but it can also be more prone to overﬁting, when the model ﬁts the training data too closely and fails to generalise successfully to new, unobserved data.   Pruning is a strategy used to minimise the complexity of a machine learning model, particularly decision trees. The aim of pruning is to eliminate branches from the tree that are not adding signiﬁcantly to the model's accuracy. Pruning can be done in two major ways:  1.  Seting a minimum number of samples needed to divide a node or a maximum depth for the tree is known as pre-pruning.  This  limits  the  complexity  of  the tree  before it  is  trained,  preventing  it  from  overﬁting to the training data.  2.  Post-pruning entails creating the whole decision tree and then eliminating branches that do not signiﬁcantly improve the  model's accuracy. When doing  this, the model is typically trained on  a subset of  the data  and tested on another subset using a validation set or cross-validation. A branch can be pruned if doing so does not materially reduce the model's accuracy on the validation set.A decision tree's generalisation performance may be enhanced and overﬁting can be decreased via pruning. Simplifying the decision rules and lowering the number of branches in the tree can also increase the model's interpretability.  Impact of distance metrics on KNN performance: The  KNN  algorithm's  performance  can  be  signiﬁcantly  impacted  by  the  choice  of  distance  metric.  The  closest neighbours of a particular data point are determined using the distance metric, which gauges how similar two data points are. The most widely used distance metrics in KNN are Euclidean distance and Manhatian distance, however additional  distance  metrics  including  Minkowski  distance,  cosine  similarity,  and  Hamming  distance  can  also  be employed.  The  characteristics  of  the  data  and  the  issue  being  solved  determine  which  distance  metric  should  be  used.  For instance, while Hamming distance is appropriate for categorical data, Euclidean distance is appropriate for continuous data. Similarly, cosine similarity is commonly used for text data and image recognition tasks.  The performance of KNN can be aﬀected by the choice of distance metric in several ways:  1.  Scaling:  The  size  of  the  input  characteristics  aﬀects  the  accuracy  of  several  distance  measures,  including Euclidean distance. Therefore, it is crucial to normalize the data before employing such distance measures to avoid characteristics with greater sizes dominating the distance computation.  2.  Sparsity: For sparse data, like text data, when most of the characteristics are zero, distance measures like cosine  similarity work betier.  3.  Noise: In the presence of noise or outliers in the data, robust distance measures such as Mahalanobis distance  can be more eﬀective than ordinary Euclidean distance.  4.  Dimensionality:  High-dimensional  data  can  suﬀer  from  the  curse  of  dimensionality,  where  the  distance between  any two  data points gets more  similar as the number of dimensions grows. As a result, KNN may perform  poorly.  In  these  circumstances,  dimensionality  reduction  methods  or  distance  metrics  created  for highly dimensional data may be employed.  In conclusion, the eﬀectiveness of the KNN algorithm can be signiﬁcantly impacted by the choice of distance metric. It is crucial to select a distance metric that is appropriate for the data and the current problem because the best distance metric depends on the characteristics of the data and the problem being solved.  Feature Importance: Decision Trees (DT) can oﬀer a measure of feature relevance that can be used to pinpoint the dataset's most important characteristics. The feature signiﬁcance score is dependent on how much each feature contributes to minimising the impurity or entropy in the decision tree.  Typically, Gini impurity or information gain are used to determine the feature signiﬁcance score in classiﬁcation trees. Information gain measures the decrease in entropy (or disorder) brought about by spliting a node on a speciﬁc feature, whereas the Gini impurity measures the likelihood of misclassifying a sample randomly chosen from a given node.  In regression trees, the feature signiﬁcance score is commonly determined using mean decrease impurity or mean decrease error. The mean decrease impurity measures the reduction in variance obtained by spliting a node on a given feature, whereas the mean decrease error measures the reduction in mean squared error achieved by spliting a node on a particular feature.  To rank the features in order of importance and choose the most crucial features for a particular task, one can use the feature importance scores. This can assist minimise the complexity of the dataset, enhance model interpretability, and perhaps increase model performance by focusing on the most important characteristics.  It is crucial to remember that feature relevance ratings might be skewed towards qualities that have several categories or are closely connected with other features. Additionally, the particular approach used to produce feature signiﬁcance scores might have an impact on them, making it crucial to interpret feature importance scores carefully and validate them using additional techniques like cross-validation or permutation importance.     