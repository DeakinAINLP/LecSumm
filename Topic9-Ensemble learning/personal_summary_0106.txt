1.   K nearest neighbor (KNN) algorithm   Decision tree (DT)   Best number of neighbours (K)   Regression tree   Classification trees   Model complexity and pruning   Decision trees advantages and disadvantages   KNN in Python   Decision trees in Python   2.   First: I read and researched all the resources and articles in topic 5 of Learning  Resources.    Second: I joined an online Python class.   Third: I have read and watched Machine Learning resources from my mother tongue to  English as well.    Fourth: I also watch, read, and learn about Python-related issues in both languages.   Lastly: I bought the Machine Learning Online Course.  GeeksforGeeks group authors (2023) “K-Nearest Neigbor (KNN) Algorithm”, Geeks for Geeks, last updated 5 May 2023.  Ihechikara Vincent Abba (2023) “KNN- Algorithm -K-Nearest Neighbors Classifiers and Model Example”, FreeCodeCamp, Published 25 January 2023.  GeeksforGeeks group authors (2023) “Decision Tree”, Geeks for Geeks, last updated 8 May 2023.  Gokul S Kumar (2020) “ Decision Trees: A step-by-step approach to building DTs”, Towards Data Science, Published 20 August 2020.  Kushal Vala (2022) “How to Get Started with Regression Trees”, Data science, Built in, published 19 July 2022.  Zach (2020) “An Introduction to Classfication and Regression Trees”, Statistics Simplified Statology, Published 22 November 2020.  8.12 KNN in Python - SIT307_SIT720 - Machine Learning (deakin.edu.au)  8.13 Decision trees in Python - SIT307_SIT720 - Machine Learning (deakin.edu.au)  Head First Python, 2nd Edition (oreilly.com) Types And Applications Of Machine Learning | Eduonix - YouTube Trang chủ - Big-O Coding (bigocoding.com)  3.  1.  K Nearest Neighbor (KNN) Algorithm:  o  KNN is a non-parametric classification algorithm that predicts the class of a data point based on the majority class of its K nearest neighbors in the feature space. It is a lazy learning algorithm, as it does not explicitly learn a model during training and instead relies on the training data for prediction.  o  o  KNN is  sensitive to the choice of K (the number of neighbors) and the distance  metric used for calculating the proximity between data points. o  KNN can be used for both classification and regression tasks. o  KNN  has  been  implemented  in  various  fields  such  as  image  recognition,  recommendation systems, and anomaly detection.  o  Compared to other classification algorithms, KNN is computationally expensive,  especially when dealing with large datasets.  2.  Decision Tree (DT):  o  Decision trees are supervised learning algorithms that recursively split the feature  space based on different criteria to create a tree-like model for prediction.  o  Each internal node in the tree represents a decision based on a feature, and each leaf  node represents a class or a predicted value.  o  Decision trees can be used for both classification and regression tasks. o  Splitting  criteria  in  decision  trees  include  Gini  impurity  and  information  gain  (entropy).  o  DT is one of the most popular algorithms in machine learning due to its simplicity  and interpretability.  3.  Best Number of Neighbors (K):  o  In KNN, the choice of K determines the number of nearest neighbors considered for classification or regression.  o  The selection of the best K value depends on the dataset and can be determined  through techniques like cross-validation or grid search.  o  Choosing the best  K value can have  a significant  impact  on the accuracy of the  model.  4.  Regression Tree:  o  Regression  trees  are  a  variant  of  decision  trees  used  for  predicting  continuous  o  values or numerical outcomes. Instead  of  predicting  classes,  regression  trees  predict  the  average  value  of  the training samples falling in each leaf node.  o  Regression trees have been used in various applications such as predicting housing  prices, stock prices, and weather patterns.  5.  Classification Trees: o  Classification  trees  are  decision  trees  specifically  designed  for  solving classification problems, where the goal is to predict discrete class labels for data points.  o  Classification  trees  have  been  used  in  various  applications  such  as  medical  diagnosis, fraud detection, and customer segmentation.  6.  Model Complexity and Pruning:  o  Decision  trees  have  the  tendency  to  overfit  the  training  data,  resulting  in  overly  complex models that may not generalize well to unseen data.  o  Pruning  is  a  technique  used  to  simplify  decision trees  by  removing  unnecessary branches  or  nodes,  reducing  model  complexity  and  improving  generalization performance.  o  Pruning  can  be  done  through  methods  such  as  reduced  error  pruning,  cost  complexity pruning, and minimum description length pruning.  7.  Decision Trees Advantages and Disadvantages:  o  Advantages:  ▪  Easy to interpret and visualize. ▪  Can handle both categorical and numerical features. ▪  Can capture non-linear relationships between features. ▪  Suitable for handling missing values and outliers. ▪  Can be used for both classification and regression tasks. ▪  Can be used for feature selection.  o  Disadvantages:  ▪  Prone to overfitting, especially with complex trees. ▪  Can  be  sensitive  to  small  changes  in  the  data,  leading  to  different  tree  structures.  ▪  Limited ability to express relationships between features. ▪  May  not  perform  well  when  the  classes  or  decision  boundaries  are  overlapping or unbalanced.  ▪  Can be biased towards features with a high number of categories. ▪  May not be suitable for high-dimensional data.  