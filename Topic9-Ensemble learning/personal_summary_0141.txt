 In  topic  eight,  we  delved  into  the  topic  of  Nonlinear  models,  focusing  on  the  K-Nearest  Neighbors  (KNN)  algorithm  and  Decision  Trees  (DT).  We  began  by  exploring  the  KNN  algorithm and its variants, discussing how it operates by finding the K nearest neighbors to a  given data point and making predictions based on the majority class among those neighbors.  Next,  we  covered  the  theory  of  KNN,  where  we  discussed  how  to  make  the  majority  of  decisions based on the close training points using the mode of the class labels, average or mean  distances and distance-weighted nearest neighbor algorithm which is also known as Shepardâ€™s  method.  We then discussed how do you pick the variable K, which holds the best number of neighbours.  Under this section we explored the importance of the variable K, the number of neighbors to  consider  and  considerations  for  choosing  the  optimal  value  of  K,  such  as  cross-validation,  evaluating the model with different ranges of K values, usage of misclassification error as a  measurement of performance etc.  Moving  on  to  Decision  Trees,  we  examined  the  partitioning  of  the  feature  space,  their  construction and application in both regression and classification tasks. We first looked at the  regression  trees  and  the  principles  of  how  this  works.  We  also  discussed  the  problems  of  performing  this  and  how  to  solve  that  problem  using  a  top-down,  greedy  approach  that  is  known as recursive binary splitting. Then we learned about classification trees, where we also  discussed about the classification error rate (E), and the solutions for that such as Gini index  and Entropy. We concentrated on different decision tree algorithms, such as ID3, C4.5, and  CART, and how they differ in terms of attribute selection measures and pruning techniques.  We mainly looked at ID3 in detail.  Then we introduced the concept of model complexity and pruning, emphasizing the trade-off  between tree complexity and generalization performance. We also learned about pre-pruning  and post pruning techniques. We discussed the advantages of decision trees, including their  interpretability, ability to handle both categorical and numerical data, and their robustness to  outliers. However, decision trees also have limitations, such as overfitting and instability with  small perturbations in the data.  We  went  through  advanced  topics  covering  the  impact  of  distance  metrics  on  KNN  performance and the feature importance of using Decision Trees.   In the programming part of the unit, we gained practical experience implementing KNN and  Decision Trees in Python. We learned how to utilize the Scikit-learn library to build KNN and  Decision  Tree  models,  select  the  optimal  number  of  neighbors  and  tree  parameters,  and  evaluate their performance using appropriate metrics.  Overall, topic eight provided a comprehensive understanding of Nonlinear models, particularly  KNN and Decision Trees. We learned about the underlying principles, algorithm variations,  and practical considerations for using these models. By combining the knowledge of KNN and  Decision  Trees,  we  gained  valuable  tools  for  tackling  a  wide  range  of  machine  learning  problems, from classification to regression tasks.  