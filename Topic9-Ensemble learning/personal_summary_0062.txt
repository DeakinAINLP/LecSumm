 Assigning weights to the neighborsâ€™ contributions to the average, with the nearer neighbors contributing more than the far neighbors, can be a valuable strategy.  To classify the test data point as the same as the nearest neighbor (NN) is the essential notion.  Target function with continuous value: The k nearest training examples average value.  Unambiguous class label: The k nearest training examples' mode of the class labels.  A decision tree represents the potential consequences of a number of connected decisions.  Decision trees can be employed to compare alternative courses of action based on their costs, advantages, and probabilities.  Typically, a decision tree has a single root node from which it branches out into potential outcomes.  Regression trees are decision trees that make use of a regression model. As an alternative, a categorization model can be fitted. Classification trees are the name given to these decision trees.  Leo Breiman coined the term "Classification and Regression Trees" (CART) to describe decision tree techniques that can be applied to classification or regression predictive modelling issues.  The most widely used method of measuring inequality is the Gini index.  There are numerous decision tree algorithms.  Three of the more well-known ones are listed below:    Entropy is used by ID3 (Iterative Dichotomiser 3).   A significantly more advanced version of ID3 that also incorporates entropy is C4.5  (the successor to ID3).    Gini impurity is used by CART (Classification and Regression Tree).  We should anticipate low possibilities of visiting numerous training locations in that sub- region if the tree is quite deep. This indicates that due to the huge variance, none of the estimations in that area are accurate.  By deleting branches of the tree that have minimal ability to classify cases, pruning is a strategy for reducing the size of decision trees.  Pre-pruning involves deciding when to stop adding nodes while the network is being built (for example, by examining entropy).  After the complete decision tree has been constructed, the characteristics are pruned using subtree Replacement.  KNN is a well-known machine learning technique that uses distance measurements to classify new data points based on how close they are to their nearest neighbours in the training set.  The distance measure that is chosen can have a big effect on how well KNN works. Among the distance metrics widely used in KNN are Euclidean distance, Manhattan distance, and cosine similarity.  The most crucial categorization features are chosen via feature selection in the widely used machine learning technique called decision trees.  Finally, we went through and practiced the python programming related to this module.  