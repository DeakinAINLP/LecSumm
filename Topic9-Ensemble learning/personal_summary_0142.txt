 In topic 8, we explored several related concepts of Support Vector Machine (SVM) such as K nearest neighbor (KNN) algorithm, Decision tree (DT).  KNN algorithm and its variants are useful technique that can be to assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.The basic idea is to label the test data point is the same as the nearest neighbour (NN).  Also, decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.A decision tree typically starts with a single root node, which branches into possible outcomes. Moreover, we learnt about classification tree which is similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response.  For decision tree, there are some algorithms,  ID3 (Iterative Dichotomiser 3)     C4.5 (Successor of ID3)   CART (Classification and Regression Tree)  Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances. Generally there are several ways of pruning trees, they are; Pre- pruning (forward pruning) and Post-pruning (backward pruning). In topic 8, we have learned about the advantages and disadvantages if decision tress. Finally, Impact of distance metrics on KNN performance is the most important thing we went through the topic.           