KNN Algorithm and its variants The K-nearest neighbors (KNN) algorithm is a technique used for classification and regression tasks. It involves assigning weights to data points based on their proximity to a test point. The basic idea is to label a test data point based on the class of its nearest neighbor(s). In KNN, the value of K determines the number of nearest neighbors considered for decision making. The algorithm can be visualized using figures. In one figure, a black circle represents a test point, and the closest neighbor is a black ellipse with a specific class label. In this case, the test point would be assigned the same class label as the nearest neighbor. In another figure, multiple training points are shown, with a new test point falling within the scope of two training points from one class and one training point from another class. By considering the majority class among the nearest neighbors (in this case, three neighbors), the test point would be labeled with the majority class. Theory of KNN The K-nearest neighbours (KNN) algorithm involves calculating the Euclidean distance between data points to find the nearest neighbours. In the case of continuous target values, the majority decision is determined by calculating the mean value of the nearest training examples. For discrete class labels, the mode of the class labels of the nearest training examples is used. The concept of Voronoi Diagrams is partially related to KNN. Voronoi diagrams partition a plane into regions based on the distance to points in a specific subset. It is similar to KNN in that it is based on closest neighbours and can result in complex boundaries and decision rules. Best number of neighbours (K) The selection of the number of neighbours (K) in the K-nearest neighbours (KNN) algorithm is an important decision that affects the shape of the decision boundary and the performance of the classifier. Choosing the right value for K is crucial. For small values of K, the decision boundary is more focused on close regions and neighbours, leading to low bias but high variance. This means the classifier pays less attention to distant points. On the other hand, larger values of K result in smoother decision boundaries, reducing variance but increasing bias. With higher K values, the classifier considers more information, even from distant training points. There is no universal rule for choosing K, as it depends on the characteristics of the training data and the desired exploration rate. For example, if you have a large training dataset, you may consider a higher value of K. Decision trees A decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities. A decision tree typically starts with a single root node, which branches into possible outcomes. Regression trees Regression trees are a type of decision tree model used for regression tasks. They partition the feature space into distinct and non-overlapping regions and fit a simple model in each region. This approach can also be applied to classification tasks, resulting in classification trees. The goal of regression trees is to find regions that minimize the training error. This is done by calculating the difference between the predicted values and the true output values for each point within a region and aiming to minimize this error. It becomes an optimization problem to find the regions that result in the least error. Finding the optimal solution by considering every possible partition of the feature space into regions is computationally infeasible due to the vast number of possibilities. Instead, a top-down, greedy approach known as recursive binary splitting is used. This approach involves iteratively splitting the feature space based on certain criteria, such as minimizing the error or maximizing the homogeneity of the regions, in a heuristic manner. By recursively splitting the feature space based on the chosen criteria, regression trees can efficiently build a model that captures the relationships between the features and the target variable, leading to effective predictions in regression tasks. Classification trees Classification and Regression Trees (CART) are decision tree algorithms used for both classification and regression tasks. In classification trees, the goal is to predict a qualitative response, such as a class label. Each test instance is assigned to the majority class (mode) of the training instances in the region it belongs to. This can be thought of as the data point "voting" itself into a region based on the majority class. To make binary splits in classification trees, the classification error rate is used as a criterion instead of the sum of square error. The classification error rate is the fraction of training instances in a region that do not belong to the most common class. It measures the certainty of distribution (COD) within a region, indicating how certain the classifier is about the decision. When the COD is close to 1, it means most of the training points in the region are voting for a certain class, indicating high certainty. On the other hand, when the COD is close to 0, the votes cannot be trusted due to a high classification error rate. In practice, alternative measures such as the Gini index and entropy are commonly used instead of the classification error rate. The Gini index measures inequality or node purity, taking a small value when the distribution of votes is low in inequality. It is a measure of total variance across the classes. Entropy is another measure that represents the average amount of information needed to classify a data point. Both Gini index and entropy are preferred over the classification error rate due to their sensitivity for tree-growing. Overall, the Gini index and entropy provide better measures of inequality and node purity for binary splits in classification trees compared to the classification error rate. They help determine the optimal splits and improve the accuracy of the classification predictions. Decision tree algorithms There are several popular algorithms for decision trees, including ID3, C4.5, and CART. ID3 uses entropy, C4.5 is an advanced version of ID3 that also uses entropy, and CART uses Gini impurity. The ID3 algorithm, developed by Ross Quinlan in 1975, is used to generate decision trees from a dataset. The steps of the ID3 algorithm are as follows: 1.  Calculate the entropy of every feature using the dataset. 2.  Split the dataset into subsets using the feature with the minimum entropy. Lower entropy values indicate a better choice for attribute selection, as it provides more information. 3.  Create a decision tree node containing that feature. 4.  Recurse on the subsets using the remaining features. The depth of the decision tree is an important consideration. A very deep tree partitions the feature space into small regions, but this may lead to low chances of visiting many training points within each sub-region, resulting in high variance and less reliable estimations. Conversely, a shallow tree with large regions may have low variance but high bias, as it oversimplifies the decision-making process. Finding the right depth for the decision tree involves balancing these factors. Model complexity and pruning Pruning is a technique used to reduce the size of decision trees by removing sections of the tree that do not contribute significantly to classifying instances. It is employed to address the issue of overfitting, where a tree becomes too complex and performs poorly on unseen data. A decision tree with a large number of regions may have only a few data points per region, leading to high variance. Conversely, a tree with a small number of regions may result in high bias. To mitigate these problems, one approach is to first grow a large tree and then prune it to obtain a smaller, more generalizable subtree. There are two main types of pruning: 1.  Pre-pruning (forward pruning): In pre-pruning, decisions about when to stop adding nodes to the tree are made during the tree-building process. For example, the process may halt when the entropy reduction from selecting different features is not significant. Pre-pruning aims to eliminate unnecessary complexity from the model. However, it may overlook cases where attributes, though individually insignificant, have a significant impact when combined. 2.  Post-pruning (backward pruning): Post-pruning involves growing the full decision tree and then evaluating the performance of subtrees. Subtrees that can be replaced by a single leaf node or feature without a significant increase in error are pruned. This process continues until no further improvements can be made. Post-pruning allows for a more comprehensive assessment of the impact of removing subtrees, considering the combined effect of attributes. Post-pruning follows a procedure where the full decision tree is built, and then subtrees are evaluated and potentially replaced by simpler structures that minimize error or impurity measures such as entropy or Gini index. Both pre-pruning and post-pruning techniques are used to prevent overfitting and improve the generalization performance of decision trees. The choice between these techniques depends on the specific problem and the characteristics of the dataset. Decision trees: advantages and disadvantages What are the unique features of decision trees? What problems are best suited for their use? Advantages   Very easy to understand, as they represent rules.   Capable of modelling nonlinear functions.   Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a Euclidean distance between two vectors having weather as a variable.) Disadvantages   Sensitive to small changes in the data. If you add few data points or change some small values, your rules can be changed!   May overfit easily. As we have said before, by building deep decision trees you are at high risk of overfitting and a high variance model.   Only axis-aligned splits. Normal decision trees split the space along each features independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.   Trees may not be as competitive in terms of accuracy as some of the other regression and classification techniques such as SVM or neural networks. 