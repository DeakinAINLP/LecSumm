 Topics Covered This Topic K nearest neighbour (KNN) KNN  is  a  non-parametric,  lazy  learning  algorithm  that  can  be  used  for  classification and  regression tasks. It works by finding the K nearest data points in the training dataset to a given test point and predicting the label or value based on the majority vote or average of the nearest neighbours.  Decision tree (DT) A  DT  is  a  tree-like  structure  used  for  both  classification  and  regression  tasks.  It  consists  of  nodes, branches,  and  leaves,  where  nodes  represent  features,  branches  represent  decisions,  and  leaves represent  output  values  or  class  labels.  DTs  are  built  by  recursively  splitting  the  dataset  based  on feature values to maximise information gain or reduce impurity.  Regression Trees Regression trees  are decision  trees  used for predicting  continuous output values. The prediction is made by taking the average value of the target variable in the leaf node.  Classification Trees Classification trees are a type of decision tree used for predicting class labels. The prediction is made by choosing the class label that is most common in the leaf node.  Gini and Entropy Gini and entropy are impurity measures used in decision trees to evaluate the quality of a split. Gini measures the probability of a randomly chosen sample being misclassified, while entropy measures the randomness or disorder in the dataset. Lower values of Gini or Entropy indicate a better split.  Model complexity and pruning Model complexity refers to the level of detail and structure in a model. Pruning is a technique used to reduce  overfitting  in  decision  trees  by  removing  branches  that  provide  little  improvement  to  the overall model performance.  Pre-pruning Pre-pruning  is  a  method  of  stopping  the  growth  of  a  decision  tree  early,  before  it  becomes  overly complex, by setting limits such as maximum depth or a minimum number of samples in a leaf node.  Post-pruning Post-pruning  simplifies  a  fully  grown  decision  tree  by  removing  branches  with  low  importance  or impact on the overall model performance.  Decision Trees: Advantages and Disadvantages Advantages include ease of interpretation, continuous and categorical data handling, and automatic feature  selection.  Disadvantages  include  sensitivity  to  small  changes  in  data,  overfitting,  and  high variance  Impact of distance metrics on KNN Performance The  choice  of  distance  metric  in  KNN  affects  the  algorithm's  performance  and  accuracy.  Common distance  metrics  include  Euclidean,  Manhattan,  and  Minkowski.  Choosing  the  appropriate  metric depends on the data and problem domain.      Feature importance of using Decision Trees (DT) DTs  can  provide  insights  into  feature  importance  by  calculating  the  total  reduction  of  impurity contributed by each feature across all splits in the tree.  Additional Content Summary This topic I found the following resources particularly helpful:     "Pattern Recognition and Machine Learning" by Christopher M. Bishop "An Introduction to Statistical Learning" by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani  Reflection Understanding these topics is vital for machine learning practitioners because they form the basis of many  common  algorithms  and  techniques.  Knowledge  of  KNN,  decision  trees,  and  their  variations allows practitioners to effectively select and apply the most suitable algorithm for a given problem. Understanding Gini, Entropy, and model complexity helps make informed decisions when designing and  optimising  models.  Additionally,  being  aware  of  the  impact  of  distance  metrics  on  KNN performance and the importance of feature selection in decision trees contributes to more effective model development and evaluation.  Topic 8 Quiz    