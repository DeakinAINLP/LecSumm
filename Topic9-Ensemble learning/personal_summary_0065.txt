Topic 8 Machine learning  KNN algorithm (K nearest Neighbour), A technique to assign weight to the contribution of data point neighbours so the nearer neighbours contribute more to the average than the more distant ones.   -  Decision tree is a map of the possible outcomes of a series of related choices -  Used to weight possible outcome another based on other factors  - -  Regression tree, are decision trees that use a regression model, if fitting a classification  model into decision tree can be called classification tree.  -  Classification tree and regression tree (CART), are used to predict a qualitative response  rather than a quantitative response.  -  Gini and entropy- Gini is used to measure out the inequality. Eg in economics, the Gini index  represents the income or wealth distribution of residents in a country  - -  Decision tree algorithms - -  Pruning, technique used to reduce size of decision tree Pruning methods include pre pruning (forwards pruning) and post pruning (backwards pruning) Pre pruning, decide during the building process when to stop adding nodes, by checking the amount of entropy reduction when we select different features. We can stop splitting nodes when the entropy reduction is not significant Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement   -  Advantages of decision tree- easy to understand, Capable of modelling nonlinear functions,  Can handle categorical variables  -  Disadvantages of decision tree- Sensitive to small changes in the data, may overfit easily,  Only axis-aligned splits. Normal decision trees split the space along each features independently, Trees may not be as competitive in terms of accuracy as some of the other regression and classification techniques such as SVM or neural networks     