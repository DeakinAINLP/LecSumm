 K nearest neighbor (KNN) algorithm  Algorithm & Variants 2:03pm   Learnt that K means is for unsupervised learning and KNN is for supervised learning. Learnt that KNN Algorithm can be used for both both classification and regression Learnt that it is a useful technique that is used to assign weights based on the neighbors. The nearer neighbors contribute more to the average than the distant ones.  K is the hyper parameter, that checks K nearest neighbors of the test point in order to  make the decision  This image above has a K value of 1, and identifies that the class is 1.     This image above has a K value of 3 and has class 0 having the majority and labels the point as 0.  Theory of KNN 2:06pm   Let’s assume an arbitrary data point is represented as:  We use the Euclidean distance between data points:  We need to perform average or mean in continuous cases and find the mode of the class labels in discrete format to find the majority of decisions based on the close training points.  Where Continuous valued target function:  We find the Mean value of the K nearest training examples  Where Discrete class label:  Mode of the class labels of the K nearest training examples Additionally, we consider the Distance-weighted nearest neighbor algorithm (Shepard’s  method)  Where it assigns weights to the neighbors based on their distance from the test  point. For example, the weight may be inverse square of the distances (1/D2)    This means that the higher the distance of the neighbor, the lower its weight.    In this image above, we see that the blue class has a majority than the red class, however the red class is closer to the test point therefore has a higher weight than the blue class which is further than the test point. This test point using Shepard’s method will label it as a red class.  KNN Best number of neighbors (K) 2:11pm   Think of K as controlling the shape of the decision boundary   For Small values of K  Restrains the region of a given prediction   Forces classifier to be more focused on the close regions and neighbors This will result in a low bias and high variance (Overfitting)  Remember that bias is the difference between your prediction and  the true value  Variance is the squared of that difference.   For Higher values of K  Asking for more information from distant training points Smoother decision boundaries   Lower variance but increases bias (Underfitting)   The image below will show an example of K values, where it shows the number of neighbours to be K = 1 to Kmax.   There is no rule of thumb in selecting Kmax since it depends on your desired rate of exploration for K  A simple and handy method  Cross-validation to partition your data into test and training samples Evaluate model with different ranges of K values   The misclassification error can be used as a measurement of performance  Classification is very time consuming because we need to find distance with all the  training instances.  Decision tree (DT) 2:20pm  Decision Tree is a role based system where it makes decisions based on a tree model that maps the possible outcomes of a series of related choices.  It weighs possible actions against one another  Costs, probabilities, and benefits  Typically starts with a single node  And then Branches into possible outcomes.  Partition of Feature Space 2:28pm  We have two feature X1 and X2 We can partition based on the decision tree model that corresponds to the splits made in  the 2 dimensional space which we can visualize (if dimenation increases we can’t visualize it).  After partitioning the feature space, we can fit a simple model in each sub-region  (𝑅_1,𝑅_2…).  We can fit a regression model. Such decision trees are called regression trees.  We use the mean of a sub region like R1 for prediction  We can also fit a classification model. Such decision trees are called classification trees.  We use the majority of a sub region like R for prediction  Usually, extremely simple models such as majority (classification) or mean (regression)  are used.  Process of building a DT 2:31pm  In what kind of criteria do we make these partitions.   Let’s start with the procedure:  We divide the feature space, i.e., the set of possible values for 𝑥1,…,𝑥𝑑 (dimensions) into 𝐽 distinct and non-overlapping regions, 𝑅1,…,𝑅J   For every instance that falls into region 𝑅𝑗, we make the same prediction, which is simply the mean (or mode) of response values for the training observations in 𝑅𝑗.  Formulation of Regression Trees 2:33pm  The overall goal of regression trees is to find regions 𝑅1,𝑅2,…,𝑅𝐽 that minimize the training error:    where  is the mean of the target values of the training instances in the 𝑗𝑡ℎ region.  For each region, we are trying to minimize the summation of all the squared distances  How do we find the solution? 2:34pm   It is computationally infeasible to consider every possible partition of the feature space into 𝐽 regions.  We use a top-down, greedy approach that is known as recursive binary splitting. Rather than using a brute-force solution, we would like to work in a heuristic way.  How the heuristic method works? 2:36pm  Select a feature 𝑥𝑗 and a threshold 𝑠 such that  Split the feature space  Regions {𝑥|𝑥_𝑗≤𝑠} and {𝑥|𝑥_𝑗≥𝑠}  Not going into the joint space of all features  leads to the best possible reduction in training error This is actually partitioning the data space  Use independent feature form such as 𝑥𝑗 with a threshold s .  Repeat the process     Looking for the best feature and the best threshold  Minimize the error in each of the resulting regions.  instead of splitting the entire feature space, we only split one of the two previously identified regions. The splitting process continues until a stopping criterion is reached.  In general, the construction process of the tree is actually a feature selection process, it's not selecting features from all the features. But it's selecting the importance of the features and putting the most important features as the root node and the less important features as the shallow internal nodes.  Classification trees 2:39pm   In the classification setting  we replace the sum of square error by the classification error rate as a criterion  for making the binary splits. The classification error rate (𝐸) is defined as the fraction of the training instances in that region that do not belong to the most common class.  𝑗𝑡ℎ region that are from 𝑘𝑡ℎ class  represents the proportion (fraction) of training instances in the      𝐶𝑜𝐷 (certainty of distribution) and close to 1 Almost all of the training points inside a region are voting for a certain  class label.  Classification error is being less sensitive for tree-growing Alternative solution:  Gini index (𝐺) is defined as    It is a measure of node purity. 𝐺 becomes small as 0 or 1.  most commonly used measurement of inequality  closes to either  Entropy defined as:   Tells us the uncertainty of something  Pattern of Error, Gini Index and Entropy for different probabilities of class distributions:   Decision Tree Algorithms 3:00pm  Here are three of the more popular ones:   ID3 (Iterative Dichotomiser 3) uses Entropy.   C4.5 (Successor of ID3)   slightly more advanced version of ID3 and also uses Entropy.  CART (Classification and Regression Tree)   uses Gini impurity.  We will look at ID3 in detail.  For the ID3 Algorithm   It’s used to generate a decision tree from a dataset  Calculate the entropy of every feature using the data set S.  Split the set 𝑆 into subsets using the feature for which entropy is minimum.  So lesser values of entropy means it should be a good choice for  selection of the attribute  Make a decision tree node containing that feature. Recurse on subsets using remaining features.   Tree Depth   If the tree is very deep   It partitions the feature space into small regions.  Small number of training points in sub-regions.   Increases variance and estimation becomes poor   If the tree is shallow Large regions   Small variance but large bias  We need to find the sweet point  Depth of the decision tree Cross-validation as discussed earlier in Lecture topic 5 and 6  Remember, you need to find or tune the proper hyperparameter which is the  depth of the tree.  Model complexity and pruning 3:06pm  Pruning is a technique that reduces the size of decision trees  Removes sections of the tree   Little power to classify instances.   The tree-building process that we described in previous steps may produce good predictions on the training set, however it may become:  Overfit (creating deep trees) Underfit (creating small number of regions)  Generally there are several ways of pruning trees:  Pre-pruning (forward pruning) 3:09pm  Decision is made during the building process  Stop adding nodes (e.g. by looking at entropy).   In case of Entropy  Check the amount of entropy reduction by selecting different  features.  Stop splitting when the entropy reduction is not significant.  Pre-pruning can be problematic  Sometimes attributes individually do not contribute much to a  decision, but combined, they may have a significant impact.  Post-pruning (backward pruning)(USED MORE OFTEN)  Post-pruning waits until the full decision tree has been built Then prunes the attributes by subtree replacement.   Replace an entire subtree with a single region or node   It reproduces the smallest error.  Select a subtree  Check – replacing it with a single node or feature incurs a small  amount of change in Entropy. If yes, trim the tree. If not, keep that subtree   Decision Trees: Advantages and Disadvantages -3:13pm  Advantages:  Decision trees are very easy to understand  Decision trees are capable of modeling nonlinear functions. Decision tree can handle categorical variable  Disadvantages:  Sensitive to small changes in the data.  Adding few data points or change some small values will change the DT  May overfit easily  Deep decision trees increases risk of overfitting and high variance model.  Only axis-aligned splits.  Considers each feature independently No joint probabilities of features  Performance is not competitive  SVM, KNN or Neural network  Impact of distance metrics on KNN performance 3:16pm  KNN classifies new data points according to their closeness  Colsest Neighbours in the training set  Using Distance measures  Effectiveness of KNN can be significantly impacted by the distance metric that is  selected  Selection distance metrics Euclidean distance, Manhattan distance, and cosine similarity etc. Cosine similarity may be a better option than Euclidean distance, for instance, in  high-dimensional data.  Feature importance of using Decision Trees (DT) 3:17pm  Decision Trees uses feature selection to determine the most important classification  features.  Decision Tree operates by recursively segmenting the data into subsets based on the  most informative features until a stopping criterion is reached.   Information gain or the Gini index  At each node of the tree, the feature with the highest score is chosen as the splitting  criterion.  The significance of each feature can be determined by considering how much it contributes.    