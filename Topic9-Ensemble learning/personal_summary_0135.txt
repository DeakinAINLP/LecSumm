This topic I have learned about two new Machine Learning algorithms Distance weighted K-Nearest Neighbors and Decision Tree. Both are mainly used for categorical data, however, can be applied on continuous data.  We have already covered normal K Nearest Neighbors in previous topics, the difference with Distance weighted KNN is that we assign weights to the neighbors depending upon its distance from the test point.  The next is Decision tree, as the name suggests, the Decision tree is a tree-based structure which makes the decision and predicts the outcome, if it predicts the categorical value, it is known as classification tree, and if it predicts continuous value, it is known as regression tree. Decision tree contains root node, branches and leaves, where we use impurity to select root nodes and branches and leaves are the final prediction.  There are few methods to calculate impurities, though Gini Impurity is the most popular and straightforward. Others are Entropy and Information gain. To avoid overfitting of the decision model, it is important to limit the number of leaves, the depth (the depth of the decision tree).  We can evaluate the classification tree and regression tree just like any other categorical or continuous machine learning model, decision tree are easy to understand, however, they are sensitive towards small changes, therefore, it is necessary to tune them using hyper-parameters.  Later, I have learned how to apply KNN, Decision tree models in Python programming language.      