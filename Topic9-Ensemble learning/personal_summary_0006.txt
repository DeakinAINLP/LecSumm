KNN A useful technique can be used which is to assign weights to the contributions of data point neighbours so that the nearer the neighbour the heavier the weight that contributed to the average then more distant ones. This helps with both classification and regressions. We label the test data points the same as the nearest neighbour (NN). As K can vary in KNN you can label a test instance to the same as the majority label of the KNN. To find the majority decisions based on the close training points you need to perform average or mean in continuous cases and find the mode of the class labels in discrete format. There is a concept related to KNN called Voronoi Diagram and is the partitioning of a plane into regions based on distance to points in a subset of the plane.  K or best number of neighbours controls the shape of the decision boundary. Small values restrain the region of a prediction and force classifiers to be more focused on the close regions and neighbours. Giving low bias and high variance. High values of K have smoother boundaries with a lower variance and higher bias. Decision Trees A decision tree is a map of the possible outcomes of a series of related choices. Weights can be used to predict the cost and benefits or probabilities. Starting from a single root node and branching out. After partitioning the feature space we can fit simple model in each sub region. We divide the feature space, the set of possible values for non overlapping regions. Then for every instance that falls in a region we make the same prediction, which is the mean, for response values for the training observations. This helps to minimize the training error. Classification and Regression Trees (CART) is a term used to refer to decision tree algorithms that can be used for classification or regression predictive modelling problems. It is similar to regression trees except it is used to predict qualitative responses rather than quantitative responses. For a classification tree we assign each test to the mode of the training instances in the region it belongs. We then replace the sum of square error by the classification error rate to make the binary split. Certainty of Distribution COD shows how certain it is that a classifier sits inside a region. If close to 1 it means all training points are voting for a certain class label so it is more certain. Gini index is most commonly used for measuring inequality. Measuring the total variance across the K class.  The ID3 algorithm is used to generate a decision tree from a dataset.  Calculate the entropy of every feature and split its subsets, make a decision tree node containing the feature, recurse on subsets using remaining features. Very deep trees partition the feature space into small regions. There is high variance of the estimators as there is low chance of visiting many training points. If the regions are big and tree is shallow than you have low variance but high bias. You can perform cross validation to help find the right depth. Pruning is the technique of reducing the size of a decision tree by removing section that provide little power to classify instances. You can pre prune, where during the building you decide to stop adding nodes by looking at entropy. Or post-pruning where once the tree is built the attributes of a subtree can be replaces with a single node. Decision trees are very easy to understand, capable of modelling nonlinear function and can handle categorical variables. Although they are sensitive to small changes in the data, may easily overfit, are only axis-aligned splits and the trees may not be as competitive in terms of accuracy. KNN is well a well known ML method that classifies new data points according to their closeness to the closest neighbours in the training set using distance. The effectiveness of KNN is impacted by distance metric selected, such as the Euclidean distance, Manhattan distance and cosine similarity. Specific metrics might be more suited depending on the problem and data type. Decision tress are a popular ML algorithm as it uses feature selection to determine the most important classification features. Segmenting out the data until a certain criteria is met to then improve the performance model. 