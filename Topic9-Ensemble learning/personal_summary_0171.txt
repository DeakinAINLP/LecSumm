This  topic  we  started  with  K  nearest  neighbour  (KNN)  algorithm  and  Decision  tree. Assigning  weights  to  the  neighbours'  contributions  to  the  average,  with  the  closer neighbours contributing more than the farther neighbours, can be a valuable strategy. Both classification and regression can benefit from this. To classify the test data point as  the  same  as  the  nearest  neighbour  (NN)  is  the  essential  notion.  We  studied  the theory  of  KNN  .  The  selection  of  the  K  value  in  K-Nearest  Neighbours  (KNN)  is important because it affects the shape of the decision boundary and the performance of the model. Choosing a small K can lead to overfitting with low bias and high variance, while  choosing  a  large  K  can  result  in  underfitting  with  low  variance  and  high  bias. Cross-validation  is  commonly  used  to  find  the  optimal  K  value  by  evaluating  the model's performance on different K values. The choice of Kmax depends on the size and complexity of the training data.  A  decision  tree  represents  the  potential  consequences  of  a  number  of  connected decisions.  Decision  trees  can  be  employed  to  compare  alternative  courses  of  action based  on  their  costs,  advantages,  and  probabilities.  Typically,  a  decision  tree  has  a single root node that branches out into potential outcomes. Regression trees partition the feature space into distinct regions and fit a simple model, such as the mean, in each region. The goal is to minimize the training error by finding the regions that result in the  least  error.  This  is  achieved  through  a  top-down,  greedy  approach  known  as recursive binary splitting. The process involves selecting a feature and threshold to split the  data,  then  recursively  splitting  the  regions  to  further  minimize  the  error.  The splitting process continues until a stopping criterion is met, such as a maximum number of instances per region or reaching a desired level of purity or sparsity. Classification trees are used to predict qualitative responses and assign test instances to the majority class of the training instances in the corresponding region. The classification error rate is used as a criterion for making binary splits, where it represents the fraction of training instances in a region that do not belong to the most common class. Alternatively, the Gini  index  and  entropy  can  be  used  to  measure  node  purity  and  inequality  in  class distribution. The Gini index measures the total variance across classes and takes a small value when the class distribution is less unequal. We even studied the decision tree algorithm .  Pruning is a technique used to reduce the size of decision trees and prevent overfitting. It  involves  removing  sections  of  the  tree  that  do  not  significantly  contribute  to classification accuracy.  Pre-pruning,  or  forward  pruning,  involves  stopping  the  tree-building  process  early based on certain criteria, such as entropy reduction. This helps eliminate unnecessary complexity in the model but may overlook the combined impact of attributes.  Post-pruning, or backward pruning, builds the entire decision tree and then prunes it by replacing subtrees with single regions or nodes. The goal is to find the smallest error while  reducing  the  complexity  of  the  tree.Both  pre-pruning  and  post-pruning techniques  aim  to  balance  model  complexity  and  performance,  helping  to  improve generalization and avoid overfitting.       We even went through advantages and disadvantages of decision tree, Decision trees have  advantages  such  as  easy  interpretability,  handling  categorical  variables,  and modeling nonlinear functions. However, they are sensitive to data changes, prone to overfitting, limited to axis-aligned splits, and may be less accurate compared to other techniques like SVM or neural networks. Decision trees are best suited for situations where  interpretability  and  nonlinear  relationships  are  important,  but  not  for  high accuracy or complex data.  Impact  of  distance  metrics  on  KNN  performance,  KNN is  a  well  known  machine learning  method  that  classifies  new  data  points  according  to  their  closeness  to  the closest  neighbours  in  the  training  set  using  distance  measures.  The  effectiveness  of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option  than  Euclidean  distance,  for  instance,  in  high-dimensional  data.  And  we implicated KNN in python for hands on experience.  