Lesson Review: Nonlinear Models Module Learning Objectives  1.  K nearest neighbour (KNN) algorithm 2.  Decision tree (DT)  Summarising the content: K-Nearest Neighbors (KNN) -  is an approach to classification that estimates how likely a data point is to be a member of one group or other groups depending on what the data points nearest to it are in.  Continuous valued target function : Mean value of the k nearest training examples Discrete class label: Mode of the class labels of the k nearest training examples  Voroni Diagram – partitioning of a plane into regions based on distance to points in a specific subset of the plane. Example in image below:  K values: Small K Values – restrains the region of a given prediction and forces the classifier to be more focused on the close regions and neighbours. This behaviour makes the classifier less likely to react to distant points. This results in low bias with High Variance. Large K Values – are asking for more information and will respond better to distant points. Larger values will have smoother decision boundaries which means lower variance but increased bias.  Decision Tree (DT) – is a map of the possible outcomes of a series of related choices. DT can be used to weigh possible actions against one another based on their costs, benefits and probabilities. A DT start with a single root node which branches into possible outcomes.  Regression Tree – Decision trees that use a regression model. Recursive binary splitting is a top- down, greedy approach used to find the best feature and threshold for splitting the data. The splitting process continues until a stop criterion is reached, such as a certain number of instances per region or the nodes becoming too pure or sparse. The Prediction of the response for a given test instance using the mean of the training instances in the region where the test observation falls.  Classification Tree – Decision trees that use a classification model. Classification trees predict qualitative response assigning test instance to the majority class in the respective regions. Error rate is used measuring the percentage of instances that do not belong to the most common class in a region.  Certainty of Distribution (COD) – indicates the certainty of a classifier’s decision within a region. A high COD implying high certainty, while a low COD indicating uncertainty.  Gini Index and Entropy - Gini Index represents inequality or node purity, while entropy captures the information content or disorder. These provide a measure of classification error used for tree growing.  Decision Tree Algorithms:  ▪  Iterative Dichotomiser 3 (ID3) – calculates the entropy of every feature using the data set. Splits the set into subsets using the feature for which entropy is minimum.  ▪  C4.5 – is a more advanced version of ID3. ▪  Classification and Regression Tree (CART) – uses Gini impurity.   Decision Tree complexity:    A tree that has a large numer of regions may have only a few data points per region  resulting in high variance.    A tree that has a small number of regions will have more data points per region resulting in  a high Bias.  Pre-pruning – a technique used to during the building process to stop adding addition nodes. By looking at the entropy reduction it is possible to stop adding additional nodes when the reduction is insignificant. This will eliminate unnecessary complexity on the model.  Post-Pruning – waits until the decision tree is fully grown and then prunes attributes by subtree replacement. This method checks which subtree removal introduces the smallest error and replace it with a single node. This method reduces the complexity of the tree while incurring only a small amount of change in Entropy.  Advantages  ▪  Easy to understand (represents rules) ▪  Capable of modelling nonlinear functions ▪  Can handle categorical variables.  Disadvantages   Sensitive to small changes in the data.   May overfit easily   Only axis-aligned splits   May not be as accurate as other  methods.  Reflecting on the content: What is the most important thing you learnt in this module? How does this relate to what you already know? Having previously covered KNN and Decision Tree models I was familiar with the content in this module which made it easier to progress through and understand. However, the content and activities in this module helped to refine my knowledge and skills in creating basic models using these methods.  Why do you think your course team wants you to learn the content of this module? The models introduced in this module are some commonly used in Machine Learning and therefore is important to understand how they work, and how to implement them. In this case understanding the strengths, weaknesses and limitations of each model helps to ensure that the correct technique is applied and appropriately tuned to the problem at hand.  Evidence of Learning Evidence of learning can be found in the attached Jupyter Notebook file containing the code and solutions from activity 8.14.  Evidence of Self-assessment, with a sufficient score (at least 90%)  