 Learning objectives:  K nearest neighbor (KNN) algorithm  - -  Decision tree (DT)  Learning summary:  -  KNN algorithm and its variants  The abbreviation KNN stands for “K-Nearest Neighbor”. It is a supervised machine  learning algorithm. The algorithm can be used to solve both classification and regression problem statements. The number of nearest neighbors to a new unknown variable that has to be predicted or classified is denoted by the symbol 'K'  -  Theory of KNN  For finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format. To summarize:  Continuous valued target function: Mean value of the k nearest training  examples  Discrete class label:  Mode of the class labels of the k nearest training examples There is another concept partially related to KNN which is called a  Voronoi Diagram. In  mathematics, a Voronoi diagram is partitioning of a plane into regions based on distance to points in a specific subset of the plane.  The higher the distance of the neighbour, the lower its weight. All training points may influence a particular instance. This method is also known as Shepard’s method.  -  Best number of neighbours (K)  Higher values of k will have smoother decision boundaries which means lower variance but increased bias. So basically, higher values of k means asking for more and more information even from distant training points.  Like most of machine learning problems, finding hyper-parameters such as k is not really straightforward. Finding the best answer is not always possible. But as a simple and handy method, you can use Cross-validation (see topic, Model Selection) to partition your data into test and training samples and evaluate your model with different ranges of k values.  -  Decision trees  A decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weigh possible actions against one another based on their  costs, benefits and probabilities.  A decision tree typically starts with a single root node, which branches into possible  outcomes.  -  Regression trees  Decision trees that use a regression model are called regression trees. We can  alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used.  Let’s start with the procedure:  We divide the feature space, i.e., the set of possible values for x1,…,xd into J  distinct and non-overlapping regions R1,…,RJ.  For every instance that falls into region Rj we make the same prediction, which is simply  the mean (or mode) of response values for the training observations in Rj.  -  Classification trees  o  Classification and Regression Trees (CART) is a term introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modeling problems. It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response. For a classification tree, we assign each test instance to the majority class (mode) of the training instances in the region where it belongs. You can consider this action as a being like a data point voting itself into a region which results in selecting the majority.  Gini and Entropy o  In practice people would prefer to use the Gini index and Entropy. The Gini index is the most commonly used measurement of inequality. For example in economics, the Gini index represents the income or wealth distribution of residents in a country.  -  Decision tree algorithms  There are variety of algorithms for decision trees. Here are three of the more popular ones:  ID3 (Iterative Dichotomiser 3) uses Entropy.  ▪ C4.5 (Successor of ID3) slightly more advanced version of ID3 and also uses  Entropy.  CART (Classification and Regression Tree) uses Gini impurity.  The ID3 Algorithm: The algorithm was developed by Ross Quinlan in 1975 (He is an  Australian who graduated from University of Sydney). It’s used to generate a decision tree from a dataset. Although this method is simple, it is an effective machine learning algorithm. The basic algorithm is as follows:  Calculate the entropy of every feature using the data set S. Split the set S into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information.  Make a decision tree node containing that feature. Recurse on subsets using remaining features.  -  Model complexity and pruning  Pruning is a technique that reduces the size of decision trees by removing sections of  tree that provide little power to classify instances.  The tree-building process that we described in previous steps may produce good predictions on the training set, but it’s likely to overfit the data, leading to poor generalization performance.  A tree that has a large number of regions may have only few data points per  region resulting in high variance.  On the other hand, having a small number of regions may result in high bias. One possible alternative is to grow a large tree, and then prune it backin order  to obtain a subtree.  Generally there are several ways of pruning trees:  Pre-pruning (forward pruning) Post-pruning (backward pruning)  Pre-pruning: In pre-pruning, we decide during the building process when to stop adding  nodes (eg. by looking at entropy).  Sometimes attributes individually do not contribute much to a decision, but  combined, they may have a significant impact.  Post-pruning: Post-pruning waits until the full decision tree has been built and then  prunes the attributes by subtree Replacement. Consider the selected subtree (in red) in the figure below. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error.  -  Decision trees : advantages and disadvantages  Advantages  Very easy to understand, as they represent rules. Capable of modelling nonlinear functions. Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot  compute a Euclidean distance between two vectors having weather as a variable.)  Disadvantages  Sensitive to small changes in the data. If you add few data points or change  some small values, your rules can be changed!    May overfit easily. As we have said before, by building deep decision trees you  are at high risk of overfitting and a high variance model.  Only axis-aligned splits. Normal decision trees split the space along each  features independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.  Trees may not be as competitive in terms of accuracy as some of the other  regression and classification techniques such as SVM or neural networks.  -  Advance topics -  Impact of distance metrics on KNN performance  KNN is a well known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data. Please use the following link for further explanation.  -  Feature importance of using Decision Trees (DT)  Decision Trees are a popular machine learning algorithm that uses feature selection to determine the most important classification features. DT operates by recursively segmenting the data into subsets based on the most informative features until a stopping criterion is reached. A criterion such as information gain or the Gini index is used to determine which feature is the most informative. At each node of the tree, the feature with the highest score is chosen as the splitting criterion. The significance of each feature can be determined by considering how much it contributes to overall improvement in the criterion. The greater the contribution, the more significant the feature. Feature importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model.      