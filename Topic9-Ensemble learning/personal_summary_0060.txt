 KNN algorithm and its variants    A useful technique is to assign weights to the contribution of neighbors of data points so that closer neighbors  contribute more to the mean than data points that are farther away.    This is useful for both classification and regression.   The basic idea is to  label  test data points as the same as nearest neighbor (NN).  The following diagram illustrates this concept.  If the black circle as the test point falls into the area where the  closest point is a  black ellipse with the class label , label this new sample as a class based on 1, nearest neighbor.  1。      Figure KNN intui,on.    Not only in KNN it may vary. Suppose  someone wants to check Use the nearest  neighbor (KNN)  of the test point to make a decision.  You can label  a test instance the same as the K nearest neighbor majority label. The figure below is an example. 3-NN classification.         Form 3− Classiﬁca,on.   As you can see, the new test point falls within the range of  two training points in the class.  0 and per person from class 1. Obviously you will trust 0 to create a class by the majority and label the points 0  .   KNN's Theory  To find the majority of decisions based on close training points, you need to do the average or average in continuous cases, and you need to find the mode of class labels in discrete form. In summary:  §  §  Continuous target function: The average  of the closest training example Discrete Class Label:  The mode   of class labels is  the closest training example  There is another concept that is partially related to KNN, which is called  Voronoi diagram. In mathematics, Voronoi diagrams divide a plane into areas based on the distance to a point in a certain subset of the plane. Take a look at the following figure.     Shape. Voronoi diagram.  As for the Voronoi diagram, we can see that it is based on the nearest neighbor. It's the same concept as KNN. Also, obviously, the data is not linearly separable, resulting in complex boundaries and decision rules. Note that the decisive plane is formed by the training example.  Imagine a scenario where we're performing 10− (10 nearest neighbors), with the only point that is actually close. The other is that the  6 neighbors that we are going to analyze have little to do with this point. The end result can actually be misleading.  But what if you assign different weights to the distances of the data points?  The question here is, what is a  distance-weighted nearest neighbor algorithm?  Basically, you can  assign weights to neighbors based on their distance from the test point. For example, the weight may be the inverse square of the distance.  This means that the greater the distance of adjacent things, the smaller their weight. All training points can affect a specific instance. This method is also known as Shepard's method.   Optimal number of neighbors (K)  Optimal number of neighbors (K)  How do you choose  a variable which one holds the number of neighbors?  How important is it to choose the right one?     One  possible is  to control the shape of the decision boundary described earlier. For  small values, we are limiting the region of a given prediction and forcing the classifier to focus more on the regions and neighborhoods that are close. We are asking classifiers not to bother with points that are quite far away. This results  in a lower bias and greater variance.  Higher values  have lower  variance because the decision boundary is smoother, but bias  increases. Basically, higher values mean  that you will be asking for more information, even from faraway training points.  As with most machine learning problems, we find hyperparameters such as:  It's actually not that simple. It is not always possible to find the best answer. However, as a simple and convenient approach, you can use cross-validation (see this topic's Model Selection)  to split the data into test and training samples to evaluate different ranges of models.  Values.  For example, the number of neighbors can be thought of as:  (cid:0)=1、。。  , m is a cross. It then performs cross-validation on all possible numbers.  (cid:0)=1、。。   The meter x   evaluates the model based on the split training and testing data. You can use the number of misclassification errors, or misclassifications, as a measure of model performance.  Finally,  you can determine which is which value by investigating the different values of $K$ and their corresponding misclassification errors.  (cid:0)=1、。。  , the meter X has the best performance based on partitioned data.  You can  define meters based   on training data points. There is no rule of thumb in choosing a meter X because it depends on  your desired search speed .  For example, when you're working on something, you  can also define 1000 training data points =1、。。。 、50。  Fig.  Here is a sample of the number of neighbors plo=ed ( ) and misclassiﬁca,on errors. As you can see, the minimum error occurs when:  (cid:0)=7。   Decision Turry  Decision Tree  A decision tree is a map of possible outcomes from a set of related selections.  Decision trees allow you to weigh possible actions against each other based on cost, benefit, and probability.     A decision tree typically starts with a single root node and branches to possible outcomes.   Regression trees  After dividing the feature space, you can fit a simple model to each subregion.  (cid:0)1、 (cid:0)2、... 。 What does this look like in practice?  First, let's look at the principle of how this works.  Decision trees that use regression models are called  regression trees. You can also fit classification models alternately. Such a decision tree is called a  classification tree. Typically, very simple models such as majority (classification) and mean (regression) are used  .  Let's start with the steps.  It divides the feature space, that is, the set of possible values. X 1, ... , X into non-overlapping separate areas 1,... 、(cid:0)(cid:0)。  1. 2.  Make the same prediction for  all instances that fall under the region. This is simply the  mean (or mode)  of the  response values of the training observaTon. (cid:0)(cid:0)。  How do we perform these actions?  The overall goal of a regression tree is to find an area.  (cid:0)1、 (cid:0)2、...  Minimize training errors:  Where ^  is the  average of  the target value of the training instance.  − Second region.       Let's say  there is a region. In this formula, for every point in the domain,  we find   the difference between the prediction and the true output i−^ . Obviously, we want to minimize this problem so that we have an area where there will be fewer errors. Once again, there is an optimization problem to solve.  How can I find a solution? Unfortunately, it is computationally impossible to consider all possible divisions of the feature space.  Region. How large will the space of possibilities be in this case?  For this reason, we have adopted a  top-down, greedy approach known as recursive binary splitting.  We  want to work in a  heuristic way rather than using a brute force solution  .  Dissolu’on  Let me explain how this heuristic technique works.  1.  2.  3.  4.  First select   the feature X and Threshold to divide the feature space into areas such as {X | X ≤ } and {X| X > } leads to as much reducHon of training errors as possible. Therefore, instead of going into the join space of all features, we  will work on independent feature forms, such as:    X with  threshold . Then repeat the process and further divide the data to ﬁnd  the best features and best thresholds to minimize errors in each resulHng area  . This Hme, however, instead of spliLng the enHre feature space, we only split one  of the  two previously idenHﬁed regions. The spliLng process conHnues unHl the Stop Criterion is reached. For example, you can conTnue unTl no more regions contain more than ﬁve  instances  , or unHl the nodes become too pure or sparse.  Forecast  Use the average (or mode)  of the training instance in the area where the test observation falls to  predict the response of a given test instance  .   Classification Tree    Classification and regression tree (CART) is a term introduced by Leo Breiman to refer    to a decision tree algorithm  that can be used for classification or regression predictive modeling problems. It is similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative response.  For a classiﬁcaTon tree, assign each test instance to the majority class (mode) of the training instance in the area to which it belongs. You can think of this action as having data points vote for an area, resulting in a majority being chosen.    The classification settings replace the sum of squared errors with the classification error percentage as a criterion for binary  splitting. The classification error rate is defined as the portion of the training instance in that area that does not belong to     the most common class. Where ^  represents the percentage of training instances  . − Region from − Class:    Essentially, distribution certainty (COD) indicates how certain it is that the classifier exists in a region.  If Ah is close to 1, this means that almost all training points in a region are voting for  a specific class label. Therefore, the classifier in this case is sure of the decision. On the other hand, if you are like, Ah is 0.5 which means that you cannot trust the vote due to the high classification error rate ( ).  However, one  problem with classification errors is low sensitivity  to tree growth.  Gini and Entropy  In fact, I prefer to use Gini Indices and Entropy. The Gini Index is the most commonly used measure of inequality. For example, in economics, the Gini index represents the distribution of income or wealth of residents of the country. The Gini index is defined as:  It also  represents the percentage  (percentage) of ^ training instances.  − from the  second region − third class. It is a measure of the sum of the overall variances.  Class.  If take a small value ^≈0, 1 for  all (why?) ）。  That's because we are faced with a low unequal distribution of votes. Therefore, the Gini index is considered a measure of the purity of a node.  Consider the following diagram: Suppose the Gini index and entropy lines represent the probability of choosing a particular class for a data point. For example, the percentage of training points in an area voting for a class label of 0  For points. As can be seen from the figure, in the corners there is a small value of the Gini index and misclassification, but in the middle area, where both classes represent the same opportunity of classification, the error is larger, showing the most even peaks. Since it is in the middle, a misclassification error occurs. If you look at the Gini index, you can see that the misclassification is formally linear, although it is not linear. In this case, Entropy seems to be smoother and better.      Shape. Gini index, diagram of entropy and classiﬁca,on error.  As an alternative to a Gini index, Entropy is defined as:  This formula has the same concept.  Let's look at another example regarding the Gini index.  Consider a two-class problem with the following split:    Figure: Example of the Gini index  As you can see, the value of the Gini index is 0  when faced with a great inequality like the first case, 1=0  and  2=6 and its value increases when dealing with evenly distributed cases such as:  1=3 and 2=3 resulting in 0.5 for the Gini exponent.   Decision Tree Algorithm  Decision trees have different algorithms.  Here are three  of the most popular:  §  §  §  ID3 (iteraHve dichotomy 3) uses entropy. C4.5 (successor to ID3) A slightly more advanced version of ID3 that also uses Entropy. CART (classiﬁcaHon and regression trees) use Gini impuriHes .  Let's take a closer look at ID3.      ID3 Argolism  The algorithm was developed by Ross Quinlan in 1975  (he is an Australian who graduated from the University of Sydney). Used to generate a decision tree from a dataset. This method is a simple but effective machine learning algorithm. The basic algorithm is as follows:  1.  2. 3.  Calculate  the entropy of all features using the dataset. Split  a set Divide into subsets using features that minimize entropy. Therefore, a small value of entropy means that it is suitable for aYribute or feature selecHon and more informa<on can be obtained. Create a decision tree node that contains  the feature. Use the remaining features to recurse the subset.  Tree depth  When you build a very deep tree  , you are basically dividing the feature space into smaller areas. If the tree is very deep, you  should expect to be unlikely to be able to visit many training points within that subregion.  This means that all estimates of the area are not adequate due to the large variance.  On the other hand, if the area is very large and the tree is shallow, you can assume that the variance of the training data points is not high, but  other issues such as  bias can occur. Shallow decision trees have high bias. It means that your decision-making process is too sweet. If you need to revise these concepts,  we'll cover them in Topics 7 and 8.  To find the sweet spot in terms of depth, you  need to create a decision tree of the appropriate depth. This  can be achieved by performing the cross-validation and other assessment methods learned in topics 5 and 6.  Note that you need to find or adjust the appropriate hyperparameter, which is the depth of the tree.   Model complexity and pruning  Pruning is a technique to reduce the size of a decision tree by removing sections of the tree that are hardly useful for classifying instances .  The tree-building process described in the previous step may produce good predictions for the training set, but it can lead to  overfitting of the data, which leads to poor generalization performance.  § § §  Trees with many regions may have few data points per region, resulTng  in large variance. On the other hand, a small number of regions can lead to a high bias . One  possible alternaTve is to grow a large tree and then prune it to get a subtree.  In general, there are several ways to prune trees.    § §  Front pruning (anterior pruning) Posterior pruning (backward pruning)  Cropping before pruning  Pre-pruning determines when to stop adding nodes during the build process  (for example, by examining entropy).  Suppose you are dividing nodes by checking the amount of entropy reduction when you select different features. If the reduction in entropy is not noticeable, you can stop splitting the node. By using this method, you eliminate unnecessary complexity of the model. However, there may be problems with this.  In some cases, the attributes do not contribute much to the decision individually, but they can have a significant impact when combined.  Let's discuss another model that can handle this problem.  A@er pruning  Post-pruning waits until a complete decision tree is built and prunes attributes by subtree replacement. Consider the subtree (red) selected in the following figure  . You can easily replace an entire subtree with a single region or node. You should make sure that this reproduces the smallest error.  Shape. ALer pruning. Sources ( Sayad 2018  )  Determine which subtree deletion causes the least error and replace it with a single leaf node.  As the diagram shows, wait until the complete decision tree is built, and then look for the subtree to see if you can replace it with a single node or feature with a slight change in entropy.   If yes, cut the tree. If not, you should keep that subtree as it may contain useful information.   Pros and cons of decision trees  What are the unique features of decision trees?  What issues are best for its use?  Highlights  § § §  It is very easy to understand because it represents the rules  . Nonlinear func<ons can be modeled. You can handle categorical variables  (that is, whether the weather is sunny or cloudy.   You cannot calculate the Euclidean distance between two vectors that have weather as a variable).  Shortpoints  § §  §  §  It is sensiHve to small changes in data  .  Adding a few data points or changing small values can change the rule. It can easily be overﬁt  . As menHoned before, building a deep decision tree increases the risk of overﬁLng and high variance models. Only division along the axis  . A typical decision tree divides space independently along each feature.   If you need to create a more complex decision tree model, you can consider concurrency and more complex scenarios when modeling the tree. Trees may not be as compe<<ve in terms of accuracy as other regression and classiﬁcaHon techniques such as SVMs and neural networks  .  2.  Provide summary of your reading list–external resources, websites, book  chapters, code libraries, etc.  3.  Reflection the knowledge that you have gained by reading contents of  this  topic with respect to machine learning.  Here are some of the key areas we'll explore this topic:  §  K Nearest Neighbor (KNN) Argolizum §  The k-nearest neighbor algorithm, also known as KNN or k-NN  , is a nonparametric supervised learning classifier that uses proximity to individual data  Classify or predict the grouping of points. It can be used for either regression or classification         problems, but it is typically used as a classification algorithm and works on the assumption that similar points can be found close to each other.  §  For classification problems, class labels are assigned based on majority vote. That is, the  label that most frequently represents around a particular data point is used.  §  Decision Tree (DT) §  Decision trees are nonparametric supervised learning algorithms that are used for  both classification and regression tasks. It has a hierarchical tree structure consisting of a root node, branches, internal nodes, and leaf nodes.  §  Types of decision trees §  Hunt's algorithm was developed in the 1960s  to model human learning in  psychology and forms the basis of many common decision tree algorithms, including: - ID3:  Ross Quinlan  is credited with the development of ID3. ID3 stands for Iterative Dichotimizer 3. The algorithm uses entropy and information gain as indicators to evaluate potential splits. - C4.5: This algorithm is considered to be a later iteraTon of  ID3, also developed by Quinlan. You can use information gain or gain ratio to evaluate split points in a decision tree. - CART: The term CART is an abbreviation for "classification and regression tree" and was introduced by Leo Breiman. This algorithm typically relies on Gini impurities to identify the ideal attribute to split. Gini impurities measure how often randomly selected attributes are incorrectly classified. When evaluating with Gini impurities, lower values are ideal.        