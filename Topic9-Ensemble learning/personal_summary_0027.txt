A  non-parametric  machine  learning  approach  called  K-Nearest  Neighbours (KNN) is utilised for both classification and regression problems. The complete training  dataset  is  kept  in  memory  by  this  instance-based  or  lazy  learning approach. KNN determines the distances between a given test instance and all training  instances  during  the  prediction  phase  and  chooses  the  K  closest neighbours based on a distance measure. When making predictions for the test instance, the algorithm then employs the labels (for classification) or values (for regression) of these neighbours.  KNN  has  a  number  of  variations  and  modifications  that  improve  its functionality or handle certain problems:  Weighted KNN: Assigns various weights to the neighbours depending on how close  they  are  to  each  other,  giving  closer  neighbours  greater  weight  in  the prediction.  Similar  to  weighted  KNN,  distance-weighted  KNN  emphasises  nearby neighbours  much  more  because  the  weights  are  inversely  proportional  to distance.  KNN with Feature Selection: Before calculating the distance, a feature selection step is used to determine which features are the most informative and to get rid of any that are unnecessary or redundant.  KNN with Feature Weighting: Assigns various weights to features depending on their significance or relevance, making sure that more informative features have a higher influence on the distance calculation.  Edited and Condensed KNN: These variations try to enhance the performance of  KNN  by  carefully  choosing  which  instances  to  keep  or  remove  from  the training  dataset.  Condensed  KNN  chooses  a  small  sample  of  instances  that adequately  represent  the  complete  dataset,  whereas  edited  KNN  removes instances that were incorrectly classified.  These KNN variations and extensions provide the ability to adapt the algorithm to various datasets and enhance its predicting performance.  Decision Trees:  A  well-liked  supervised  machine  learning  technique  called  decision  trees  is utilised  for  both  classification  and  regression  tasks.  They  are  built  using  a hierarchical network of nodes that represent  choices and results based on the values of the features. Each  branch on an internal node represents a potential outcome  for  a  test  on  a  particular  feature.  The  conclusion  or  forecast  is represented by the leaf nodes. Decision trees are capable of capturing non-linear correlations  and  interactions  between  information  and  offer  interpretable  and simple-to-understand models.  Regression Trees:  A particular kind of decision tree used for regression tasks is called a regression tree. They project outputs that are continuous numerical numbers. Regression trees break data into segments in order to reduce the variance or mean squared error of the target variable within each segment. The mean or median value of the training instances connected to each leaf node serves as the forecast in most cases.  Classification Trees:  Another  sort  of  decision  tree  used  for  classification  problems  is  the classification  tree.  They  anticipate  outputs  to  be  discrete  class  labels.  The separation  or  purity  of  classes  inside  each  split  is  maximised  during  the classification  tree  splitting  procedure.  To  assess  the  quality  of  splits,  various impurity metrics, such as Gini impurity or entropy, are frequently utilised. The majority class or the class with the largest probability distribution among  the training cases connected to that leaf node is normally the prediction at each leaf node.  By  using  various  splitting  algorithms  and  metrics,  both  regression  trees  and classification trees  can  handle  multi-class  situations.  Regression  and classification trees are examples of decision trees, which offer models that are easy  to  understand,  can  handle  non-linear  relationships,  and  are  resistant  to outliers. They can, however, overfit and be sensitive to slight variations in the training  data.  Pruning,  ensemble  approaches  (such  as  Random  Forests),  and regularisation are among strategies that can be used to address these problems and enhance the performance of Decision Trees.  The term "model complexity" describes a machine learning model's capacity to identify  and  express  the  underlying  relationships  and  patterns  in  the  data.  In machine learning, it is a crucial factor to take into account since overly complex models can result in overfitting, where the model memorises the training data but  struggles  to  generalise  to  new  data.  However,  overly  simplistic  models might not be able to adequately represent the intricacies of the data, resulting in underfitting and subpar performance.  Pruning is a method for dealing with overfitting in  machine learning models, especially in decision trees. In order to optimise and simplify a decision tree, pruning includes deleting some branches or nodes.        