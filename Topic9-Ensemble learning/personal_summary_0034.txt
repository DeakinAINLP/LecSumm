This topic we explore  K nearest neighbour (KNN) Decision trees (DT)  KNN algorithm and its variants A useful technique can be to assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.  This is useful for both classification and regression.  The basic idea is to label the test data point is the same as the nearest neighbour.  Theory of KNN For finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format. To summarise:  Continuous valued target function:  Mean value of the k nearest training examples  Discrete class label:  Mode of the class labels of the k nearest training examples  There is another concept partially related to KNN which is called a Voronoi Diagram. In mathematics, a Voronoi diagram is partitioning of a plane into regions based on distance to points in a specific   subset of the plane  Best number of neighbours (K) How do you pick the variable K which holds the number of neighbours and how important is selecting the right K?  For small values of K we are restraining the region of prediction and forcing our classifier to more focused on close regions and neighbours this make its not care as much about distant points. This results in low bias and high variances.  Higher values of K have smoother decision boundaries which means lower variance but increased bias.  Finding the right K is not straightforward. There isn’t always a best answer, but simple method is to use cross-validation to partition your data into test and training samples and evaluate the model on a range of K values and finding the one with the smallest misclassification error.  Decision trees A decision tree is a map of the possible outcomes of a series of related choices.  Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.  Typically starts with a single root node and branches into possible outcomes.  Regression trees Decision trees that use a regression model are called regression trees. We can alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used.    This becomes computationally infeasible as the number of regjions grows the space of possibilities grows massively.  For this reason, we take a top-down, greedy approach that is known as recursive binary splitting. Rather than using a brute-force solution, we would like to work in a heuristic way.  Classification trees Classification and Regression Trees (CART) is a term introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modelling problems.  It’s like regression trees, except that it is used to predict a qualitative response rather than a quantitative response. For a classification tree, we assign each test instance to the majority class (mode) of the training instances in the region where it belongs. You can consider this action as a being like a data point voting itself into a region which results in selecting the majority.  Decision tree algorithms  ID3 (iterative dichotomiser 3) uses entropy   C4.5 (successor of ID3) slightly more advanced version of ID3 also uses entropy CART uses gini impurity  ID3 Algorithm  1.  Calculate the entropy of every feature using the data set S. Split the set S into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information.  2.  Make a decision tree node containing that feature. 3.  Recurse on subsets using remaining features.  Tree depth If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, when the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances however you may have other problems such as bias. You will have a high bias in shallow decision trees. It means your decision making process is too naïve.     You need to create a decision tree of the right depth, to find the sweet spot in terms of depth  Model complexity and pruning Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances.  The tree-building process that we described in previous steps may produce good predictions on the training set, but it’s likely to overfit the data, leading to poor generalization performance.  A tree that has a large number of regions may have only few data points per region resulting  in high variance.  On the other hand, having a small number of regions may result in high bias. One possible alternative is to grow a large tree, and then prune it backin order to obtain a  subtree.  Generally there are several ways of pruning trees:  Pre-pruning (forward pruning) Post-pruning (backward pruning)  Pre-pruning In pre-pruning, we decide during the building process when to stop adding nodes (eg. by looking at entropy). Let’s say we are splitting nodes by checking the amount of entropy reduction when we select different features. We can stop splitting nodes when the entropy reduction is not significant. By using this method we are eliminating an unnecessary complexity on the model. However, this may be problematic.  Sometimes attributes individually do not contribute much to a decision, but combined, they may have a significant impact.  Post-pruning Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement. Consider the selected subtree (in red) in the figure below. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error.  Decision trees advantages and disadvantages Advantages  Very easy to understand, as they represent rules. Capable of modelling nonlinear functions. Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a  Euclidean distance between two vectors having weather as a variable.)  Disadvantages  Sensitive to small changes in the data. If you add few data points or change some small  values, your rules can be changed!    May overfit easily. As we have said before, by building deep decision trees you are at high  risk of overfitting and a high variance model.  Only axis-aligned splits. Normal decision trees split the space along each features  independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.  Trees may not be as competitive in terms of accuracy as some of the other regression and  classification techniques such as SVM or neural networks.    Impact of distance metrics on KNN performance KNN is a well known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data.  Feature importance of using Decision Trees (DT) Decision Trees are a popular machine learning algorithm that uses feature selection to determine the most important classification features. DT operates by recursively segmenting the data into subsets based on the most informative features until a stopping criterion is reached. A criterion such as information gain or the Gini index is used to determine which feature is the most informative. At each node of the tree, the feature with the highest score is chosen as the splitting criterion. The significance of each feature can be determined by considering how much it contributes to overall improvement in the criterion. The greater the contribution, the more significant the feature. Feature importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model.      