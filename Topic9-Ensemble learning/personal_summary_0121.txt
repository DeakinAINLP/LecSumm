  K nearest neighbor (KNN) algorithm   Decision tree (DT)  KNN algorithm and its variants:  KNN (K-Nearest Neighbors) algorithm is a supervised machine learning algorithm used for classification and regression tasks. It is a non-parametric algorithm that makes predictions based on the similarity (closeness) of data points in the feature space.  The basic idea behind KNN is that similar data points tend to belong to the same class or have similar output values. In the case of classification, the algorithm assigns a class label to a test data point by considering the class labels of its k nearest neighbors in the training dataset. The class label that occurs most frequently among the neighbors is assigned to the test data point.  There are a few variants of the KNN algorithm that can be used depending on the specific requirements and characteristics of the data  Distance Metrics:  Euclidean Distance: The most common distance metric used in KNN. It calculates the straight-line distance between two points in the feature space.  Manhattan (Cityblock) Distance: It calculates the distance between two points by summing the absolute differences of their coordinates along each dimension.  Cosine Distance: It measures the cosine of the angle between two vectors, which represents their similarity.  Weighted KNN: Instead of simply considering the majority vote of the k nearest neighbors, weighted KNN assigns weights to the neighbors based on their distance to the test data point. Closer neighbors have higher weights, and their class labels contribute more to the final prediction.  Radius-based Neighbors: In addition to considering a fixed number of nearest neighbors (k), this variant considers all training data points within a certain radius from the test data point. The class label is determined by majority voting among the neighbors within the radius.  KD-Tree: KD-Tree is a data structure that partitions the feature space into regions to speed up the search for nearest neighbors. It organizes the training data points in a binary tree structure based on their feature values, allowing for efficient neighbour search.  These variants offer different ways to measure similarity and make predictions in KNN, allowing for more flexibility and adaptability to different types of data and problem domains.  Decision trees:    A decision tree is a map of the possible outcomes of a series of related  choices.    Decision trees can be used to weigh possible actions against one another  based on their costs, benefits and probabilities.    A decision tree typically starts with a single root node, which branches  into possible outcomes.  Regression trees:  Regression trees are a type of decision tree algorithm used for solving regression problems. While classification trees predict categorical class labels, regression trees predict continuous numerical values as the output.  The structure and construction of regression trees are similar to decision trees. The key difference lies in how the splits and predictions are made at each node.  The process of constructing a regression tree involves recursively partitioning the data based on the values of input features, creating a hierarchy of nodes and branches. Each internal node represents a feature and a splitting criterion, while each leaf node represents a predicted numerical value.  Classification trees:  Classification trees are a type of decision tree algorithm used for solving classification problems. They are supervised machine learning models that predict categorical class labels based on input features.  The construction and structure of classification trees are like other decision trees. The algorithm recursively partitions the data based on the values of input features, creating a hierarchical structure of nodes and branches. Each internal node represents a feature and a splitting criterion, while each leaf node represents a predicted class label.  The splitting criterion in classification trees aims to minimize impurity or maximize information gain at each node. Common impurity measures used include the Gini Index and entropy. The algorithm searches for the best feature and threshold that minimize the impurity or maximize the information gain in the resulting partitions.  Model complexity and pruning:  Model complexity refers to the degree of intricacy or sophistication of a machine learning model. In the context of decision trees, model complexity is determined by the depth and breadth of the tree, as well as the number of nodes and branches.  Pruning is a technique used to reduce the complexity of decision trees and prevent overfitting. Overfitting occurs when a model captures noise or random variations in the training data, leading to poor generalization on unseen data. Pruning helps to strike a balance between capturing the patterns in the training data and maintaining the ability to generalize to new data.  There are two common approaches to pruning decision trees:    Pre-Pruning: Pre-pruning involves stopping the growth of the tree early,  before it becomes too complex.    Post-Pruning: Post-pruning involves growing the tree to its full  complexity and then selectively removing nodes or branches. This is done by iteratively evaluating the impact of removing each node or branch on a validation set or using statistical tests. If removing a node or branch improves performance on the validation set, it is pruned.  Pruning helps to prevent overfitting by reducing the complexity of the decision tree. A simpler tree with fewer nodes and branches is less likely to memorize noise in the training data and has a better chance of generalizing well to unseen data.  Pruning can improve the performance of decision trees by reducing variance, improving interpretability, and reducing computational resources required for training and prediction. However, it's important to strike a balance and avoid excessive pruning, which can lead to underfitting and loss of important patterns in the data. The optimal level of pruning depends on the specific dataset and problem at hand and may require experimentation and validation.   Decision tree advantages and disadvantages:  Advantages    Very easy to understand, as they represent rules.   Capable of modelling nonlinear functions.   Can handle categorical variables (i.e. weather being sunny vs cloudy. We  cannot compute a Euclidean distance between two vectors having weather as a variable.)  Disadvantages    Sensitive to small changes in the data. If you add few data points or  change some small values, your rules can be changed!    May overfit easily. As we have said before, by building deep decision trees you are at high risk of overfitting and a high variance model.    Only axis-aligned splits. Normal decision trees split the space along each features independently. If we need to make a more complex decision tree model, we can consider joint probabilities or more complicated scenarios while modelling the tree.    Trees may not be as competitive in terms of accuracy as some of the other regression and classification techniques such as SVM or neural networks.  We also learnt the impact of distance metrics on KNN performance and future importance of using decision trees.  We also learnt how to use KNN and decision trees in python.     