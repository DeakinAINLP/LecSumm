 This  topic  in  the  Machine  Learning  unit  we  were  introduced  to  K  nearest  neighbour  (KNN)  algorithm and Decision Trees, we learnt:  -  KNN is a technique in regression and classification in which weights are added to the neighbor  points such as closer points would have more contribution than further points in the average  calculations.  -  Voronoi Diagram is a plane partitioning technique based on the distances between the points  in the plane and specific points in the plane, resulting in decisions rules based on the resultant  partitioning.  -  The chosen number of K points will shape the resultant decision boundaries. Having small K  value will limit the classifier in close training points while neglecting more distant ones, leading  to low bias and high variance. The opposite could be said about choosing a large K value, which  will lead to low variance but large bias.  -  Decision trees are maps showing all the possible outcomes resulting from all potential inputs.  They usually start with a single root node with branches appearing as we go further in the tree  with  potential  outcomes.  Trees  that  use  regression  or  classification  models  are  called  regression or classification trees, respectively.  -  A top-down greedy approach is used to find the optimal partition for the tree. In that approach,  a feature is selected, and the feature space is divided based on that feature such as the division  will lead to least training error. Then, another feature is selected, and the feature space is split  again such as the split leads to the minimum training error. This process is repeated for all  features until a stopping criterion is satisfied.  -  One of the Decision tree algorithms is the ID3 Algorithm. In that algorithm, the entropy index  is calculated for every feature in the dataset. Then the dataset is split into subsets using the  feature with the lowest entropy value and a tree node is made that contains that feature. This  process is repeated for the rest of the features.  -  As the depth of the tree increases, the lower chance that having many training points within  each  sub-region,  which  decreases  the  quality  of  the  estimation  due  to  the  high  variance.  However, having the tree not deep enough or shallow, other problems such as high bias could  occur.   -  One technique to reduce the size of the trees is called pruning. In this technique, nodes or  sections of the tree that arenâ€™t contributing strongly to the classification or regression process  are removed from the tree. This could happen while creating the tree by stopping adding more  depth to the tree when entropy reduction becomes insignificant, or after creating the tree by  removing the nodes that will lead to the smallest change in entropy.  -  Simplicity, capability of handling nonlinear functions and categorical data, are all advantages  of Decision trees. However, Decision trees have the limitations of being sensitive to changes  in data and susceptible to overfitting.  