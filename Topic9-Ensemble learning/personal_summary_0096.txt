 KNN algorithm and its variants:   A useful technique can be to assign weights to the contribution of data point neighbours, so the nearer neighbours contribute more to the average than more distant ones. And this is useful for both classification and regression. The basic idea is to label the test data point is the same as the nearest neighbour (NN). K in KNN can vary. Let’s say someone would like to check. nearest neighbours (KNN) of the test point in order to decide. You can label a test instance to the same as the majority label of the K-nearest neighbours.   Theory of KNN:    Best number of neighbours (K): You can think K of as controlling the shape of the decision boundary we talked about earlier. For small values of K, we are restraining the region of a given prediction and forcing our classifier to be more focused on the close regions and neighbours. We are asking the classifier not to care about distant points. This will result in a low bias and high variance. Finding the best answer is not always possible. But as a simple and handy method, you can use Cross-validation to partition your data into test and training samples and evaluate your model with different ranges of K values.  Decision Trees: A decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities. A decision tree typically starts with a single root node, which branches into possible outcomes. In the video you will see an example of how a decision tree can be used to predict commute time.    Regression Trees:  VI.  Classification Trees: Classification and Regression Trees (CART) is a term introduced by Leo Bierman to refer to decision tree algorithms that can be used for classification or regression predictive modelling problems. It’s like regression trees, except that it is used to predict a qualitative response rather than a quantitative response.  Gini and Entropy:  Decision Tree Algorithm:  The ID3 Algorithm: 1. Calculate the entropy of every feature using the data set S. Split the set S into subsets using the feature for which entropy is minimum. So lesser values of entropy mean it should be a good choice for selection of the attribute or feature and it will gain more information. 2. Make a decision tree node containing that feature. 3. Recurse on subsets using remaining features.  Tree depth: If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, when the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances however you may have other problems such as bias. You will have a high bias in shallow decision trees  VIII.  Model complexity and pruning:  Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances.   Pre-pruning: Splitting nodes by checking the amount of entropy reduction when we select different features. Then stopping splitting nodes when the entropy reduction is not significant. By using this method, we are eliminating an unnecessary complexity on the model. However, this may be problematic. Sometimes attributes individually do not contribute much to a decision, but combined, they may have a significant impact.  Post-pruning: Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error. Decision trees advantages and disadvantages:   Impact of distance metrics on KNN performance: The performance of KNN (K-Nearest Neighbours) algorithm can be significantly influenced by the choice of distance metrics used to measure the similarity between data points. The distance metric used in KNN algorithm determines which neighbouring points are selected to make the classification decision. Therefore, selecting an appropriate distance metric is crucial for achieving high classification accuracy.  