 Topic 8 Topics to cover ❑ K nearest neighbor (KNN) ❑Decision tree (DT)  1.  K Nearest Neighbor (KNN) Algorithm  The KNN algorithm was discussed extensively. KNN is a type of instance-based learning that classifies objects based on closest training examples in the feature space. KNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry.  How KNN works: The algorithm calculates the distance (Euclidean, Manhattan, etc.) between the test point and all the training points, selecting the 'K' training points that are nearest to the test point. The test point is then assigned the most common class among its K nearest neighbors.  Choosing the right 'K': This is crucial as a smaller 'K' value can capture more noise and lead to overfitting, while a larger 'K' can lead to underfitting. Cross-validation is often used to choose the optimal 'K'.  Weights based on distance: This technique assigns more importance to closer points. The farther the neighbor, the lower the weight assigned.  2.  Decision Tree (DT) The decision tree is a type of supervised learning algorithm mostly used for classification problems. It works by partitioning the data into subsets based on feature values. This process is called 'splitting,' and the reverse process is 'pruning.'  How a Decision Tree works: Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. The tree is built top-down, choosing a variable at each step that best splits the set of data.  Types of Decision Trees: Mainly, there are two types - Classification Trees used when the response variable is categorical, and Regression Trees used when the response is numeric.  Building a Decision Tree: The process involves partitioning data into distinct and non- overlapping regions. For each instance that falls into one region, the same prediction is made, which is simply the mean (or mode) of the response values for the training instances in that region.  Pruning: The process of pruning is about controlling the size of the tree at the end of its development. This is essential to prevent overfitting.   Decision Tree algorithms: Various algorithms were covered, including ID3, C4.5, and CART. Each uses a different metric for choosing the node to split on - entropy for ID3 and C4.5, and Gini impurity for CART.  3.1 Impact of distance metrics on KNN performance: It was discussed how different distance metrics can significantly affect the performance of KNN.  3.2 Importance of feature selection in Decision Trees: Decision Trees inherently perform feature selection, choosing the optimal feature to split on at each node. This has a major impact on the performance and interpretability of the tree.  3.  Provide summary of your reading list–external resources, websites, book chapters,  code libraries, etc.  1. Most Popular Distance Metrics Used in KNN: https://www.kdnuggets.com/2020/11/most- popular-distance-metrics-knn.html  2. United States of Voronoi: https://youtu.be/b_uvofsYl9s  3.  Regression Trees: https://youtu.be/w4MnOA14pYs  4.Sayad, S 2018, Decision Tree - Overfitting: http://www.saedsayad.com/decision_tree_overfitting.htm  5. Top 5 advantages and disadvantages of Decision Tree Algorithm: https://dhirajkumarblog.medium.com/top-5-advantages-and-disadvantages-of-decision- tree-algorithm-428ebd199d9a  6. Decision Trees in Machine Learning:  https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052  