 The first topic for this topic was KNN and the basic idea behind this technique is to label the test  data point the same as the nearest neighbour. In other words, we utilize the labels of the majority of the  nearest neighbours to make a prediction on a data point. In the case of choosing the majority, we have to  choose an appropriate calculation depending on the problem. If we have a classification problem, we can  use the mode of the class labels and if we have a regression problem, we can use the average or mean  distances. In this algorithm, K will be our hyperparameter and it essentially allows us to choose how many  neighbours we want to include to make a prediction. K will control the shape of the decision boundary  and small values of k restricts the region and makes the model focussed on closer regions and  neighbours. Inversely, higher values cover a larger area and allows for smoother decision boundaries.  One of the main things to note is the fact that while the learning process of this is simple, it is very  computationally expensive.  The next topic to cover is Decision Trees and the basic idea is asking questions and breaking  down the responses into branches that go into new nodes (which in itself, can a feature or partitioned  feature) which in itself can be further questions and finally, the leaf node will be our predictions/target  value. The process in building a decision tree firstly involves dividing the feature space  which will be our  possible values into distinct and non-overlapping regions during the training time. Once the tree has been  built, we will have the class labels or means from each region, to which we can assign new data points to  by pushing it into the tree. Since the goal here is to find the correct region, we want to minimize the error  in regard to that by using a top-down greedy approach known as recursive binary split. Something I  thought was important to note is in this process, we are essentially performing feature selection by  ensuring that the most important feature will be at the root and the subsequent nodes/feature will be  based on importance. Also, there are two metrics we can use for building a classification decision tree  known as Gini index and Entropy. Based on the example provided during the lecture on how to build the  tree, if we were to look at the Gini index to get the root node feature, we would calculate the Gini index  for all features and use the one that has the lowest value, followed by a split to subsets and recursively  repeat and select nodes. One thing to consider though is we need to be able to stop as if we get deeper  into the tree, we get smaller regions and higher variance, leading to poor estimations (overfitting).  Conversely, if we have a shallow tree, we will get lower variance, but higher bias (underfitting). Once  again, we need to find a good trade off point. We can use pruning to control this depth of the tree. Pre-  pruning is done during the tree building process where we stop the process by looking at entropy values  for example. Post-pruning is done after the building where we look at the significance of the features and  remove accordingly. Overall, the advantage of this technique is it is easier to understand, good for non-  linear problems and can also handle categorical variables. Disadvantages are that it is sensitive to small changes in the data, can overfit due to deep trees, considers each feature independently and  performance is not as competitive compared to other techniques such as SVM or Neural Networks.   