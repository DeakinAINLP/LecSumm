Topic 8: Nonlinear models (KNN and DT)  8.2 KNN algorithm and its variants  One technique that is used in KNN is to assign more weight to the points in the data that are nearer than the more distant ones.  8.3 Theory of KNN  8.4 Best number of neighbours (K)  The lower the number of K the more restrained the classifier will be, whereas with higher values of K there will be lower variance and better decision boundaries however with increased bias.  8.5 Decision trees  Decision trees are a type of supervised learning which involves the splitting of data based on a certain parameter. Decision trees use decision nodes and leaves, the leaves are the final outcomes/decisions, whereas the nodes are where the data is split, i.e., Stall and Accident in the figure above.  8.6 Regression trees  Regression trees are decision trees that use a regression model. ‘Usually, extremely simple models such as majority (classification) or mean (regression) are used.’  8.7 Classification trees  Classification trees are decision trees that use classification problems, working with quantitative data instead of qualitative data.  8.8 Decision tree algorithms  Here are three of the more popular ones:    ID3 (Iterative Dichotomiser 3) uses Entropy.    C4.5 (Successor of ID3)  slightly more advanced version of ID3 and also uses Entropy.    CART (Classification and Regression Tree)  uses Gini impurity.  In the case of a deep tree, there is low chance that many of the training points in that region should be visited. Contrary to this, a wide and shallow tree would reveal you don’t have a huge variance, however there can be bias.  ‘You need to create a decision tree of the right depth, to find the sweet spot in terms of depth. You can achieve that by performing the cross validation and other evaluation methods you have learned in topics 5 and 6.’  8.11 Advance topics  KNN can be largely affected by the distance metric chosen to be used. ‘Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN.’  “Decision Trees are a popular machine learning algorithm that uses feature selection to determine the most important classification features. DT operates by recursively segmenting the data into subsets ba sed on the most informative features until a stopping criterion is reached. A criterion such as informati on gain or the Gini index is used to determine which feature is the most informative. At each node of t he tree, the feature with the highest score is chosen as the splitting criterion. The significance of each f eature can be determined by considering how much it contributes to overall improvement in the criteri on. The greater the contribution, the more significant the feature. Feature importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model. “  Decision Tree 50-50% Split:  Decision Tree 70-30% Split:  In the above comparison it can be noted that the metrics overall appear to be in favour of the 70-30% split in this case. In the 50-50% the data has less training data, thus it may underfit more than the 70- 30% because it has less data to work with to learn the intricacies of certain patterns that may be present.  With the 70-30% split the model has access to more training data. While this increases the risk of potential overfitting when compared to the 50-50% split, in general it will allow for the model to have a better understanding and thus perhaps better performance, which is in the results the case.               