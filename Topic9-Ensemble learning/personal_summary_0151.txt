KNN can be used to allocate weights to neighbors where nearer neighbors will weigh greater than the distant ones. The is typically used in classiﬁca9on and regression. The fundamental idea is to label the test data point similar to the nearest neighbor (NN).  KNN is most used for classiﬁca9on wherein its classiﬁca9on is based on how its nearest neighbors have been classiﬁed. The nearest neighbors or close training points can be iden9ﬁed through performing mean in con9nuous cases and mode in discrete format. The Shepard’s method is when weights are assigned to the neighbors based on their distance from the test point.  KNN algorithm is based on feature similarity. The process of ﬁnding the value for k is called parameter tuning. If the value of k is too low, a skewed answer results from low bias, whereas if the k value is too big, a lot of data points are regarded and 9me to process increases by a lot, it also increases bias. The most common way of choosing k is by using the square root of n where n is the total number of data points. An odd value of K is usually selected to avoid confusion between two classes of data. Another way of ﬁnding the k value is through the cross valida9on method to par99on data into training and test sets and evalua9ng the model with diﬀerent ranges of k by incorpora9ng misclassiﬁca9on error  in measuring the performance of the model. Based upon this, the value of k that yields the best performance can be chosen based on the par99oned data.  A decision tree is a hierarchical model that branches out every possible consequence including event chance outcomes based on their costs, beneﬁts, and probabili9es. It starts with a single root node that eventually branches out to possible outcomes. Regression trees are decision trees that use a regression model. This starts by dividing the feature space into possible values for the data points into dis9nct and non-overlapping regions, and for every instance that falls into a speciﬁed region, a predic9on will be made which is done on the mean or mode of response values for the trained observa9ons of the dataset. This can be achieved through the heuris9c method.  Classiﬁca9on and regression trees are used for classiﬁca9on or regression predic9ve modeling problems. It is used to predict a qualita9ve response instead of a quan9ta9ve one. There are three popular algorithms for decision trees mainly ID3 which uses entropy, C4.5 which is a bit more advanced version of ID3 and uses Entropy and CART that uses Gini impurity. A deep tree is par99oning the feature solace into small regions meaning it has a decreased chance of visi9ng training points within that speciﬁed region resul9ng in high variance Whereas shallow decision trees have bias.  Pruning reduces the decision tree size by removing sec9ons that contribute very liTle power to classify the instances. Pruning includes pre-pruning which includes when to stop adding nodes in a decision process and post-pruning is when a full decision tree is built then prunes its aTributes by subtree Replacement.  