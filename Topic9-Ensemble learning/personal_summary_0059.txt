Topic 8. Nonlinear models (KNN and DT)  KNN algorithms and its variants        KNN algorithm : useful technique can be to assign weights to the contribution of data point neighbours so the nearer neighbours conrtribute more to the average than more distant ones (simply, it assumes that similar things exist in close proximity) : useful for both classification and regression Label the test data point the same as the nearest neighbour(NN) K can vary Label a test instance to the same as the majority label of KNN.  Theory of KNN    Continuous valued target function  : Mean value of the k nearest training examples    Discrete class label  : Mode of the class labels of the k nearest training examples    Voronoi Diagram  : partitioning of a plane into regions based on distance to points in a specific subset of the plane    Distance-weighted nearest neighbor algorithm(Shepard`s method)  : assign weights to the neighbors based on their distance from the test point : weight may be inverse square of the distances : higher the distance, lower its weight  Best number of neighbours(K)    Small values of K : restraining the region of a given prediction and forcing classifier to become more focused on the close regions and neighbours. : low bias and high variance    Higher values of K  : has smoother decision boundaries : lower variance but increased bias.    Cross-validation  : to partition data into test and training samples and evaluate model with different ranges of K values : can use the misclassification error as a measurement of performance  Decision trees    Decision trees  : a map of the possible outcomes of a series of related to choices : can be used to weigh possible actions against one another based on their costs, benefits and probabilities   Regression trees  : after partitioning the feature space, we can fit a regression model, and it is called regression tree : goal of regression trees is to find regions that minimize the training error  : usually, extremely simple models such as majority(classification) or mean(regression) are used. 1.  A top down and greedy approach (recursive binary splitting) 2.  Heuristic method(solution)     Select a feature and a threshold: split the feature space. Regions. Leads to the best  possible reduction in training error     Not going into the joint space of all features: use independent feature form such as  with a threshold     Repeat the process: looking for the best feature and the best threshold. Minimize  the error in each of the resulting regions. Split one of thew two previously identified regions. Splitting process continues until a stopping criterion is reached.    classification trees : if we fit a classification model, it is called classification trees : it is used to predict a qualitative response : replace the sum of square error by the classification error rate as a criterion for making the binary splits. : the classification error rate: the fraction of the training instances in that region that do not belong to the most common class : CoD(Certainty of Distribution) and close to 1 : one of the problems of classification error is that it is less sensitive for tree growing : classification error is being less sensitive for tree-growing : Solution 1.  Gini index: measure of node purity.  Used to evaluate the quality of a split at a particular  node. 0 indicates pure set and 1 indicates a completely impure set  2.  Entropy: pattern of error. Quantifies the degree of uncertainty or disorder in a set of  data. Goal is to minimize entropy by finding the best splits in the decision tree that lead to more pure subsets.  Decision tree algorithms    C4.5(Successor of ID3)  : slightly more advanced version of ID3 and also uses Entropy    CART(Classification and Regression Tree)      : uses Gini impurity ID3(iterative Dichotomiser3): uses Entropy 1.  Calculate the entropy of every feature using the dataset S. 2.  Split the set S into subsets using the feature for which entropy is minimum. 3.  Make a decision tree node containing that feature 4.  Recurse on subsets using remaining features Tree depth(Cross validation is needed) 1.  If tree is very deep    Partitions the feature space into small regions    Small number of training points in sub-regions   If tree is shallow      Small variance but large bias  Increases variance and estimation becomes poor  Large regions  2.   Model complexity and pruning      Pre-pruning (forward pruning) : decide during the building process when to stop adding nodes by looking at entropy     In case of entropy : check the amount of entropy reduction when selecting different features : stop splitting when the entropy reduction is not significant : can be problematic  Post pruning(backward pruning) : prunes by subtree replacement after full tree is built     Replace an entire subtree with a single region or node  : reproduces the smallest error     Select a subtree and check if replacing it with a single node or feature incurs only small amount of change in Entropy, if yest trim the tree, if not keep the subtree  Advantages and disadvantages of decision trees    Advantages     Very easy to understand as they represent rules    Capable of modelling nonlinear functions    Can handle categorical variables    Disadvantages     Sensitive to small changes in the data    May overfit easily-risk of overfitting and a high variance model    Only axis-aligned splits â€“ consider each feature independently and no joint  probabilities of features     Performance is not competitive  Impact of distance metrics on KNN performance    The effectiveness of KNN is impacted by distance metrics     Euclidean distance, Manhattan distance and cosine similarity    Cosine similarity is better than Euclidean in a high dimensional data  Feature importance of using Decision Trees (DT)    DT operates by recursively segmenting the data into subsets based on the most informative  features until a stopping criterion is reached     Information gain or the Gini index    At each node of the tree, the feature with the highest score is chosen as the splitting  criterion     The significance of each feature can be determined by considering how much it  contributes       