 In this module we learned about two new supervised learning models which are KNN (K nearest neighbors) and Decision trees. In KNN, the basic idea is that the data point creates a boundary and before predicting it looks at the nodes in that boundary, hence its neighbors and classifies that data point as the majority of its neighbors. The only hyperparameter is K. The best way to find K is to predict using various K values, then plot the values on a graph. When we evaluate the graph, we can find a sweet spot where K results in the best performance. Small values of K can lead to low bias but high variance and high values of K result in high bias and low variance, that is why it is important to find a balance between the trade-offs. A reason KNN is not used in real world applications is that it is very time consuming.  Suppose we have a million data points in a dataset, we cannot evaluate knn for each data point as it will take a lot of time.  Decision trees are another supervised learning model. It is a very interpretive model which results in low performance, that is the trade off with the model. The working is fairly simple. You can use three metrics for its working such as entropy, Gini index etc. You evaluate for the metric for the null model and then using that you compute the value for each feature and based on the values you choose the feature for splitting. It is based on statistics and probability. If the tree is very deep, the feature space would be small, resulting in high variance. But if the feature space is large or tree is shallow it would result in low variance high bias. Here as well we need to find a sweet spot as with all the models to optimize performance.   Pruning is a technique where we remove branches from a tree. There are two types of pre-pruning and post-pruning. Then, we discussed the advantages and disadvantages of decision trees. Then we learned how to apply theoretical knowledge to practical using python and various libraries.  