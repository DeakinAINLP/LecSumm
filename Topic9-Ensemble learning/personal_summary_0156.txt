During our topic 8 of the machine learning course, we focused on two important  algorithms in machine learning: the K-nearest neighbors (KNN) algorithm and decision trees.  We began by discussing the basics of the KNN algorithm, its variants, and how it works in  practice which was very informative in my case. We then explored the various distances  metrics that are used to measure similarity between data points, including Euclidean distance,  Manhattan distance, and Minkowski distance.  Next, we delved into the theory of the KNN algorithm and discussed how it can be  used for both classification and regression tasks. We learned about the role of  hyperparameters in the KNN algorithm, including the number of neighbors (K), distance  metric, and weight function. We also examined the advantages and limitations of the KNN  algorithm.  Next, we shifted our focus to decision trees, which are another popular algorithm used  in both classification and regression tasks. We explored the different types of decision trees,  including regression trees and classification trees. We learned about how decision trees work,  how to build them, and the algorithms used for decision tree construction. I had not  understood the concepts well at first but I later on found very good resources online which  really helped in getting a clearer understanding.  We also covered the topic of model complexity and pruning in decision trees, which  involves adjusting the size and complexity of a decision tree to prevent overfitting. We  examined the advantages and disadvantages of decision trees, including their interpretability  and ability to handle missing data.  Lastly, we delved into advanced topics related to KNN and decision trees, including  ensemble learning techniques such as bagging and boosting. We learned about how these  techniques can be used to improve the performance of KNN and decision trees. To  complement our theoretical understanding of KNN and decision trees, we also learned how to  implement these algorithms in Python using popular machine learning libraries such as scikit-  learn. We worked with various datasets and learned how to tune hyperparameters, evaluate  model performance, and make predictions using KNN and decision trees.  Overall, topic 8 was an informative and practical topic that expanded my knowledge  of these two important algorithms in machine learning. I gained an understanding of the  strengths and limitations of KNN and decision trees and learned how to apply these  algorithms to real-world problems using Python. The knowledge gained during this topic will  be useful for building accurate and efficient models for various real-world applications and in  my career as a data scientist.  