 Summarise the main points that were covered. Main points covered in topic 8 of the unit content:   K nearest neighbour algorithm (KNN)   Decision trees  Reflection My reflection on the knowledge gained this topic from reading the unit contents for this topic with respect to machine learning.  This is a reflection on the knowledge that I gained during topic 8 in regard to machine learning.  This topic I learnt about the K nearest neighbour algorithm (KNN). It is a supervised learning algorithm that it generally used as a classification algorithm but can also be used for regression. A data point is labelled according to the labelled data that is within the same location (it’s neighbours).  The algorithm uses a value to determine how many of the closest data neighbouring points the algorithm view to assist in making the decision. There is also an altered version of this algorithm called Distance-weighted nearest neighbour, which is also known as Shepard’s algorithm. When viewing the n closest neighbours to classify the data point, the neighbours that are located closer are weighted so that they have more influence on the outcome. Selecting the n value for the number of neighbours to view is not a straightforward process. Similar to other machine learning algorithms, determining the hyper parameters is not a simple task. However, one approach is to evaluate the model using cross validation, so viewing the model with many different values of n neighbours, to find the best hyperparameter.  This topic I also gained knowledge on the topic of decision trees. Decision trees are another supervised learning algorithm, used for classification tasks as well as regression tasks. The structure of the Decision tree contains a root node, internal nodes, leaf nodes, and branches to connect the nodes. The leaf nodes are the possible outcomes, whereas the decision nodes are where the data is split. There are multiple decision tree algorithms, and this topic the unit content discussed the ID3 (Iterative Dichotomiser 3) algorithm.  Also discussed was the pruning of decision trees. The pruning of trees is primary done to reduce the likelihood of overfitting, by removing sections of the tree that don’t heavily impact the classification of data points. There are two techniques for pruning, pre-pruning, and post-pruning. Pre-pruning is used before the construction of the decision tree. For example, we can decide when to stop splitting the nodes based on the entropy reduction, if it is not significate then stop splitting the nodes. Post- pruning waits until after the decision tree is built, then it replaces the sub-trees that have little influence on the correct result with a leaf node.   