 This topic, we focus mainly on k-Nearest Neighbours (k-NN) and Decision Trees.  These are the main points covered this topic:  1.  k-Nearest Neighbours (k-NN):  As a simple, instance-based learning method, the k-NN algorithm was  introduced. The algorithm calculates new instances using a similarity metric  (e.g., distance functions). It was pointed out that k-NN performs better when  the data is scaled and struggles when faced with many input variables.  Furthermore, k-NN makes no assumptions about the problem's functional  form.  2.  Cross-Validation:  The concept of k-fold cross-validation was discussed, emphasising its role in  estimating machine learning model accuracy. It was pointed out that  increasing the number of folds (K) increases computational time but  increases confidence in the validation result. The Leave-One-Out Cross-  Validation particular case of k=N was also discussed.  3.  Decision Trees:  The Decision Tree algorithm was introduced as an effective solution for  regression and classification problems. The main idea is to build a decision  tree based on the values of the input features. The concept of tree splitting  based on classification error minimisation was discussed, but it was stressed  that this approach does not guarantee an optimal tree.  4.  Overfitting and Tree Depth:  The effect of tree depth on model accuracy was investigated. It was  demonstrated that increasing the depth of a tree could result in overfitting.  As a result, determining the optimal tree depth is critical to the model's  performance.  5.  Model Evaluation Metrics:  Precision, recall, and accuracy were emphasised as evaluation metrics,  particularly in imbalanced class problems.  6.  Feature Importance:  In decision trees, the concept of feature importance was investigated. More  features may result in lower bias, but they may also result in overfitting.  7.  Handling Missing Data:  The k-NN method was presented as a method for imputing missing values  for both categorical and continuous variables.  8.  Disadvantages of KNN:  It was mentioned during the discussion that the k-NN algorithm's drawbacks  are that it is sensitive to the scale of features and irrelevant features, requires  significant memory to store training data, and is computationally expensive  during the testing phase.  