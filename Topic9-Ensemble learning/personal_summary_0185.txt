Lesson Review Topic 8: Nonlinear models (KNN and DT)  To assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones can be a useful technique for both regression and classification. To label the test data point is the same as the nearest neighbour (NN) is the basic idea. K in KNN can vary.    Continuous valued target function:  Mean value of the k nearest training examples    Discrete class label:  Mode of the class labels of the k nearest training examples  A Voronoi diagram(concept partially related to KNN) is partitioning of a plane into regions based on distance to points in a specific subset of the plane. The same concept as KNN, it is based on closest neighbours. The data is not linearly separable and it results in complex boundaries and decision rules. The training examples form the decision surface. Shepard’s method - All training points may influence a particular instance.  K can be a controlling shape of the decision boundary. Asking the classifier not to care about fairly distant points results in a low bias and high variance. Higher values of K will have smoother decision boundaries which means lower variance but increased bias. Cross-validation can be used to partition data into test and training samples and evaluate model with different ranges of K values.  A map of the possible outcomes of a series of related choices is called a decision tree. They can be used to weigh possible actions against one another based on their costs, benefits and probabilities. It typically starts with a single root node, which branches into possible outcomes.  Decision trees that use a regression model are called regression trees. We can alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used. A top-down, greedy approach is known as recursive binary splitting.  A term introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modeling problems is called Classification and Regression Trees (CART). It is used to predict a qualitative response rather than a quantitative response. In the classification setting, replace the sum of square error by the classification error rate as a criterion for making the binary splits. The fraction of the training instances in that region that do not belong to the most common class is defined as the classification error rate E. If CoD is close to 1, it means almost all of the training points inside a region are voting for a certain class label. On the other hand, when CoD is 0.5 it means we can not trust the votes because there is a high classification error rate (E). One of the problems of classification error is that it’s less sensitive for tree-growing.  The Gini index is the most commonly used measurement of inequality. Gini index is therefore considered a measure of node purity because it faces with a low inequality distribution in the votes.  There are three of the more popular Decision Tree Algorithms:  §  ID3 (Iterative Dichotomiser 3) uses Entropy.   §  C4.5 (Successor of ID3)  slightly more advanced version of ID3 and also uses Entropy.  §  CART (Classification and Regression Tree)  uses Gini impurity.  ID3 is used to generate a decision tree from a dataset. It is an effective machine learning algorithm.  A technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances is called Pruning. There are two ways of pruning a tree: Pre-pruning (decided during the building process when to stop adding nodes (eg. by looking at entropy)) and Post-pruning(Waiting until the full decision tree has been built and then prunes the attributes by subtree Replacement).  A well known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures is called KNN. By the distance metric that is selected, the effectiveness of KNN can be significantly impacted. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data. To improve the performance of the model, feature importance can be used to identify the most relevant features for classification and for feature selection.  