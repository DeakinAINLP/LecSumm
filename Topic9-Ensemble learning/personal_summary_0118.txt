In this topic, the utilization of K-nearest neighbors and decision tree classifiers in supervised learning are the main topics.  KNN algorithm and variants  K-nearest neighbours finds the distance between a query and all the samples in the data, selecting the specified number of examples(K) closest to the query. It makes highly accurate predictions and can be used for both classification and regression problems. It’s ideal for non-linear data since there’s no assumption about underlying data  For continuous target functions, the average or mean of the nearest training examples is used, while for discrete class labels, the mode of the class labels of the nearest training examples is used.  For smaller values of K, the decision boundary becomes more focused on the close regions and neighbors, disregarding distant points. This leads to a low bias and high variance in the classifier. On the other hand, higher values of K result in smoother decision boundaries, considering information from distant training points. This reduces variance but increases bias.  Selecting the optimal value for K is not a straightforward task and there is no definitive answer. However, one approach is to use cross-validation to partition the data into training and test samples and evaluate the model's performance with different values of K. The misclassification error, indicating the number of misclassifications, can be used as a performance metric.  Decision trees  Decision trees(DT) predicts values of responses by learning decision rules derived from features which can be used both for regression and classification model. As a tree-like structure that represents a series of decisions and their possible consequences, it’s easy to validate the algorithm using statistical tests.  Regression trees  Target variables can take continuous values instead of class labels in leaves. When the values of the outcome at the terminal nodes are numeric, the terminal node of the tree can be constant values.  A regression tree is built through a process known as binary recursive partitioning, which is an iterative process that splits the data into partitions or branches, and then continues splitting each partition into smaller groups as the method moves up each branch.  Classification trees  Unlike regression trees, it is used when the output variable is categorical and classification and regression trees(CART) can be used to predict qualitative responses.  In a classification tree, each test instance is assigned to the majority class of the training instances in its corresponding region. This can be seen as a voting process, where the majority determines the class assignment. The classification error rate is used as a criterion for making binary splits in the tree. It represents the fraction of training instances in a region that do not belong to the most common class.  The Gini index and Entropy are commonly used measurements for splitting criteria in practice. The Gini index represents the inequality in a distribution and serves as a measure of node purity. It takes   a small value when the votes are distributed equally among the classes. On the other hand, Entropy is a measure of the impurity or disorder in the distribution. It provides a smoother representation compared to the Gini index.  One example for Gini index in economics can be poverty analysis. The Gini Index is useful in studying poverty levels within a population. By analysing income distributions, policymakers can determine the proportion of the population living below the poverty line and monitor changes in poverty rates over time.  Decision tree algorithms  The ID3 algorithm is used to generate decision trees from a dataset. The basic steps of the ID3 algorithm are as follows: calculate the entropy of each feature in the dataset, split the dataset into subsets based on the feature with the minimum entropy (indicating greater information gain), create a decision tree node with that feature, and recursively repeat the process on the subsets using the remaining features.  A deep tree with many partitions creates smaller regions in the feature space, potentially resulting in low chances of encountering many training points within each sub-region. This can lead to high variance and unreliable estimations.  To find the optimal depth, we can use cross-validation and other evaluation methods. And it’s also important to tune the hypermarameter of the tree to get the best performance.  Pruning  Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances.  Pre-pruning (forward pruning): In pre-pruning, the decision to stop adding nodes is made during the tree-building process.  Post-pruning (backward pruning): Post-pruning occurs after the full decision tree has been built. Post-pruning allows for more comprehensive evaluation of the tree and can capture attribute combinations that have a significant impact on decisions.  Advantages and disadvantages of decision tree model  Advantages  Disadvantages  Easy to understand and interpret.  Overfitting can occur.  Can handle both categorical and numerical data.  Decision trees can be sensitive to small changes in the data.  Can handle missing values and outliers.  They can be biased towards certain outcomes.  Advantages  Disadvantages  Can be used for classification and regression problems.  Large decision trees can be hard to interpret.  Can help identify important features in the data.  They may not work well with certain types of data.  Impact of distance metrics on KNN performance  The choice of distance metric has a significant impact on the effectiveness of KNN. Commonly used distance metrics include Euclidean distance, Manhattan distance, and cosine similarity  Feature importance of using Decision Trees(DT)  Information gain or the Gini index are commonly used criteria to determine the informativeness of features. At each node of the tree, the feature with the highest score is chosen as the splitting criterion  