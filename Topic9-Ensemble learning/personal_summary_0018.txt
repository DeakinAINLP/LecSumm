KNN works by taking a given data point, comparing how far it is from its neighbours and comparing that to the count of each kind of label that those neighbours have and using that data to figure out what the given point should be classified as.  A Voronin diagram is a visualisation that shows partitions based on how far points are from each other, depending on where a point falls it can be easily used to figure out the closest point.  A challenge of KNN is deciding how many neighbours to use as a hyperparameter. One method of working this out is to simply run the algorithm N number of times and going with the value that has the lowest misclassification error.  Decision trees are a map of possible outcomes based on a series of choices, often comparison.  A decision tree that uses a regression model is known as a regression tree.  Algorithmically this is done by recursively splitting at a point that produces the greatest reduction to training error. Each concurrent split will be split twice resulting in an even tree, however this can result in overfitting and often needs to be pruned to improve model accuracy, precision and recall.  A classification tree is used to produce qualitative result.  Gini index is used to measure node purity, this purity can tell us for a given space how likely we are to produce classification error.  Deeper trees result in high variance while shallow trees result in high bias.  