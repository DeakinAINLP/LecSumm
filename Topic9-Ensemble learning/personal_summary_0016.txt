Lesson Overview â€“ Non-linear Models (KNN and DT)  KNN (K-nearest neighbours) is a machine learning algorithm that can be used for both classification and regression.  The idea is to label a data point by locating the k nearest point in the training data set.  As KNN can be used for both discrete data as well as continuous data, to calculate the closest point, average or mean will need to be found for continuous cases, whereas for discrete variables, mode of class labels will need to be found.   Vrononoi diagram is a way to divide space into regions basing on a set of seeds/points.   Finding the optimal K is crucial in ensuring the successful of a model, one  simple  method  which  can  be  used  to  find  k  is  using  cross-validation.    Euclidean,  Manhattan, Chebyshev,  Minkowski,  Mahalanobis  and  cosine  distance  are  all  important  metrics  for  the  KNN classifier and can have a significant impact on the performance.  There is not a universal metrics which is appropriate for all dataset, the choice of distance metric will depend on the characteristics of the dataset presented and the classification problem.  However, there are some distance metrics which may be less affected by added noises.  Decision  Tree  (DT)  is  another  supervised  machine  learning  algorithm  which  can  be  used  for  both continuous and discrete variable.  Decision tree contains nodes representing a possible action, and branches defining possible outcomes, the tree continues down the different nodes and branches until a  conclusion  is  made.    Regression  trees  and  classification  trees  are  two  types  of  decision  trees,  a classification  tree  is  useful  when  the  outcome  is  discrete  and  regression  tree  is  useful  when  the outcome is continuous.  There are three popular decision trees algorithms, and these include ID3, C4.5 and CART.  ID3 is used for classification only, ID3 calculates the entropy of every feature using data set S and splitting the S into subsets where entropy is minimal.  C4.5 builds on top of ID3 and can work with both continuous data and categorical data.  CART is use for both classification and regression classification; CART repeatedly splits the data based on the feature that provides the best classification at each level.  To ensure data is not overfitted, it is important pruning is performed to reduce the size of a decision tree.  Pruning is a process where small tree sections, with little power to classified, are removed. There are two ways where trees can be pruned, pre-pruning and post-pruning.  In pre-pruning, the decision can be made during the building process of when nodes should no longer be added. However, pre- pruning can may stop splitting too early and cut out attributes which may seem to have little impact initially.  Therefore, post-pruning is often done instead, and unlike pre-pruning, post-pruning is only done after the tree is grown and pruning of subtree is then done to remove branches that are not useful.  Decision  trees are  useful  as  it  can handle  non-linear  functions, categorical  variables  and  is easy  to understand.    However,  it  is  sensitive  to  small  changes  in  the  data,  can  overfit  easily,  less  accurate comparing to other regression and classification techniques, and it is only an axis-aligned splits. 