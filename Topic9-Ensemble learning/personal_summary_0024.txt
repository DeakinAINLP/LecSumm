Topic 8 taught us about the principles of KNN (K Nearest Neighbour) algorithm and decision trees.  KNN  In KNN the number of neighbours that influence the classification of a new data point is known as K.  Measuring the performance of a KNN can be construed as a measurement of misclassification (Errors). Presumably this could be Mean Square error calculations (or accuracy).  K is a hyper-parameter for KNN, but there is “pre-emptive” method to estimate a value of K. This means the selection of a K value will be determined in an iterative set of cross-validation models.  Decision Trees  Decision trees are a “follow the bouncing ball” method. In a sequence of decisions that draw down a particular path, noting that the final result may be similar to the final result of other pathways.  Decision Trees may use a regression or classification model, generally classification use a majority and regression a mean of response value for training.  It is computationally intensive to consider every possible partition of the feature space into decision tree regions. An approach to optimise this is a recursive binary splitting, rather than brute-force. This is known as a heuristic solution.  This method has a “threshold” for splitting the feature down “nodes or regions” of tree. Changing the threshold based on error to achieve an optimisation and increased accuracy. To prevent the computational cost, a stopping criteria (number of layers or levels) that the decision tree proceeds with before moving to a new feature is implemented.  A tool to assess the “equality and balance” of the decisions is the GINI Index, which “qualifies” the probability of a particular pathway being chosen by an input datapoint. An alternative is to use Entropy.  Some types of decision trees are ID3 = Iterative Dichotomiser 3 – Uses Entropy, C4.5 (Successor of ID3) – Uses Entropy, and Classification and Regression Tree (CART) – Uses Gini Index (Impurity).  Tuning the depth of the tree is a key hyperparameter, it is desirable that in training data that all regions of the tree have a subset of results but also that there is enough distribution to accurately classify and delineate differences. This is primarily driven by how many decisions there are, i.e the depth of tree.  Small number of regions may result in BIAS, a large number may result in variance.  Technique known as pruning which can be done pre-modelling or post-modelling. Post modelling has the most benefit as results are observable which show which elements of the tree could be pruned effectively. Pre-pruning it is just removing nodes based on the entropy values.  Advantages of Decision Trees    Easy to understand, not inscrutable like other ML algorithms   Able to be justified, which may be of importance for safety critical functions   Capable of modelling non-linear functions    Can handle categorical values, reducing the need for encoding (and isn’t distance related)  Disadvantages    Sensitive to small changes   Easy to overfit if the tree is deep (High variance)   Only axis-aligned splits   Not as competitive in comparison to SVM or neural networks in accuracy   Can have large computational requirements for large trees  