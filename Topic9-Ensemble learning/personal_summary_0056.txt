Topic 8 Summary Nonlinear models (KNN and DT)  K Nearest Neighbour & Decision Trees algorithms Useful in classification & regression  Continuous valued target function:  Mean value of k nearest training examples Discrete class label  Mode of class labels of k nearest training examples  Voronoi Diagram  Partition planes into regions based on distance to points in specific subset  Finding the best k  find minimum value of Misclassification error and select k  split data into training and test cases  run through K = 1, ..., k_max  Decision trees  Regression trees  1. Divide feature space into districts & non-overlapping regions  2. Make prediction for every instance that fall into region R_j, mean or mode  Classification trees  predicts quantitative response.  assign each test instance to majority class (mode) of training instances in the region  Decision tree algs:  ID3 (iterative Dichotomiser 3) - uses Entropy  C4.5 (Successor of ID3) - slightly more advanced version of ID3, uses Entropy  CART (Classification and Regression Tree) - Gini impurity  ID3 Algorithm  1. Calculate entropy for every feature. Split data set into subset. Less entropy means, good choice for  selection of attribute/feature & gain more information  2. Make decision tree node that has feature  3. Recurse on subsets using remaining features  Tree Depth if deep (partitioned the feature space into small regions) = low chances of visiting training points in sub  region. All estimations in regions not good, because high variance  if shallow & regions is big = training data points not have high variance. Allows for Bias. Decision making too naive.  Model complexity & pruning Pruning: reduces size of decision trees  DT adv:  easy to understand, has rules  can model nonlinear functions  can handle categorical variables  DT disadv:  sensitive to small changes  May overfit easily  Only axis-alighned splits  Not as competitive in accuracy compared to other regression & classification techniques, SVM or neural networks.   