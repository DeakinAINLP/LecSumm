This topic we learned about non-linear models   Usually used for classiﬁcation and regression problems The k-nearest neighbours, KNN algorithm is instance based and a nonlinear. It basically works on, predicting based on the similarity of the data points. It assigns labels for the majority class used for classiﬁcation or for regression it computes the average of the target values.   The knn algorithm uses distance metrics to be able to quantify the similarity between separate data points. This is because it assumes that the data points with similar features tend to have a smaller target values. There are many distance metrics that the knn algorithm can use, such as the Manhattan distance, or the Minkowski distance, or the most famous one, the Euclidean distance. Basically, what KNN algorithm does is it assumes the data points within a certain distance of one another must have something in common, and therefore can be used to predict whatever the target value is.   As it is stated in its name, the KNN algorithm requires choosing the optimal number of neighbours denoted by K. this is the most crucial part of the algorithm, as not choosing the correct, or the appropriate amount of neighbours may lead to multiple issues. Once such issue is over ﬁtting and happens when the number of neighbours is small, which makes it sensitive to outliers. The other issue is under ﬁtting and happens when the number of neighbours is too large therefore, the algorithm cannot predict the target value accurately. Therefore, choosing the number of K depends completely on the complexity of the problem.   Another model used for classiﬁcation regression problems are decision trees. The nodes of the decision trees are based on a hierarchal structure, which represent decisions from the features, and class labels, or the target values are represented through what is called the leaf nodes. Trees become trees through the recursive, splitting of the data through the speciﬁed, thresholds and features, which allow for information to be gathered and help to reduce outliers. Through this process, the tree can then be used for predictions.   Another form of trees or decision Trees are what's called regression trees and they are speciﬁcally used for regression problems. Unlike decision, trees regression trees only predict continuous, target values. The good thing about regression trees is that they are able to handled diﬀerent forms of features such as continuous and categorical, which makes it very good at capturing nonlinear relationships.   Now we have what's called classiﬁcation trees, which are based from decision trees, and as the name suggests are used speciﬁcally for classiﬁcation tasks. Where the work is they get the feature value and assign what is called class probabilities or they predict the class labels. The good thing about classiﬁcation trees is that they handle numeric and lake. Regression. Trees handle, categorical features, which helps for interpretation of the data set.   One issue that may occur with decision trees is the overﬁtting of the training data, if the tree has grown too large/too deep. To control this complexity, and to prevent this over ﬁtting, we have a technique that is called pruning. Pruning is the process of simplifying, the tree based on either removing the node or collapsing the node. There are many diﬀerent pruning techniques which include cost complexity and reduce error pruning to try and maintain the optimal complexity and the optimal accuracy.  