K Nearest Neighbours   K nearest neighbours (KNN) is a supervised machine learning algorithm and can be used  for both classification and regression problems   K nearest neighbours (KNN) method that uses ‘proximity’ to classify or predict data  points, assigns a new data point to the class of its k closest neighbours in the training set.   Importance & Influence of K:   Determines how many neighbours are considered for the classification or prediction, thus  controls the shape of the decision boundary   Small k focuses more focus on the close regions and neighbours meaning the decision boundary is more sensitive to noise and outliers, and can lead to low bias and high variance (overfitting).  Large k incorporates more information from distant training points, meaning that the decision  boundary is smoother and more general, and can leads to high bias and low variance (underfitting)   Cross-validation can be used to find the optimal k value in conjunction with misclassification error  can be used as a measurement of performance   Shepard's distance weighted method: is a variant of KNN that assigns ‘weights’ to the neighbours based on their distance to the target point. The closer the neighbour, the higher the weight. This reduces the effect of distant neighbours on the classification or prediction  Decision Trees Overview   Decision trees are a type of supervised learning algorithm that can perform  both classification and regression tasks. Usually, extremely simple models such as majority (classification) or mean (regression) are used   A decision tree is a map of the possible outcomes of a series of related choices, which can be useful in visually outlining the potential outcomes, costs, and consequences of a complex decision, based on their costs, probabilities, and benefits  .   Building a Decision Tree:   Using a simple top-down greedy approach that iteratively divides each region into  two smaller distinct non-overlapping regions based on the best feature and threshold that minimize a certain criterion (such as the sum of squared errors for regression or the entropy for classification) until each subset is homogeneous or pure.   The splitting process starts from a single node that contains all the data and continues  recursively until a stopping criterion is met.   The resulting tree structure consists of a root node, internal nodes, and leaf nodes.  Characteristics of Decision Trees   Tree Depth   Deep trees may cause over-fitting, especially when the feature space is small or the number of training points per region is small. This can increase the variance of the model and reduce its generalization ability.   Shallow trees may cause underfitting, especially when the feature space is large or the regions are  too coarse. This can increase the bias of the model and reduce its accuracy.   Optimizing trees   Use k-fold cross validation to evaluate the performance of different tree depths and select the  optimal one.   Pruning is a technique to reduce the size of trees and avoid over-fitting, and there are two main  types of pruning:  Forward pruning: during the tree building process, stop splitting or adding nodes when the entropy  reduction is insignificant or below a threshold.   Backward pruning: after the tree is built, replace subtrees with leaf nodes based on the minimal  amount of entropy change.  Decision Tree Disadvantages:   Can easily overfit, sensitive to small changes in data, do not consider joint probability of features  Performance not competitive to SVM, KNN or neural networks.  Populare DT Algorithms   Iterative Dichotomies 3 (ID3): Calculate the entropy of every feature., recursively splitting the data into subsets using the feature with the minimum entropy to build the tree. Only handles categorical attributes and does not prune the tree.   C4.5: An extension of ID3 that can handle both categorical and numerical attributes. It also uses entropy as a measure of information gain. It prunes the tree using a confidence factor to avoid overfitting.   Classification and Regression Tree (CART): Suitable for both classification and regression problems. It uses Gini impurity as a measure of information gain to split the nodes and build the tree. It prunes the tree using a cost- complexity parameter to avoid overfitting  