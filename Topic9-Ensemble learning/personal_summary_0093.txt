Summary:  8.2/ 8.3:  Assigning weights to neighbours of data points based on their contribution can be useful in letting closer points contribute more to the mean.  This is done by labelling the same way as nearest neighbour. Weights are assigned to points based on their proximity to the test point, it may be the inverse square of the distances.  The higher the distance to the neighbour the lower its weight.  8.4:  Selecting the right value for K is important as it controls the shape of the decision boundary. A low K value entails constraining the classifier to developing more focused regions. This results in a low bias and high variance.  Higher K will have smoother decision boundaries, lower variance and increased bias. This entails more information from more distant points.  Cross validation can help ascertain hyperparameter values such as K. Using misclassifications and iterating over differing values of K to measure performance and select a favourable value.  The maximum K explored may be on the order of 5% of training points, but this is highly dependent.  8.5:  A decision tree defines possible outcomes of a series of choices. They are used to weigh actions based on their pros, cons and probabilities.  They start with a single root node typically and branch down decisions at each level.  8.6:  Regression trees are decision trees that use a regression model.  The feature space is divided, and for each instance that falls into a region the same prediction is made which is the mean of response values for the training set in the region.  The goal is to find the regions that minimise training error.  8.7/ 8.8:  Classification and Regression trees (CART) are decision tree algorithms that can be used for classification or regression problems.  Algorithms for decision trees exist, one is ID3. It is used to generate a decision tree from a dataset.  ID3 involves calculating the entropy of every feature of the data set, splitting it into subsets using the feature for which entropy is lowest. It then makes decision tree nodes  based on these outcomes, and the process is recursed on subsets with remaining features. A deep tree effectively results in partitioning and may lead to issues.  8.9:  Pruning allows us to deal with model complexity by reducing the decision tree. This is done by removing the sections that have little power to classify. Pre pruning kills nodes during the building process by looking at entropy. Post pruning is done after the tree is built in entirety.  8.10/ 8.11:  Decision trees are easy to understand and are capable of non linear functions, categorical values. However, they are sensitive to small changes in data, can overfit easily and may have poorer performance in comparison to other techniques.  KNN can be heavily impacted by the distance metric used.  