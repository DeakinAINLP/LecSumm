Topic 8: Nonlinear Models (KNN and DT)  K-Nearest Neighbours Classification (KNN):  KNN is a non-linear algorithm used to classify datapoints based on the distance between them. The KNN algorithm is mostly used in supervised machine learning to build classification and regression models. The KNN algorithm focuses on making predictions and classifying datapoints and labelling them based on their characteristics. It is important to ensure and set the hyperparameters before building a model. KNN makes use of various distance metrics such as ‘Minkowski’, ‘Manhattan," "Euclidean, "Cityblock," and ‘Cosine’ . The default method is ‘Minkowski’.  Before implementing the KNN algorithm, it is important to determine the number of n- neighbour parameters, as this helps in determining the shape of the decision boundary. For instance, a smaller k value would lead to a higher number of class labels than required, and a higher k value would lead to a lower number of class labels but potentially lead to a loss of information within the data.  Decision Trees:  The decision tree algorithm is a supervised algorithm that is used for classification and regression tasks. A decision tree can be imagined as a flow chart that maps the possible outcomes, and the decision tree algorithm makes use of probabilities to determine the best outcome possible and makes use of parameters such as Gini and entropy to classify the datapoints. The decision tree algorithm is very easy to apply, but there is a high risk of ‘overfitting’.  To avoid overfitting while building a decision tree, we make use of techniques such as pruning, which is classified into  1.  Pre-pruning. 2.  Post-pruning.      