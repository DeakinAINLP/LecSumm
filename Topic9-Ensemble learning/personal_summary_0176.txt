 Main points summary  -  KNN algorithm. The basic idea of K-NN is to label the test data point depending only on a number, K, of the nearest neighbours, generally by majority. How do you pick the number of neighbours, K? A low value of K will will create a model with low bias and high variance, and the model will therefore be prone to overfitting. On the other hand, a high value of K will create a model with low variance and high bias, which could result in underfitting. Finding the best value for K is not straightforward, but one way to do it is using Cross-validation and evaluating your model with different values of K.  -  Decision trees. A decision tree is a map of the possible outcomes of a series of  related choices. Decision trees that use a regression model are called regression trees. Decision trees that use a classification model are called classification trees, which are similar to regression trees except that they predict a qualitative response rather than a quantitative response. Examples of splitting criteria that can be used for classification trees are classification error, gini index and entropy.  -  Pruning is a technique that reduces the size of decision trees by removing  sections of tree that provide little power to classify instances.  -  The effectiveness of KNN can be significantly impacted by the distance metric  that is selected. Examples of distance metrics are Euclidean distance, Manhattan distance and Cosine Similarity.  