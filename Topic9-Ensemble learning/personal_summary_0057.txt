KNN algorithm and its variants  Assigning weights to the neighbours' contributions to the average can make good use of the closer neighbours' greater average contribution than the farther neighbours. Both regression and classification can benefit from this. The key tenet is that the test data point should be labelled as the same as the nearest neighbour (NN). K in KNN can also change, though. Consider a situation where someone needs to evaluate the test point's closest neighbours (KNN) before making a choice. A test instance can be labelled using the majority label of the K-nearest neighbours.  In continuous circumstances, you must conduct average or mean, and in discrete formats, you must locate the mode of the class labels to determine most decisions based on the nearby training points. In conclusion,    Mean value of the k closest training samples for the continuous valued target  function.    Discrete class label: The k closest training examples' class labels, on average.  Another idea known as a Voronoi Diagram is also somewhat connected to KNN. A Voronoi diagram in mathematics divides a plane into regions according to how close points are to each other in a particular subset of the plane. You can see that it is based on closest neighbours, like how KNN works, as a Voronoi diagram. Clearly, the data cannot be separated in a linear manner, which leads to complex decision-making and boundary structures. The training examples shape the decision surface, so keep that in mind.  In essence, based on the neighbours' distance from the test site, we can assign weights to them. Accordingly, the weight of the neighbour decreases with increasing distance. A specific occurrence may be impacted by all training points. This approach is sometimes referred to as Shepard's method.  Best number of neighbours (K)  How are the neighbours, represented by the variable K, chosen? How crucial is picking the proper K?  K can be viewed as dictating how the decision boundary's shape is determined, as we previously discussed. For low values of K, we restrict the prediction region and make our  classifier concentrate more on the nearby regions and neighbours. We're asking the classifier to disregard far-off spots. Low bias and large variance are the outcomes of this.  Higher K values will result in smoother decision boundaries, which will result in lower variance but higher bias. Higher K values essentially translate to requesting for more information from training points farther away.  Finding hyper-parameters like K is not as simple as it may seem, like most machine learning tasks. It's not always possible to find the optimal solution. But as a quick and convenient technique, you may use cross-validation to divide your data into test and training samples and assess your model using several K value ranges. Finally, depending on our partitioned data, we can determine which K has the greatest performance by examining several values of $K$ and their related misclassification error.  Decision Trees  A decision tree is a diagram representing the potential consequences of several connected options. Using decision trees, it can compare various course of actions based on their probabilities, costs, and benefits. Normally, a decision tree has one root node at the beginning that branches out into potential outcomes.  Regression Trees  Regression trees are a type of decision tree that employ a regression model. Alternately, we might fit a classification model. Classification trees are what these decision trees are known as. Typically, very basic models like majority (classification) or mean (regression) are employed. Finding regions that minimise the training error is the main objective of regression trees.  Find the difference between the predicted and actual output values for each point in the region, according to the formula.  It goes without saying that we want to reduce this issue as much as possible to create regions with lower mistake rates. We need to find a solution to another optimisation issue. How can we identify the answer? Regrettably, considering every conceivable division of the feature space into areas is computationally impractical. How expansive would the possible outcomes become in this scenario? We therefore adopt a top- down, greedy strategy known as recursive binary splitting. We would prefer to employ a heuristic approach rather than a brute-force one.  Utilising the mean (or mode) of the training examples in the area where the test observation falls, we predict the response for a specific test instance.  Classification Trees  Classification and Regression Trees (CART) is a term coined by Leo Breiman to describe decision tree techniques that can be applied to classification or regression predictive modelling issues. Similar to regression trees but used to forecast qualitative responses as opposed to quantitative ones. In a classification tree, each test instance is given the class that makes up the majority (mode) of the training instances in the region to which it  belongs. You can think of this activity as being like a data point voting itself into a region and choosing the majority.  As a criterion for creating the binary divides in the classification setting, the classification error rate is used in place of the sum of square errors. The percentage of training cases in a region that do not fall into the most prevalent class is known as the classification error rate, or E. In essence, Certainty of Distribution (COD) measures how confidently a classifier sits within a region. In an area, practically all of the training points are voting for one class label if Cod is close to 1. Therefore, in this instance, the classifier is confident in their choice. Conversely, a Cod of 0.5 indicates a high categorisation error rate (E) and a lack of confidence in the results of the vote. However, categorisation error has the drawback of being less sensitive to the growth of trees.  Gini index and Entropy - In actuality, users favour the Gini index and entropy. The most popular way to quantify inequality is via the Gini index. It represents the overall variation among the K classes. For any k, a small number ≈ 0,1 is required (why?). because the votes are distributed with little inequity. Gini index is thus regarded as a gauge of node purity.  Decision Tree Algorithms  The 3 most popular decision tree algorithms are:    ID3 (Iterative Dichotomiser 3) uses Entropy. Ross Quinlan, an Australian with a degree from the University of Sydney, created the algorithm in 1975. With the help of a dataset, it creates a decision tree. Although this approach is straightforward, it is a powerful machine learning technique. Here is the fundamental algorithm:  1.  Using the data set, determine each feature's entropy. Use the characteristic  that has the lowest entropy to divide the set into subsets. As a result, attributes or features with lower entropy levels should be chosen because they will provide more information.  2.  Create a node in the decision tree that contains that feature. 3.  Use the remaining features to iterate over subgroups.    C4.5 (Successor of ID3) slightly more advanced version of ID3 and uses Entropy.   CART( Classification and Regression Tree) uses Gini impurity.  Tree Depth  The feature space is essentially divided into smaller sections if a sufficiently deep tree is constructed. The likelihood of visiting many training points in that sub-region should be minimal if the tree is quite deep. This indicates that the significant variance makes all of the estimates in that area unreliable.     On the other hand, when the regions are very large and the tree is shallow, it is possible to conclude that the training data points do not have huge variances, but there may be additional issues, such as bias. In shallow decision trees, your bias will be high. It implies that your decision-making is overly naïve.  To find the optimal depth, you must build a decision tree with the appropriate level of depth. Cross validation and other evaluation techniques that you learnt in previous topics can be used to do that. Keep in mind that you must locate or adjust the correct hyperparameter, which is the depth of the tree.  Model Complexity and Pruning  Pruning is a method for shrinking decision trees by deleting branches that have minimal ability to classify instances. The tree-building procedure we discussed in earlier sections may yield accurate predictions on the training set, but it is probably going to overfit the data, which will result in subpar generalisation performance.    There may be few data points per region in a tree with several regions, resulting in a  huge variance.    On the other hand, having fewer areas may lead to higher prejudice.   A different option is to grow a big tree and then cut it back to get a subtree.  There are often various techniques for pruning trees:    Pre-pruning (forward pruning)  Pre-pruning involves choosing when to stop adding nodes while the network is being built, for example by examining entropy.  Let's imagine that we are dividing nodes by gauging how much entropy is lost when certain attributes are chosen. When there is no discernible entropy reduction, we can cease splitting nodes. We reduce the model's needless complexity by employing this technique. This could cause issues, though. Sometimes characteristics work together to make a judgement even when they don't individually contribute much.    Post-pruning (backward pruning)  The attributes are pruned by subtree Replacement during post-pruning after the complete decision tree has been constructed.  A single area or node can quickly take the role of a subtree in its entirety. We must make sure that this replicates the least error. Choose the subtree that causes the smallest error to be removed, then replace it with a single leaf node. Wait until the entire decision tree is constructed, then look for subtrees to see if you can replace it with a single node or feature while just slightly changing the entropy. If so, prune the tree. In the absence of that, you ought to maintain that subtree since it probably contains important information.     Decision Tree Advantages    As they stand for rules, they are quite simple to understand.   Able to simulate nonlinear functions.   Can handle categorical data, such as whether it is sunny or cloudy outside. Since  weather is a variable, we are unable to compute a Euclidean distance between two vectors.)  Decision Tree Disadvantages    Sensitive to slight modifications in the data. Your rules can be altered if you add a  few extra data points or change a few tiny numbers!    Prone to overfitting. Building extensive decision trees puts you at a great risk of  overfitting and a high variance model, as we have already stated.    Axis-aligned splits only. Traditional decision trees divide the space along each feature  separately. When modelling the tree, we can take joint probabilities or more complex scenarios into account if we need to create a more complex decision tree model. In comparison to other regression and classification approaches like SVM or neural networks, trees might not be as accurate.    Impact of distance metrics on KNN performance  Using distance measures, the well-known machine learning technique KNN categorises new data points based on how close they are to their nearest neighbours in the training set. The distance metric that is chosen can have a big impact on KNN's performance. Distance metrics that are often used in KNN include Euclidean distance, Manhattan distance, and cosine similarity. Depending on the issue and the data, other metrics of distance might be more appropriate. For example, in high-dimensional data, cosine similarity may be preferable to Euclidean distance.  Feature importance of using Decision Trees (DT)  Popular machine learning algorithms like decision trees employ feature selection to decide which features are most crucial for classification. For DT to work, the data is iteratively segmented into subsets based on the most informative features until a stopping threshold is achieved. The most informative characteristic is chosen using a metric like information gain or the Gini index. The feature with the greatest score at each node of the tree is selected as the splitting criterion. The importance of each element can be assessed by considering its contribution to the criterion's overall improvement. The feature becomes more significant the more substantial the contribution. To enhance the performance of the model, feature importance can be utilised to find the most essential features for categorisation and feature selection.  