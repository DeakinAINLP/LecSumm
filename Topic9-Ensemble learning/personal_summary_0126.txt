 Topic 8 focused mainly on two common non linear models K nearest  neighbor (KNN) and Decision tree (DT).  For KNN, we looked at different variants of KNN and implication of k  value in cluster formation and thus model performance.  We also delved into various decision trees including regression tree and  classification tree and their application. Various algorithms for decision  trees were also explored including  ID3 (Iterative Dichotomiser 3) which  uses Entropy, C4.5 (Successor of ID3)  which is also based on entropy but  slightly more advanced  and CART (Classification and Regression Tree)  that uses  Gini impurity.  We also discussed the concept of tree pruning to avoid model complexity  and issue of overfitting and impact of various distances on KNN model,  concluding with advantages and disadvantages of decision trees.  