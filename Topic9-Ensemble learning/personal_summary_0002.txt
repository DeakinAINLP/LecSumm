topic 8 covered KNN alg, variants, and theory KNN is a lazy algorithm used for classification and regression tasks because it is lazy memory usage is higher works by finding the k nearest neighbour of a new data point and then making a decision based on the majority class. variants include weighted KNN closer neighbours have a greater influence on the outcome radius-based KNN all neighbours within a specified radius are considered the gist of KNN is that similar data points will likely have similar outcomes **optimal number of neighbours k in-order for KNN to perform well an optimal value k must be chosen, representing the best number of neighbours if a k value is too small there is a chance of overfitting if a k value is too large there is a chance of underfitting to find value k techniques like cross-validation can be used which involves training and testing the data on different subset of the data and k value with the lowest error rate is chosen. decision trees, regression trees, and classification trees they work by recursively splitting data into subsets based on a data's features this results in a tree data structure classification trees are decision trees that are for classification tasks whereas regression trees are used for regression tasks each leaf node represents a label or predicted value common decision trees are ID3, C4.5, and CART. complexity and pruning in the context of decision trees, model complexity is referring to the depth and number of nodes in the tree complex trees can lead to overfitting whereas simple trees can result in underfitting pruning is the technique used to reduce the complexity of a decision tree by removing the less important node and branches this improves the models generalisation and reduces overfitting advantages vs disadvantages advantages can handle both numerical and categorical data very interpretable handles outliers well disadvantages tendency to overfit sensitive to small data changes 