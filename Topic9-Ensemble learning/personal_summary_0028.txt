KNN algorithm and its variants and KNNTheory  KNN is a useful technique used for classification and regression. It works by assigning weights to the span of data point neighbours so that the closer neighbours contribute to the average of that area more than the distant neighbours. The nearest neighbours, K is a determined amount in which one of its test points is used to make a decision. The KNN works at heart by labelling a new data point and looking at its closest neighbor(s), using either their average value or their most common class. A divide plane can be used to break the points into clusters based on how close they are to other points, which is similar to KNN. This is called Voronoi diagrams.  Can we use other distance metrics in KNN? Please share your answers in the Student Discussion.  As said in the module page, the Euclidean distance which is a common measure used to determine the distance between two data points. Other metrics which can be used are ones such as the Manhattan distance is also a valuable metric which evaluates the sum of the absolute differences between coordinates of two data points.  8.4 Best number of neighbours (K)  The classifier focuses its attention on the nearby regions and neighbors in which K is smaller, making it more sensitive to noise and less generalizable. When K is large, the classifier is smoother and more stable, but it may ignore some important details. The best value of K is not easy to find, and cross- validation can help to compare different values of K. The best value of K minimizes the number of wrong labels. The best value of K also depends on how much the classifier wants to explore new regions and how many training data points are available. 