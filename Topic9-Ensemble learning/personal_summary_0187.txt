In this topic I learned about K nearest neighbour (KNN) algorithm, Decision tree (DT). Let’s discussed these and related topics in brieﬂy.  KNN algorithm and its variants: It is supervised machine learning algorithm used for both classiﬁcation and regression tasks. It is a non-parametric method that makes the predictions based on similarity of a new data points to its K nearest neighbours in the training dataset. There is another concept that is ‘Voronoi Diagram’ partially Related to KNN.  KNN algorithm has its variants as well and these are weighted KNN, Radius-base KNN, KNN with feature weighting, KNN with distance weighting, and KNN with kernel functions.  Best number of neighbours (K): Selecting the best number of neighbours (K) in K-nearest neighbours (KNN) algorithm is an important task as it signiﬁcantly impacts the algorithm performance. To ﬁnd the K is not straigh(cid:414)orward and can ﬁnd with cross-validation.  Decision Trees (DT): Decision Tree is a supervised machine learning algorithm and used for both classiﬁcation and regression tasks. Its is a graphical representation of a series of decision or rules that can be applied to dataset to predict the value of target variable. A decision tree typically starts with single root node, which branches into possible outcomes.  Regression Tree: A regression tree is type of decision tree used for regression tasks. It is supervised machine learning algorithm that predicts continuous numeric values as the target variable.  Answers to Python:  Q1: As we can see the result from KNN, we got high accuracy on the test set with high precision, recall, and F1-score for each class. The result shows that the KNN algorithm is very eﬀective at classifying the digit classes in the ‘digits’ dataset.  On the other hand, SVM may be more sensitive to the choice of hyperparameters and kernel function. So, we can say that the KNN algorithm is more ﬂexible and robust for the classiﬁcation tasks.  Q2: when we split the data by 50 50%, the model has less data to learn from and this result in lower accuracy due to overﬁtting and underﬁtting. The model may not be able to capture the full complexity of the dataset and resulting poor generalisation to unseen data.  When we split the data 70-30%, we can see the accuracy bit higher than 50-50% splitting and this is because model has more data to learn and can capture the complexity of dataset and can make better predictions in unseen data.  