During this lesson, KNN and Decision trees, How to create a decision tree, and advantages and disadvantages are discussed.  KNN  Very important to Machine Learning because of its explainability. In KNN the nearest neighbor is used to do the prediction. KNN works for both classification and regression models. This can be used for recommendations.  The prediction will be the majority of the nearest neighbours.  In KNN the only hyperparameter we need to give is K.  K- How many nearest neighbours should be selected to build the model?  The problem is how can we find the optimal K value.  When K is greater than 1 then there are many neighbors. To predict we need to consider all those neighbors. When considering neighbours  In the Classification problem - we consider the large proportion of the class label as the prediction  In the Regression problem - we consider the average or mean of the neighbours. We need to  consider the distance to the neighbour as well.  When the data point is closer to the prediction, it gives a higher weight to that data point.  K controls the shape of the decision boundary.  A small value of K - low bias, High variance  A higher value of K - lower variance, higher Bias  After the circled point, when K increased the misclassification error is raised.  the y-axis is different for regression- MSE  If the dataset is big, it’s time-consuming because we need to find the distance between all  the training instances.  KNN is a good starting point to get the lower bound of the ML model performance.  Decision Tree  Make decisions based on a tree structure.  In the above example, we have 3 features. Each internal node is a feature.  Leave nodes are predictions.  Decision tree is the most interpretable ML model.  Deep learning models are based on this.  It is not compatible with batch supervised Learning models like Support Machines.  High Interpretability but accuracy needs to be higher.  Partition of Feature Space  After partitioning the feature space, we can fit a simple model in each region.  - When we fit a regression model the decision tree is called a regression tree - When we fit a classification model the decision tree is called a classification tree  Formulation of Regression Trees  The goal is to find the regions that minimize the training error.  We try to minimize the training error for each prediction.  How can we minimize the training error?  The solution is a Greedy Search method. We use the Greedy search method when the data set is large.  It searches for the most important feature.  Do not try the joint space of all the features.  Must define constraints for depth and misclassification errors.  Computing classification errors all the time is not practical. When the tree is becoming deeper and deeper the tree is less sensitive. Therefore people use the Gini index and entropy which are more sensitive.  Gini Index  Entropy  We prefer a smooth matrix. In real-world scenarios, we don’t have a lot of training examples. If the size of the training set is small we prefer a smoother matrix. To get rid of overfitting and underfitting problems.  Decision Tree Algorithms  ● ID3 (Iterative Dichotomiser 3)  -  uses Entropy  ● C4.5 (Successor of ID3)  -  slightly more advanced version of ID3 and also uses Entropy  ● CART (Classification and Regression Tree)  -  uses Gini impurity  The tree is very deep  - - -  Small regions A small number of training points in sub-regions. Increases variance and estimation becomes poor  The tree is shallow  - -  Large regions Small variance but a large bias  We need to find the sweet point  - Acceptable classification accuracy and a small tree.  Model Complexity and Pruning  Model complexity- Depth of the tree.  Pruning can be pre-pruning or post-pruning  Pre-pruning  When building the tree pruning simultaneously  Post-pruning  Finish building the tree and do pruning to remove some branches  Most of the time we use post-pruning.  The advantages of Decision Trees:  ● easy to understand ● can model nonlinear functions. ● can handle categorical variable  The Disadvantages of Decision Tree  ● Sensitive to small changes in the data. ● May overfit easily ● Only axis-aligned splits. ●  Performance is not competitive   