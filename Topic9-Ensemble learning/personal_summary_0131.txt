 The K-Nearest Neighbours (“KNN”) algorithm can be used for both classification and regression problems. It works by labelling a new data point based on the labels of its nearest neighbours. Weights can be assigned to neighbours, allowing closer data points having more influence. This approach accounts for the possibility of multiple classes being nearby but trusts the majority class for the classification decision.  The selection of k in the KNN algorithm is crucial. A small k value may result in low bias and high variance as it focuses on closer regions, whereas a high k value will increase bias as it incorporates more distant points. Cross-validation can be used to help find the optimal k value, involving partitioning the data into test and training samples and assessing the model performance with different k values.  Regression trees are a type of decision tree that fit a regression model to the data. The model partitions the feature space into distinct and non-overlapping regions. For every instance which falls into these regions, the model will predict the mean or mode of the response values for the training observations. The aim is to minimise the training error which can be achieved through recursive binary splitting. This method selects a feature and a threshold that best reduces the training error and continue to split the data until a stopping criterion is met.  Classification trees are generally used to predict qualitative attributes rather than quantitative attributes. Each instance is assigned to the majority class of the training instances in the region to which it belongs. As the classification error rate is less sensitive for tree-growing, the Gini index and Entropy are considered a better metric. These metrics are measures of node purity, representing total variances across classes.  Finding the right tree depth is an important component, as deep trees can lead to high variance and reduced accuracy in the estimations, whereas shallow trees can result in higher bias. Techniques, such as cross validation can be used to finding the right tree depth.  Pruning is a technique which can also help reduce the size of a decision tree by removing sections which provide minimal influence on the classification. This can occur using the following approaches:    Pre-pruning – This occurs during the building process and nodes may no longer be added by reviewing key metrics such as entropy (i.e. when entropy reduction is no longer significant, no more nodes would be added).    Post-pruning – Occurs once the tree has been fully developed, where an entire sub-  tree may be replaced with a single region or node.  