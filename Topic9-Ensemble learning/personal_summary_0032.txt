 This topic covered the nonlinear machine learning models: K-nearest neighbours and decision trees. The following points were covered:  KNN:    KNN can be used for both classification and regression problems. The idea is to label data points based on a specified number of its nearest neighbours (based on their Euclidean distance from the point). In classification problems, a point is labelled as the mode of the K points nearest to it (i.e., the majority class of the neighbours). In regression problems, a point is labelled as the mean value of its K nearest neighbours.  o  KNN is useful when data is non-linearly separable and there are complex  boundaries.  Weights can be assigned to points based on distance from the subject point. In this way, points that are further away contribute less to the determination of the point’s value or label.  A low choice for K results in low bias and high variance, as the classifier does not take distant points into consideration. A high value of K has a smoother decision boundary which results in lower variance but increased bias. Cross-validation can be used to attempt to pick a good value for K.  Decision Trees:   A DT is a map of the possible outcomes of a series of related choices. A DT starts with a root node, and then branches out with possible outcomes from the decision at that, and each subsequent, node. Decision Trees that use a regression model are called regression trees. After partitioning the feature space, a simply regression model if fit in each sub-region. Alternatively, classification models can also be fit, known as classification trees. It is computationally infeasible to consider every possible partition, and so a heuristic solution known as recursive binary splitting is used. At each step, a feature and threshold is chosen for the split that leads to the highest possible reduction in training error. In classification, either the Gini impuritys or entropy are used. Both measure the purity that a certain node achieves when classifying instances. For example, a node that correctly classifies every instance into two separate classes is completely pure, and therefore has a low entropy/Gini impurity. As the number of misclassifications at the node increase, the purity of the node decreases and the measures for entropy/Gini impurity increase. The idea is to minimise these measures at each node.    The difference in the entropy/Gini index measures are how they are  calculated.  There are a variety of algorithms for Decision Trees, such as: ID3 – Iterative Dichotomiser (entropy)  ▪ C4.5  - Successor of ID3 (entropy) CART – Classification and Regression Tree (Gini Impurity) The ID3 algorithm works by first calculating the entropy of every feature in a dataset, then splitting the subset using the feature with the lowest entropy (most amount of information). This become the first node. The process is repeated for all remaining features  to build out the tree. Trees that are very deep have high variance, whereas trees that are very shallow have high bias. The depth of the tree is a hyperparameter that needs to be tuned for the data.  Pruning is a process whereby sections of a tree that provide little  information are removed from the tree. In this way, large trees can be pruned to reduce the variance (overfitting).  There are a few ways to prune a tree, two of which are pre- and post- pruning. Pre-pruning involves deciding during the building process to stop adding nodes (by examining the entropy and determining when it is not significant). Post-pruning means to build the tree out first, and then prune attributes by adding subtree replacement (i.e., decide which sections can be removed and replaced with a single node). This addresses some of the problems that can arise with pre-pruning. For example, some attributes with low significance may be pre-pruned, but the combined information from pre-pruned nodes may be significant. Decision trees can be an advantage model to go with as they are easy to understand, are capable of modelling nonlinear functions and can handle categorical variables directly without the need for encoding. Numerical variables also do not need to scaled before being fed to a decision tree, as the decision tree does not make decisions based on distances, and so features on different scales do not cause the same problems that they do in linear models.  On the other hand, there are disadvantages to decision trees and these include that they are sensitive to small changes in data (i.e., adding additional observations may require the tree to be rebuilt), they may overfit easily and they may also not be as competitive in terms of accuracy as other models (such as SVMs)  How to implement KNN and Decision Trees in Python using the scikit-learn  library  