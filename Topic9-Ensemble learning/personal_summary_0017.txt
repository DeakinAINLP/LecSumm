The contents found within this module introduces upon the concept of both K Nearest Neighbours (KNN) and Decision trees (DT). First the module explains what the KNN algorithm is, its unique technique of identifying its nearest neighbours from the source datapoint, and how it allows the nearer neighbours to contribute more to the average compared to the distant neighbours. The module compares this algorithm to the Voronoi Diagram and provides examples of how its nearest neighbours are found. The module then provides a further explanation of Decision trees, what they are and how they work, while providing visualisations on how decision trees look like. Furthermore, the module provides and explains the many variants of decision trees, such as Regression trees and Classification trees. The module also explains on the other decision tree algorithms that utilises the concepts of Gini and Entropy, such as ID3, C4.5 and CART. Finally, the module explains the cleaning process of decision trees through the technique of pruning. This can be applied during the building process (Pre-pruning) or after the building process (post-pruning).  Through the completion of this module, I am able to understand upon how K nearest Neighbours work through finding its nearest neighbours, and on the concepts of Decision trees and the many variants within it. Through the understanding of this module, I am able to complete the pass tasks provided within it through implementing a KNN algorithm within the dataset, evaluating its performance, further implementing a Decision Tree algorithm and finally comparing its performance to datasets trained through 50-50% and 70-30%.  Overall, after completing this module, I am able to gain an understanding of K Nearest Neighbours and Decision Trees. Through this, I can implement these concepts in a practical manner through python and then analyse and compare these two algorithms.  