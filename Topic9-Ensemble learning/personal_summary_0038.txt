 KNN algorithm labels test data points based on the nearest neighbourâ€™s class, and can assign weights to neighbours so closer ones contribute more to the average.  KNN uses Euclidean distance to find the majority of decisions based on close training points, using mean value for continuous cases and mode for discrete class labels. Distance-weighted nearest neighbour algorithm assigns weights based on distance, with higher distance resulting in lower weight.  Choosing the right number of neighbours (K) is important for the decision boundary in classification; small K values result in low bias and high variance, while larger K values lead to lower variance and increased bias. Cross-validation can help find the best K value by evaluating misclassification errors for different K values.  Decision trees are maps of possible outcomes from a series of choices, used to weigh actions based on costs, benefits, and probabilities. They consist of decision nodes and leaves, representing decisions and final outcomes.  Regression trees divide the feature space into distinct regions and make predictions based on the mean of response values for training observations in each region. Recursive binary splitting, a top-down greedy approach, is used to find the best feature and threshold for splitting the data to minimize error.  Classification and Regression Trees (CART) are decision tree algorithms used for classification or regression predictive modelling problems. Gini index and Entropy are measures of node purity and are preferred over classification error rate for tree-growing.  Popular decision tree algorithms include ID3, C4.5, and CART. ID3 uses entropy and is a simple yet effective machine learning algorithm. Finding the right tree depth is important to balance high variance and high bias.  Decision trees are easy to understand, model nonlinear functions, and handle categorical variables. However, they are sensitive to small data changes, may overfit, have axis-aligned splits, and may not be as accurate as other techniques like SVM or neural networks.  KNN performance is affected by distance metrics like Euclidean distance, Manhattan distance, and cosine similarity. Decision Trees use feature selection to determine important classification features, improving model performance.  