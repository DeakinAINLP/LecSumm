Evidence of Learning  Module 8: Nonlinear models (KNN and DT)  Module Learning Outcomes –  I certify that I have successfully learned and understood the following topics.  1.  KNN algorithm and its variants. 2.  Theory of KNN 3.  Best number of neighbours 4.  Decision trees 5.  Regression trees 6.  Classification trees 7.  Decision tree algorithm 8.  Model complexity and pruning 9.  Advantages and disadvantages of decision trees 10. Advance topics  Summary and Reflection –  The  pages  below  contain  the  handwritten  summary  referring  to  the  given  learning resources. It includes all the important points of the topic 8 module.  In this module, we first learnt about the KNN algorithm and its variants. Then we dived deep into  the  theory  of  KNN,  in  that  part  of  the  module  we  were  introduced  to  the  Voronoi diagram. Then we learnt about K, which is the best number of neighbours.  Then we were introduced  to  the  decision  tree,  the  regression  tree  and  the  classification  tree.  Then  we learnt the decision tree algorithm in detail.   In the next part of the module, we got to know about  the  model  complexity  and  pruning  with  its  different  occasions  of  pruning.  Then  we learnt about the advantages and disadvantages of decision trees. Then we were introduced to two more advanced topics: The impact of distance metrics on KNN performance, and the feature importance of using decision trees.  When we started topic 8, I was not familiar with any of these concepts at all. I knew a little bit about KNN but not in this depth. So, it was very interesting and exciting to learn about a new side of machine learning which affects every and all aspects of it. And the practical part done  with  Python  was  a  little  bit  confusing  at  first  but  was  able  to  understand  all  the necessary commands and ways of implementation by referring to both given resources and online resources.   Activity 8.3  Can we use other distance metrics in KNN?  Yes, in K-nearest neighbors (KNN) algorithm, you can use other distance metrics besides the commonly  used  Euclidean distance.  The  choice of distance metric  in  KNN  determines how similarity or dissimilarity is measured between data points.  Here are some commonly used alternative distance metrics in KNN:  1.  Manhattan  distance  (also  known  as  city  block  distance  or  L1  distance):  It  measures  the sum  of  absolute  differences  between  the  coordinates  of  two  points.  It  is  calculated  as  the sum of the absolute differences between the corresponding coordinates of two points.  2.  Minkowski  distance:  It  is  a  generalization  of  both  Euclidean  and  Manhattan  distances. When  p  =  1,  it  is  equivalent  to  Manhattan  distance,  and  when  p  =  2,  it  is  equivalent  to Euclidean distance.  3.  Cosine  similarity:  Instead  of  measuring  distance,  cosine  similarity  measures  the  angle between two vectors. It is commonly used when dealing with text data or high-dimensional data.  It  is  calculated  as  the  dot  product  of  two  vectors  divided  by  the  product  of  their magnitudes.  4. Hamming distance: It is used for comparing binary vectors of equal length. It counts the number of positions at which the corresponding elements of two vectors are different.  These  are  just  a  few  examples,  and  there  are  several  other  distance  metrics  available depending  on  the  nature  of  your  data  and  the  problem  you  are  trying  to  solve.  It  is important  to  choose  an  appropriate  distance  metric  based  on  the  characteristics  of  your data and the specific requirements of your problem.  Activity 8.5   These  trees  are  built  on  a  series  of  choices  or  decisions  in  a  particular  order.  What’s  an example of a decision tree from your everyday life?  An example of a decision tree from everyday life could be the process of deciding what to wear for the day based on the weather. Here's a simplified representation of a decision tree for choosing an outfit:  1. Is it raining?  - Yes: Grab an umbrella and proceed to the next question.  - No: Move to the next question.  2. Is it hot outside?  - Yes: Wear a light and breathable outfit.  - No: Move to the next question.  3. Is it cold outside?  - Yes: Wear warm clothing, such as a sweater or jacket.  - No: Move to the next question.  4. Is it a special occasion?  - Yes: Dress up in formal attire.  - No: Wear casual clothes.  This  decision  tree  helps  guide  the  decision-making  process  by  considering  various  factors and conditions. By answering each question, you arrive at a specific outcome or choice for what  to  wear  based  on  the  given  situation.  Decision  trees  in  everyday  life  can  be  more complex and involve multiple branches and criteria, but this example demonstrates the basic concept.  Activity 8.7  Can you think of some real-world usages of the Gini Index in economics?  Certainly!  The  Gini  index  is  a  commonly  used  measure  of  income  or  wealth  inequality  in economics. Here are some real-world usages of the Gini index:  1. Income Inequality Analysis: The Gini index is often used to measure and compare income inequality  among  different  countries  or  regions.  Economists  and  policymakers  utilize  the index  to  understand  the  distribution  of  income  within  a  society  and  identify  disparities.  It provides  a  quantitative  measure  to  assess  the  effectiveness  of  policies  aimed  at  reducing income inequality.  2.  Poverty  Analysis:  The  Gini  index  is  useful  for  evaluating  the  extent  of  poverty  within  a population.  By  measuring  income  inequality,  it  helps  identify  the  proportion  of  the  population  that  is  most  affected  by  poverty.  The  index  enables  policymakers  to  monitor poverty levels over time and evaluate the impact of poverty-alleviation programs.  3.  Social  Welfare  Policies:  When  designing  and evaluating  social welfare  programs,  such  as income redistribution policies or targeted assistance programs, the Gini index plays a crucial role. Policymakers utilize the index to assess the effectiveness of these programs in reducing inequality and improving the overall welfare of the population.  4.  Economic  Development:  The  Gini  index  is  often  used  as  an  indicator  of  economic development. Countries with lower levels of income inequality tend to have more stable and sustainable  economic  growth.  The  index  helps  economists  and  policymakers  identify  areas where  interventions  are  needed  to  promote  inclusive  growth  and  reduce  inequality,  thus fostering long-term economic development.  5.  International  Comparisons:  The  Gini  index  allows  for  comparisons  of  income  or  wealth inequality  across  different  countries.  By  examining  variations  in  the  index,  economists  can gain insights into the factors contributing to inequality in various socio-economic contexts. This information can be used to learn from successful policies implemented in countries with lower inequality levels.  6. Policy Impact Assessment: When introducing economic policies or reforms, the Gini index can  be  employed  to  assess  their  impact  on  income  or  wealth  distribution.  By  measuring changes in the index before and after policy implementation, policymakers can evaluate the intended and unintended consequences of their actions and make adjustments accordingly.  Overall, the Gini index serves as a valuable tool in economic analysis, policy formulation, and monitoring progress in addressing income or wealth inequality within societies.  Activity 8.9  Can you identify any relationships between the need for pruning and over-fitting?  Yes, there is a relationship between the need for pruning and overfitting in machine learning models.  Pruning  is  a  technique  used  to  reduce  the  complexity  of  a  model  by  removing unnecessary  or  redundant  parts,  such  as  individual  nodes  or entire branches  of  a decision tree  or  connections  in  a  neural  network.  Overfitting,  on  the  other  hand,  occurs  when  a model becomes too complex and starts to fit the training data too closely, resulting in poor generalization to unseen data.  Here are a few relationships between pruning and overfitting:  1. Overfitting Reduction: Pruning can help mitigate overfitting by reducing the complexity of a model. When a model is excessively complex, it may memorize the training data instead of learning the underlying patterns and relationships. By pruning unnecessary components, the model's capacity is reduced, leading to a simpler and more generalized representation that is less prone to overfitting.  2. Regularization: Pruning can be considered a form of regularization, which is a technique used  to  prevent  overfitting.  Regularization  methods  aim  to  introduce  constraints  on  the  model's  complexity,  and  pruning  achieves  this  by  removing  unnecessary  parameters  or structures.  By  applying  pruning  as  a  regularization  technique,  the  model's  capacity  is controlled, making it less likely to overfit the training data.  3.  Feature  Selection:  In  some  cases,  pruning  can  be  seen  as  a  form  of  feature  selection. Overfitting can occur when a model learns from noisy or irrelevant features in the training data. Pruning can help identify and remove such features, focusing the model's attention on the  most  informative  and  relevant  features.  This  process  can  improve  the  model's generalization performance by reducing the influence of noisy or irrelevant information.  4.  Post-Pruning  Evaluation:  Pruning  can  also  serve  as  a  means  to  detect  and  alleviate overfitting after training a model. By evaluating the model's performance on a validation set during  or  after  pruning,  it  is  possible  to  observe  changes  in  the  model's  generalization capability. If pruning results in improved performance on the validation set, it indicates that overfitting has been reduced.  In summary, pruning helps address overfitting by reducing the complexity of a model, acting as  a  regularization  technique,  and  aiding  in  feature  selection.  By  removing  unnecessary components and controlling the model's capacity, pruning can improve a model's ability to generalize well to unseen data and mitigate the negative effects of overfitting.  Activity 8.10  Can you think of any more advantages or disadvantages of decision trees?  Certainly!  Decision  trees  offer  several  advantages  and  disadvantages.  Here  are  some additional points to consider:  Advantages of Decision Trees:  Interpretability:  Decision  trees  provide  a  transparent  and  easily  1. interpretable representation  of  the  decision-making  process.  The  tree  structure,  with  its  branches  and nodes, allows for clear visualization and understanding of how decisions are made based on the input features.  2.  Feature  Importance:  Decision  trees  can  measure  the  importance  of  features  in  the decision-making process. By evaluating the splits in the tree, one can identify which features have  the  most  significant  impact  on  the  outcome.  This  information  can  be  valuable  for feature selection or gaining insights into the underlying data.  3. Handling Missing Values and Outliers: Decision trees can handle missing values in the data without  requiring  imputation.  They  simply  make  decisions  based  on  available  features. Additionally, decision trees are robust to outliers since they partition the data into segments based on splits rather than relying on summary statistics.  4.  Nonlinear  Relationships:  Decision  trees  can  capture  nonlinear  relationships  between features  and  the  target  variable.  Through  a  series  of  splits,  the  tree  can  identify  complex interactions  and  dependencies  in  the  data,  enabling  more  flexible  modeling  compared  to linear models.  Disadvantages of Decision Trees:  1.  Overfitting:  Decision  trees  have  a  tendency  to  overfit  the  training  data  if  not  properly controlled. They can create overly complex trees that perfectly fit the training data but fail to generalize  well  to  new,  unseen  data.  Techniques  like  pruning  and  regularization  are  often used to address this issue.  2. Lack of Robustness: Decision trees are sensitive to small variations in the training data. A slight  change  in  the  data  can  result  in  a  completely  different  tree  structure.  This  lack  of robustness makes decision trees more prone to instability compared to other algorithms.  3.  Discretization  Bias:  Decision  trees  treat  continuous  features  as  discrete  by  selecting specific threshold values for splitting. This discretization can introduce bias and result in the loss of information present in continuous variables.  4.  Class  Imbalance:  Decision  trees  can  struggle  to  handle  datasets  with  imbalanced  class distributions. In such cases, the tree may become biased toward the majority class, leading to  poor  predictive  performance  for  the  minority  class.  Techniques  like  resampling  or adjusting class weights can be applied to alleviate this issue.  5.  Inconsistent  Variable  Importance:  The  importance  of  features  in  decision  trees  can  be inconsistent  across  different  runs  or  variations  of  the  tree.  Depending  on  the  particular structure  of  the  tree,  the  perceived  importance  of  features  may  change,  making  it  less reliable for feature selection or ranking.  It's  important  to  weigh  these  advantages  and  disadvantages  when  considering  the  use  of decision trees in a particular context. They are versatile and widely used, but proper tuning and precautions are necessary to optimize their performance.  