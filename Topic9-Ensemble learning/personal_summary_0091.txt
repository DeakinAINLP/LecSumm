Topic 8 Summary Notes â€“ KNN and DT    Brief introduction as to what the KNN algorithm is and its variants in 8.2. From my  understanding it is an algorithm that labels the nearest neighbors with a certain weight so that the nearest ones contribute mostly to the average rather than the distant ones.   Should note that in order to find the majority of decisions based on neighboring training    points we must find the mean value of k nearest training examples for continuous cases, and we must find the mode of the discrete class labels. In order to find the best hyper-parameter K for your issue it can be hard but a simple way to do this can be to use cross-validation and then evaluate the model and try different K values to see which one works the best.    8.5 introduces decision trees which are shown to be a map of possible outcomes based on related choices. It is a good way of seeing how different choices can lead to different outcomes of different costs, values, and probabilities.    We can fit a regression model with decision trees to make them regression trees or we could  even use a classification model to make a classification tree. These are good to draw conclusions about a dataset.    The Gini index and Entropy can be used for measuring both inequality and node purity respectively. Gini index basically measures the probability for a random instance to be misclassified when chosen at random.    Decision tree algorithms include ID3(Iterative Dichotomiser 3), C4.5(Successor of ID3) and  CART(Classification and Regression tree). ID3 uses Entropy and is used to generate a decision tree from a dataset.    Pruning is a method we can use to reduce the size of decision trees if they are overbearing  and lack power to classify instances in certain sections. Doing this could avoid poor performance such as high variance, high bias, etc.    Advantages of decision trees include: easy to comprehend, being able to model nonlinear  functions, categorical variables can be handled.    Disadvantages include: small changes in data can completely change the ruleset, overfitting and high variance can happen easily, can lack accuracy compared to other methods such as SVM.    