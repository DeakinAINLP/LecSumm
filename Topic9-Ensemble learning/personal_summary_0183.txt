 Key Learning:   The  K-Nearest  Neighbours  (KNN)  algorithm,  utilized  for  both  classification  and  regression, assigns  labels  to  test  data  points  based  on  the  class  labels  of  their  nearest  neighbours. Variants of this algorithm can consider a variable number of neighbours ('k') and implement weighting schemes, where closer neighbours contribute more significantly to the decision, resulting  in  the  labelling  of  a  test  instance  to  match  the  majority  label  of  its  k-nearest neighbours.    The K-Nearest Neighbours (KNN) algorithm classifies data points based on the labels of their 'k'  nearest  neighbours,  using  Euclidean  distance  for  proximity;  continuous  values  use  the mean,  while  discrete  labels  use  the  mode.  Variations  of  KNN  can  implement  weight assignments based on distance from the test point, known as a distance-weighted nearest neighbour algorithm or Shepard's method, where closer neighbours have greater influence, thereby mitigating the influence of irrelevant distant neighbours.    The selection of 'k' in KNN, which represents the number of neighbours, is crucial as it shapes the decision boundary; smaller 'k' values create more focus on closer regions, leading to low bias and high variance, while larger 'k' values result in smoother boundaries, implying lower variance  and  higher  bias.  Determining  the  optimal  'k'  is  typically  achieved  through  cross- validation,  partitioning  the  data  into  test  and  training  sets,  and  evaluating  the  model's performance (often using misclassification error as a measure) across a range of 'k' values to find the one that minimizes the error.    A decision tree is a graphical representation used for decision-making, which maps possible outcomes  of  various  related  choices,  allowing  for  weighing  actions  based  on  their  costs, benefits, and probabilities, often applied in areas like predicting commute time.    Regression trees are a type of decision tree that partition the feature space into distinct, non- overlapping regions, predicting the response for instances based on the mean of the training observations in the region they fall into. The tree is constructed using a top-down, greedy approach known as recursive binary splitting, which iteratively selects a feature and threshold to minimize the training error, and continues until a stopping criterion, such as a maximum number of instances per region, is met.    Classification trees, part of Classification and Regression Trees (CART), predict a qualitative response  by  assigning  each  test  instance  to  the  majority  class  of  training  instances  in  the region where it falls. To optimize the binary splits, metrics like the Gini index or entropy are used, which are measures of node purity and inequality among class labels in a region, being more sensitive than the classification error rate for growing the tree.    Decision  tree  algorithms  like  ID3,  C4.5,  and  CART  build  decision  trees  from  datasets  using measures such as entropy or Gini impurity to create subsets of the data based on features, with tree depth playing a crucial role in controlling bias and variance. The optimal tree depth, acting as a hyperparameter, is typically determined through methods like cross-validation, aiming to find a balance between the depth of the tree and the variance and bias in the model.   Entropy  is  a  measure  of  impurity  in  a  dataset;  it  quantifies  the  amount  of  information  or uncertainty,  with  higher  entropy  indicating  greater  information  content.  In  decision  tree algorithms,  the  concept  of  information  gain,  which  measures  the  importance  of  a  given  attribute  in  discriminating  between  classes,  is  used  to  decide  the  ordering  of  attributes  in nodes.  The  information  gain  is  calculated  as  the  difference  between  the  parent  node's entropy and the weighted average entropy of the child nodes. The attribute with the highest information gain is chosen to be the root of the tree, with child nodes constructed for each value  of  the  attribute.  This  process  is  recursively  repeated,  splitting  the  dataset  based  on different attributes until a stopping criterion is met, resulting in a decision tree that maximises information gain at each split.    Pruning is a technique in decision tree algorithms that mitigates overfitting by reducing the tree's  complexity  through  the  removal  of  sections  that  contribute  minimally  to  instance classification. This can be achieved via pre-pruning, which halts node creation during the tree building  process  when  entropy  reduction  is  insignificant,  or  post-pruning,  which  trims subtrees from a fully built tree when their removal results in a small increase in entropy.   Decision trees are advantageous due to their simplicity, interpretability, and ability to model nonlinear  functions  and  handle  categorical  variables.  However,  they  are  sensitive  to  small changes in data, prone to overfitting, limited to axis-aligned splits, and may not offer the same level of accuracy as other techniques such as SVMs or neural networks.    The performance of the K-Nearest Neighbours (KNN) algorithm can greatly vary depending on  the  chosen  distance  metric,  such  as  Euclidean  distance,  Manhattan  distance,  or  cosine similarity,  with  some  metrics  more  suitable  depending  on  the  problem  and  data  type.  In contrast, Decision Trees (DT) use feature selection to determine the most critical classification features,  with  feature  importance  gauged  by  its  contribution  to  improving  the  chosen criterion such as information gain or the Gini index.  Load "digits" datasets from SKlearn. Classify digit classes using KNN. split the dataset into training and testing sets using train_test_split() function from sklearn.model_selection. Then evaluate using sklearn.metrics accuracy_score,  