 KNN algorithm and its variants  To understand this algorithm, let's look at a straightforward scenario. A distribution of red circles (RC) and green squares (GS) is shown below:  You want to learn the blue star's (BS) class. Only RC or GS and nothing else can be BS. The nearest neighbour, denoted by "K" in the KNN method, is who we want to cast our vote. K will be set at 3. We will thus draw a circle with BS at its centre that is just large enough to contain three data points on the plane.  The three locations that are closest to BS are all RC. As a result, we can state with a high degree of certainty that the BS should be in class RC. As the closest neighbor gave RC three votes, the decision became clear.  An arbitrary data point can be represented as,  The Euclidean distance between data points can be denoted as,  x = [𝑥1,𝑥2, . . . , 𝑥𝑑] ∈  𝑅𝑑  𝑑  𝑑𝑖𝑠𝑡(x𝑖 , x𝑗 ) = √∑(𝑥𝑖𝑟 − 𝑥𝑗𝑟)2  𝑟=1  For finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format. To summarise:  ▪  Continuous valued target function: Mean value of the k nearest training examples ▪  Discrete class label:  Mode of the class labels of the k nearest training examples       Best number of neighbors (K)  The following are the different boundaries separating the two classes with different values of K.  For small values of K, we are restraining the region of a given prediction and forcing our classifier to be more focused on the close regions and neighbors. This will result in a low bias and high variance. Higher values of K will have smoother decision boundaries which means lower variance but increased bias.  Like most of machine learning problems, finding hyper-parameters such as K is not really straightforward. Cross-validation can be used to partition your data into test and training samples and evaluate your model with different ranges of K values.  By exploring different values of K and their corresponding misclassification error, we can decide which 𝐾 = 1, . . , 𝐾𝑚𝑎𝑥 has the best performance based on our partitioned data.      Decision trees  A decision tree is a map of the possible outcomes of a series of related choices.  Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.  A decision tree typically starts with a single root node, which branches into possible outcomes.  Regression trees  Decision trees that use a regression model are called regression trees. We can alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used.  Heuristic Method  1.  We first select a feature 𝑥𝑗 and a threshold ‘s’ such that splitting the feature space into the regions  {𝑥|𝑥𝑗 ≤ 𝑠} and {𝑥|𝑥𝑗 > 𝑠} leads to the best possible reduction in training error. So we are not going into the joint space of all features, but we work on a independent feature form such as 𝑥𝑗 with a threshold s.  2.  Next, we repeat the process, looking for the best feature and the best threshold in order to  split the data further to minimize the error in each of the resulting regions.  3.  However, this time, instead of splitting the entire feature space, we only split one of the  two previously identified regions.  4.  The splitting process continues until a stopping criterion is reached. For example, we may continue until no region contains more than five instances or the nodes are getting too pure or sparse.       Classification  trees  Classification and Regression Trees (CART) refer to decision tree algorithms that can be used for classification or regression predictive modeling problems. It is a structural mapping of binary decisions that lead to a decision about the class (interpretation) of an object (such as a pixel).  It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response.  In the classification setting, we replace the sum of square error by the classification error  rate as a criterion for making the binary splits. The classification error rate E is defined as the fraction of  ^ the training instances in that region that do not belong to the most common class. Where 𝑝 represents the proportion (fraction) of training instances in the j- region that are from k-th class:  𝑗𝑘  𝐸 = 1 − 𝑚𝑎𝑥  𝑘  ^ 𝑗𝑘 𝑝  Basically, Certainty of Distribution (COD) shows how certain it is that a classifier sits inside a region.  Let 𝐶𝑜𝐷 = 𝑚𝑎𝑥  𝑘  ^ 𝑗𝑘 𝑝  If CoD is close to 1, it means almost all of the training points inside a region are voting for a certain class label. On the other hand, when CoD is 0.5 it means we can not trust the votes because there is a high classification error rate (E).  But one of the problems of classification error is that it’s less sensitive for tree-growing.  Gini and Entropy  Consider the following figure, let say the Gini index and Entropy lines are representing the probability of selecting a particular class for a data point. For example, the fraction of training points in a region which are voting for class label 0 for a point. As you can see in the figure, in the corners we have small values of Gini Index and mis-classification, but in the middle area which both classes are representing the same chance for classification, we have high error and it shows the most equality peak in the middle which results in high mis-classification error. If you check the Gini index, it is not linear but mis-classification is linear in form. It seems Entropy is smoother and better in this case.       As an alternative to Gini index, Entropy, is defined as:  𝐾  ^ 𝐷 = − ∑ 𝑝  ^ 𝑗𝑘𝑙𝑜𝑔𝑝  𝑗𝑘  𝑘=1  Again we have the same concept in this formula too.  Decision tree algorithms  Popular algorithms for decision trees:  ID3 (Iterative Dichotomiser 3) -uses Entropy.     C4.5 (Successor of ID3) - slightly more advanced version of ID3 and also uses Entropy.   CART (Classification and Regression Tree)- uses Gini impurity.             The ID3 Algorithm  1.  Calculate the entropy of every feature using the data set S. Split the set S into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information.  2.  Make a decision tree node containing that feature. 3.  Recurse on subsets using remaining features.  Tree depth  If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, when the regions are very big and you have a shallow tree, even though it cannot have high variances it will have a high bias in shallow decision trees.  Model complexity and pruning  Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances.  Ways of pruning trees:    Pre-pruning (forward pruning)   Post-pruning (backward pruning)  Pre-pruning  In pre-pruning, we decide during the building process when to stop adding nodes (eg. by looking at entropy).  Let’s say we are splitting nodes by checking the amount of entropy reduction when we select different features. We can stop splitting nodes when the entropy reduction is not significant. By using this method we are eliminating an unnecessary complexity on the model.         Post-pruning  Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement. Consider the selected subtree (in red) in the figure below. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error.  Decision trees: advantages and disadvantages  Advantages    Very easy to understand, as they represent rules.   Capable of modelling nonlinear functions.   Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute  a Euclidean distance between two vectors having weather as a variable.)  Disadvantages    Sensitive to small changes in the data. If you add few data points or change some small  values, your rules can be changed!    May overfit easily. As we have said before, by building deep decision trees you are at  high risk of overfitting and a high variance model.    Only axis-aligned splits. Normal decision trees split the space along each features  independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.    Trees may not be as competitive in terms of accuracy as some of the other regression and  classification techniques such as SVM or neural networks.   