Summary and Reflection of Topic 8:  KNN and its variants    Assign weights to the contribution  of data point neighbours so the  nearer neighbours contribute  more to the average than more distant ones. This is useful for both classification and regression.    The basic idea is to label the test data point the same as the nearest neighbour (NN) (Technique  1) and/or label a test instance to the same as the majority label of the KNN (Technique 2)  Finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format.  Distance-Weighed  Nearest  Neighbour  Algorithm  –  assign  weights  to  the  neighbours  based  on  their distance from the test point, e.g.: weights may be inverse sequence of the distances: the higher the distance of  the  neighbour  the  lower  its  weight.  All  training  points  may  influence  a  particular  instance.  Method: Shepard’s method.  Best number of neighbours – can perform cross-validation for every possible number and evaluate the model based on the training and test data you have partitioned and be guided by the misclassification error i.e. the number of misclassifications, as a measurement of performance in your models.  Decision trees    A decision tree is a map of the possible outcomes of a series of related choices.   Decision trees can be used to weigh possible actions against one another based on their costs,  benefits and probabilities.    A decision tree typically starts with a single root node, which branches into possible outcomes.  Basic flow of the Decision Tree - If there is a mixture of a result in a sub node, there is scope to split further to try and get it to be only one category. This is termed the purity of the node.  A leaf node would be one which contains only one category.  A node which is impure can be branched further for improving purity. However, most of the time we do not necessarily go down to the point where each leaf is ‘pure’.  Decision  trees  that  use  a regression  model are  called regression  trees.  We  can  alternately  fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used.  Feature spaces are portioned into a number of regions using a top-down, greedy approach that is known as recursive binary splitting. Rather than using a brute-force solution, we work in a heuristic way / method:  1.  Select a feature and a threshold such that splitting the feature space into the regions such that it  leads to the best possible reduction in training error.  2.  Repeat  the  process,  looking  for  the best  feature and  the best  threshold in  order  to split the  data  further to minimize the error in each of the resulting regions.  3.  However,  this  time, instead of  splitting  the  entire  feature  space,  we  only  split  one  of  the  two  previously identified regions.  4.  The splitting process continues until a stopping criterion is reached.   In the classification setting, we replace the sum of square error by the classification error rate as a criterion for making the binary splits. The classification error rate is defined as the fraction of the training instances in that region that do not belong to the most common class.  Variable selection criterion - selected on a complex statistical criterion which is applied at each decision node. Done via two approaches:  1. Entropy and Information Gain  2. Gini Index  Both criteria  are  broadly similar  and seek to  determine  which variable would split  the  data  to  lead  to  the underlying child nodes being most homogenous or pure.  Certainty of Distribution (COD) shows how certain it is that a classifier sits inside a region. If close to 1, it means almost all of the training points inside a region are voting for a certain class label.  Classifier in this case is certain about the decision. On the other hand, when it is 0.5 it means we cannot trust the votes because there is a high classification error rate.  There are variety of algorithms for decision trees. Three of the more popular ones:  ID3 (Iterative Dichotomiser 3) uses Entropy.     C4.5 (Successor of ID3) slightly more advanced version of ID3 and also uses Entropy.   CART (Classification and Regression Tree) uses Gini impurity.  Entropy is a measure of disorder or impurity in a node. Thus, a node with more variable composition, such as 2 of a specific category and 2 of a different category (example Pass versus Fail) would be considered to  have  higher  Entropy  than  a  node  which  has  only  Pass  or  only  Fail.  The  maximum  level  of  entropy  or disorder is given by 1 and minimum entropy is given by a value 0. That is, leaf nodes which have all instances belonging to 1 class would have an entropy of 0. Whereas the entropy for a node where the classes are divided  equally  would  be  1.  To  split  a  sub note  –  we calculate  the  Entropy  and  Information Gain  for  the remaining variables.  We select the variable that shows the highest Information Gain.  If the Decision Tree continues to split we have another problem which is that of overfitting.  Gini Index - The other way of splitting a decision tree is via the Gini Index. The Entropy and Information Gain method focuses on purity and impurity in a node. The Gini Index or Impurity measures the probability for a random instance being misclassified when chosen randomly. The lower the Gini Index, the better the lower the likelihood of misclassification. The Gini Index has a minimum (highest level of purity) of 0. It has a maximum value of .5. If Gini Index is .5, it indicates a random assignment of classes.  The  Gini  index  has  a  maximum  impurity  is  0.5  and  maximum  purity  is  0,  whereas  Entropy  has  a maximum impurity of 1 and maximum purity is 0.  In general, most leaf nodes are not pure and, hence for categorical prediction, we use the modal value for prediction. If it is a numerical prediction (regression tree), we predict the mean value of the target values at each leaf node.  Overfitting and Decision Trees    Overfitting can be a big challenge with Decision Trees = may lead to a complex tree structure which may not generalize well to a test scenario. This is because each leaf will represent a very specific set of attribute combinations that are seen in the training data, and the tree will not be able to classify attribute combinations not seen in the training data.  Several ways are used to prevent the decision tree from becoming too unwieldy.  Pre-pruning or Early stopping: Preventing the tree from growing too big or deep  Pre-pruning = technique refers to the early stopping of the growth of the decision tree. The pre-pruning technique involves tuning the hyperparameters of the decision tree model prior to the training pipeline. The hyperparameters of the DecisionTreeClassifier in SkLearn:    include max_depth, min_samples_leaf, min_samples_split  Can be tuned to early stop the growth of the tree and prevent the model from overfitting. The best way is to use the sklearn implementation of the GridSearchCV technique to find the best set of hyperparameters for a Decision Tree model.  That is, in pre-pruning, we decide during the building process when to stop adding nodes (eg. by looking at entropy).  Let’s  say  we  are  splitting  nodes  by  checking  the  amount  of  entropy  reduction  when  we  select  different features. We can stop splitting nodes when the entropy reduction is not significant. By using this method, we are eliminating an unnecessary complexity on the model.  Sometimes  attributes  individually  do  not  contribute  much  to  a  decision,  but combined,  they  may  have  a significant impact.  A  challenge  with  the  early  stopping  approach  is that it faces  a  ‘horizon’  problem, where  an  early stopping may prevent some more fruitful splits down the line.  Post Pruning = technique, we allow the tree to grow to its maximum depth. Then we remove parts of the tree to prevent overfitting. We effectively consider subtrees of the full tree which are evaluated on a criterion and then removed. Hence, we are effectively going ‘up’ the tree and converting leaves to nodes and subtrees. The criteria whether a particular consolidation goes through or not is usually MSE for regression trees and classification error for classification trees.  Check which subtree removal introduces the smallest error and replace it with a single leaf node while incurring only a small amount of change in Entropy.  A  challenge  with  post  pruning  is  that  a  decision  tree  can  grow  very  deep  and  large  and  hence evaluating every branch can be computationally expensive.  Ensembling (another technique) = using averages of multiple models such as Random Forest  Advantages    Very easy to understand, as they represent rules.   Capable of modelling nonlinear functions.   Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a  Euclidean distance between two vectors having weather as a variable.)  Disadvantages    Sensitive to small changes in the data. If you add few data points or change some small values,  your rules can be changed!    May overfit easily. As we have said before, by building deep decision trees you are at high risk of  overfitting and a high variance model.    Only axis-aligned splits. Normal decision trees split the space along each features independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.    Trees  may  not  be  as competitive in  terms  of  accuracy  as  some  of  the  other  regression  and  classification techniques such as SVM or neural networks.  Impact of distance metrics on KNN performance    KNN is a well-known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan  distance,  and  cosine  similarity  are  a  few  of  the  distance  metrics  that  are  frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data.  The  above  summary  has  been  created  using  the  full  course  material  for  Topic  8  (including  the relevant YouTube clips)    