Summary/Reflection of the main points during topic 8:  k-Nearest Neighbours (k-NN) Algorithm and Its Variants    k-NN is a type of instance-based learning where new examples are classified based on their similarity to existing examples in the training dataset.    Variants of k-NN include: weighted k-NN (distance of neighbours is factored into the  decision), k-D trees (a data structure to hold the training dataset that speeds up finding nearest neighbours), and Locality Sensitive Hashing (a technique for high-dimensional spaces).  Theory Behind k-NN    k-NN operates on the principle that similar examples have similar output variables.    The algorithm measures the similarity between examples using a distance metric, such as  Euclidean or Manhattan distance.    The 'k' in k-NN refers to the number of neighbours the algorithm checks to make its  prediction.       Finding the Best Number of Neighbours (k)    Choosing the right k is critical. A smaller k makes the model sensitive to noise, whereas a  larger k could make the model too generalized.    Methods to choose k include cross-validation (finding the k that results in the lowest  validation error) and the elbow method (plotting the error against different values of k and choosing the 'elbow' point).  Decision Trees    Decision trees make predictions by splitting the dataset into subsets based on the feature  values. These splits are represented as a tree structure.    Each node in the tree represents a feature in the dataset, and each branch represents a  decision rule.      Regression Trees    Regression trees are decision trees where the target variable is continuous.    Predictions are made by traversing the tree from the root to a leaf and taking the mean value  of the target variable in that leaf node.      Classification Trees    Classification trees are decision trees where the target variable is categorical.    Predictions are made by traversing the tree from the root to a leaf and taking the most  common class of the target variable in that leaf node.       Decision Tree Algorithms    Algorithms for building decision trees include ID3, C4.5, and CART.    ID3 and C4.5 can only handle categorical features, while CART can handle both categorical and numerical features.    These algorithms differ in how they choose which feature to split on at each node.  Model Complexity and Pruning    Model complexity refers to the number of parameters and the structure of a machine  learning model. Complex models are more likely to overfit the training data.    Pruning is a technique to reduce the complexity of decision trees. It involves removing the  branches that add little to no predictive power to prevent overfitting.    Types of pruning include reduced error pruning (remove each decision node and check if it improves validation error), and cost complexity pruning (use a complexity parameter to control the size of the tree).  Pros/Cons of Decision Trees  Pros:    Easy to understand and interpret.    Can handle both numerical and categorical data.    Can handle multi-output problems.    Requires little data pre-processing, i.e., no need for scaling or normalization.    Able to model nonlinear relationships.  Cons:    Can easily overfit or underfit the data if not properly tuned.    Can be sensitive to small changes in the data, causing large changes in the structure of the  optimal decision tree.    Decision-tree learners can create biased trees if some classes dominate.  Impacts of Distance Metrics on KNN Performance    The choice of distance metric can significantly affect the performance of k-NN. It should be  chosen based on the characteristics of the data.    Euclidean distance works well for numerical data but can be affected by differences in scales  across features.    Manhattan distance can be a better choice when dealing with high dimensional data.    For categorical data, Hamming distance or other similarity measures could be used.    The choice of distance metric should be validated using cross-validation or other model  selection techniques.     Feature Importance Using Decision Trees    Feature importance in decision trees is calculated based on the amount that each feature decreases the weighted impurity in a tree.    A feature with a high importance value has contributed more to the decision-making in the  tree and hence is more crucial for making accurate predictions.      Features with low importance may not be necessary for prediction and could potentially be removed to simplify the model.  Feature importance can help with interpretability of the model and provide insights into the underlying data and the prediction task.                            References/Sources:  1.  Sayad, S. (no date) K Nearest Neighbors - Regression. Available at:  https://www.saedsayad.com/k_nearest_neighbors_reg.htm (Accessed: 19 May 2023).  2.  Masters in Data Science (no date) A Beginnerâ€™s Guide to Decision Trees. Available at:  https://www.mastersindatascience.org/learning/machine-learning-algorithms/decision-tree/ (Accessed: 19 May 2023).  3.  Timofeev, N. (2004) Classification and Regression Trees (CART) Theory and Applications. Available at: https://www.semanticscholar.org/paper/Classification-and-Regression- Trees%28CART%29Theory-and-Timofeev/c77b4aff121e3c2c2c18bef72b36286bb3a6d375 (Accessed: 19 May 2023).  4.  Solver (no date) Decision Tree Classification. Available at:  https://www.solver.com/classification-tree (Accessed: 19 May 2023).  5.  Chauhan, N. (2018) How to find the optimal value of K in KNN?. Available at:  https://towardsdatascience.com/how-to-find-the-optimal-value-of-k-in-knn-35d936e554eb (Accessed: 19 May 2023).  6.  Amazon Web Services (2021) Amazon SageMaker now supports K-Nearest Neighbors (k-NN)  classification and regression. Available at: https://aws.amazon.com/blogs/machine- learning/amazon-sagemaker-supports-knn-classification-and-regression/ (Accessed: 19 May 2023).                     Quiz:     