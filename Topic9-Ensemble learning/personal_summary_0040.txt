In this topic we studied K nearest neighbor algorithm (KNN) and decision tree, regression tree and its formulation and classification tree.  KNN algorithm is used for both classification and regression task on labeled data, it is a non-parametric algorithm that does not make assumptions about the shape of the data, so it can be used when there is no prior knowledge of underlying distribution of the data. It is easy to interpret and implement, it has a low bias, so it can fit to complex data patterns, but if the value of K is too small then it can suffer high variance and overfitting. It can handle both continuous and categorical data. Selecting the best value for K depends on the specific dataset and problem at hand.  A decision tree is a flowchart-like structure where each internal node represents a feature or attribute, each branch represents a decision rule, and each leaf node represents the outcome or the target variable. The decision tree algorithm uses the features of the training data to create a tree-like model that can be used to make predictions or decisions. Its construction involves selecting the best feature at each step that optimally splits the data based on some criteria. The criteria can vary depending on the specific implementation, but commonly used measures include Gini impurity and information gain. The goal is to find the features that provide the most discriminatory power to separate the data into different classes or categories. Once the decision tree is constructed, it can be used to make predictions on new, unseen data. Starting from the root node, each feature in the tree is evaluated based on the decision rules until a leaf node is reached, which corresponds to the predicted outcome or class. The decision rules are typically binary, meaning that the tree branches into two at each internal node, based on a threshold or condition related to the selected feature. Decision trees is capable of handling both numerical and categorical data, they can handle both classification and regression tasks. Decision trees are prone to overfitting, where the model becomes too specific to the training data and performs poorly on new data. Techniques such as pruning, limiting the tree depth, or using ensemble methods like random forest or gradient boosting can help mitigate overfitting and improve performance.  Regression tree is a type of decision tree used for regression tasks, it predicts continuous numerical values instead of categorical classes. They are useful when the relationship between features and the target is non-linear or exhibits complex interactions.  A classification tree is a decision tree used for classification tasks predicts the class or category to which an input belongs. It is used when the relationship between the features and the target variable is non- linear and exhibits complex decision boundaries. They can handle both binary and multi-class classification problems.     