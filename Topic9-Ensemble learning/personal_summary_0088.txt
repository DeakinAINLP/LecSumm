 The K Nearest Neighbours algorithm can be used for both classification and regression tasks, and works to classify unlabelled data points, or to predict continuous values in regression tasks. As it is a supervised learning algorithm, it begins with labelled training data which is memorised by the algorithm. It then uses a distance metric such as Euclidean distance to identify the K nearest neighbours of an unlabelled data point in relation to the training data. Care needs to be taken when choosing a distance metrics and the optimum value K considering the nature of the dataset.  Shepard’s method is a technique that can be used to estimate the value of a point based on its surroundings. It assigns weights to data points based on their distance from the target, indicating their influence on the predicted value. A weighted average is then calculated using these values.  Decision trees are a common machine learning algorithm, and are structured using a sequence of logical condition on the features of a dataset. The structure can be easily visualised as a series of nodes which represent condition, and branches connection them to possible outcomes. They can be used for both classification and regression problems. While classification trees use this structure to classify data points, regression trees predict continuous data as the output all by following the path from the root to a leaf node.  Gini index and Entropy are metrics that can be used with decision trees. Entropy measures the amount of uncertainty or randomness in class labels. It calculates the entropy using the likelihood of a class occurring. Gini index is a measure of node purity, as it calculates that likelihood of incorrectly classifying an random instance. It is commonly used to measure inequality, and is often used to represent the wealth distribution of a country.  The Iterative Dichotomiser 3 algorithm is a type of decision tree algorithm that works with classification tasks. The algorithm first calculates the entropy of each feature of a dataset, and then splits the dataset into subsets using the feature with low value entropy. A decision tree node is made using that feature, and the process is repeated recursively to build the rest of the tree.  Pruning is a technique used with decision trees to improve performance and prevent overfitting. Subtrees or nodes can be removed from the tree if they do not significantly decrease the overall accuracy of the model. Pruning can also be guided by a cost-complexity measure to balance a tree’s complexity and accuracy. This can not only help prevent overfitting, but can also make the tree easier to interpret, and improve its accuracy on unseen data.  