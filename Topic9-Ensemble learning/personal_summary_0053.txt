 KNN (K-Nearest Neighbour) is a classification and regression technique is where an algorithm determines (predicts) where an unknown data point belongs when added to a known data set. The unknown is classified by a simple neighbourly vote, where the class of close neighbours “wins.” It’s most popular use is for predictive decision making.  Determining K  You can think of K as controlling the shape of the decision boundary. Its value represents the number of “nearest neighbours” which should be considered when offering judgement on which group the new data point belongs to.  KNN Limitations  For small values of K, the reference region is restrained, which means that the classifier focuses on data points closer to it rather than distant points. This results in a lower bias with higher variance.  On the other hand, higher values of K gather more information from distant training points, which means smoother decision boundaries, lower variance but increased bias. Essentially, larger values of K reduces the effect of noise when classifying, but it also makes boundaries between classes less distinct.  Decision Trees  A decision tree maps the possible outcomes of a series of related choices. It weighs the costs, benefits and probabilities of potential actions against one another.  As its name suggests, it’s a hierarchical tree like structure which consists of a root node, branches, internal nodes and leaf nodes.  A decision tree starts with a root node. The outgoing branches from the root node then feed into the internal nodes, also known as branches (decision nodes). Leaf nodes (terminal nodes) represent all the outcomes possible from within the dataset.  Regression Trees  A regression tree is essentially a decision tree that’s used for regression tasks.  Regression trees differ from classification trees in that the outcome of a model is a prediction of a “continuous” quantity rather than a prediction if an observation belongs to a specific or “discrete” category.  Classification Trees  Classification trees are algorithms that can be used for classification or regression predictive modelling problems. It is similar to a Regression tree, except that a qualitative result is predicted, rather than a quantitative one.  A classification tree assigns each data point, essentially voting itself into the majority class of training instances in the region where it belongs.    Decision tree Algorithms  There are 3 popular algorithms used in decision trees:   ID3 (Iterative Dichotomiser 3) uses Entropy. When entropy is 0, the dataset is completely homogeneous, meaning that each instance belongs to the same class.   C4.5, which is the successor of ID3, is slightly more advanced and also uses Entropy.  CART (Classification and Regression Tree) uses Gini impurity, which is a score that evaluates  how accurate a split is among the classified groups.  Tree depth  If a tree is very deep, we should expect low chances of visiting many training points in that sub- region. This means all the estimations in that region are not good because of the high variance.  If a tree is very shallow, you can infer that the training data points do not have high variances however you may have other problems such as bias.  Tree Pruning  Pruning reduces the size of decision trees by removing parts that do not assist in classification. Decision trees are susceptible to overfitting and effective pruning can reduce this likelihood.  Trees can be;   Pre-pruned: decision made to stop adding nodes (e.g. using entropy) during building  Post-pruned: waits until a full decision tree has been built and before pruning attributes    