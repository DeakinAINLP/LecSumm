K Nearest Neighbours (KNN):  The  supervised  machine  learning  method  K  Nearest  Neighbours  (KNN)  is  utilised  for  both classification and regression applications. It is a straightforward yet effective algorithm that bases its predictions on how similar the input data is to its neighbours.  Similar data points frequently have similar labels or values, which is the core tenet of KNN. In other words, a new data point is likely to have a comparable label if it is adjacent to a group of labelled data points. By locating the k nearest neighbours of a new data point and figuring out the  dominant  class  (for  classification)  or  average  value  (for  regression)  among  those neighbours, KNN uses this idea to create predictions.  The KNN algorithm's essential elements and phases are listed below:  Data: A labelled training dataset with each data point having a set of features and an associated label or value is necessary for KNN.  KNN  uses  a  distance  metric  (such  as  the  Manhattan  distance  or  the  Euclidean  distance)  to determine how similar two data points are. The type of distance measure chosen relies on the data's makeup.  K value: K is a hyperparameter that specifies how many neighbours will be taken into account while making predictions. It should be carefully chosen because a little number might result in overfitting and a big value could induce bias.  KNN determines the distances between the new data point and every other data point in the training set in order to create a prediction for a new data point. Then, depending on the shortest distances, it chooses the k nearest neighbours.  Classification by majority vote: For classification tasks, KNN chooses the class that has the most members among its k neighbours and gives that class to the new data point.  Average  (regression):  KNN  calculates  the average value  of  the target  variable  among  the  k neighbours and applies that value to the new data point for regression tasks.  KNN  provides  a  number  of  benefits,  including  its  simplicity,  use,  and  capacity  for  both category  and  numerical  data.  It  does,  however,  have  certain  restrictions.  These  include  its sensitivity to the feature scaling, its testing phase computing cost, especially for big datasets, and its dependence on the distance metric.  It is crucial to preprocess the data, choose an acceptable distance measure, and adjust the value of k based on the task at hand if you want to utilise KNN efficiently. Additionally, because KNN  may  be  computationally  expensive  for  high-dimensional  data,  it  is  better  suited  for smaller datasets or datasets with fewer characteristics.  Overall, KNN is a flexible and popular machine learning algorithm, especially for its ease of use and natural method of producing predictions based on the similarity of data points.  Decision Tree:  Decision trees are supervised machine learning algorithms with a wide range of applications that  may  be  utilised  for  both  classification  and  regression  tasks.  They  offer  a  concise  and understandable illustration of the decision-making processes based on the input attributes.  The basic goal of decision trees is to produce homogenous subsets with regard to the target variable by segmenting the input space into regions or subsets depending on the feature values. Each leaf node of the tree represents a prediction or a class label, whereas each interior node reflects a judgement based on a particular attribute.  The main elements and procedures for creating a decision tree are as follows:  Data: A labelled training dataset with each data point having a set of features and an associated class label or value is necessary for decision trees.  The selection of the splitting criterion affects how the characteristics are chosen at each node to divide the data. Information gain, entropy, and Gini impurity are common splitting criteria. Finding  the  split  that  maximises  the  homogeneity  or  purity  of  the  ensuing  subsets  is  the objective.  Recursive partitioning: Decision trees divide the data in a top-down, recursive manner. The algorithm chooses the optimal split based on the selected criterion and generates child nodes starting  from  the  root  node.  When  a  stopping  requirement  is  reached  (such  as  reaching  a maximum depth or a minimum number of samples per leaf), the algorithm iteratively resumes for each subset of data.  Pruning  (optional):  Following  the  original  decision  tree's  construction,  overfitting  can  be decreased by using pruning strategies. Pruning entails eliminating or compressing nodes that make minimal contributions to the prediction performance. This enhances the tree's capacity for generalisation.  To produce predictions, fresh data points move through the decision tree by adhering to the node's decision  criteria  until they reach the leaf  node. The class label  (for classification) or value (for regression) of the new data point is represented by the prediction at the leaf node.  The  interpretability,  adaptability  to  both  categorical  and  numerical  data,  and  resistance  to outliers  and  missing  values  are  only  a  few  benefits  of  decision  trees.  They  may  record interactions, nonlinear connections, and varying degrees of relevance.  Decision trees do have certain drawbacks, though. If they are not sufficiently regularised, they can easily overfit the training data. They could have trouble understanding multidimensional data or intricate relationships. Decision trees are also susceptible to instability, which means that even little changes in the data might result in alternative decision bounds.  Classification and Regression Trees (CART):  A  popular  decision  tree  approach  that  may  be  used  for  both  classification  and  regression problems is the Classification and Regression Trees (CART) algorithm. Based on the properties of the input, it constructs binary trees to generate predictions.  To  build  the  tree,  the  CART  method  uses  recursive  binary  splitting.  An  overview  of  the algorithm's operation for classification and regression tasks is given below:  Trees of classification      Information: Training datasets for classification trees must be labelled, with each data point having a set of characteristics and a matching class label.    The Gini impurity or entropy is frequently utilised as the splitting criteria. Based on the class labels assigned to the data points in a node, it calculates the impurity or disorder of that node.    Recursive  splitting:  The  algorithm  chooses  the  optimal  split  based  on  the  selected criterion and produces two child nodes, starting from the root node. When a stopping requirement is reached (such as reaching a maximum depth or a minimum number of samples per leaf), the algorithm iteratively resumes for each subset of data.    To produce predictions, fresh data points move through the decision tree by adhering to  the  node's  decision  criteria  until  they  reach  the  leaf  node.  The  projected  class  is determined by the training samples' majority class labels in that leaf node.  Trees with Regression:    Data: To train regression trees, you need a dataset that has been labelled and contains  data points with a set of features and a numerical value for each one.    The mean absolute error (MAE) or mean squared error (MSE) are frequently employed as  the  splitting  criterion.  These  metrics  calculate  the  target  variable's  variance  or average absolute difference inside a node.    Recursive splitting: The method recursively divides the data according to the selected criterion,  producing  two  child  nodes  at  each  split,  much  like  classification  trees  do. Until the halting requirement is satisfied, the splitting process goes on.    The mean or median value of the target variable for the training samples at that leaf  node is typically the projected value at each leaf node for regression tasks.  CART  provides  a  number  of  benefits.  It  can  accept  missing  data,  manage  category  and numerical  characteristics,  and  record  intricate  non-linear  connections.  For  figuring  out  the underlying patterns in the data, CART trees also offer interpretable decision rules.  However,  when  the  tree  is  too  deep  and  complicated,  CART  is  vulnerable  to  overfitting. Pruning methods may be used to alleviate this problem by reducing the size of the tree and enhancing generalisation.  