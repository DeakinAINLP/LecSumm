Topic 8 – Nonlinear models (KNN and Decision Trees)  KNN algorithm and its variants    Assigning weights to neighbouring data points; therefore, the closer/neighbouring data points  contribute more to the average compared to the distant and isolated ones    K can varied so that that labelled data instance can have the same as the majority label of the  KNN algorithm.  KNN theory    Finding the mode of the class labels in discrete format (requires finding Euclidean distance  between test point and data instances which are arbitrary values)  o  Continuous valued target function:  ▪  Mean value of the k nearest training examples  o  Discrete class label  ▪  Mode of the class labels of the k nearest training examples    Assigning weights to the neighbours based on their distance from the test point.[1]  o  Therefore, the further a data instance is from the test point, the lower its weight is  relative to that test point  The best number of neighbours (K)    Higher K value means smoother and lower variance, but more bias.   Lower K value means lower bias, and but higher variance.   By using Cross-validation for every possible number of K, misclassification error can be used, to  evaluate the performance of each model based on K    From this result, the best K can be decided; lower the error, the less errors there are in the  clustering.  Decision Trees    Mapping of possible outcomes and probabilities of actions of a series of events from the main  question    This will allow weigh values/actions against one another based on the cost, benefits and  probabilities[2]  Regression trees    Regression trees are decision trees which implement regression models (classification trees)    How to create a classification tree (majority)  o  Divide the feature space (set of possible xis ) into J distinct and non-overlapping regions  R1….Rj  o  For each instance that is filtered into Rj, make same prediction; this is mean of the  response values for the training observations in Rj    Minimising training errors function    Required to to use optimisation problem to minimise the problem so that it has regions that  result is less error    This requires a recursive binary splitting.  Classification trees    Like the regression trees, however, can predict a qualitative response rather than a quantitative  response.    Replace SSE with classification error rate for making binary splits.    The Certainty of Distribution indicates how certain it is that a classifier sits inside a region; where  CoD is close to 1, it would signify most of the training points inside a region are voting for a certain class label.  Gini and Entropy    Used to measure inequality    It measures the node purity; as it encounters low inequality distribution in the votes.  Decision tree algorithms The ID3 algorithm    Generate a decision tree from a dataset  o  Calculate the entropy of each feature data set S; split the set S into subsets using the  feature for which entropy was minimum. o  Construct decision tree node for a feature o  Recurse on subsets using remaining features.  Tree depth    Partitioning the feature space into smaller regions; the deeper the tree, the less likely the  estimations would be accurate and can lead to higher variance.  Model complexity and pruning    Reduces the size of decision trees; reduces the power to classify instances which have little  influences on the decision tree (in way like dimensional reduction)    Tree building can lead to over fitting.  o  A tree has large number of regions may have only few data points per region resulting  high variance.  o  Having lower number of regions = higher bias  Pre-pruning    When to stop including nodes during the building stage   Pre-pruning can be executed when an entropy reduction can be less significant.  Post-pruning    Prunes attributes by subtree replacement, until the full decision tree has completed.   Check subtrees after built and identify whether they can replace it with a single node or feature,  while incurring only a small amount of change in Entropy.  Advantages and disadvantages of decision trees Advantages    Can be easily formulated.   Can model nonlinear functions.   Can deal with categorical variables.  Disadvantages    Rules could change due to manipulation of sensitive and new data.   Can lead to high variance; overfitting (this is the case if there are deep decision trees which are  not pruned) SVM and NN have proven to be more advantageous in terms of accuracy.   