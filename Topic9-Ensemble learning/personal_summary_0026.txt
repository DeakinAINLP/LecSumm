KNN  K-nearest neighbours (KNN) is a process that increases the weight of the K training data points  closest to the data point being predicted.  For classification problems result is the Mode of the class labels of nearest K training labels.  For regression problems result is the mean of the nearest K training labels  For example, KNN applied ot a classification problem would see the distance from the point to all training points calculated.- The K nearest training data points would then be found.  The classes of each K trainin points would be counted, with the largest number of classes being the  result.  Shepards method is an approach that weights the neighbours by their distance from the data point so  that their infuence reduces as their distance increases.  The value of K sets the shape of the decision boundary which determines whether a training point is  considered. It is a trade off between considering too few or too many points.  Low values of K are likely to be noisy. That is, they have high variance, and low bias.  High values of K are subject to misclassification as samples with low frequency may be outvoted by  distant but more frequent categories. That is, they have low variance and high bias.  Selecting K requires trial and error to find a suitable value. There is no approach gauranteed to give  the best result.  Cross-validation and evaluating misclassification for different values of K from  K = 1, . . . , Kmax  is a  good approach to find a suitable value.  Kmax  must be selected manually for the model.  Impact of distance metrics on KNN performance  KNN is suitable for use with a wide range of distance metrics to determine which neighours are  nearest.  A range of studies have found analysed the relative performance of the different measures.  KNN performance suffers limited degradation (20%) from up to 90% noise.  Performance between different but similar distance measures is similar. For example, using a  distance measure that is simply converted to another through some constant operation does not  significantly differentiate performance.  There is not a single best measure to use for all types of data. A number of studies have identified  Manhattan as the best performer. More recently the Hassanat algorithm has found success.  Whilst some evidence exists that Hassanat outperforms Manhattan, it is also slower. Depending on  the task at hand, manhattan may be faster and offer sufficient performance.  Decision Trees  Decision trees map outcomes based on choices starting from some root. Each node is a choice that  must be made, and its branches are the options that can be chosen. Leaf nodes are outcomes that  have resulted from that options chosen to get to that leaf.  Decision trees can be applied to continuous or classification problems.  Regression Trees  Regression trees are applied to continous problems.  The data space is divided into a series of regions.  Each choice node in the tree predict which region the data point being predicted will fall into.  The predicted value is the mean of the region that the is reached for a given value.   Checking all possible region combinations is infeasible, thus a heuristic approach named recursive  binary splitting is used to find determine regions with a sufficiently minimised error.  At a high level the recursive binary split is performed as follows:  . at each level of the tree the algorithm finds a feature and threshold such that dividing the  remaining region at that threshold minimises training error.  . The algorithm then selects one of the regions and repeats recursively.  . The recursion terminates on some condition, such as a minimum number of data points, region  purity, or region sparsity.  Prediction is then performed by taking the new data point and following the decision tree until the  region for the data point is identified. The prediction is then the mean of the outcomes for the data  points in that region.  Classification Trees  Classification trees are similar to regression trees, but make predictions using the mode of the region  containing the datapoint, rather then the mean.  When determining regions different error metrics are minimised instead of the sum of square error.  Decision Tree Algorithms  A range of decision tree algorithms exist, including the following methods:  ID3 (Iterative Dichotomiser 3)  C4.5  CART (classification and regression 3).  C4.5 succeeds ID3, both of which use entropy to evaluate error.  CART uses Gini impurity to evaluate error.  ID3 Algorithm  Calculate entropy of all points in data set.  The point with least entropy becomes the threshold for the node.  Recursively continue through sub-sets.  Tree Depth  A deep tree means that regions are small, thus contain few samples. This increases variance.  A shallow tree means that regions are large, thus contain many samples. This increases bais.  Therefore the depth of the tree must be selected based on evaluation and tuning of  hyperparameters.  Model Complexity and Pruning  Pruning is the process of removing branches that provide low value to point classification from a tree  in order to reduce model complexity.  Pre-pruning occurs while a decision tree is being built. For example, stopping a branch when it does  not significantly reduce entropy.  Post-pruning occurs once the tree has been built and uses a process called subtree replacement.  Subtree replacement iterates through decision nodes to identify subtrees which can be replaced  while minimising change to model entropy.  Post-pruning is preferred as it avoids the need to estimate when a branch should stop growing, as  required by the pre-pruning approach.  Decision Trees Advantages and Disadvantages  Advantages:  Easy to understand how the process works  Can be applied to non-linear functions  Can be used for categorical values  Disadvantages:  Require splits in data to be aligned with axis. Avoiding this issue requires data pre-processing.  May be less accurate then SVM or neural networks.  Small changes in data cause model to change  Vulnerable to overfitting.  Feature Importance of using Decision Trees  Feature selection plays a critical role in the performance and efficiency of many machine learning  algorithms.  Implementing a decision tree inherently prioritises features by using the recursive binary splitting  approach to prioritise features for classification.  It follows that the features most important (ie closest to root) in a decision tree will be most important  for other machine learning approaches.  As such, a decision tree can be used as a preliminary step to prioritse features for use in a different  training method.  