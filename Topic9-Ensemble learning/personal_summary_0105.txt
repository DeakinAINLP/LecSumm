 ● K-Nearset Neighbor (KNN) is a popular and easy to understand algorithm. The  fundamental working of KNN is that it groups a bunch of data points in a cluster based on how closely they are associated with other members of the same cluster. It can be seen as a voting system where the closest neighbours of a data point have the most say in what that data point should be classified as and the farther the other data points, the less influence they would have. The number of nearest neighbours is k.  ● The question arises, what should be the values of k to be considered? Unfortunately, there is no heuristic rule about this. We need to run a number of trials with different values of k and plot the accuracy/misclassification error and make a decision based on that. Choose a k with the highest accuracy and lowest misclassification error.  ● Another important factor of KNN is the distance metric, as it uses distance as a measure of similarity, the distance metric plays a significant role in how KNN looks for similarity. There are many different metrics including the usual Euclidean distance, Manhattan/city block distance, cosine similarity, etc.  ● A Decision Tree (DT) is another classification algorithm that is hierarchical in structure  and branches out to smaller and smaller nodes until it reaches a decision, except that it is not as straightforward. There is a chance of entropy in the data which makes the data uncertain or impure.  ● The way Decision Tree handles Entropy is by using a metric known as Information Gain. Information gain checks for what features the entropy reduces or in other words, which features are actually more significant than others and can be taken as a deciding factor when splitting a node.  ● But if we keep splitting the nodes, then we may end up overfitting the data. To prevent this we can tune a hyperparameter called max depth. It is the number of levels up to which the nodes can split and that would be the final level where the decisions would be made.  ● Max depth is not enough to avoid overfitting, we also need to prune the decision tree  and remove branches that provide low to no contribution to the classification of the data. There are two common ways to prune a decision tree, pre-pruning and post-pruning. ● In pre-pruning, we keep a check on entropy reduction and stop the growth of the tree when the entropy is low while in post-pruning, we wait for the whole tree to grow and then check for subnodes/branches that are insignificant  