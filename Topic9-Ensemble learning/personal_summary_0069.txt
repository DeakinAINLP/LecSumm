For topic 8 , we looked at nonlinear models, such as KNN (K nearest neighbour) and DTs (Decision Trees). This topic was split into these key areas to help us learn:  -  KNN algorithm and its variants, which explains a technique of assigning weights to data  point neighbours so that any neighbours that may be located nearby can contribute more to the average than ones that may be further away.  -  Theory of KNN, which goes more in depth about how we use KNN with data, and an explanation on a Voronoi diagram, which allows us to chart nearest neighbours, and is separated by boundaries and decision rules. These are formed from the training examples that are provided.  -  Best number of neighbours (K), and how important it is to pick the right K value. While it is difficult, this can be achieved through cross-validation, so that data can be separated into testing and training samples, which can help to evaluate the used MLL with different ranges of K values. We also learnt that higher K values will tend to have smoother decision boundaries but higher bias, and vice versa for lower,  -  Decision Trees, which are maps that produce a layout of possible outcomes based on a series of related choices. It starts with a single root node, and then branches into multiple possible outcomes.  -  Regression Trees, which are essentially variants of decision trees, which instead of being utilized for classification purposes, they help to approximate real-valued functions.  -  Classification Trees, which are another variant of decision trees, which map out binary decisions which then ultimately lead to a decision about that class. These trees are like regression trees; however, it can be used to predict qualitative responses, rather than quantitative responses. To use classification trees, we can assign each testing instance to the mode (majority class) of the training instances in the region being observed. We also learned about Gini index and Entropy, which are commonly used in measuring inequality.  -  Decision Tree Algorithms, which explained the vast variety of algorithms we can use with  decision trees, including:  -  ID3 (Iterative Dichotomiser 3, which uses Entropy)  -  C4.5 (Successor of ID3, more advanced, uses Entropy)  -  CART (Classification and Regression Tree, uses Gini impurity)               Lachlan Connor Patrick Geary, 221260728  -  Model complexity and pruning, where pruning is a technique that is used to reduce the size of a decision tree by removing sections of a tree that provide little to no power to classify instances. We also learnt about the stages of pruning, such as: Pre-pruning: which is stopping the tree before it adds all the nodes and classifying the training set by looking at entropy. Post-pruning: which waits until the full decision tree has been completed, and then prunes the tree’s attributes via subtree replacement.  -  -  -  Decision trees and their advantages and disadvantages, and what they are best suited to:  Advantages Easy to understand.  - -  Can model non-linear functions. -  Able to handle categorical variables  Disadvantages  Sensitive to small changes in data Potential to overfit data easily  - - -  Only axis-aligned splits  -  -  Impact of distance metrics on KNN performance, which explained different distance metrics, such as Euclidean, Manhattan and cosine. It explained how specific distance metrics may be better suited depending on the data and the problem. It also demonstrated an example of how cosine similarity could be a better choice over Euclidean distance, if you are dealing with high-dimensional data.  Feature importance of using decision trees, which can be used to identify more relevant features for classification and to help feature selection improve the overall performance of the model.  Reading List  This is my reading list of topic 8 to help me gain a better understanding of the material that was provided for us, as well as help me program the pass task coding questions:  Solver: (https://www.solver.com/regression-trees)  Clark Labs(https://clarklabs.org/classification-tree- analysis/#:~:text=A%20classification%20tree%20is%20a,that%20leads%20to%20categorical%20decis ions.)  Displayr(https://www.displayr.com/machine-learning-pruning-decision- trees/#:~:text=As%20the%20names%20suggest%2C%20pre,tree%20after%20it%20has%20finished.)  Data Camp (https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn)  Scikit Learn (https://scikit- learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)            Lachlan Connor Patrick Geary, 221260728  Reflection  Overall, this topic was quite simple and quick, a lot of this information was already learnt near the start of the trimester, so it was nice to go back and build on that foundational information.  21/05/2023, 01:34  : Topic 8 Quiz - SIT307_SIT720 - Machine Learning - CloudDeakin  Topic 8 Quiz  LACHLAN CONNOR PATRICK GEARY (username: lpgeary)  Attempt 2  Written: 21 May, 2023 1:31 AM - 21 May, 2023 1:34 AM  Submission View  Your quiz has been submitted successfully.  Question 6  For any two models where the first is using more number of features than the second, which of  these is correct?  The first model is more likely to overfit compared to the second model  The first model is more likely to underfit compared to the second model  The first model is more likely to be biased compared to the second model  The first model is less likely to be biased compared to the second model  Attempt Score:  9 / 10 - 90 %  Overall Grade (highest attempt):  9 / 10 - 90 %  Done  https://d2l.deakin.edu.au/d2l/lms/quizzing/user/quiz_submissions_attempt.d2l?isprv=0&qi=149808&ai=10269927&isInPopup=0&cfql=0&fromQB=… 1/1   