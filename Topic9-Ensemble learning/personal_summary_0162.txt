 The K-nearest neighbours (KNN) technique is a well-known supervised machine learning algorithm that may be used for classification and regression applications. It is a non-parametric method that makes no assumptions about the data distribution. Instead, it produces predictions in the feature space based on the K closest neighbours to a particular data point.  The K-nearest neighbours (KNN) approach is based on the idea that data points with comparable attributes are more likely to belong to the same class or have similar goal values. The method produces predictions by locating and utilising the labels or values of the K closest neighbours to a given data point in the feature space.  A decision tree is a widely used supervised machine learning technique for classification and regression applications. Each internal node represents a characteristic or attribute, each branch represents a decision or rule, and each leaf node represents the conclusion or class label.  Regression trees are a type of decision tree that is especially built for predicting a continuous target variable rather than class labels. Regression trees operate by dividing the feature space into discrete areas, each with its own predicted value.  Classification trees are a form of decision tree method that is especially designed to solve classification issues by assigning categorical class labels to data instances based on their feature values. Classification trees divide the feature space into different areas, each of which is assigned a predicted class label.  There are multiple common decision tree algorithms, each with its own set of traits and modifications. They are ID3 (Iterative Dichotomiser 3), C4.5 (Successor to ID3), and CART (Classification and Regression Tree).  One of the first decision tree algorithms was ID3. It is primarily intended for categorical target variables and employs information gain as the splitting criterion. To build the decision tree, ID3 uses a top-down, greedy search. C4.5 is an ID3 extension that fixes some of its shortcomings. It accepts both category and numerical characteristics and splits based on the information gain ratio. C4.5 additionally handles missing values in the dataset and enables for decision tree pruning to prevent overfitting. CART is a flexible decision tree method that may be used for classification as well as regression. It splits classification trees using the Gini impurity and regression trees using the mean squared error (MSE). CART builds binary decision trees by dividing data into two groups at each internal node.  The amount of sophistication or intricacy in a machine learning model is referred to as model complexity. The depth and breadth of the tree, i.e., the number of levels and branches or splits, indicate model complexity in the context of decision trees. A more complicated decision tree has more nodes, which might result in overfitting if not adequately regulated.  Pruning is a strategy for reducing decision tree complexity and mitigating overfitting. It entails eliminating or compressing certain nodes, branches, or subtrees from the tree while retaining or enhancing generalisation performance. Pruning aids in the creation of a smaller, more generalised decision tree, reducing the danger of catching noise or patterns in the training data that may not generalise well to unknown data.   