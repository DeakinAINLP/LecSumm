KNN algorithm and its variants  A useful technique can be to assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.  This is useful for both classification and regression.  For finding the majority of decisions based on the close training points, you need to perform average or mean in continuous cases and you need to find the mode of the class labels in discrete format. To summarise:   Continuous valued target function:  Mean value of the k nearest training examples   Discrete class label:  There is another concept partially related to KNN which is called a Voronoi Diagram. In mathematics, a Voronoi diagram is partitioning of a plane into regions based on distance to points in a specific subset of the plane.  A Voronoi diagram is based on closest neighbours; the same concept as KNN. Also clearly the data is not linearly separable and it results in complex boundaries and decision rules. Remember that the decision surface is formed by the training examples.  Imagine a scenario in which we are performing 10−NN (10 Nearest Neighbours), and the only real close points are  points. The other 6 neighbours we are going to analyse are not really related to this point. The final result actually could be misleading!  But, what if we assign a different weights to distances of data points? So the question here is what is a  distance-weighted nearest neighbour algorithm?  Basically we can assign weights to the neighbours based on their distancef rom the test point. For example, weight may be inverse square of the distances.  This means the higher the distance of the neighbour, the lower its weight. All training points may influence a particular instance. This method is also known as Shepard’s method.  You can think of k as controlling the shape of the decision boundary we talked about earlier. For small values of k, we are restraining the region of a given prediction and forcing our classifier to be more focused on the close regions and neighbours. We are asking the classifier not to care about fairly distant points. This will result in a low bias and high variance.  Higher values of k will have smoother decision boundaries which means lower variance but increased bias. So basically, higher values of k means asking for more and more information even from distant training points.   Like most of machine learning problems, finding hyper-parameters such as k is not really straightforward. Finding the best answer is not always possible. But as a simple and handy method, you can use Cross-validation (see topic, Model Selection) to partition your data into test and training samples and evaluate your model with different ranges of k values.  Decision trees  A decision tree is a map of the possible outcomes of a series of related choices.  Decision trees can be used to weigh possible actions against one another based on their costs, benefits and probabilities.  A decision tree typically starts with a single root node, which branches into possible outcomes.  Regression trees  Decision trees that use a regression model are called regression trees. We can alternately fit a classification model. Such decision trees are called classification trees. Usually, extremely simple models such as majority (classification) or mean (regression) are used.  Procedure:  1.  We divide the feature space, i.e., the set of possible values into distinct and non-overlapping  regions.  2.  For every instance that falls into region we make the same prediction, which is simply  the mean (or mode) of response values for the training observations.  The overall goal of regression trees is to find regions that minimize the training error.  We take a top-down, greedy approach that is known as recursive binary splitting. Rather than using a brute-force solution, we would like to work in a heuristic way.  Classification trees  Classification and Regression Trees (CART) is a term introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modeling problems.  It’s similar to regression trees, except that it is used to predict a qualitative response rather than a quantitative response. For a classification tree, we assign each test instance to the majority class (mode) of the training instances in the region where it belongs. You can consider this action as a being like a data point voting itself into a region which results in selecting the majority.  In the classification setting, we replace the sum of square error by the classification error rate as a criterion for making the binary splits. The classification error rate E is defined as the fraction of the training instances in that region that do not belong to the most common class.  Gini and Entropy  In practice people would prefer to use the Gini index and Entropy. The Gini index is the most commonly used measurement of inequality. For example in economics, the Gini index represents the income or wealth distribution of residents in a country.  Because it faces with a low inequality distribution in the votes. Gini index is therefore considered a measure of node purity.  Decision tree algorithms  There are variety of algorithms for decision trees. Here are three of the more popular ones:    ID3 (Iterative Dichotomiser 3) uses Entropy.  C4.5 (Successor of ID3)  slightly more advanced version of ID3 and also uses Entropy.  CART (Classification and Regression Tree)  uses Gini impurity.  The ID3 Algorithm  The algorithm was developed by Ross Quinlan in 1975 (He is an Australian who graduated from University of Sydney). It’s used to generate a decision tree from a dataset. Although this method is simple, it is an effective machine learning algorithm. The basic algorithm is as follows:  1.  Calculate the entropy of every feature using the data set S. Split the set S into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the attribute or feature and it will gain more information.  2.  Make a decision tree node containing that feature. 3.  Recurse on subsets using remaining features.  Tree depth  If you build a very deep tree, you are basically partitioning the feature space into small regions. If the tree is very deep, we should expect low chances of visiting many training points in that sub-region. This means all the estimations in that region are not good because of the high variance.  On the other hand, when the regions are very big and you have a shallow tree, you can infer that the training data points do not have high variances however you may have other problems such as bias. You will have a high bias in shallow decision trees. It means your decision making process is too naive.  You need to create a decision tree of the right depth, to find the sweet spot in terms of depth. You can achieve that by performing the cross validation and other evaluation methods.  Remember, you need to find or tune the proper hyperparameter which is the depth of the tree.  Model complexity and pruning  Pruning is a technique that reduces the size of decision trees by removing sections of tree that provide little power to classify instances.   The tree-building process that we described in previous steps may produce good predictions on the training set, but it’s likely to overfit the data, leading to poor generalization performance.  A tree that has a large number of regions may have only few data points per region resulting  in high variance.  On the other hand, having a small number of regions may result in high bias. One possible alternative is to grow a large tree, and then prune it back in order to obtain a subtree.  Generally there are several ways of pruning trees:  Pre-pruning (forward pruning) Post-pruning (backward pruning)  Pre-pruning  In pre-pruning, we decide during the building process when to stop adding nodes (eg. by looking at entropy).  Let’s say we are splitting nodes by checking the amount of entropy reduction when we select different features. We can stop splitting nodes when the entropy reduction is not significant. By using this method we are eliminating an unnecessary complexity on the model. However, this may be problematic.  Sometimes attributes individually do not contribute much to a decision, but combined, they may have a significant impact.  Let’s discuss another model which can handle this problem.  Post-pruning  Post-pruning waits until the full decision tree has been built and then prunes the attributes by subtree Replacement. Consider the selected subtree (in red) in the figure below. We can easily replace an entire subtree with a single region or node. We need to check that this reproduces the smallest error.  Check which subtree removal introduces the smallest error and replace it with a single leaf node.  As the figure shows, you wait until the full decision tree has been built, then go for subtrees and check whether you can replace it with a single node or feature, while incurring only a small amount of change in Entropy.  If yes, trim the tree. If not, you should keep that subtree because it probably has useful information.  Decision trees : advantages and disadvantages  What are the unique features of decision trees? What problems are best suited for their use?  Advantages   Very easy to understand, as they represent rules.  Capable of modelling nonlinear functions.  Can handle categorical variables (i.e. weather being sunny vs cloudy. We cannot compute a Euclidean distance between two vectors having weather as a variable.)   Disadvantages   Sensitive to small changes in the data. If you add few data points or change some small values, your rules can be changed!   May overfit easily. As we have said before, by building deep decision trees you are at high risk of overfitting and a high variance model.   Only axis-aligned splits. Normal decision trees split the space along each features  independently. If we need to make a more complex decision tree model we can consider joint probabilities or more complicated scenarios while modelling the tree.   Trees may not be as competitive in terms of accuracy as some of the other regression and  classification techniques such as SVM or neural networks.  Impact of distance metrics on KNN performance  KNN is a well known machine learning method that classifies new data points according to their closeness to the closest neighbours in the training set using distance measures. The effectiveness of KNN can be significantly impacted by the distance metric that is selected. Euclidean distance, Manhattan distance, and cosine similarity are a few of the distance metrics that are frequently employed in KNN. Other distance measurements might be more suited depending on the problem and the type of data. Cosine similarity may be a better option than Euclidean distance, for instance, in high-dimensional data. Please use the following link for further explanation.  Feature importance of using Decision Trees (DT)  Decision Trees are a popular machine learning algorithm that uses feature selection to determine the m ost important classification features. DT operates by recursively segmenting the data into subsets based on the most informative features until a stopping criterion is reached. A criterion such as information gai n or the Gini index is used to determine which feature is the most informative. At each node of the tree, the feature with the highest score is chosen as the splitting criterion. The significance of each feature ca n be determined by considering how much it contributes to overall improvement in the criterion. The gr eater the contribution, the more significant the feature. Feature importance can be used to identify the most relevant features for classification and for feature selection to improve the performance of the model.  