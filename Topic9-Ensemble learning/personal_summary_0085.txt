In this topic we have learn more about KNN and Decision Trees (Non-linear models).  K-Nearest Neighbors (KNN) and Decision Tree (DT) are two commonly used machine learning  algorithms in classification problems.  KNN is a non-parametric algorithm that classifies data based on the similarity of the new data  instance to the existing data instances in the dataset. KNN works by finding the K-nearest neighbors  to the new data instance and classifying the new instance based on the majority class of the K-  nearest neighbors. The distance metric used to calculate the similarity between instances can be  Euclidean distance, Manhattan distance, cosine similarity, and so on.  Decision Tree, on the other hand, is a tree-based model that recursively partitions the dataset into  smaller subsets based on the feature that provides the most information gain. Decision Trees can be  used for both classification and regression problems. The decision tree consists of nodes, branches,  and leaves. The nodes represent a decision or a test on a particular feature, the branches represent  the outcomes of the decision or the possible values of the feature, and the leaves represent the final  decision or the predicted class.  Both KNN and DT have their own strengths and weaknesses. KNN is simple and easy to understand,  but it can be computationally expensive when working with large datasets. KNN also suffers from  the curse of dimensionality, where the performance of the algorithm decreases as the number of  features increases. DT can handle both categorical and numerical data, and it is easy to interpret and  visualize the decision-making process. However, DT can be prone to overfitting if the tree is too  complex and may not generalize well to unseen data.  In practice, the choice of algorithm depends on the characteristics of the dataset and the problem at  hand. It is often a good idea to try multiple algorithms and compare their performances before  choosing the best one.  