The topic of this topic is K Nearest Neighbours (KNN) & Decision Trees (DT). KNN & DT are both supervised learning methods capable of classification and regression. KNN is a so called “lazy learner”   in   that   it   really   just   creates   a   data   storage   structure   for   the   training   data   that   can   be efficiently traversed to find k nearest neighbours from which a voting or averaging metric (i.e. mode for discrete data and mean for continuous data) is then computed to make a prediction. In many ways DT are similar to KNN. DT also use voting or averaging to perform predictions from a sub set of data that is considered to have similar features to the input data a prediction is being sought for. But instead of using the K nearest (distance wise) neighbours to find the most similar sub set, DT make a sequence of decisions to split the data into sub sets that have more similarity within the subsets after the split based on feature similarity metrics such as “impurity” minimisation for classification or MSE minimisation for regression. In both KNN and DT, the data is searched to find a subset of data that is considered to most closely match the input data, and then an aggregate metric (mean or mode etc) is computed from that most similar subset of training data in order to make a prediction.  READING To revise, I read the relevant sections of Hands-on Machine Learning with Scikit-Learn, Keras & relevant   section   of   the   course   notes. TensorFlow   by   Aurelien   Geron   and   the  SUMMARY OF THE MAIN POINTS FOR TOPIC 8 LEARNING  KNN Algorithm Outline:   Training: Build a data structure that allows finding “training” data points that are close in  distance to a “test” data point e.g. construct a kd-tree based on euclidean distance.   Testing/Prediction: Search for the k “training” data points that are nearest to the “test” data point and compute an aggregate measure of those k training data points as a prediction i.e. the mode or mean for discrete or continuous data respectively.  DT Algorithm Outline:  Training:   Recursively:  ▪ for each feature compute the optimal splitting of the data into 2 sub groups that separately have a reduced classification or regression error. The error measures can be for instance MSE for continuous data or entropy or gini index for discrete data. ▪ Choose the feature that makes the largest contribution to error reduction and split the  training data at with the optimal criteria for that feature,  ▪ repeat for each of the sub data set splits   Splitting can continue on any of the remaining features until various stopping criteria have been met such as each leaf node having the min number of training sample data points, or some maximum depth of the tree has been reached.   Testing/Prediction: Traverse the Decision Tree to find the sub group (leaf node) that best matches the test data point, and compute an aggregate measure of those k training data points as a prediction i.e. the mode or mean for discrete or continuous data respectively.    Pruning: is a technique that can improve model performance, reduce overfitting etc. Essentially nodes are removed under if certain criteria is met i.e. entropy reduction is not significant enough (according to some defined threshold). Pruning can be done while training or after training or both.   Major Advantages: Interpretability; Interpreting a DT is trivial  Major Disadvantages: Overfitting; Easily overfits without parameter tuning, pruning etc.    