Overview In topic 8, we focused on the following learning goals:    K nearest neighbor (KNN)   Decision tree (DT)  KNN Algorithm and its Variants The basic idea is to label the test data point the same as the nearest neighbour. This is useful so we can assign weights to the contribution of data point neighbours so the nearer neighbours contribute more to the average than more distant ones.  Example: if a black circle is placed in a region which is closest to a black ellipse with class label of 1, based on the nearest neighbour, this circle is labelled as class 1 as well.  KNN can vary however, for example, someone could check K nearest neighbours of the test point in order to make a decision; you can label a test instance to the same as the majority label of the k- nearest neighbours.  Theory of KNN We need to perform average or mean in continuous cases and to ﬁnd the mode of the class labels in discrete format in order to ﬁnd the majority of decisions based on KNN.  Best Number of Neighbours (K) For small values of K, we are restraining the region of a given prediction and forcing our classiﬁer to focus on close regions and ignore fairly distant points. This results in a low bias and high variance.   Higher values of K will mean smoother decision boundaries which means lower variance but increased bias. Basically, higher values of K means asking for more and more information even from distant training points.  Finding hyper-parameters such as K is not straightiorward. But you can use Cross-Validation to partition your data into test and training samples and evaluate your model with diﬀerent ranges of K values.  Decision Trees A decision tree is a map of the possible outcomes of a series of related choices. Decision trees can be used to weight possible actions against one another based on their costs, beneﬁts and probabilities.  Regression Trees Decision trees that use a regression model are called regression trees.  Watch:  htps://youtu.be/w4MnOA14pYs   Classiﬁcation Trees Just like regression trees, this term “classiﬁcation trees” refer to decision tree algorithms that can be used for classiﬁcation predictive modelling problems. It’s used to predict qualitative response. We replace the sum of square error by the classiﬁcation error rate as a criterion for making the binary splits.  Certainty of Distribution (COD) shows how certain it is that a classiﬁer sits inside a region. If COD is close to 1, it means almost all the training points inside a region are voting for a certain class label. So the classiﬁer is certain about the decision.  When COD is 0.5 it mean we cannot trust the votes because there’s a high classiﬁcation error.  Decision Tree Algorithms Some of the populate decision tree algorithms:    ID3 (Iterative Dichotomiser 3) uses entropy   C4.5 (Successor of ID3)   CART (Classiﬁcation and Regression Tree) uses Gini impurity  The ID3 Algorithm It’s used to generate a decision tree from a dataset. The basic algorithms is as follows:  1. Calculate the entropy of every feature using the dataset S.  a.  Split S into subsets using the feature for which entropy is minimum. So lesser values of entropy means it should be a good choice for selection of the feature and it will gain more information.  2. Make a decision tree node containing that feature 3. Recurse on subsets using remaining features   Deep Tree vs Shallow Tree Deep tree means you have low chances of visiting many training points in that sub-region. (high variance)  Shallow tree means you don’t have high variance, but you may have high bias.  To ﬁnd the sweet spot in terms of depth, you can perform cross- validation and other evaluation methods. Remember, you need to ﬁnd the proper hyperparameter which is the depth of the tree.  (refer back to topic 5-8)  Model Complexity and Pruning A technique that reduces the size of the decision trees by removing sections of tree that provide litle power to classify instances.  To avoid deep tree vs shallow tree problem, we can alternatively grow a large tree and then prune it back in order to obtain a subtree.  Ways of Pruning Trees Pre-pruning (forward pruning): In pre-pruning, we decide when to stop adding nodes during the building process (e.g. by looking at entropy)  Post-pruning (backward pruning): Here, we wait until the full decision tree is built, then check subtree and check whether to replace it with a single node or feature while incurring only a small amount of change in entropy.  Advantages and Disadvantages of Decision Trees + Very easy to understand as they represent rules.  + capable of modelling nonlinear functions  + Can handle categorical variables (i.e. weather being sunny vs cloudy)   -  Sensitive to small changes in the data. -  May overﬁt easily -  Only axis-aligned splits -  Trees may not be as competitive in terms of accuracy as some of the other regression and classiﬁcation techniques such as SVM or neural networks  Impact of Distance Metrics on KNN Performance KNN is a well know machine learning method that classiﬁes new data points according to their closeness to the closest neighbours in the training set using distance measures. It’s highly aﬀected by the distance metric that is selected. Cosine similarity may be beter than Euclidean distance in high-dimensional data.  Feature Importance of Using Decision Trees DTs are a popular machine learning algorithms. They work by recursively segmenting the data into subsets based on the most informative features until a stopping criterion is reach (i.e. Gini index is used to determine which feature is the most informative)  