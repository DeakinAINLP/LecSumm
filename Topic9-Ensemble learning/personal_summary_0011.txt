This topic’s content is a look into the K Nearest Neighbor (KNN) algorithm. and the Decision Tree. Another two highly important ML concepts. KNN Algorithm The KNN Algorithm is a simple machine learning algorithm that can classify or predict new instances of data based on the data points closest to it. This is where the name K Nearest Neighbor comes from. The model works by looking at K number of data points closest to the new data point and classifies this based on the average classification of the data points around it. There is also a distance weighted version of the KNN algorithm where the closer a data point is to the one being tested, the more it contributes to the average and ultimately to the classification assigned to the data point. Best Number of Neighbors (K) As with all models that we have learnt, it is not always easy to pick a hyperparameter such as K. The number of neighbors used will ultimately affect the variance and bias of the model. Using a higher K value will lead to lower variance but at the cost of increased bias. To determine the optimal K value, cross-validation can be used. In cross-validation, the data is split into train and test datasets. The model is then trained on the train dataset and tested on the test dataset. In the case of KNN, we can repeat the training for each possible value of K starting at 1 and then the testing. This can all be compiled and the optimal K value chosen. Decision Trees Decision Trees (DTs) are a supervised learning algorithm which can be used for classification and regression tasks. DTs are in essence a map of possible outcomes based on choices that are made and can be used to weight the risks, rewards, and probabilities of a certain decision. Regression Trees Regression Trees are a form of Decision Trees that use a regression model. Regression Trees work by finding a splitting criterion with the goal of partitioning the data into different subsets that have similar target values. This will continue until a stopping criterion is met at which point there will be leaf nodes that contain predicted values. Classification Trees Classification Trees are similar to Regression Trees except that CTs are used to predict qualitative responses as opposed to quantitative. It is otherwise very similar to Regression Trees. Decision Tree Algorithms There are a number of different algorithms that can be used for Decision Trees but 3 of the more popular ones are: - Iterative Dichotomiser 3 (ID3) -  C4.5 which is a successor to ID3 -  Classification and Regression Tree (CART) Taking a deeper look at ID3, this algorithm works in the following three steps: 1.  The entropy of every feature within the data set is calculated. The dataset is then split into subsets based on the feature for which entropy is minimum. The entropy being minimum suggests that this feature is a good choice for selection as more information will be gained by it. 2.  A decision tree node is made containing that feature. 3.  The subsets are recursed using all remaining features. When using any decision tree model, it is important to monitor the tree depth as a tree that is too deep will have a high variance whilst a tree that is too shallow will have a high bias. As such, it is important to perform evaluations on the model such as cross validation to find the optimal depth of the tree. Pruning Pruning is a process when making a decision tree model that, like pruning an actual tree, reduces the size of the tree. This is done by removing tree sections that do little to help classify instances.  As mentioned, a tree that is too shallow will have a high bias whilst a tree that is too deep will have a high variance, this is why pruning is necessary. Pruning is generally done one of two ways, either pre-pruning or post-pruning. Pre-pruning is done whilst building the tree. This is done by stopping the addition of nodes based on a specific criterion. This criterion could be that when the entropy reduction of splitting nodes is insignificant, the building does not continue. Post-pruning is done after the full decision tree is built. The tree’s attributes are then pruned by using subtree replacement. In subtree replacement, it is examined which subtree can be removed whilst introducing the lowest error. Once this is determined, the subtree is replaced with a single node. Assuming the subtree was relatively insignificant, this should only create a small change in entropy. Advantages and Disadvantages of Decision Trees Whilst decision trees are very useful, they do also have some drawbacks, both are detailed below. Advantages: -  DTs are easy to understand given that they are representations of rules. -  DTs are capable of modelling nonlinear functions. -  DTs can handle categorical variables that can’t be handled by other models. Disadvantages: -  DTs are extremely sensitive to small changes. The addition of a few data points could completely change the DT’s rules. -  DTs are very prone to overfitting. -  DTs can only make axis-aligned splits which can make modelling more complex DTs challenging. -  DTs are not as accurate as other regression and classification techniques. Impact of Distance Metrics on KNN Performance Given that there are multiple distance metrics that can be used for the distance calculation in a KNN model, it is important to select the correct metric based on the characteristic of the data. Each of these has its own drawbacks that might affect the model performance. Euclidean distance is the most commonly used metric for KNN. It is used to measure the straight-line distance between any two data points. Whilst this metric works well for datasets with continuous features that have similar scales, it is often not suitable for high-dimensional datasets or datasets with features that have different scales. As such, applying Euclidean distance to a dataset like this could lead to poor model performance. Manhattan distance, sometimes called city block distance or L1 norm, calculates the distance of datapoints by calculating the absolute differences in the coordinates of the two datapoints. Due to this, it works better than Euclidean distance on datasets with features that have different scales as well as datasets with categorical features. Cosine similarity is used in high-dimensional feature spaces. It works by calculating the cosine of the angle between the datapoints rather than the length of the distance between them. It will therefore perform far better in high-dimensional spaces than Euclidean would. Feature Importance of using Decision Trees DTs are able to provide information on the importance of the features within a dataset by determining how much each features contributes to the final decision making. Several key-points are considered when determining feature important. These include: - The Splitting Criteria - The Gini Importance - The Information Gain from the node - The Feature Usage or number of times the feature is used in order to split nodes across the tree.       