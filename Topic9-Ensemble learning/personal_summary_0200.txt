This topicâ€™s content focus mainly on KNN algorithms and trees such as, decision, regression, and classification trees.  KNN Algorithm  KNN stands for K-Nearest-Neighbors which is a supervised machine learning method. A useful technique for KNN and its variant is to assign weights to the contribution of data point neighbors. We need to label the test data point as the Nearest neighbor, then we simply label the point to the closest nearing neighbors and label it.  The above formula calculates the distance between the data points and its neighbors. Based on its result, the Euclidean formula will give the output of the nearest neighbor.  Best number of neighbors (K)  The number of neighbors K are very important as it will determines the shapes of the data. Furthermore, higher values of K will have smoother decision boundaries while having lower variance but increased bias. We can determine the maximum values of K by using the training points.  Decision Trees  Decision tree is a map of possible outcomes of related choices. They can be used to weigh possible actions against one another based on the cost, probabilities and benefits. Furthermore, they starts with a root node then branching on into different outcomes.  Regression trees  Regression trees is a decision tree-based algorithm that predicts numerical or continuous values. It uses optimal splits to minimize variance of the predicted values within each leaf. These trees are really popular for business companies all around the world.  Classification Trees  Classification Trees is used for classification or regression predicting models. It is used to predict a qualitative response rather than a quantitative response.  Furthermore, I have also learnt about Ginti and Entropy which is used for the measurement of inequality. Gini index is defined as:  On the other hand, Entropy is defined as  