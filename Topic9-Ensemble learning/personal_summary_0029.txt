K-NEAREST NEIGHBOURS  K-Nearest Neighbours is a supervised learning model used in both classification and regression models. KNN assigns weights based on the neighbours, where nearer neighbours contribute more than distant ones. A test data instance should be assigned the same label as the majority of its nearest neighbours. The only hyper parameter is K (the number of nearest neighbours). Finding the correct value of K determines the accuracy of the model.  This majority voting mechanism is dependent on the type of problem. For classification problems the mode of the class labels is used. For regression problems instead the average or mean distances are taken. In some cases distance-weighted algorithm is used, which assigns weights based on their distance from the test point. These weights can be the inverse square of distances, so lower distances have higher weights.  The value of K changes the shape of the decision boundary. Small values force the model to focus on close, dense regions, creating low bias and high variance. High values instead look at distant neighbours, lowering variance but increasing bias. The ideal value of case will find the middle-ground between bias and variance, though in practice this can be difficult.  Finding the ideal value is typically done through cross-validation and evaluation on different values of k. The evaluation type depends on if it is a regression or classification model. This makes the learning process simple as only a single hyper parameter needs finding. Classification problems are time consuming due to the pairwise distances of every training instance.  KNN is rarely practical for real-world problems but can be used as a foundation to find a minimal boundary for performance. Its primary weakness is that it is computationally expensive for large data sets. Distance metric selected has a large impact on KNN.  DECISION TREES  Decision trees map all the possible outcomes of a series of choice, where choices are based on some constraint on a feature. Typically starting from a single node and branching into a tree-like structure. Simple but not as accurate as some models.  Feature space can be partitioned into regions. Each region will represent a leaf-node in the tree (the ultimate outcome of a chain of decisions). This will generate s + 1 regions, where s is the number of splits. A model can be fit to each sub-region after partitioning. The type of models used inform the type of tree – regression models are regression trees and classification models are classification trees.  BUILDING DT  The process of building a DT starts with partitioning the feature space into distinct and non-overlapping regions. Then for all data instances that fall into a given region R, they should have the same prediction, the mean of training data for regression and mode of training data for classification. Naturally this process incurs a training  error that needs to be minimised.  MINIMISING ERROR  Finding every possible partition of feature space is computationally infeasible (factorial runtimes) so instead a top-down greedy algorithm is used, recursive binary splitting. Select the best feature Xi and threshold s. Split the feature space into two regions where features below the threshold are put in the left region, features above the threshold to the right region. The “best” feature and threshold combination is the one that leads to the best possible reduction in training error. Then the process is repeated, selecting the next best feature and threshold from ONE of the two created regions. This recursive process repeats until a stopping criterion is reached. Stopping criteria could include tree depth limit, size of regions, etctera.  The classification error rate is used as the criterion for binary splits. This CER (E) is the fraction of training instances in that region that don’t belong to the most common class. However it’s not very sensitive for tree growing so alternatives like Gini Index and Entropy are often used. Gini Index measures node purity. Entropy is a probabilistic metric. Ultimately the goal is to reach a smoother classification error.  Tree depth plays an important role in determining the accuracy of the model. A deep tree can lead to high variance because specific regions are less likely to be visited due to the sheer volume of them. On the other hand a shallow tree can introduce bias. A good balance should be met. Unimportant branches can be removed from a tree to reduce its size and reduce overfitting. Pre-pruning can be incorporated by deciding when to stop adding regions while the tree is under construction. On measuring the Entropy reduction when adding a node, should it be sufficiently low then that node won’t be split. However this method doesn’t take into account the potential combined significance of all potential regions that sub-tree would have generated.  Having only a small number of regions can lead to high bias, so the pruning needs to be controlled. Building a tree and then pruning it through post-pruning solves this problem. Post-pruning is implemented on the full decision tree and removes uninformative branches by replacing them with a single leaf node. The subtree is chosen by which one introduces the least error, thus lowest change in entropy, on removal. 