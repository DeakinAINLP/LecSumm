 This topic, we have learned about linear methods for classification models. In this  topic, we first talked about KNNs. It is a useful technique that can be used to assign weights to the contribution of neighbouring data points. The nearer the data point, the greater the weight assigned. The K in KNNs refers to the number of neighbours around a specific data point. So, 3-NNs (which stands for ‘Nearest Neighbours’) implies that a particular data point is being assigned wait based of its three nearest neighbours. KNNs also use the Euclidean distance to measure the distance between two data points w with respect to the point being considered. It also uses the Voronoi Diagram, which partitions a plane into regions based on distance two points in a specific subset of the plane.  In this topic, we also have learned how to select the suitable number of nearest  neighbours (or K). You can think of this number is controlling the shape of a decision boundary just like when we talked about that of SVMs. So, for small number of values, we are restraining the region of a given prediction and forcing the model to focus on close regions and neighbours, which means high variance but low bias. Likewise, the higher values of K will have smoother decision boundaries, which means low variance but high bias. One method we can use is Cross-Validation, where we can evaluate the model based on the training and test data. You can also use misclassification error, which can decide which value gives the best performance. This implies the best value of K will have the lowest misclassification error.  In this topic we have also learned about decision trees, which is basically a map for all possible outcomes from a series of related choices. They can be used to weight possible actions against one another based on their costs, benefits, and probabilities. A decision tree typically starts with a single root node, which branches into possible outcomes, or more branches. There are known to be two types of decision trees: classification trees and regression trees. They can also be called CART, which stands for Classification and Regression Trees. It's like regression trees, except it is used to predict a qualitative response rather than a quantitative response. It works by dividing in this feature space into a set of possible non- overlapping regions. We find these regions for the data points first, to minimise the training error. We take on a top-down, greedy approach for this that is known as ‘recursive binary splitting’.  A regression tree would take on the sum of squared error as the misclassification error. But for the classification counterpart, however, we use the Gini Index (Considered as a measure of node purity), Entropy, and Information Gain. We have also come about three of the most popular decision trees: ID3, C4.5, and CART. In case of overfitting or underfitting in the decision tree model, we can use pruning as a technique to reduce the size of decision trees by removing sections of trees that provide little importance to classify data instances. There are known to be two types of pruning techniques: Pre-pruning and Post-pruning. PRI pruning allows you to decide during the building process when to stop adding new nodes, why post pruning wait till the full decision tree has been built and then reduces the attributes wherever unnecessary. Decision trees are very easy to understand, capable of modelling nonlinear functions, and can handle categorical variables. However, they are sensitive to small changes in the data, which can lead to overfitting very easily. Furthermore,    they may not be as competitive in terms of accuracy as to other techniques like SVM and neural networks.  