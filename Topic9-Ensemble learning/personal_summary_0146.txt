Following up last topic’s concepts of SVM (Support Vector Machine), this topic we explored the concepts of KNN (K nearest neighbor) algorithm,its variants and Decision tree (DT).  ● KNN is a method used for making predictions based on the data points that are closest to a new data point. KNN calculates the distance between the new point and all the training points. The most commonly used distance metric is Euclidean distance, but other metrics such as Manhattan(cityblock) or cosine distance can also be used.  Then the algorithm selects the K nearest neighbors to the new data point based on the calculated distances. For classification tasks, the algorithm counts the number of neighbors in each class and assigns the class label with the highest count as the prediction for the new data point, whereas, for regression tasks, the algorithm takes the average of the target values of the K nearest neighbors as the predicted value.  Once the prediction is made, the algorithm outputs the predicted label or value for the new data point.  ● The best number of neighbors (K) in K-Nearest Neighbors (KNN) algorithm depends on  the specific dataset and problem at hand. There is no one-size-fits-all, however Cross-validation techniques, such as k-fold cross-validation, can be used to evaluate the performance of the model for different values of K. By comparing the performance metrics for different K values.  ● A decision tree is like a flowchart that helps make decisions, it consists of a tree has branches that represent different choices or conditions, and each branch leads to more branches or an outcome. Decision trees are easy to understand and can handle different types of data.  ● Some of the most popular Decision Tree Algorithms are:  1.  ID3: It looks at different characteristics of the data and chooses the most informative one to make decisions. It keeps doing this recursively to create a tree that helps classify or predict outcomes.  2. C4.5: It is an improved version of ID3 that handles more types of data. It  uses a ratio to determine which characteristic is the most useful in making decisions  3. CART: It can be used for classification or regression tasks. It uses a  measure called Gini index to find the best characteristic to split the data and create a binary tree.  Proof of completion of topic 8 Quiz  (Pass activity task will be in a separate .ipynb file)   