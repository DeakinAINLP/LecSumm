 A highly parallel architecture and networks of interconnected neurones characterise our brain. Artificial neural networks (ANNs) were developed because of biological neural systems. What distinguishes neural networks?   When significantly nonlinear support exists in the underlying functions or decision    boundaries, linear models may not be adequate. While creating nonlinear functions, vector machines employ fixed feature transformations that depend on the kernel function. For instance, if you choose a linear kernel, it stands to reason that the output of your model will also be linear.   What happens if you are uncertain about the kernels or prefer to learn these properties directly from the data? With the use of neural networks, feature modifications can be learned from data.  Two ANN research teams are tackling this issue in the field of machine learning:   One team models and studies the brain using ANN.  The other group utilises the brain as inspiration to create ANNs that are efficient  learning devices, however this approach might not produce a genuine model of the brain.  Neural System Basics  Let's introduce the main participants in a neural network system while keeping in mind the brain: Typically, a neural network (machine) will have an input layer, one or more hidden layers, and it has an output layer, combiners (sum functions), and nonlinear activation functions.  Complexity- Because neural networks work with high dimensional inputs and multi-label classification, we can build larger, more complicated neural networks. Just keep in mind that if you don't provide your neural network enough training data, it could over-fit.  Perceptron Algorithm  A single layer neural network called a perceptron is a linear classifier (binary). The term "neural network" refers to a multi-layer perceptron. In conclusion, a perceptron is a  straightforward neural network that is utilised for binary classification. There is simply one layer and one node.  Motivation for Multilayer Perceptron  Using a single line to divide the data points according to their class labels is it always possible?  Think about the AND and OR logical gates. We can demonstrate that in class labels of 1 and 0, both issues are linearly separable. Based on the labels of the classes, we can quickly choose a line to split. However, what about the XOR issue? Linearly separating this issue is not possible. As it is impossible to divide the data points by a single line according to their class designations. A new neural network was created to address these kinds of issues, but this time with a layer in the centre. The ability of a multilayer perceptron (MLP) to represent the XOR problem was later demonstrated.  Multilayer Perceptron  A perceptron's capacity for representation is extremely limited. A multi-layer network is required for complex, non-linear decision surfaces.  If you recall, a perceptron's sign function gave us a -1,1(binary) output. However, by selecting a more complex activation function, the network can integrate the inputs in more complex ways, giving it a wider range of functions to mimic. A number between 0 and 1 with an s-shaped distribution is produced by non-linear functions like the logistic, commonly known as the sigmoid function. In a multi-layer network, node selection should be continuous but should also be a continuous meaningful function, such the sigmoid function. We have a function, like a perceptron, but this time it is a sigmoid function.  Feedforward Neural Network  An Artificial Neural Network (ANN) that uses feedforward learning is one in which connections between units do not cycle. Information in this network only travels in one direction: forwards, from the input nodes to the output nodes via any hidden nodes that may exist. The network contains neither loops nor cycles.  The term "multi-layer perceptron" (MLP) is also used to describe a multi-layer feed-forward neural network (NN). Since the model consists of many layers of models that resemble logistic regression (with continuous non-linearities) as opposed to multiple perceptron (with discontinuous non-linearities), the label "MLP" is really a fairly accurate description of the model.  Think of a two-layer network with input, hidden, and output layers. You should consider the following highly significant facts regarding this network:   There are two types of weights in the output, which is a vector: input, hidden input,  and output.   The input layer relays the input vector only; it makes no calculations. It may contain  more than one hidden layer.   The fact that it is not required to be fully linked is another intriguing feature.  Though it initially appears to have a straightforward structure and style, its strength will become apparent. One of the earliest uses of ANNs in self-driving cars and image processing is depicted in the figure below. The output of this ANN is the degree of steering in a specific direction, as one might anticipate.  Now let's examine the MLP's conceptual underpinnings. Finding the network weights will enable the projected values to be as accurate as feasible given the input and intended output. In other words, we want to minimise the error, which is the difference between the output's true value and the anticipated value. Finding w to minimise the error function is the above-mentioned optimisation challenge. Gradient-descent minimisation will be used in this situation. E(w) is not convex; instead, it is a complicated function that may have numerous local minima. We will employ the Backpropagation technique to resolve this issue.  Detour: Gradient-based Optimisation  When using gradient-based optimisation techniques, the search directions are determined by the function's gradient at the current point. You can apply a similar technique to maximise f(x), known as gradient-ascent, based on prior descriptions of how to maximise the function f.  Gradient-descent locates the optimal point by determining the actual movement in the direction of the optimal point as well as its magnitude. Even in ANNs, it is a very helpful method. If we apply this concept to the error function we previously used, we may reduce the difference between the actual output and the expected output throughout each iteration. We require an update rule (where t stands for the current training sample) to minimise this function based on the gradient-descent.  So, rather than minimising E(w), the stochastic gradient descent (SGD) minimises the instantaneous approximation obtained by employing only the t-th instance. In a stochastic sense, SGD is easy to implement and is assured to arrive at a local minimum.  Backpropagation Algorithm  MLP training is done using the backpropagation technique. The fundamental idea behind training MLPs is a stochastic gradient-descent rule, as you discovered in the last session. The training data serves as the input for the gradient-descent rule that we have already demonstrated. The weights will be initialised at the beginning, and you will continue to adjust them until the stopping criteria is satisfied (which may be accuracy, iterations, or amount of change).  Backpropagation is a method that is frequently used in machine learning to train feedforward artificial neural networks or other parameterized networks with differentiable nodes. Reverse accumulation or the reverse form of automated differentiation are other names for it. For a single input-output example, backpropagation efficiently calculates the gradient of a loss function with respect to the network weights, computing the gradient one  layer at a time and iterating backwards from the last layer to avoid duplicating calculations of intermediate terms in the chain rule. This can be derived from dynamic programming. Most often, gradient descent or its derivatives, like stochastic gradient descent, are used. Backpropagation strictly only relates to the algorithm used to compute the gradient and not how the gradient is applied.  Introduction to Deep Learning  A deep learning model is intended to continuously analyse data using reasoning that is akin to how a human would come to conclusions. Deep learning use ANN-like layered algorithms to do this.  Deep learning techniques use sophisticated neural networks. Many practical tasks (such handwritten digit identification and image recognition) have been learned by them with success. Common deep learning architectures include:   networks that use convolution  Autoencoders  Broad Belief Networks  Boltzmann Machines  Boltzmann machines with restrictions  Deep Boltzmann Systems  In-depth neural networks  Convolutional Neural Network (CNN)  The mammalian visual cortex, the area of the brain where visual information is processed, serves as a model for the architecture of a convolutional neural network (CNN or ConvNet). Specific neurones in the visual cortex only activate when certain occurrences are visible in the field of view. One neurone may only fire when you are gazing at a horizontal line, while another may only fire when you are looking at a left-sloping diagonal line. Images are processed by our brains in layers of increasing complexity. Lines and curves are basic characteristics that are distinguished in the first layer. At deeper levels, the brain can distinguish between, say, a house or a bird, depending on how the edges and colours are arranged.  Three fundamental ideas make up CNNs:   Sparse Interactions: Sparse weights are used in a smaller kernel rather than the  entire input for sparse interactions. The number of parameters is lowered as a result. The term "kernel" in CNN often refers to an operator used to change the data encoded in the pixels by applying it to the entire image.   Parameter Sharing: When applying to various places (sliding windows), a kernel  utilises the same set of weights.   Translation Invariance: An object is said to be invariant if you can still identify it as such even if its appearance changes in some way. In general, this is advantageous  since it enables the separation of an object's identity or category from the particulars of the visual input.  LeNe5  Yann LeCun unveiled the original CNN, dubbed LeNe5. Fundamental was the LeNet5 architecture. Specifically, the understanding that image features are dispersed over the entire image and that convolutions with learnable parameters are a useful method to extract related features at many locations with minimal parameters.  Finally, the following is a list of LeNet5 features:   using a series of three layers in a convolutional neural network: To extract spatial characteristics, convolution is used in combination with pooling and nonlinearity.   subsamples utilising the spatial mean of maps.   To reduce the overall computing cost, an MLP is typically used as the final classifier.  tanh- or sigmoidal-based nonlinearity  Application of CNN  Each network layer in a CNN serves as a detection filter for certain features or patterns contained in the original data. Large features that are very simple to identify and interpret are detected by a CNN's initial layers. To discover a specific portion of a pattern or texture, each image filter patch is used, in other words. A substantial data set is required to train these deep networks; otherwise, the findings may be meaningless.  What then has given deep learning a boost in the field:   Ability to use new training methods to model larger models.  substantial ImageNet dataset  using graphical processing units (GPU) that are quick.  The most crucial answers to the numerous problems deep learning where first encountering are these three elements.  Autoencoder  A neural network that can manage multiple hidden layers in its structure is called an autoencoder. An autoencoder's goal is to discover a representation (encoding) for a set of data, usually to reduce the dimensionality of the data. This neural network is trained to try to replicate its input in its output. It contains a secret layer Z on the inside that defines the code used to represent the input. For learning generative models of data, the autoencoder concept has recently gained in popularity.  An autoencoder learns to condense data from the input layer into a compact code, which it can subsequently decompress into data that is nearly identical to the original. As a result, the Autoencoder is compelled to do dimensionality reduction. Since we essentially want outputs that are the same as inputs, the loss function of the autoencoder should be able to detect differences between input and output.  Another method of feature learning is with  autoencoders. If there are limitations on the number of nodes in the hidden layers, the solution is trivial, meaning it is absurdly straightforward and of little interest. A PCA is a linear Autoencoder with KN. Its strength comes from its non-linearity.  The original feature vector can be recreated using this hidden layer of codes following the learning process. This hidden layer, which is an encoded representation of the inputs, is significantly more significant and less substantial. The encoded hidden layer or h_3 could therefore be used instead of the entire big feature vector (v).  