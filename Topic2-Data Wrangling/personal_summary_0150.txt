  Summary:  o  More Supervised Learning:  ▪  Motivation to Neural Networks: Artificial neural networks (ANNs) play an important  role  in  machine  learning  and  AI,  modelling  complicated patterns  and  predicting  difficulties.  They  have  remarkable  pattern recognition, flexibility, noise tolerance, the capacity to handle enormous datasets,  and  the  ability  to  predict  non-linear  connections.  These characteristics  make  them  valuable  in  real-world  applications  like  as recommendation  systems,  self-driving  cars,  medical  diagnostics,  and language  translation.  However,  neural  networks  may  be  complicated, computationally costly, and frequently require a large amount of data to operate properly. They are also referred to as a "black box" owing to the difficulties  in  deciphering  its  conclusions.  Despite  these  difficulties, understanding ANNs is critical for people interested in machine learning and artificial intelligence.  ▪  Perceptron: The Perceptron is a simple neural network design that works as a binary classifier. It aggregates input into a weighted total, returning 1 if the value exceeds a given threshold, otherwise  -1 (or 0). Initialising weights and thresholds, calculating actual output, and adjusting weights based on the difference between desired and actual output are all part of perceptron learning. If the training set is linearly separable, the technique can  obtain  a  perfect  classifier  in  a  finite  number  of  steps.  It  will  not, however,  converge  if  the  classes  are  not  linearly  separable.  The Perceptron,  despite  its  simplicity,  has  opened  the  path  for  more complicated neural networks. Nonetheless, its single-layer structure can only  categorise  data  that  is  linearly  separable.  By  incorporating  hidden layers  and  nonlinear  activation  factors,  Multilayer  Perceptron’s  bypass this barrier, allowing for larger categorisation problem solutions.  ▪  Multi-layer  Perceptron  (MLP):  A  Multilayer  Perceptron  (MLP)  is  a sophisticated  artificial  neural  network  that  outperforms  the  ordinary perceptron. It has at least three node layers (input, hidden, and output) and  uses  a  non-linear  activation  function  on  its  inputs.  MLPs  learn, predict,  calculate  error,  and  update  weights  using  a  backpropagation method.  Given  enough  nodes  and  training  data,  it  can  simulate  any function and is utilised in a variety of applications including as voice and picture recognition. MLPs, on the other hand, have downsides. Because of their complexity, they can be difficult to comprehend, can overfit the training data if not adequately regularised, can suffer from disappearing or bursting gradients when training deep networks, and training can be computationally  demanding.  Regardless,  MLPs are  an  important  tool  in machine learning and deep learning.  ▪  Connections  to  Deep  Learning:  Deep  Learning  is  a  subtype  of  machine learning  that  processes  data  in  layers  using  algorithms  inspired  by  the   layers  and  complicated  organisation  of  the  brain,  notably  artificial  neural  networks.  Machines learn from labelled data to predict outcomes in supervised learning. Deep learning  networks  may  also  be  taught  supervised,  mapping  input  to output using such data. MLPs (Multilayer Perceptron’s) are a basic type of deep neural network. The  phrase  "deep  learning"  is  usually  linked  with  networks  that  have numerous  hidden topologies,  such  as Convolutional  Neural  Networks  (CNNs)  and  Recurrent  Neural  Networks (RNNs). CNNs use convolutional layers that glide over the input data to analyse grid-like  data,  such  as  pictures.  RNNs  include  loops  that  allow  for  the information,  making  them  excellent  for  processing persistence  of sequential data such as time series or natural language. Deep  learning  models  are  often  trained  by  stochastic  gradient  descent variations, which are frequently combined with backpropagation. Despite their complexity, deep learning models' capacity to learn from enormous amounts  of  data  and  capture  complicated  patterns  has  resulted  in substantial advances in a variety of machine learning disciplines.    Reading list: Lecture Slides, Lecture Recordings, Learning Contents.   My  reflections:  Because  of  their  superior  pattern  recognition,  flexibility,  noise tolerance,  and  capacity  to  handle  big  datasets  and  non-linear  connections, artificial neural networks (ANNs) are critical for AI and machine learning, modelling complicated patterns and predictions. They may, however, be sophisticated and computationally intensive, and their decision-making process can be difficult to understand. A  basic  binary  classifier,  the  Perceptron  combines  input  and  produces  a  result depending on a threshold. It can discover a perfect classifier for data that is linearly separable, but not for data that is not linearly separable. It paved the way for more complicated neural networks. MLPs are sophisticated neural networks with at least three layers of nodes that employ non-linear activation functions. They are utilised in a variety of applications and can represent  any  function  given  enough  nodes  and  training  data.  They  can,  however, overfit, have gradient problems, and their training can be computationally demanding. Deep learning, a subtype of machine learning, processes data in layers using algorithms inspired by brain anatomy. Supervised deep learning networks may be trained. While MLPs  are  a  fundamental  type  of  deep  neural  network,  the  phrase  "deep  learning" typically refers to networks with many hidden layers and complicated topologies, such as  Convolutional  Neural  Networks  (CNNs)  and  Recurrent  Neural  Networks  (RNNs). Deep learning models, despite their complexity, can learn from vast volumes of data and  recognise  complicated  patterns,  leading  to  substantial  advances  in  numerous machine learning disciplines.  