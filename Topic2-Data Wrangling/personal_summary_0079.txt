This topic, we will talk about Artificial Neural Networks (commonly called  ANNs). We will also talk about Neural Networks that will become a base of a field in Machine learning that we call Deep Learning.  In this topic, we learned that the development of ANNs is in fact motivated by  biological neural systems. We learned about some of the concepts from Neurology like Neurons and Brain Function. Neural Networks happen to distinguish themselves in a way as follows. Linear models may not be sufficient when the underlying functions or decision boundaries are extremely nonlinear. Likewise, SVMs happen to construct nonlinear functions but use fixed feature transformations, which depends on the kernel function. Neural Networks on the other hand, allow the feature transformations to be learnt from data. This topic, we also learned some of the basics of the Neural system basics. A typical neural network comprises of input layers, hidden layers, combiners (sum functions), nonlinear activation functions, and output layers. Neural Networks are usually more complex, bigger neural networks because neural networks are compatible with high dimensional inputs and multi-label classification. One thing to note is that having a more complex neural networks can result in over-fitting if you are not providing enough training data. We also deep dived into the mathematical concepts in the making of the Perceptron Algorithm. We also learned that a feedforward neural network is the kind of ANN where connections between units do not form a cycle. In this network, the information moves in only forward, from the input nodes, through the hidden nodes (if any) and to the output nodes.  We also learned that a multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer Perceptron (MLP). The term MLP is really an accurate name because the model comprises multiple layers of logistic regression like models rather than multiple perceptions. A multilayer perceptron resembles an XOR Logic Gate while separating data instances. Some of the important facts about this network are as follows: the output we get is a vector. There are two types of weights in an MLP: one between the input and hidden layer and one in between hidden and output layer. The input layer only relays the input vector. Hidden layers can be more than one in number. We later learned about the formulation of the MLP including the Gradient Descent Optimization. We also looked at the backpropagation algorithm which is used for training MLPs. In essence, this algorithm helps in adjusting the weights of the MLP along the way before it reaches the stopping criteria.  We moved on to Deep Learning and we learned some of the concepts of the basics at a basic level. A deep learning model is designed to continually analyse data with a logic structure like how a human would draw conclusions. To achieve this, deep learning uses a layered structure of algorithms like ANNs. Deep learning methods are advanced neural networks. They have been successful in learning many real-world tasks, including handwritten digit recognition, and image recognition. Some of the common Deep learning architectures include Convolutional Networks and Autoencoders. Convolutional Neural Networks (or simply just CNN) every network layer acts as a detection filter for the presence of specific features or patterns present in the original data. The first layers in a CNN detect large features that can be    recognized and interpreted relatively easily.  For such architectures, please keep in mind that you need a large set of data for feeding these deep networks, otherwise the results are useless. They also need special hardware called a GPU to compensate for the computation time it usually takes. An Autoencoder is a neural network which can handle many hidden layers in its structure. The aim of an Autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. This type of neural network is trained to attempt to copy its input to its output. Internally, it has a hidden layer that describes a code used to represent the input. Recently, the Autoencoder concept has become more widely used for learning generative models.  