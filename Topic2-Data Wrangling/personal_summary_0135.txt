1.1  Motivation and inspiration Development of Artificial Neural Networks (ANNs) is motivated by biological neural systems. Neural Networks allow the feature transformations to be learnt from data.  1.2  Neural system basics There is always an input layer which is responsible for taking the input features. We can have more complex, bigger neural networks because neural networks are compatible with high dimensional inputs and multi-label classification. More complex neural networks may lead to overfitting if not enough training data is provided.  1.3  Perceptron algorithm Perceptron  is  a  linear  classifier  (binary)  and  is  a  single  layer  neural  network.  A  multi-layer perceptron is called a neural network.  1.4  Motivation for multilayer perceptron Multilayer perceptron (MLP) can represent the XOR problem.  1.5  Multilayer perceptron A perceptron is quite weak in what it can represent. For complex, non-linear decision surfaces, we need a multi-layer network.  1.6  Backpropagation Algorithm The basic concept of training MLPs is a stochastic gradient-descent rule.  1.7  MLP in Python  1.8  Introduction to Deep Learning A deep learning model is designed to continually analyze data with a logic structure similar to how a human would draw conclusions. Deep learning methods are advanced neural networks.  1.9  Convolutional Neural Networks The  architecture  of  a  Convolutional  Neural  Network  (CNN  or  ConvNet)  is  modelled  after  the mammalian visual cortex. CNNs are made of three basic concepts:  Sparse interactions Parameter sharing Translation invariance  1.10 Autoencoder An Autoencoder is a neural network which can handle many hidden layers in its structure.  The aim of an Autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.  