In this topic we were introduced to the concept of Perceptron and Deep Learning. Deep Learning is a kind of Supervised Learning. Neural Networks are used in Deep learning. Linear machine learning models and SVMs which depends on kernel function might not be sufficient for prediction in case of extremely non-linear data. In such cases Neural Networks are being utilized. It can also be used for linear modelling. ANN (Artificial Neural Networks) are motivated by biological neural systems. An ANN consists of one input layer, one output layer and one or more hidden layers. Complexity of model increases with increase in hidden layers.  Perceptron is a simple neural network used for binary classification. It has only one layer with single node. It takes a set of inputs, multiplies each input by its corresponding weight, and then applies an activation function to produce an output. The activation function is typically a step function that maps the weighted sum of inputs to either a 0 or 1, representing two classes or binary states. The learning algorithm follows these steps:   Initialize the weights and the bias term to small random values.   For each training example, compute the weighted sum of the inputs.   Apply the activation function to the weighted sum to obtain the predicted output.   Compare the predicted output with the desired output.   Adjust the weights and the bias term based on the error.   Repeat steps 2-5 for a specified number of iterations or until convergence.  Perceptron is weak for complex, non-linear decision surfaces, hence we use multi-layer network. We can use different activation functions like Sigmoid function in Multi-layer Perceptron (MLP) to capture input data and get output data. The MLP consists of an input layer, one or more hidden layers, and an output layer. A feed-forward neural network (also known as a feed-forward neural network or a feed-forward artificial neural network) is a type of artificial neural network where the information flows only in one direction, from the input layer to the output layer. Multi Layer feed forward Neural Network is also known as MLP.  The multi-layer perceptron (MLP) is formulated by defining the network architecture, choosing an activation function, and specifying a loss function. The network consists of input, hidden, and output layers. The activation function introduces non-linearity to the network, and forward propagation computes the weighted sum of inputs and applies the activation function to obtain outputs. A loss function measures the network's performance by comparing predicted outputs with desired outputs. Back-propagation calculates gradients and updates weights and biases. Training involves iterating forward propagation and back-propagation on a training dataset. Finally, the trained network the multi-layer perceptron (MLP) is formulated by defining the network architecture, choosing an activation function, and specifying a loss function. The network consists of input, hidden, and output layers. The activation function introduces non- linearity to the network, and forward propagation computes the weighted sum of inputs and applies the activation function to obtain outputs. A loss function measures the network's performance by comparing predicted outputs with desired outputs. Back-propagation calculates   gradients and updates weights and biases. Training involves iterating forward propagation and back-propagation on a training dataset. Finally, the trained network can make predictions by feeding new data through the network and make predictions by feeding new data through the network. There are some disadvantages with back propagation like the tendency of network to memorize all training samples leads to poor generalization which usually happens when there is too many hidden nodes,  Deep Learning models are advanced neural networks. They are used in handwritten digit recognition, image recognition etcetera. Convolutional Neural networks, Autoencoders, Deep Neural Networks etcetera are some common Deep Learning architectures. CNN or Convolutional Neural Network. The key idea behind CNNs is to exploit the local spatial correlations present in grid-like data. Unlike traditional neural networks, which process each input feature independently, CNNs apply convolutional operations to capture spatial patterns and relationships. This is achieved through the use of convolutional layers, pooling layers, and fully connected layers.  