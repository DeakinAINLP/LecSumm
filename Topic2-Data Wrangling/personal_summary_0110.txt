Topic 10  Pass Task  1.Summarise the main points that are covered in this topic.  I have learned about the below topics in topic10.    Neural system basics   Perceptron algorithms   Multilayer perceptron   Backpropagation algorithm  Introduction to Deep learning   Convolutional Neural Networks   Autoencoder   Deep learning with python  2. Provide summary of your reading list â€“ external resources, websites, book chapters, code libraries, etc.  I have read the all the modules in the unit site for topic 9.  The below are the references:  Learn Python - Free Interactive Python Tutorial  https://www.w3schools.com/python/default.asp  3. Reflect on the knowledge that you have gained by reading the contents of this topic with respect to machine learning.  Neural system basics:  Neural systems, also known as neural networks or artificial neural networks, are computing systems inspired by the structure and function of the human brain. They consist of interconnected artificial neurons, or nodes, that process and transmit information. Neural systems are used for various tasks, including pattern recognition, classification, regression, and optimization.      Here are some key components and concepts related to neural systems:  1. Neurons: Neurons are the basic building blocks of a neural system. They receive input signals, apply transformations, and produce output signals. Neurons are interconnected in layers to form a network.  2. Layers: A neural network typically consists of multiple layers of neurons. The input layer receives the initial input data, and the output layer produces the final output or prediction. There can be one or more hidden layers between the input and output layers, which perform intermediate computations.  3. Weights and Biases: Each connection between neurons in a neural network is associated with a weight. Weights determine the strength of the connection and affect the influence of one neuron's output on another's input. Biases are additional values added to the inputs of neurons, introducing an offset or threshold.  4. Activation Function: An activation function is applied to the output of each neuron. It introduces non-linearity and determines whether the neuron should be activated or not based on its input. Common activation functions include sigmoid, ReLU (Rectified Linear Unit), and softmax.  5. Forward Propagation: Forward propagation is the process of passing input data through the neural network layer by layer, applying weight and bias transformations and activation functions. It produces the final output or prediction of the network.  6. Loss Function: A loss function measures the discrepancy between the predicted output of a neural network and the actual output. It quantifies the network's performance and is used to optimize the network's weights and biases during training.  7. Backpropagation: Backpropagation is an algorithm used to update the weights and biases of a neural network based on the calculated loss. It involves propagating the error gradient from the output layer back to the input layer, adjusting the parameters to minimize the loss.          8. Training: Training a neural network involves iteratively presenting training examples, calculating the loss, and updating the network's parameters using backpropagation. The goal is to minimize the loss and improve the network's performance on the training data.  9. Validation and Testing: After training, a neural network is evaluated on separate validation and testing datasets. The validation dataset helps in tuning hyperparameters and assessing the generalization performance of the network. The testing dataset provides a final evaluation of the network's performance on unseen data.  10. Hyperparameters: Hyperparameters are configuration settings of a neural network that are set before training and affect the learning process. Examples include the learning rate, number of hidden layers, number of neurons per layer, and activation functions.  Neural systems are powerful machine learning models capable of learning complex patterns and making accurate predictions. However, they require careful design, tuning of hyperparameters, and sufficient training data to achieve optimal performance.  Perceptron Algorithm:  The Perceptron algorithm is a simple yet fundamental algorithm for binary classification. It is a type of linear classifier that learns to classify input data into two classes based on a linear decision boundary. The algorithm was introduced by Frank Rosenblatt in 1957 and is considered one of the building blocks of neural networks.  Here are the main steps of the Perceptron algorithm:  1. Initialize Weights and Bias: Initialize the weights (coefficients) and bias term (intercept) to small random values or zeros.  2. Input and Output Data: Provide the input data features (X) and corresponding target outputs (y) for training.  3. Activation Function: Choose an activation function, commonly the step function or sign function, which maps the weighted sum of inputs to a binary output (-1 or 1).         4. Forward Propagation: Multiply the input features with their respective weights, add the bias term, and pass the result through the activation function to obtain the predicted output.  5. Update Weights: Compare the predicted output with the true target output. If they match, no changes are made. If they differ, update the weights and bias term based on a learning rate and the error. The learning rate determines the step size for updating the parameters.  6. Iterate: Repeat steps 4 and 5 for all training examples and continue iterating until convergence or a maximum number of iterations is reached. Convergence occurs when the algorithm finds a set of weights that correctly classifies all training examples.  The Perceptron algorithm has some key properties:  - It works well for linearly separable data, where the two classes can be separated by a hyperplane.  - It is a single-layer neural network with no hidden layers.  - It is a type of online learning algorithm, meaning it updates the weights after each training example.  - It converges if the data is linearly separable; otherwise, it may not converge.  - It is computationally efficient and can be used for large datasets.  The Perceptron algorithm can be extended to handle multi-class classification using techniques like One-vs-All or One-vs-One. It is also a foundational concept for more complex neural network architectures, such as feedforward neural networks.  However, it's important to note that the Perceptron algorithm has limitations. It cannot learn non-linear decision boundaries, so it may not perform well on datasets with complex patterns. Additionally, it does not provide probabilistic outputs like some other classifiers.          Overall, the Perceptron algorithm serves as a fundamental concept in machine learning and forms the basis for more advanced neural network models.  Multilayer perceptron:  A Multilayer Perceptron (MLP) is a type of artificial neural network that consists of multiple layers of interconnected neurons, including input, hidden, and output layers. It is a powerful and flexible model that can learn complex patterns and solve a wide range of machine learning tasks, including classification, regression, and even unsupervised learning.  Here are the key aspects and characteristics of a Multilayer Perceptron:  1. Architecture: An MLP is composed of an input layer, one or more hidden layers, and an output layer. Each layer consists of multiple neurons (also called units or nodes), which are interconnected through weighted connections.  2. Activation Function: Each neuron in an MLP applies an activation function to the weighted sum of its inputs. Commonly used activation functions include sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU). The choice of activation function affects the model's capacity to represent non-linear relationships.  3. Forward Propagation: The forward propagation process involves passing the input data through the network from the input layer to the output layer. The weighted sum of inputs is computed for each neuron, passed through the activation function, and propagated to the next layer.  4. Backpropagation: The backpropagation algorithm is used to train the MLP by adjusting the weights in the network. It involves calculating the error between the predicted output and the true target output, propagating this error backward through the network, and updating the weights using gradient descent optimization. This process iterates until the network converges to a satisfactory solution.  5. Training: MLPs are typically trained using a supervised learning approach, where labeled training data is used to update the weights. The training process involves repeatedly presenting         the training examples to the network, calculating the errors, and updating the weights using the backpropagation algorithm. The objective is to minimize a loss function that quantifies the discrepancy between the predicted outputs and the true outputs.  6. Overfitting and Regularization: MLPs are prone to overfitting, where the model learns the training data too well but fails to generalize to unseen data. Regularization techniques, such as weight decay or dropout, are often employed to prevent overfitting and improve generalization performance.  7. Hyperparameters: MLPs have several hyperparameters that need to be tuned, including the number of hidden layers, the number of neurons in each layer, the learning rate, the activation function, and regularization parameters. Finding the optimal set of hyperparameters often requires experimentation and validation on a separate validation set.  MLPs have been widely used in various domains, including computer vision, natural language processing, and recommendation systems. They can capture complex relationships in the data and learn intricate decision boundaries, making them suitable for handling non-linear and high- dimensional problems.  However, MLPs also have some limitations. They can be sensitive to the initial weights and require careful initialization. Training large MLPs on complex datasets may be computationally intensive and prone to overfitting. Additionally, interpreting the learned representations and understanding the internal workings of MLPs can be challenging.  In summary, Multilayer Perceptrons are powerful neural network models capable of learning complex patterns and solving a wide range of machine learning tasks. They have been widely adopted and form the foundation for more advanced neural network architectures. Proper tuning of hyperparameters and regularization techniques is crucial for achieving good performance and avoiding overfitting.  4. Attempt the quiz given in topicly content  and add screenshot of your score (Â³85% is considered completion of the task) in this report.         Quiz:      