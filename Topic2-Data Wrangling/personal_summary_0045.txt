The topic of this topic is “biologically inspired” learning methods referred to as artificial neural networks, including single and multi layer perceptrons and deep neural networks. ANNs allow for the creation of highly non linear models with no prior assumptions about the general functional form of the solution i.e. the do have no fixed or pre-determined “kernel”, instead, the functional solution is entirely learned from the data.  READING To revise I read Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow by Aurelien Geron and I also read the relevant sections of the course notes and sklearn documentation.  SUMMARY OF THE MAIN POINTS FOR TOPIC 10 LEARNING  A Perceptron is a computational unit that takes 1 or more inputs, applies a functional transform called an activation function (sign in this case) to produce a binary output (-1 or 1); as per fig 1.  if we replace the sign function with other non-linear functions such as tanh, sigmoid, relu etc, and we combine multiple perceptrons ( The use of non-linear activation functions in a MLP allows it to make complex decisions, not just binary ones as a perceptron does.  A MLP is trained by propagating/feeding input data forwards through the network to produce an output where it is compared to a known result/label and a loss value is computed. This loss is used to adjust the weights, by computing the gradient of the loss function with respect to the weights to propagate the error information backwards through the network from the output all the way back to the inputs. Weights are continuously adjusted until the loss is within certain defined tolerance limits (or perhaps some  max number of iterations has been reached to avoid infinite attempts) using algorithms such as gradient descent which ensures that a locally optimal solution can be found. Multi-class outputs can be achieved with functions such as softmax which.  Deep Neural Networks DNNs are really just an extension of MLPs in that they are just deeper i.e. they  have  more  layers.  Some  might  argue   that  MLPs  do  not  have  some  of  the  more  complex features of a DNN such as drop out or pooling etc but the core feature is in the name, they’re deep neural networks.  Some common DNNs are convolutional neural networks and autoencoders.  Convolutional Neural Networks (CNN) are almost exclusively used for vision tasks and they have functionality that resemble parts of the human visual system. Principally a CNN learns a finite set of filters that represent features which are then convolved with patches of the input to detect the presence of those features. CNNs typically create many small features at the layers closest to the input that represent simple features such as lines or circles, and then as information is propagated through the network, these features are combine into a more complex representations of an object. Fig 3. below shows the representations of simple features in layers 1 and 2 of a CNN.  Another very common DNN is the autoencoder. Autoencoders essentially learn how to reconstruct their input, but they do it by learning an encoding of the input in a lower dimensional form as well as learning a decoding of the low dimensional form back to the original; or something that ideally closely resembles the original. An autoencoder is somewhat like PCA or lossy compression.  SCREEN SHOT OF RESULTS FOR QUIZ 10   