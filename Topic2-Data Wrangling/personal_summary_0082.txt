Introduction to Machine Learning Task 10.1P  Reflection  This topic's module covered Artificial Neural Networks (ANNs), their components, and applications. We started with the most basic network known as a Perceptron before moving onto Multi Layered Perceptrons and then onto Deep Learning.  The practical task required us to create a Multi Layered Perceptron (MLP) model with 16 hidden layers and train it on the Sklearn “fetch_lfw_people” dataset which is made up of famous face images. The Sklearn packages make it quite simple to implement a MLP model, however, tuning the hyperparameters to get a reasonable performance was quite difficult. Obviously, a MLP isn’t the ideal model for image recognition, and we should instead use a Convolutional Neural Network which is designed for a task like this.  Neural Systems  Our brain consists of billions of inter-connected neurons and a highly paralleled architecture. This structure is the key motivation behind the development of Artificial Neural Networks. ANNs are designed to learn from data by processing it through interconnected layers of artificial neurons (or nodes). These layers include an input layer, one or more hidden layers, and an output layer. Each node applies a specific mathematical function to the inputs it receives and passes the result on to the next layer.  Perceptrons and Multilayer perceptrons  A perceptron is the simplest form of a neural network and can be thought of as the building block for more complex networks such as ANNs. A perceptron is essentially a binary classifier that maps a set of input data to an output decision.  A perceptron works by taking an input, or a series of inputs based on the number of features being passed, and applying a weight to each of those inputs based on the perceived importance of each. The perceptron then calculates the weighted sum of the inputs before adding a bias term to the weighted sum. Lastly, the result is passed through an activation function, for example a step function, outputting a 1 or 0 which is the final result and the prediction made by the model.  The perceptron is only able to handle linearly separable problems. The example given in the module demonstrates the effectiveness with AND and OR logic operations and how the perceptron is unable to process an XOR problem as it isn’t linearly separable. The solution to this limitation is a Multilayer Perceptron or MLP for short.  Multilayered perceptrons work in a similar way, but as the name suggests have at least three layers. Rather than just an input and output, MLPs also have at least one hidden layer which makes them a simple version of an ANN. In an MLP, the output from each perceptron in one layer is the input to the perceptron’s in the next layer. The data flows from the input layer, through the hidden layer/s, to the output layer in a process called forward propagation.  This added complexity is what allows MLPs to deal with non- linearly separable problems like the XOR example given above.  Backpropagation  Backpropagation, short for "backward propagation of errors," is a method used to train neural networks, including multilayered perceptrons. It updates the weights and biases of the network by calculating the gradient of the loss function once the network has finished processing the training data.  As mentioned above, passing the data from the input layer to the output layer is referred to as forward propagation. Once the output has been produced, the loss can be calculated using some loss function which is based on the difference between the predicted and the actual values. This loss is then back propagated through the network calculating the partial derivatives of the loss with respect to each weight and bias essentially determining how much each weight and bias contributed to the overall loss. Finally, the weights and biases are then updated ready to start the process all over again.  Deep Learning  Deep learning models are designed to continuously analyse data with a logic structure operating similar to how a human brains logic structure would. Deep learning models differ from ANNs in terms of their depth. Where ANNs may only have one or two hidden layers, Deep Learning models have many more, hence deep in the name.  There are many deep learning models designed for different purposes, in the module we focused on Convolutional Neural Networks (CNN or ConvNet) which is modeled on the mammalian visual cortex. CNNs are designed to process structured grid data such as images and are made up of serval convolutional layers with each filtering more complex features the deeper into the network the data travels.  One of the key advantages of CNNs over fully connected networks is that they are translation- invariant, meaning that one the network learns a pattern in one area of the image, it can recognise it in any other area of the image.  