 Our brain is made up of interconnected networks of neurones and has a highly parallel design. Biological brain systems inspire the development of Artificial brain Networks (ANNs).  When the underlying functions or decision limits are very nonlinear, linear models may not be sufficient. Vector machines can build nonlinear functions; however, they require fixed feature transformations that vary depending on the kernel function. For example, if you use a linear kernel, your model output will be a linear model. Data-driven feature modifications may be learned using neural networks.  The brain is critical in processing both physical and mental inputs and producing appropriate reactions depending on the information it receives. The brain receives physical or mental stimuli as input, processes them, and then creates an output if necessary. For instance, suppose you see a dog. Your brain analyses the visual and audible data and, depending on your previous experiences, develops a desire to pet the dog or run away, among other things.  With the brain in mind, the following are the primary actors in a neural network system:  A typical neural network (machine) contains an input layer, one or more hidden layers, combiners (sum functions), nonlinear activation functions, and an output layer.  Because neural networks are compatible with high-dimensional inputs and multi-label classification, we can create more complicated, larger neural networks. The graphic above depicts a sophisticated neural network with three hidden layers and one high-dimensional input layer with a three- dimensional output layer. If you don't provide enough training data, a more sophisticated neural network might overfit.  The Perceptron method is one of the earliest and most basic neural network models for binary classification tasks. It was created in the late 1950s by Frank Rosenblatt and may be thought of as a building block for more complicated neural network architectures.  The Multilayer Perceptron (MLP) is a sort of artificial neural network composed of numerous layers of linked artificial neurones (sometimes referred to as nodes or units). It is a very effective and commonly used neural network design for classification and regression problems.  Backpropagation is an important component in the training of artificial neural networks, notably multilayer perceptrons (MLPs). It is a frequently used strategy for changing the network's weights and biases to minimise the error between expected and real outputs.  A deep learning model is intended to analyse data indefinitely using a logic framework similar to how a human would derive conclusions. Deep learning does this by employing a layered framework of algorithms like ANNs.  Deep learning techniques rely on sophisticated neural networks. They have learned several real- world tasks successfully (for example, handwritten digit recognition and picture recognition). Deep learning architectures that are often used include:     o  Convolutional Neural Networks o  Autoencoders o  Networks of Deep Belief o  Machines based on the Boltzmann equation. o  Boltzmann Machines with Restrictions o  Machines using Deep Boltzmann Equations o  Deep Neural Networks (DNNs)  A Convolutional Neural Network (CNN or ConvNet) architecture is built after the mammalian visual cortex, the portion of the brain where visual information is processed.  Specific neurones in the visual cortex activate only when certain occurrences are in the field of vision. For example, one neurone may fire only when you look at a left-sloping diagonal line, while another may fire only when you gaze at a horizontal line. Our brains analyse pictures in more complicated levels. The first layer differentiates fundamental features such as lines and curves. At deeper levels, the brain recognises a pattern of edges and colours as, say, a house or a bird.       