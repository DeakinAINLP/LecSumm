This topic we focused on:   Neural Network   Deep Learning   Perceptron and multilayer perceptron So why Neural Network? Why we got here and the reason of we might not using other algorithms? Linear models may not be sufficient when underlying functions or decision boundaries are extremely nonlinear; SVM can construct nonlinear functions but use fixed feature transformations, which depends on the kernel function. What if we are not sure about the kernels or we prefer to learn these features from the data itself? ANN allow the feature transformations to be learnt from data. Neural System Basics: The major players in a neural network system:   A layer as an input   One or many hidden layers   Combiners (sum functions)   Nonlinear activation functions   Output layer In the first example on the left, we have 3 features input, one hidden layer and 2 output features. We can have more complex neural networks which are bigger and have a high dimensional inputs and multi-label classification. One of the issues of more complex neural network can result in over fitting if we could not provide enough training data. Perceptron Algorithm: Perceptron is a linear classifier (binary) and is a single layer neural network. A multilayer perceptron is called a neural network. So, perception is a simple neural network used for binary classification. It has only one layer with single node. Given weight w, the perception linearly divides input space into two regions; above and under the hyperplane or decision surface. To see how confident, we are from our hyperplane and data classification, we need to find the distance of a point to the line. If it`s close, then we cannot be so confident about it, but when it`s far from the hyperplane, we can be more confident.  So, the data is linearly separable. The steps of the perceptron algorithm are as follows: 1. Initialize w=0 2.  Retrieve next input x and desired output y   Compute actual output   Compute output error   Update weight (between 0 to 1)   Repeat from step 2 until convergence It puts each data point in a loop and at each iteration it updates the w. if w coverages to some values, then no update and the current weight already correctly classify current sample. Multilayer perceptron: The question is that if it`s always possible to separate the data points based on their class label with a single line? We can use AND and OR functions, to solve some problem and linearly solve the problem. Multilayer perceptron: For complex, nonlinear decision surfaces, we need a multilayer network. If you remember the sign function in a perceptron, we had a -1,1 output. But with choosing a more complex activation function we allow the network to combine the inputs in more complex ways and provides a richer capability in the functions they can model. Non-linear functions like the logistic, (also called the sigmoid function), output a value between 0 and 1 with an s-shaped distribution. Choice of node in a multi-layer network should be continuous but it should be a continuous meaningful function such as the sigmoid function. Feedforward Neural Network: This is an ANN where connections between units do not form a cycle. In this form, the information moves in only one direction, forward, from the input nodes, through the hidden nodes and to the outputs. So, no cycle or loops in the network. In MLP, the model comprises multilayers of logistic regression like models (with continuous non-linearities) rather than multiple perceptron (with discontinuous non-linearities). Although this may not be a good choice, we will continue using the term MLP. What we need to know about the model:   The output is a vector   There are two types of weights here: input to hidden, hidden to output   The input layer just relays the input vector   We can have more than one hidden layer   Another interesting feature is that it does not have to be fully connected In MLP, if we consider x as an input and y as output, the aim is to find the network weights w; so that the predicted values will be close as possible to the real ones. It means, we need to minimise the error which is the difference of the predicted value compared to the real true value of the output. At this case, we use gradient-descent for minimisation. E(w) is not convex, rather it is complex function with potentially many local minima. To solve this problem, we use an algorithm called Backpropagation. In the Gradient-based Optimization, we need to find the true direction towards the optimal point and the magnitude of the movement towards it and find the optimal point. The following figure is just an illustration of the gradient-descent method on the sample convex function. Backpropagation Algorithm: Backpropagation is a widely used algorithm in the field of artificial neural networks, specifically for training multi-layer neural networks through gradient descent optimization. It is a key component of training deep learning models. The goal of backpropagation is to adjust the weights and biases of a neural network to minimize the difference between the predicted output and the desired output for a given input. It achieves this by iteratively propagating the error backwards through the network and adjusting the weights accordingly. Here's a high-level overview of the backpropagation algorithm: 1.  Forward Pass: In the forward pass, the input data is fed into the network, and the activations and outputs of each layer are computed successively. Starting from the input layer, each layer applies a linear transformation (dot product between inputs and weights, plus bias) followed by a non-linear activation function (e.g., sigmoid, ReLU) to produce the output of that layer. The outputs are then passed to the next layer until the final output is obtained. 2.  Calculate Loss: The output of the network is compared to the desired output using a loss function. The loss function quantifies the discrepancy between the predicted output and the true output, serving as a measure of how well the network is performing. 3.  Backward Pass: The error, computed as the derivative of the loss with respect to the output of the network, is propagated backward through the layers. Starting from the output layer, the error is backpropagated by computing the gradients of the loss with respect to the weights and biases of each layer. 4.  Weight and Bias Updates: The gradients obtained in the backward pass are used to update the weights and biases of the network. The weights and biases are adjusted in the opposite direction of the gradient to minimize the loss. The specific update rule used is typically a variant of gradient descent, such as stochastic gradient descent (SGD) or Adam. Iterative Process: Steps 1 to 4 are repeated for multiple iterations or epochs until the network converges to a satisfactory level of performance or reaches a predefined stopping criterion. 5. The backpropagation algorithm exploits the chain rule of calculus to compute the gradients efficiently, propagating the error backwards layer by layer. This allows the algorithm to adjust the weights of each layer proportionally to their contribution to the overall error, thus fine-tuning the network's performance. Backpropagation has been instrumental in training deep neural networks and has played a significant role in the success of deep learning in various domains, including image recognition, natural language processing, and reinforcement learning. Deep Learning: Deep Learning methods are advanced neural networks. They have been successful in learning many real-world tasks. Some of the common Deep Learning architectures are:   Convolutional Networks   Autoencoders   Deep Belief Networks   Boltzmann Machines   Restricted Boltzmann Machines   Deep Boltzmann Machines   Deep Neural Networks Convolutional Neural Networks: Convolutional Neural Networks (CNNs) are a type of deep learning model specifically designed for processing structured grid-like data, such as images or sequential data like time series. CNNs have been highly successful in computer vision tasks, including image classification, object detection, and image segmentation. The key idea behind CNNs is the utilization of convolutional layers, which are responsible for automatically learning hierarchical representations from input data. Here are the main components and concepts of CNNs: 1.  Convolutional Layers: Convolutional layers consist of a set of learnable filters, also known as convolutional kernels. These filters are small-sized matrices that slide over the input data using a convolution operation. The convolution operation involves element-wise multiplication of the filter with local patches of the input data, followed by summation, producing a feature map. Multiple filters are used to extract different features from the input. Convolutional layers capture spatial relationships and local patterns in the data. 2.  Pooling Layers: Pooling layers are used to downsample the feature maps obtained from the convolutional layers. Common pooling operations include max pooling, average pooling, or L2-norm pooling. Pooling helps in reducing the spatial dimensions of the feature maps while retaining the important information. It also provides translational invariance by selecting the most prominent features. 3.  Activation Functions: Activation functions introduce non-linearities to the output of convolutional and pooling layers. Popular choices include Rectified Linear Units (ReLU), sigmoid, and hyperbolic tangent (tanh). ReLU is commonly used due to its simplicity and the ability to alleviate the vanishing gradient problem 4.  Fully Connected Layers: Following the convolutional and pooling layers, CNNs often include one or more fully connected layers. These layers connect every neuron from the previous layer to every neuron in the subsequent layer, similar to traditional neural networks. Fully connected layers capture high-level abstractions by combining features learned in earlier layers. The final fully connected layer produces the output, which can be used for tasks like classification or regression. 5.  Training and Backpropagation: CNNs are trained using the backpropagation algorithm, which involves forward propagation of input data through the network, computing the loss or error, and then backpropagating the error to update the weights using gradient descent or its variants. The weights are updated iteratively to minimize the loss function. CNNs are capable of automatically learning hierarchical representations from raw data, enabling them to capture complex patterns and features. The convolutional layers help in local feature extraction, while the pooling layers reduce spatial dimensions and enhance important features. By stacking multiple convolutional and pooling layers, CNNs can learn increasingly abstract and high-level representations of the input data. The success of CNNs in computer vision tasks can be attributed to their ability to exploit translation invariance, parameter sharing, and hierarchical feature learning, which are particularly effective for processing grid-like data such as images. LeNe5: LeNet-5 is considered one of the pioneering CNN models and has played a crucial role in popularizing the use of CNNs for computer vision tasks. The LeNet-5 architecture was primarily designed for handwritten digit recognition, specifically for the recognition of handwritten ZIP codes on envelopes in the early days of automated mail processing. It consists of the following key components: 1. Input Layer: LeNet-5 takes as input a grayscale image of size 32x32 pixels. 2.  Convolutional Layers: LeNet-5 consists of two convolutional layers. The first convolutional layer applies six 5x5 filters to the input image, followed by a subsampling operation (typically max pooling) with a 2x2 window. The second convolutional layer applies sixteen 5x5 filters to the output of the first layer, again followed by subsampling. 3.  Fully Connected Layers: After the convolutional layers, LeNet-5 includes three fully connected layers. The first fully connected layer has 120 neurons, followed by a second fully connected layer with 84 neurons. Finally, a third fully connected layer with the desired number of output neurons is used for classification. In the original LeNet-5, the output layer had 10 neurons, corresponding to the 10 possible digits (0-9). 4.  Activation Functions: Throughout the network, LeNet-5 uses the sigmoid activation function,which was common at the time of its development. However, it's worth noting that modern CNN architectures typically use rectified linear units (ReLU) as activation functions for improved training and convergence. 5.  Training and Backpropagation: LeNet-5 is trained using the backpropagation algorithm, similar to other neural networks. The weights are updated iteratively using gradient descent or its variants to minimize a chosen loss function, such as cross-entropy loss. LeNet-5's architecture leverages convolutional layers to extract local features from the input images and pooling layers to downsample and retain important information. The fully connected layers at the end of the network combine these features to perform classification. It demonstrated impressive performance on handwritten digit recognition tasks, which paved the way for subsequent advancements in CNN architectures. While LeNet-5 itself may not be widely used in modern computer vision applications due to its simplicity, it remains a significant milestone in the development of convolutional neural networks and serves as a foundation for more complex and advanced architectures that followed, such as AlexNet, VGGNet, and ResNet. Autoencoder: It`s a neural network which can handle many layers in its structure. The aim of an Autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. It is designed to learn an efficient representation of the input data by training the network to reconstruct its own inputs. Autoencoders consist of two main components: an encoder and a decoder. 1.  Encoder: The encoder takes an input and transforms it into a lower-dimensional representation, often referred to as a "latent space" or "encoding." The encoder typically consists of one or more hidden layers that gradually reduce the dimensionality of the input. The final layer of the encoder represents the compressed representation of the input data. 2.  Decoder: The decoder takes the low-dimensional representation produced by the encoder and aims to reconstruct the original input. Similar to the encoder, the decoder usually consists of one or more hidden layers that gradually increase the dimensionality of the representation until it matches the dimensionality of the input data. The final layer of the decoder generates the output, which should ideally be a close approximation of the original input. During training, an autoencoder is optimized to minimize the reconstruction error between the input and the output. The most commonly used loss function for autoencoders is the mean squared error (MSE), which measures the difference between the input and the reconstructed output. By minimizing this error, the autoencoder learns to capture the essential features of the input data and discard unnecessary information. Autoencoders have several applications, including: 1.  Dimensionality Reduction: Autoencoders can be used to learn a compact representation of high-dimensional data, effectively reducing the dimensionality. The compressed representation obtained from the encoder can be used for tasks such as visualization, clustering, or as input for other models. 2.  Anomaly Detection: Autoencoders can learn to reconstruct normal patterns from the training data. When presented with anomalous or novel input, the reconstruction error tends to be higher. This property can be leveraged for anomaly detection, where significant deviations from the learned patterns are flagged as anomalies. 3.  Denoising: Autoencoders can be trained to reconstruct clean input from noisy or corrupted data. By training the model on noisy samples and comparing the output with the original clean data, the autoencoder learns to denoise the input by capturing the underlying structure. 4.  Feature Learning: Autoencoders can be used as feature learners by training on unlabeled data. The compressed representation obtained from the encoder can be used as a set of meaningful features for subsequent supervised learning tasks, such as classification or regression. Autoencoders come in various forms and variations, such as sparse autoencoders, variational autoencoders (VAEs), denoising autoencoders, and stacked autoencoders. Each variation introduces different constraints or modifications to the basic architecture to improve specific aspects of the model, such as sparsity, generative capabilities, or robustness to noise. Overall, autoencoders provide a powerful framework for unsupervised learning, dimensionality reduction, and learning efficient representations of data, with applications in various domains such as computer vision, natural language processing, and anomaly detection. 