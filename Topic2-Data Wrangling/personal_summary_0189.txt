 NEURAL NETWORKS:  Neural networks,  which  are  computational models,  were  inspired  by  neural  connections  in  the human brain. They are made up of interconnected layers of nodes called "neurones." To  produce an output, each neurone takes inputs, applies a set of weights, adds a bias, and then  passes the sum through an activation function. The output is then passed on to neurones in  the next layer as input. The network learns by adjusting the weights and biases so that the  difference between the actual and predicted outputs is as tiny as possible.  NEURONS:  A neurone is a fundamental processing unit in the context of neural networks. It takes one or  more  inputs,  multiplies  each  by  weight,  adds  a  bias  term,  and  then  applies  an  activation  function to the sum. The output is then forwarded to the following layer. Weights and biases  are learnable parameters that the network adjusts based on prediction errors during training.  ACTIVATION FUNCTIONS:  Non-linearity  is  introduced  into  a  neuron's  output  by  activation  functions.  Because  of  this  nonlinearity, neural networks can learn from mistakes and make complex predictions. Sigmoid,  tanh, and ReLU are examples of standard activation functions.  LOSS FUNCTIONS:  This topic, the concept of loss functions was introduced. A loss function measures how closely  our  predictions  match  the  actual  values.  It  calculates  the  difference  between  the  model's  predicted  and  actual  output.  The  goal  of  training  is  to  minimise  this  loss  function.  COST OF LOSS FUNCTION:  The loss function's cost quantifies the difference between the network's prediction and output.  The goal of training is to minimise this loss. Mean squared error is a standard loss function  for regression tasks and cross-entropy loss for classification tasks.  BACKPROPAGATION:  Backpropagation is an essential method for neural network training. Backpropagation is an  algorithm  that  efficiently  computes  the  gradient  of  the  loss  function  about  the  network  weights.  During  training,  this  algorithm  is  critical  for  adjusting  network  weights.  It  uses  the  chain rule of calculus to propagate errors backwards through the network, allowing the model  to learn from its errors.  BACKPROPAGATION AND GRADIENT DESCENT:  Backpropagation is a neural network training algorithm. It uses the chain rule to calculate the  gradient of the loss function for each weight, propagating the gradient backwards through  the network. After obtaining the gradients, they are used in the gradient descent optimisation  algorithm to adjust the weights and biases to minimise the loss.  PERCEPTRONS:  Perceptrons are the most basic type of neural network, also known as a single-layer neural  network. A perceptron takes multiple inputs, applies weights, incorporates a bias term, and  then passes the result through an activation function to generate an output. It serves as the  foundation  for  more  complex,  multi-layered  neural  networks.  A  perceptron's  weights  and  biases are adjusted during learning to help the network make accurate predictions.  BIAS AND WEIGHT TERMS:   Bias is a constant added to a neuroneâ€™s input in a neural network. It allows the model to be  more flexible to better fit the data. The neurone would only be a linear function of its input if  there  was  no  bias  term.  In  a  neural  network,  the  weight  matrix  represents  the  strength  of  connections between neurones in adjacent layers. These weights are adjusted during training,  allowing the network to learn from the data.  OVERFITTING AND REGULARIZATION TECHNIQUES:  Bias is a constant added to a neuron's input in a neural network. It enables the model to be  more flexible to better fit the data. Without a bias term, the neurone would only be a linear  function  of  its  input.  The  weight  matrix  in  a  neural  network  represents  the  strength  of  connections between neurones in adjacent layers. These weights are adjusted during training,  allowing the network to learn from the data.  LEARNING RATE, EPOCHS, AND BATCH SIZE:  The learning rate is a hyperparameter that controls how much the weights are adjusted each  time the weights are updated in response to the estimated error. The number of complete  passes through the training dataset is called epochs. The batch size refers to the number of  training examples used in a single iteration. These hyperparameters significantly impact the  learning process and the model's performance.   