Input Layer, Hidden Layers, Combiners, Nonlinear Activation Functions, and Output Layer are the main building blocks of a conventional neural network. Neural networks may learn to categorise or predict input data based on patterns and relationships in the training data by combining these elements and modifying the weights and biases during the training phase.  Perceptron algorithm is a linear classifier. Each feature is given a weight after a set of input features is used. The output is then calculated using a step function once the algorithm has calculated the weighted total of the inputs. The activation function, or step function, converts the computed sum to a binary output, often 0 or 1, by acting as a mapping. Once it discovers a set of weights that correctly classifies the input data, it iterates through the training samples again and again, updating the weights until it reaches the maximum number of iterations or finds a set of weights that does not. Using the learned weights as a basis, the Perceptron can be trained to predict the class labels of fresh, unseen samples.  To alleviate the single-layer Perceptron model's shortcomings in processing complicated and nonlinear patterns, the multilayer Perceptron (MLP) was developed. Only linearly separable input could be processed by the single-layer Perceptron. In order to overcome this restriction, multilayer perceptron adds extra hidden layers in between the input and output layers. Multiple neurons make up each hidden layer, which enables the network to learn and reflect more intricate links between input and output.  Backpropagation is used for training neural networks. The gradients of the network's parameters with respect to a given loss function are effectively calculated by this gradient-based optimization approach. The network's weights and biases are then updated using these gradients, steadily enhancing the network's performance throughout the training phase.  