Neural Networks and Deep Learning  An artificial intelligence technique called a neural network instructs computers to analyze data in a way that is modelled after the human brain. Deep learning is a sort of machine learning that employs interconnected neurons or nodes in a layered framework to mimic the human brain.  Computers can make intelligent decisions with minimal human intervention thanks to neural networks. This is why they can learn and model complex, nonlinear relationships between input and output data.  Our brain has networks of inter-connected neurons and a highly-parallel architecture. Development of Artificial Neural Networks(ANNs) is motivated by biological neural systems.  Neurons  There are billions of neurons in your brain. Estimates range from 50 billion to 500 billion. (Woodford 2018)  Brain function  The physical distribution of tasks in the brain is as below,  The brain takes physical or mental stimuli as input, processes it and, if necessary, produces an output. For example, perhaps you see a dog. Your brain processes that visual and auditory input and, depending on your past experiences, produces a desire to pat the dog or run away etc.  Neural system basics  With the brain in mind, the major players in a neural network system are as follows:    a typical neural network (machine) has an input layer;      it has one or many hidden layers; it has combiners (sum functions); it has nonlinear activation functions; it has an output layer.  Simple neural network architecture  A basic neural network has interconnected artificial neurons in three layers:    Input Layer  The input layer is where data from the outside world enters an artificial neural network. Data is processed, analysed, or categorised by input nodes before being forwarded to the following layer.    Hidden Layer  It is possible for artificial neural networks to have a lot of hidden layers. Each hidden layer evaluates the output from the preceding layer, refines it, and then sends it to the following layer.    Output Layer  The output layer presents the overall outcome of the artificial neural network's data processing. It may include one or more nodes.  The following figure is shows two examples of simple neural network structures.  As you can see, there is always an input layer which is responsible for taking the input features. In the example on the left we can see that the input layer has 3 dimensions. The output layer has 2 dimensions. It has only one hidden later with size of 4.  Complexity  We can have more complex, bigger neural networks because neural networks are compatible with high dimensional inputs and multi-label classification. As you can see in the figure above, the diagram shows a complex neural network with 3 hidden layers and one high dimensional input layer with a 3-dimensional output layer.  Remember having a more complex neural network can result in over-fitting if you are not providing enough training data.  Perceptron algorithm  Perceptron is a linear classifier (binary) and is a single layer neural network. A multi -layer perceptron is called a neural network.  A perceptron is a simple neural network used for binary classification. It has only one layer with a single node.  Now lets visualize a 2-dimensional example. In the following figure we are explaining the problem that the perceptron tries to solve.  It appears to be a simple classification problem. For now we will assume that the data is linearly separable.          Multilayer Perceptron  Consider the AND and OR logical gates or functions. As you can see in the figure, we can show that both of these problems are linearly separable in 1 and 0 class labels:  We can easily find a line to divide based on the class labels. But what about the XOR problem? This problem is not linearly separable. As you can see in the next figure (below), it is impossible to separate the data points based on their class labels with a single line.  These kinds of problems were a motivation to develop a new neural network, but this time with a layer in the middle to handle these cases. Later it was proven that a multilayer perceptron (MLP) can represent the XOR problem.     Multilayer perceptron  A perceptron is quite weak in what it can represent. For complex, non-linear decision surfaces, we need a multi-layer network.  Non-linear functions like the logistic, (also called the sigmoid function), output a value between 0 and 1 with an s-shaped distribution. Choice of node in a multi-layer network should be continuous but it should be a continuous meaningful function such as the sigmoid function.       Artificial neural networks can be categorized by how the data flows from the input node to the output node. Below are some examples:  Feedforward neural networks  Data is processed in one direction, from the input node to the output node, by feedforward neural networks. Each node in one layer is linked to each node in the layer above it. To make better predictions over time, a feedforward network employs a feedback process. There are no cycles or loops in the network.  A multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer Perceptron (MLP). The model comprises multiple layers of logistic regression like models (with continuous non-linearities) rather than multiple perceptron (with discontinuous non-linearities).  Backpropagation algorithm  Artificial neural networks use corrective feedback loops to continuously learn and enhance their prediction modelling. You can think of the data flowing through the neural network's numerous channels as it moves from the input node to the output node. In order to translate the input node to the right output node, only one path can be considered proper. The neural network employs a feedback loop that functions as follows to identify this path:  1.  Each node makes a guess about the next node in the path.     2.  It checks if the guess was correct. Nodes assign higher weight values to paths that lead to  more correct guesses and lower weight values to node paths that lead to incorrect guesses.  3.  For the next data point, the nodes make a new prediction using the higher weight paths and  then repeat Step 1.  