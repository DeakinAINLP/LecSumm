NEURAL NETWORKS  Artificial Neural Networks are inspired by the parallel and inter-connected neurons of a brain. Neural networks can be used for problems where it’s not known ahead of time what type of decision boundary will be used. Linear models fail on nonlinear boundaries and SVMs require fixed feature transformations, set by the kernel function. ANN can be used to form models on the human brain structure, or be inspired by but drift away from the brain system. ANNs use a neuron network, taking inputs and generating outputs, like a brain.  Neural Networks contain an input layer, one or multiple of hidden layers, activation functions, and an output layer. Data is fed through the input layer and nodes are activated and passed onto the hidden layer nodes based on activation functions, eventually being funnelled to the output layer. The number of nodes in the input layer is said to be its dimension, or size. Likewise for the output layer. Input and output layers need not have the same dimension. Hidden layers may also vary in dimension.  PERCEPTRON  Perceptron is a single-layer single-dimension neural network that generates a binary linear classifier. This is the single-layer version of a neural network. Each feature has a corresponding weight and an associated bias term. The output is calculated with a sum function:  Y = sign[v(x,w)], where v(x,w) = xTw.  In otherwords y(x,y) = sign[wTx].  Perceptron divides the space into two regions using a boundary hyperplane (linear function). The sign of v(x,y) determines which side of the hyperplane a point sits on, while the magnitude of v(x,w) determines its distance from the hyperplane. Perceptron finds the weight vector to classify two classes. Multilayer perceptron is required for solving the XOR problem (exclusive OR operator). Single-layer perceptron can only solve linearly separable problems.  Multilayer Perceptron (MLP) is far more versatile than a standard perceptron, being able to tackle non-linear problems.  Nodes in an MLP network use meaningful continuous functions to output values between 0 and 1. Feedforward Neural Networks pass outputs in a single direction, so there are no cycles in the network. Perceptrons are feedforward and thus MLPs are an example of multi-layered feedforward networks.  For MLPs calculations are performed in the hidden layers. Input and outputs are both vectors. There’s also two types of weights, input-hidden and hidden-output. There can be multiple hidden layers. This structure is extremely powerful.  The goal of the MLP is to minimise the error between the predicted output and true value of given inputs. This is done through finding and updating the weights through back propagation using the gradient descent function.  CONVOLUTIONAL NEURAL NETWORKS  AND AUTOENCODING  Modelled after the visual input process component of a brain, this is among the most important deep learning methods. This model is based on the premise that certain neurons only activate when seeing specific visual input. The network isn’t fully connected, instead different nodes focus on different local regions. Sparse Interactions, Paramater sharing, and translation invariance are the key components of CNNs.  Autoencoders are mainly used as a dimensionality reduction technique and for feature learning. Using many hidden layers they learn how to encode a set of data into a more compact form, then decode it back into the original input data. The compact hidden layer can be extracted as the encoded form, a smaller and meaningful representation of data. What makes it stand out from PCA is that it can work non-linearly.        