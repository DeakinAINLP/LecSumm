Summary and Reflection of Topic 10:  Development of Artificial Neural Networks (ANNs) is motivated by biological neural systems.  Why the need for neural networks:      linear  models  may  not  be sufficient when  the  underlying  functions  or  decision  boundaries  are extremely nonlinear support vector machines can construct nonlinear functions but use fixed feature transformations, which depends on the kernel function. For example, while you are using linear kernel, obviously your model output will be a linear model.    Neural Networks allow the feature transformations to be learnt from data (in the event you are not  sure about the kernels or perhaps prefer to learn these features.  Major players in a neural network system:        typical neural network (machine) has an input layer; it has one or many hidden layers; it has combiners (sum functions); it has nonlinear activation functions; it has an output layer.  We can have more complex, bigger neural networks because neural networks are compatible with high dimensional  inputs  and  multi-label  classification  =  this  can  result  in  over-fitting  if  you  are  not  providing enough training data.  Perceptron  is  a  linear  classifier  (binary)  and  is  a single  layer neural  network.  A  multi-layer perceptron is called a neural network.  Motivation for a neural network = It is not always possible to separate the data points based on their class label with a single line.  The AND and OR logical gates or functions are linearly separable.  The XOR problem is not linearly separable = it is impossible to separate the data points based on their class labels with a single line. XOR is actually OR but not AND that is OR is true but not AND is true.  A perceptron is quite weak in what it can represent.  Multilayer perceptron = required for complex, non-linear decision surfaces.  Choosing a more complex activation function allows the network to combine the inputs in more complex ways and in turn provides a richer capability in the functions they can model.  Non-linear functions like the logistic, (also called the sigmoid function), output a value between 0 and 1 with an s-shaped distribution. Choice of node in a multi-layer network should be continuous but it should be a continuous meaningful function such as the sigmoid function.  A feedforward neural network is an Artificial Neural Network (ANN) where connections between units do not form a cycle.    The  information  moves  in  only  one  direction, forward,  from  the  input  nodes,  through  the  hidden  nodes (if any) and to the output nodes.    No cycles or loops in the network.  A multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer Perceptron (MLP).  Backpropagation = algorithm for training feedforward neural networks.     computes the gradient of the loss function with respect to the network weights. very  efficient,  rather  than  naively  directly  computing  the  gradient  concerning  each  weight.  This efficiency  makes  it  possible  to  use  gradient  methods  to  train  multi-layer  networks  and  update weights to minimize loss; variants such as gradient descent or stochastic gradient descent are often used.  A deep learning model is designed to continually analyze data with a logic structure similar to how a human would draw conclusions. To achieve this, deep learning uses a layered structure of algorithms similar to ANNs.  Deep learning methods are advanced neural networks. They have been successful in learning many real world  tasks  (e.g.  handwritten  digit  recognition,  image  recognition).  Some  of  the  common  Deep  learning architectures are:    Convolutional Networks   Autoencoders   Deep Belief Networks   Boltzmann Machines   Restricted Boltzmann Machines   Deep Boltzmann Machines   Deep Neural Network  Convolutional Neural Networks are used for analyzing images. However, they can also be used for other data analysis or classification. Pick out or detect patterns and make sense of them. It has hidden layers called convolutional layers. This is its basis.    Convolution layers receives input, transforms the input in some way and then outputs it in some way to another layer.  With each convolutional layer we need to stipulate the number of filters (or CNN weights) it needs to have. Detect edgers / squares / etc. The deeper our network goes (the more complex the detection) – rather than edges / simple shapes but specific objects / eyes / ears/ hair and beaks even or a full dog / cats etc.    Relatively small matrix – we decide number of rows / columns and they are initialized with random numbers. Filter is going to convolve against each 3 by 3 matrix. Can think of these filters as pattern detectors.  Must have a good amount of data whilst working with many layers of structures such as CNN otherwise you are in danger of over fitting based on your data.  An Autoencoder is a neural network which can handle many hidden layers in its structure.  The aim of an Autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.  Hidden layer – information represented in a lower density (i.e. the input data).  All neural networks are composite functions – meaning that they function of functions. The more layers a network has, the more nested functions it has.   The above summary has been created using the full course material for Topic 10 (including the relevant YouTube clips)     