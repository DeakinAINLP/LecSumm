1. Summarize the main points that is covered in this topic. A. Motivation and Inspiration: Development of Artificial Neural Networks (ANNs) is motivated by biological neural systems. Neural Networks allow the feature transformations to be learnt from data.  B. Neural System Basics a) has an input layer; b) has one or many hidden layers; c) has combiners (sum functions); d) has nonlinear activation functions; e) has an output layer.  C. Perceptron Algorithm  a) Perceptron is a linear classifier (binary) and is a single layer neural network b) A multi-layer perceptron is called a neural network.  c)  D. Multilayer Perceptron  a) Need multilayer as a single line cannot always separate the points b) A feed-forward neural network is an Artificial Neural Network (ANN) where the  information only moves forward, from the input nodes, through the hidden nodes (if any) and to the output nodes.  c) A multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer  Perceptron (MLP)  d) Try to do an optimisation problem: find w to minimise the error function:  e) f) Detour: Gradient-based Optimization g) Gradient-based optimisation methods are operated with the search directions  defined by the gradient of the function at the current point.  h) A gradient represents the direction that produces the steepest increase in the  function  i) Stochastic Gradient-Descent (SGD) minimises the instantaneous approximation  E. Backpropagation Algorithm  a) backpropagation aims to minimize the cost function by adjusting networkâ€™s  weights and biases.  b) The level of adjustment is determined by the gradients of the cost function with  respect to those parameters.  F. Convolutional Neural Networks  a) The architecture of a Convolutional Neural Network (CNN or ConvNet) is modelled  after the mammalian visual cortex, the part of the brain where visual input is processed.  b) Within the visual cortex, specific neurons fire only when particular phenomena are  in the field of vision.  c) Sparse interactions: Sparse weights within a smaller kernel instead of the whole  input. This helps reduce the number of parameters.  d) Parameter sharing: A kernel uses the same set of weights while applying to  different locations.  e) Translation invariance: Invariance means that you can recognize an object as an  f)  object, even when its appearance varies in some way. LeNet5: a convolutional neural network that uses a sequence of 3 layers: convolution, pooling, non-linearity; uses convolution to extract spatial features; subsamples using spatial average of maps; nonlinearity in the form of tanh or sigmoid; an MLP as final classifier; uses a sparse connection matrix between layers to avoid large computational cost  G. Autoencoder:  a) An Autoencoder is a neural network which can handle many hidden layers in its  structure.  b) The aim is to learn a representation (encoding) for data, b/c dimensionality  reduction  c) An Autoencoder learns to compress data from the input layer into a short code, and then uncompress that code into something that closely matches the original data.  