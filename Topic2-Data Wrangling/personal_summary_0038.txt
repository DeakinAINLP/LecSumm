Artificial Neural Networks(ANN) are a subset of machine learning algorithms and are designed to recognize patterns and relationships in data, which would allow computers to do the tasks that would normally require a human to do. The two groups working with ANN are either using ANN to study and model the brain or using the brain as a model to design ANNs.  The major components in a neural system are, an input layer, one or many hidden layers, combiners, nonlinear activation functions and an output layer. Since neural networks are compatible with high dimensional inputs and multi label classification, we are able to have more complex and bigger neural networks.  The goal of the perceptron algorithm is to find a decision boundary that separates the input data into two classes. It is a simple neural network used for binary classification and has only one layer with a single node. The algorithm can’t solve problems that require nonlinear decision boundaries, but serves as a basis for more complex neural network architectures, for example a multi-layer perceptron. The multi-layer perceptron is a feedforward neural network, which means that information only flows in one direction, from the input layer, through the hidden layers, to the output layer.  The backpropagation algorithm is a widely used technique for training artificial neural networks including the multi-layer perceptrons. It adjusts the weights and biases of a neural network based on the computed gradients of a predefined loss function. It starts by initializing the weights and biases of the network to small random values, then feeds the training example through the network and computes the weighted sum of inputs at each neuron and apply an activation function to produce the output of each neuron. Next it compares the network’s output with the true output and computes the loss. Then, starting from the output layer, it calculates the gradients of the loss function with respect to the weights and biases. Finally, it adjusts the weights and biases of the network using an optimization algorithm, typically gradient descent or one of its variants. The algorithm repeats these steps multiple times for each set or until convergence.  A deep learning model is designed to continually analyse data with a logic structure similar to how a human would draw conclusions. Some common architectures are: autoencoders, deep belief networks, convolutional networks, and deep neural networks. The architecture of a convolution neural network is modelled after the part of the brain where visual input is processed. CNNs are made up of three basic concepts, sparse interactions, parameter sharing, and translation invariance. The very first convolutional neural network was called Lenet5. It was designed for handwritten digit recognition and was trained using the backpropagation algorithm. It demonstrated the effectiveness of convolutional neural networks for image analysis and paved the way for more complex and powerful CNN architectures used in modern deep learning applications.  An Autoencoder is a neural network which can handle many hidden layers in its structure, its aim is to learn a representation for a set of data, which is mainly used for dimensionality reduction. The network attempts to copy its input to its output. The encoder learns to compress data from the input layer into a short code, and then uncompresses the code into something that looks similar to the original data.      