 I.  Neural System Basics: The component of neural system includes: - Input layers. -  Hidden layers. -  Combiners (sum functions). -  Non-linear activation functions -  Output layers  Complexity: We can have more complex, bigger neural networks because neural networks are compatible with high dimensional inputs and multi-label classification with 3 hidden layers and one high dimensional input layer with a 3- dimensional output layer.  II.  Perceptron algorithm: Perceptron is a linear classifier (binary) and is a single layer neural network. A multi-layer perceptron is called a neural network. Perceptron structure illustration:    x1 to xm features with w1 corresponding weights. We also have the w0 as bias term. A sum function will calculate a value, which later will be presented as the output y^.    III.  Multilayer perception:  A perceptron is quite weak in what it can represent. For complex, non-linear   decision surfaces, we need a multi-layer network.  Feedforward neural networks: A feedforward neural network is an Artificial Neural Network (ANN) where connections between units do not form a cycle. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network. A multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer Perceptron (MLP). The term MLP is really an accurate name because the model comprises multiple layers of logistic regression like models (with continuous non- linearities) rather than multiple perceptronâ€™s (with discontinuous non-linearities). Although this may not be a good choice, we will continue using the term MLP!   Notes on MLP:  MLP Formulation:  IV.  Backpropagation Algorithm:     Backpropagation is an algorithm used in the field of artificial neural networks for training a multi-layered feedforward network. It is a key component of many popular neural network architectures, such as deep learning models. The Algorithm includes these steps: 1.  Initialization: Initialize the weights and biases of the neural network with small  random values.  2.  Forward Propagation: Take a set of input values and pass them through the network  to generate an output prediction.  3.   Calculate the Error: Compare the predicted output with the desired output and  calculate the error or loss.  4.  Backward Propagation: This is where the actual backpropagation happens. The error is propagated back through the network, starting from the output layer and moving backward to the input layer.  5.  Weight and Bias Updates: Update the weights and biases of the network using the calculated gradients. This is typically done using an optimization algorithm, such as gradient descent or one of its variants.  