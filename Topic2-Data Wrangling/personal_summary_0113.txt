Artiﬁcial Neural Networks: Computer models known as Artiﬁcial Neural Networks (ANNs) are modelled a(cid:332)er the structure and operation of biological neural networks, notably the human brain. ANNs are a crucial element in the study of machine learning and  have  proved  eﬀective  in  addressing  a  variety  of  challenging  issues,  such  as  patiern  identiﬁcation,  picture recognition, and natural language processing.  A neurone, which is sometimes referred to as a node or perceptron, is the fundamental component of an artiﬁcial neural network. To create a network, neurones are linked together in layers. An ANN has three diﬀerent sorts of layers, which are:  1.  Data input layer: This layer accepts data input and transmits it  to the following layer. A characteristic or  property of the input data is represented by each neurone in the input layer.  2.  Between the input and output layers are layers known as hidden layers. They use weighted connections to process  the  input  data  and  nonlinear  activation  functions  to  create  the  outputs.  Complex  linkages  and patierns in the data are captured by hidden layers.  3.  Based on the data processed in the hidden layers, the output layer creates the network's ultimate output. The sort of issue the network is intended to address determines how many neurones are present in the output layer. For instance, in a binary classiﬁcation issue, the output layer may have one neurone for each class.  In a neural network, connections between neurones are weighted to reﬂect the strength of their connections. The network modiﬁes these weights through a procedure known as training throughout the learning phase. Presenting the  network  with  labelled  training  data,  comparing  the  results  to  what  was  anticipated,  and  then  changing  the weights to reduce the discrepancy are common steps in the training process.  Backpropagation is a well-liked training algorithm for ANNs. It entails propagating the output layer's mistakes back through the network and modifying the weights as necessary. Until the network reaches the required degree of accuracy or convergence, this iterative procedure is continued.  Based on their structure and connection patierns, ANNs may be grouped. Typical kinds include:  1.  Feedforward Neural Networks (FNNs): In FNNs, there are no loops or cycles; instead, information ﬂows from the input layer to the output layer in a single direction. They are the most basic kind of neural network and are frequently employed for classiﬁcation and regression problems.  2.  Recurrent  neural  networks  (RNNs):  RNNs  are  equipped  with connections  that  let  data  move  in loops or cycles. This makes it possible for them to analyse time series data or sequential data in which earlier inputs or outputs have an impact on the present ones. Natural language processing and speech recognition are two common activities that employ RNNs.  3.  Convolutional  Neural  Networks  (CNNs):  CNNs  are  made  to  handle  data  that  has  a  grid  structure,  such spectrograms of music or pictures. To capture spatial hierarchies and extract pertinent information, they employ specialised layers called convolutional layers, which carry out local operations on discrete areas of the input data.     4.  GANs, or generative adversarial networks: The generator and the discriminator, the two neural networks that make up a GAN, are in competition with one another. While the discriminator network tries to tell the diﬀerence between actual and fake data, the generator network creates samples of synthetic data. GANs have proved eﬀective in performing tasks like data synthesis and picture production.  The use of ANNs has substantially improved the science of artiﬁcial intelligence and shown outstanding capabilities in several ﬁelds. However, they also have drawbacks such long training durations, managing enormous datasets, choosing the right architectures, and overﬁting. Artiﬁcial neural networks continue to perform betier and be more useful because of ongoing research and developments.  Perceptron algorithm: A straigh(cid:414)orward  supervised  learning  approach for  binary  classiﬁcation tasks  is the perceptron  algorithm. Frank Rosenblati created it in the late 1950s, and it is one of the fundamental algorithms in the ﬁeld of neural networks.  The notion of a perceptron, a kind of artiﬁcial neurone or node in a neural network, is the basis of the perceptron algorithm. It generates a binary output based on a threshold by multiplying several inputs by appropriate weights. The approach ﬁnds a decision boundary that divides the two classes of data by adjusting the perceptron's weights during training.  A detailed explanation of the perceptron algorithm is given below:  1.  Initialise the weights: Give the perceptron's starting weights tiny, random numbers or zeros. Each weight is a measure of how signiﬁcant or helpful the related input atiribute is.  2.  Repeat the training data loop: Take the following actions for each training example:  a.  The weighted sum is calculated by multiplying each input characteristic by the associated weight and  b.  adding the results. Use the activation function: Apply an activation function, usually a step function or a sign function, to the weighted sum. The perceptron's output is determined by its activation function.  c.  Update the weights: If the perceptron's output does not correspond to the predicted output for the current training example, change the weights to lower the error. Voici how the weight update rule is provided: W = Learning Rate * Expected Output - Perceptron Output * Input Feature  A hyperparameter that regulates how much weight changes are made is learning rate. The perceptron's rate of acclimatisation to the training set of data is determined by this. Although slower convergence is the result of  poorer  learning  rates,  generalisation  may  be  improved.  Each  training  example  should  be  repeated  in stages a, b, and c, with the weights being adjusted a(cid:332)er each example.  3.  Iterate  over  the  training  data  several  times,  or  until  a  stopping  condition  is  satisﬁed.  This  is  known  as iterating through the training data. A frequent ending point is when the perceptron converges to a decision boundary that divides the classes or when it reaches a predetermined degree of accuracy.  4.  Application of steps a and b to the input characteristics of the new data allows the perceptron to be trained  to categorise fresh, unknown data.  The perceptron technique can only locate a decision border if the data can be separated linearly, thus it's vital to remember that. If the data cannot be separated linearly, the method will not succeed. However, by adding more layers  and  nonlinear  activation  functions,  more  sophisticated  neural  network  topologies,  including  multilayer perceptrons (MLPs) and deep neural networks, can handle complicated nonlinear patierns.  The perceptron method set the groundwork for more advanced algorithms utilised today as well as for subsequent advancements in neural networks and machine learning.  Multilayer perceptron: An artiﬁcial neural network (ANN) called a multilayer perceptron (MLP) is made up of numerous layers of linked nodes,  or  neurones.  It  is  a  feedforward  neural  network,  which  means  that  there  are  no  loops  or  cycles  in  the information ﬂow as it passes from the input layer through the hidden layers to the output layer.  The  existence  of  one  or  more  hidden  layers  between  the  input  and  output  layers  is  an  MLP's  distinguishing characteristic. Numerous neurones make up each layer, and each layer is completely linked to the layers above and below it. To put it another way, every neurone in a layer is linked to every neurone in the layer above it.  In  comparison  to  a  single-layer  perceptron,  an  MLP's  structure  enables  it  to  learn  complicated  non-linear correlations and handle more challenging issues. For the network to extract higher-level features and carry out more complex calculations, an MLP needs a way to capture and analyse intermediate representations of the input data.  An MLP's neurones use an activation function to add nonlinearity to the network. The sigmoid function, hyperbolic tangent (tanh) function, and rectiﬁed linear unit (ReLU) function are frequently used activation functions in MLPs. The weighted sum of inputs is subjected to non-linear changes by the activation function, allowing the network to simulate intricate interactions between inputs and outputs.  Backpropagation,  a  technique  for  training  MLPs,  combines  gradient  descent  with  forwards  propagation.  During training, the network is shown labelled training examples, and the results are contrasted with what is anticipated. Gradient  descent  is  then  used  to  change  the  weights  in  the  network  to  minimise  the  diﬀerence  between  the anticipated  output  and  the  expected  output.  Until  the  network  reaches  the  required  degree  of  accuracy  or convergence, this iterative procedure is continued.  Several tasks, including classiﬁcation, regression, and patiern recognition, have been successfully completed using MLPs. They have been applied to many diﬀerent ﬁelds, including image identiﬁcation, natural language processing, ﬁnancial analysis, and many more. MLPs do, however, have certain drawbacks, such as the potential for overﬁting when working with high-dimensional data or a lack of training instances. Several strategies, including regularisation, dropout, and early ending, can be used to lessen these diﬃculties.  For  more  complex  neural  networks,  such  convolutional  neural  networks  (CNNs)  and  recurrent  neural  networks (RNNs), MLPs act as a core framework. These networks expand on the ideas of MLPs by adding more layers and specialised structures to deal with kinds of data or jobs.  Backpropagation Algorithm: Multilayer Perceptrons (MLPs) and other artiﬁcial neural networks are frequently trained using the backpropagation technique.  It  allows  the  network  to  learn  from  labelled  training  samples  by  enabling  it  to  modify  its  weights depending on the variance between the anticipated output and the expected output. The weights of the network are updated through the backpropagation method, which combines gradient descent with forwards propagation.  A detailed explanation of the backpropagation algorithm is given below:  1. Initialise the network's weights: Begin by seting the network's weights to tiny, random numbers or zeros. 2.  Forwards propagation involves sending a training example  from the input layer to the output layer while  monitoring each neuron's activations and weighted sums. a.  To calculate the weighted total, multiply each neuron's input by its associated weight before adding them  all up.  b.  Use the activation function: To obtain the activation value, run the weighted sum through each neuron's  activation function.  c.  Send activations to the following layer: Send each neuron's activations from one layer to the next layer's  neurones as inputs.  3.  Do the output error calculation: Compare the network's anticipated result to what the current           training example should have produced. Calculate the  discrepancy between  the outputs that  were predicted and those that were expected, usually using a loss function like mean squared error or cross-entropy.  4.  Backwards  propagation:  Update  the  weights  by  propagating  the  mistake  backwards  across  the  network. Calculating  the  gradient  of  the  error  with  respect  to  each  weight  in  the  network  is  the  ﬁrst  step  in  this procedure. a.  Calculate the gradient of the output layer by computing the error in relation to the weighted sum of each output layer neurone. This gradient shows how the error might vary depending on how the weighted sum was modiﬁed.  b.  Backpropagate the gradients: Using the chain rule of derivatives, backpropagate the gradients across the network from the output layer to the hidden layers. The gradients of the neurones in the layer below are used to determine the gradient of each hidden neurone.  c.  Update  the  weights:  To  reduce  error,  update  the  network's  weights  a(cid:332)er  computing  the  gradients. Gradient descent or an optimisation technique like stochastic gradient descent (SGD) or Adam are used to update the weights.  5.  Repetition  is  required:  For  each  training  sample  in  the  dataset,  repeat  steps  2  through  4.  It  is  normal to continue  this  procedure  of  forwards  propagation,  error  calculation,  and  backwards  propagation  across  a number of epochs or until a stopping requirement is satisﬁed.  The network may modify the weights depending on mistakes that have been transmitied backwards through the network thanks to the backpropagation method. The network learns to minimise the gap between its projected outputs  and  the  anticipated  outputs  by  repeatedly  updating  the  weights,  which  enhances  its  performance  on unobserved data.  Although  backpropagation  is  useful  for  training  neural  networks,  it  can  have  drawbacks  including  overﬁting  or becoming stuck in local minima. To overcome these diﬃculties and improve the training process, a few methods and changes, including regularisation, dropout, and alternative optimisation algorithms, have been created.  Introduction to Deep Learning: A deep learning model is made to continuously analyse data using a logical framework that resembles the way a person would come to conclusions. Deep learning employs layered algorithms with a structure like ANNs to do this.  Advanced  neural  networks  are  used  in  deep  learning  techniques.  Numerous  tasks  from  the  actual  world,  such picture and handwritien digit identiﬁcation, have been successfully learned by them. Several popular deep learning architectures include:    Networks with convolutions   Autoencoders   Networks of Deep Belief   Boltzmann apparatus   Machines with a Boltzmann limit   Boltzmann Deep Machines   Deep Neural Systems  Convolutional neural network: An artiﬁcial neural network called a convolutional neural network (CNN) is made particularly for processing input that has a grid-like structure, including pictures, audio spectrograms, or time series data. They are quite good in computer vision, object identiﬁcation, and picture recognition tasks.  Convolutional layers, which are specialised layers that apply ﬁlters or kernels to the input data, are the fundamental component of CNNs. By methodically sliding across the input, these ﬁlters carry out local operations by computing dot products between the ﬁlter weights and the corresponding input values. Various characteristics or patierns in the input data are highlighted in the subsequent outputs, which are referred to as feature maps.  The following are some fundamental CNN ideas:  1.  Layers having several ﬁlters that convolve with the input data are known as convolutional layers. Each ﬁlter  creates a feature map by identifying various characteristics, such as edges, textures, or forms.  2.  Pooling Layers: The most crucial data is  preserved while the  spatial dimensions of  the feature  maps are reduced through the usage of pooling layers. Max, average, and sum pooling are a few common pooling processes.  3.  Activation  Functions:  Convolutional  and  pooling  layer  outputs  are  subjected  to  element-by-element application of activation functions, such as ReLU (Rectiﬁed Linear Unit). They add non-linearity and make it possible for the network to simulate intricate interactions.  4.  Fully Connected Layers: Convolutional and pooling layers are frequently followed by fully connected layers in  CNNs.  Similar to  conventional  multilayer  perceptrons,  these  layers  link  every neurone  in  one  layer  to every neurone in the following layer. They execute classiﬁcation or regression tasks and capture high-level representations.  5.  Backpropagation and Training: The backpropagation algorithm is frequently used to train CNNs. Gradient Descent Optimisation is used to alter the weights of the ﬁlters and fully connected layers depending on the discrepancy between the network's predictions and the anticipated outputs.  CNNs  are  able  to  automatically  learn  hierarchical  data  representations  because  of  their  distinctive  architectural structure. While later layers gradually pick up more abstract and complicated elements, the early layers capture low-level  information  like  edges  and  textures.  Because  of  its  hierarchical  nature,  CNNs  can  eﬃciently  extract pertinent characteristics and generalise well to novel, untried data.  In  several  applications, such as  image classiﬁcation,  object  identiﬁcation,  image segmentation,  and even  natural language processing tasks like text categorisation, CNNs have achieved exceptional success. They have contributed signiﬁcantly to the development of computer vision and have shown superior performance on several benchmark datasets.  It's important to note that numerous CNN designs, including LeNet, AlexNet, VGGNet, GoogLeNet (Inception), and ResNet,  have  become  very  well-liked.  Although  the  number  and  conﬁguration  of  layers,  ﬁlter  sizes,  and  other architectural  decisions  vary  amongst  these  designs,  they  nonetheless  adhere  to  the  essential  concepts  of convolutional layers and hierarchical feature learning.  Autoencoder: A method for unsupervised learning that is a member of the artiﬁcial neural network family is called an autoencoder. It is intended to develop eﬀective data representations by precisely recreating the input. Data compression, feature extraction, and dimensionality reduction are all common uses for autoencoders.  An encoder and a decoder are the two main components of an autoencoder's design, and they are coupled by a botileneck layer. The  encoder  takes the input  data  and  maps  it  to  a latent  space  or encoding, which is a  lower- dimensional representation. The most signiﬁcant characteristics and patierns of the input data are captured by the latent space. Following that, the decoder uses the encoded representation to recover the original input data.  Various ﬁelds, such as computer vision, natural language processing, and recommendation systems, have made use of autoencoders. The activities of data exploration, denoising, and the creation of new samples are made easier by the framework they oﬀer for learning meaningful representations of complicated data.     