Motivation and Inspiration  The brain consists of a highly parallel architecture of inter-connected neurons and the development of Artificial Neural Networks (ANN) is motivated by these biological neural systems. What is different about neural networks?  -  Linear models are not always sufficient functions or decision boundaries are nonlinear. -  While support vector machines construct nonlinear functions, they use fixed feature  transformations.  -  Neural networks allow for feature transformations to be learnt from the data itself  The two areas of ANN research are:  -  Using ANN to study and model the brain -  Using the brain as motivation to design ANN’s that don’t necessarily model the brain but aim  to be effective learning machines.  Neural System Basics  The major components in a neural network system are:  -  An input layer (typical of most neural networks) -  One or more hidden layers -  Combiners (sum functions) -  Nonlinear activation functions -  An output layer  There is typically always an input layer which as the name suggests is responsible for taking in the input features.  Since neural networks are compatible with high-dimensional inputs and multi-label classification there are often bigger and more complex neural networks. Although a more complex neural network can result in over-fitting if an insufficient amount of training data is supplied.  Perceptron Algorithm  Perceptron is a linear classifier and is a single layer neural network used for binary classification, a multi-layer perceptron is also a neural network.  Perceptron steps in solving problem:  Initialise w = 0  - -  Retrieve next x input and desired y  o  Compute actual input o  Compute output error o  Update weight o  Repeat step 2 until convergence  Motivation for Multilayer perceptron  Is separating data points with a single line based on their class label always possible? While we can separate AND or OR problems with a single line we cannot do the same with XOR problems. For   XOR problems we need a layer in the middle which results in the creation and use of multilayer perceptron so that we can represent this problem.  Multilayer perceptron  Since a perceptron is weak in what can be represented, we need multi-layer networks to represent non-linear complex decision surfaces. If we use a complex activation function with our neural network we can combine the inputs in more complex ways and be able to model functions better, this is not doable with the binary output from a single layer perceptron.  An ANN where connections do not form a cycle is known as a feedforward neural network. Since there are no cycles in the network input information only goes in one direction from the input nodes to the output nodes.  A multi-layer feed-forward neural network can also be called a Mult-layer Perceptron (MLP), the model is made from multiple layers of logistic regression like models instead of multiple perceptron’s.  Multi-layer perceptron facts  -  The output is a vector -  There are two kinds of weights: input → hidden and hidden → output -  The input layer does no computation as it only relays the input vector - -  It can have more than one hidden layer It does not have to be fully connected  Introduction to Deep Learning  A deep learning model is supposed to continually analyse data with a logical structure that aims to replicate that of a human, this is done with a layered structure of algorithms similar to ANNs. Deep learning methods are advanced neural networks.  Convolutional Neural Networks  A convolutional neural network (CNN) is based off of the mammalian visual cortex which is where the brain processes visual input. Within this cortex specific neurons only fire when looking at specific phenomena.  CNNs are made of three basic concepts:  -  Sparse interactions: Sparse weights within a smaller kernel as opposed to the whole input  which helps to reduce the number of parameters.  -  Parameter sharing: A kernel uses the same set of weight while applying to different locations. -  Translation invariance: Invariance refers to being able to recognise an object as an object even  if its appearance varies in some way.  Autoencoder  An autoencoder is a neural network that can handle many hidden layers in its structure which aims to learn a representation (encoding) for a set of data, usually for the purpose of dimensionality. These kinds of neural networks are trained to copy their input to their output.      