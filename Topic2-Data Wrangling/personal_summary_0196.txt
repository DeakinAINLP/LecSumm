The  need  to  model  non-  linear and  complex  relationships  have  given  rise  to  the  need  for  coming up  with algorithm that mimics the way biological neuron’s function which are the Artificial Neural Networks (ANN). Two groups of ANN researchers are working on this issue: one utilises ANN to investigate and model the brain, and the other uses the brain as the inspiration to develop ANNs as efficient learning machines.  ANN’s have many applications like image recognition, speech recognition, learn complex patterns and others. Like the functioning of brain where there are multiple neurons that act to get a response in result, ANN also has input layer, hidden layers, and output layer. The input layer takes the features as input and processes it to the hidden layers which can be decided based on the model requirements and the processed data is extracted from output layer. Input and output layers count does not have to be equal, that is, input layer with 8 feature inputs, processed through three hidden layers can result in output layer which has four feature or output points. Since neural networks can handle high-dimensional inputs and multi-label classification, we can build them bigger and more complex according to requirement. But it should be known that increasing the complexity may result in overfitting. For linear binary classification a single layer network known as perceptron is used while multi- layer  perceptron  is  nothing  but  a  neural  network.  Based  on  the  distance  of  classified  datapoints  from  the defined hyperplane line the confidence about the decision is measured. That is if the point is far enough from the  line,  then  there  is  higher  confidence  but  if  the  point  is  near  the  line  there  is  little  confidence  in  the classification.  This  perceptron  algorithm  helps  solve  classification  problem  of  linear  separability.  The algorithm involves computing outcomes from inputs, calculating output errors, updating weights accordingly and continue repeating this till convergence. If training examples cannot be separated linearly, convergence is not guaranteed. The availability of linearly non separatable datasets have given rise to the need of using neural networks. Later the existence of XOR problem in Multilayer Perceptron (MLP) was identified. While single layer  perceptron  uses  sign  function  as  the  activation  function,  with  increase  in  complexity  of  model  and introduction  of  non-  linearity  various  other  activation  functions  like  logistic,  relu,  tanh  etc  have  been introduced.  Feedforward neural networks does not involve any loops or cycles, but the data transfer is in one direction only, MLP is nothing but a multi-layer feedforward neural network. MLP is a flexible strong machine learning model  for  tasks  like  classification,  regression,  and  pattern  recognition.  For  formulation  of  efficient  MLP model, it is essential to minimise error functions. Gradient- descent which uses Backpropagation is used for minimisation of the error functions. A good method for minimising functions like the error function is gradient descent.  Initialising  random,  it  requires  descending  the  surface  in  the  direction  of  the  steepest  decline. Stochastic  Gradient-Descent  (SGD)  is  used  to  guarantee  a  local  minimum  and  reduce  the  instantaneous approximation of utilising only the current training sample. In backpropagation algorithm, training data is initialized as input and weights that are initialized at the start are updated until stopping criteria is met. Python codes are studied for the above discussed techniques.  Deep learning algorithms process data through multiple "layers" of neural network algorithms, with each layer sending a simplified version of the input information to the next layer. Deep learning models evaluate data and learn tasks like image and handwritten digit identification using ANNs and advanced neural networks. Convolutional networks (CNNs) are an important part of deep learning designs. Autoencoders, Deep Belief Networks,  Boltzmann  machines  Deep  neural  Networks  are  other  Deep  learning  architectures.  CNNs  are mostly used for analysing visual imagery and learns directly from data. CNN functioning is like mammalian visual cortex where neurons are activated only when the phenomenon is in the field of vision. To limit the number of parameters and enable the abstraction of an object's identification from the visual input, CNNs make advantage of sparse interactions, parameter sharing, and translation invariance. LeNet5 is one of the very first CNN that was developed and involved five layers. The input layer stores the image's pixel values, the convolutional layer controls neurones' output, the rectified linear unit (ReLu) introduces an elementwise activation function, the pooling layer downscales the image, and the fully connected layers attempt to derive class rankings from the activations. This network proved to be the inspiration for many modern architectural designs. Large models, innovative training methods, and quick GPUs have made deep learning feasible and  various applications can be observed. An autoencoder is a neural network that develops an encoding for a piece of data, usually with the goal of reducing dimensionality. It is trained to condense input layer data into a  brief  code,  which  it  can  then  decompress  into  data  that  closely  resembles  the  input  layer  data.  The Autoencoder's loss function should be able to determine the difference between the input and the output. The original feature vector can be recreated using autoencoders, an efficient feature learning technique, but the transformed hidden layer is more compact and significant. Autoencoders uses the techniques of encoding and decoding for dimensionality reduction and feature extraction. Autoencoders provide an effective framework for learning data representations and have been used in a variety of fields, including recommendation systems, natural language processing, and image processing. Finally, python codes are studied for the same.  