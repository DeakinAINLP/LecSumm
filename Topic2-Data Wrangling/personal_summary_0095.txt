Summary  10.2:  Linear models break down in predictive power in cases where decision boundaries are extremely non-linear. Artificial neural networks present a possible alternative to these learning machines. This involves study of biological neural networks, the human nervous system.  10.3:  A neural network consists of an input layer, output layer and one or multiple hidden layers. It also contains combiners and activation functions.  The input layer takes the input features, and output layers display the output features. These can handle high dimensions, but may result in overfitting due to complexity.  10.4/ 10.5:  Perceptron is a linear classifier, consisting of a single layer neural network. A multi- layer perceptron is a neural network. For each point the hyperplane is updated to reach a final plane/ line.  A multi-layer perception can represent a non-linear problem, whereas a single layer can only represent a linear binary classifier.  10.6:  Feedforward neural networks do not form a cycle and information only moves in a singular, forward direction. A multi-layer feedforward neural net is a multi-layer perceptron. It comprises multiple layers of logistic regression.  It results in a vector and relies on input vectors, computation occurs in the hidden layers.  10.7:  Backpropogation is used for training multi-layer perceptrons. It works by computing the gradient of a loss function with regard to weights of the network. It computes this one layer at a time, working backward from the ultimate layer.         10.9/ 10.10/ 10.11:  Deep learning continually analyses data with a layered structure similar to artificial neural networks.  Convolutional neural networks are modelled after the mammalian visual cortex. They have sparse weights within a smaller kernel. This reduces the volume of parameters.  Each network layer is a detection filter, detecting the presence of specific features of patterns present in the data. Initial layers detect large features which are easy to interpret. Deeper layers detect more refined features.  10.12:  Autoencoders are neural networks that handle many hidden layers. They learn a representation for a set of data. This is often to reduce dimensionality. It learns to compress data from the input layer into short form, and decompress it into something closely matching.  