  Perceptron Algorithm The Perceptron algorithm is a two-class (binary) classification machine learning algorithm. It is a type of neural network model, perhaps the simplest type of neural network model. It consists of a single node or neuron that takes a row of data as input and predicts a class label. This  is  achieved  by  calculating  the  weighted  sum  of  the  inputs  and  a  bias  (set  to  1).  The weighted sum of the input of the model is called the activation.  If the activation is above 0.0, the model will output 1.0; otherwise, it will output 0.0.  Activation = Weights * Inputs + Bias    Predict 1: If Activation > 0.0   Predict 0: If Activation <= 0.0  Given that the inputs are multiplied by model coefficients, like linear regression and logistic regression, it is good practice to normalize or standardize data prior to using the model. The  Perceptron  is  a  linear  classification  algorithm.  This  means  that  it  learns  a  decision boundary that separates two classes using a line (called a hyperplane) in the feature space. As such, it is appropriate for those problems where the classes can be separated well by a line or linear model, referred to as linearly separable. The  coefficients  of  the  model  are  referred  to  as  input  weights  and  are  trained  using  the stochastic gradient descent optimization algorithm.  Mul$-layer Perceptron A  multilayer  perceptron  (MLP)  is  a  deep,  artificial  neural  network.  A  neural  network  is comprised of layers of nodes which activate at various levels depending on the previous layer’s nodes. When thinking about neural networks, it may be helpful to isolate your thinking to a single node in the network. Multilayer perceptron refers to a neural network with at least three layers of nodes, an input layer, some number of intermediate layers, and an output layer. Each node in a given layer is connected to every node in the adjacent layers. The input layer is just that, it is the way the network takes in data. The intermediate layer(s) are the computational machine of the network, they  actually  transform  the  input  to  the  output. The  output  layer  is  the  way  that  results  are obtained from the neural network. In a simple network where the responses are binary, there would likely be only one node in the output layer, which outputs a probability like in logistic regression.  Backpropagation Algorithm Backpropagation  is  one  of  the  common  terms  while  one  is  learning  Introduction  to  Neural Networks. It is an algorithm used in the training of neural networks to adjust the weights of a single neuron by moving backward from the output of the neuron. It involves the process of adjusting the weights of the inputs to a neural network in order to minimize errors. The process starts  with  randomly  generating  weights  for  the  network,  then  using  backpropagation  to optimize them to the model. This is one of the essential components of a Neural Network for it to have good performance.   Introduc$on to Deep Learning Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.  Convolutional Neural Network (CNN) Convolutional  Neural  Network  (CNN)  is  the  extended  version  of  artificial  neural  networks (ANN) which is predominantly used to extract the feature from the grid-like matrix dataset. In a regular Neural Network there are three types of layers:    Input  Layers:  It’s  the  layer  in  which  we  give  input  to  our  model.  The  number  of neurons in this layer is equal to the total number of features in our data (number of pixels in the case of an image).    Hidden Layer: The input from the Input layer is then feed into the hidden layer. There can be many hidden layers depending upon our model and data size. Each hidden layer can have different numbers of neurons which are generally greater than the number of features. The output from each layer is computed by matrix multiplication of output of the  previous  layer  with  learnable  weights  of  that  layer  and  then  by  the  addition  of learnable biases followed by activation function which makes the network nonlinear.   Output Layer: The output from the hidden layer is then fed into a logistic function like sigmoid or softmax which converts the output of each class into the probability score of each class.  