Topic  10  was  a  very  insightful  and  enriching  topic,  where  we  learnt  about  advanced cutting-edge topics related to non-linear models, such as artificial neural networks (ANN). These non-linear models help us draw inferences from non-linear data.  We were initially given a brief of the working of the human brain with the help of neurons and how these connections help us act based on our past experience. For example, our eyes look at ice cream and based on past experience and preference, we decide to eat it. In this case, the eyes are the input layer, the layer of neurons that process our past experiences and preferences serve as the ‘hidden layer’ and  our mouth eating the ice cream serves as the output layer. Artificial Neural Networks (ANN) seek to mimic this very behavior of the human brain. They primarily consist of 3 layers:  1.  An input layer 2.  An output layer 3.  Multiple hidden layers  The more the number of hidden layers, the more the complexity of the neural network. The higher the dimension of the input layer, the higher the complexity of the model. The connections  between  these  layers  may  have  ‘weights’  which  correspond  to  the importance of that particular connection and accordingly reflect in the output of the neural network.  Perceptron Algorithm: When the neural network has only one hidden layer, it is called a perceptron algorithm. Vice-versa, a multi-layer perceptron algorithm is called a neural network.  We  were  given  the  example  of  Neuroevolution  of  augmenting  topologies (NEAT),  which  is  one  of  the  most  popular  Neural-Network  based  methods  used  in developing games.  We then touched upon the significance of the multi-layer perceptron. In case of AND and OR functions, it is possible to separate the data points based on their class label with a single line. However, this is not possible for an XOR function, and this is where a multi- layer perceptron model helps.  Feedforward  Neural  Network:  We  further  explored  a  sub-type  of  multi-layer perceptron(MLP)  called  the  feedforward  neural  network.  In  this  case,  there  is  no feedback/loop/circle from the output back to the input. Instead, the data only moves in one direction – the forward direction, from the input layer, processed through the hidden layers and then through the output layer. These neural networks found their use in early      applications  of  image  processing  and  autonomous  self-driven  cars.  We  also  touched upon the concept of gradient-based optimization.  Backpropagation  Algorithm:  Unlike the backpropagation algorithm actually has a  loop/feedback mechanism, where the neural network  iteratively  changes  the  weights  to  reach  an  optimal  output,  based  on  certain criteria. This criteria that determines when the feedback stops can be one of the following:  feedforward  network,  the  above  1.  Number of defined iterations are reached. 2.  Desired accuracy is obtained. 3.  Desired amount of change is obtained.  Deep  Learning:  We  were  then  introduced  to  the  very  interesting  concept  of  Deep Learning, which are advanced neural networks used to continually analyse data with a logical structure and draw conclusions from them, akin to how a human brain would. It finds its applications in image recognition, hand-written digit recognition, etc. Some types of deep learning are:  1.  Convolutional Neural Networks (CNN) 2.  Auto Encoders 3.  Deep Neural Networks 4.  Botzman machines, and so on.  Convolutional  Neural  Networks(CNN):  The  CNN  is  modelled  after  the  mammalian visual vortex, which is the part of a mammal’s brain where visual input is processed. This implies that neurons are fired based on the objects in the line of vision. Features of CNN include:  1.  Sparse interactions: These help to reduce the number of parameters. 2.  Parameter sharing 3.  Translation invariance  We  concluded  the  topic  by  learning  about  few  applications  of  CNN  and  a  brief  on Autoencoders. Overall, this topic was very enriching in terms of knowledge and was a perfect end to this very insightful unit.          