 This topic’s topic was focussed on neural networks. To start off with, I think it would be good to  cover the basic structure of a neural network. We essentially have three main portions: an input layer, n  number of hidden layers and an output layer. As the name implies, the input later is for our input data  which passes it to the hidden layer which performs computations which then finally passes to the output  layer which contains our predictions. We have the concept of a neuron which takes some input, applies  some weights, and performs computations (activation functions) which then produces an output. We also  have something known as a perceptron which is a simple neural network containing a single layer with  one node. Here, each input node will have a weight value which then at the node, we have a summation  function to get the weighted sum of all inputs, which then goes into an activation function (uses a  threshold to figure out if 0 or 1) to produce the output of 0 or 1 for a binary classification problem.  Ultimately, the goal of the perceptron is to learn the optimal weight vector to create a decision boundary  so that we can correctly classify two classes. The general process of this is to initialize w to 0, compute  the error based on output and actual output and update the weight (to minimize the error) until  convergence.  Perceptrons themselves are quite weak and will need a multi-layer perceptron for more  complex and non-linear problems with different activation functions (introducing a sigmoid node that  uses the sigmoid function).  In terms of optimizing weights, we have an algorithm known as Backpropagation which utilizes  gradient descent (stochastic gradient descent) based on the error that gets back propagated from the  network. Ultimately, we aim to minimize the error using this algorithm. In terms of the overall process,  we have our training input data where we initialize weights to random numbers between -0.5 and 0.5.  For each sample, we then iteratively propagate the input forward and calculates values for hidden nodes  and the output node. We then back propagate the error to update the weights. Once we have met the  stopping criteria, we will have an output network with trained weights. It is worth mentioning some  issues with back propagation where we have the local minima, basically meaning we cannot get to the  global minimum. This can have some possible fixes by introducing a momentum term. The other issue is  overfitting where the network can memorize all training samples which can happen due to too many  hidden nodes. Some fixes include using cross validation, weight decaying (regularization techniques) and  reducing the hidden nodes which reduces the model’s complexity.  Briefly, we have more advanced deep neural networks which include Convolutional Neural  Networks which is good for computer vision problems.  There’s also deep autoencoder which generates  output that resembles the input data.   