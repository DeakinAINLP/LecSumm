 Neural Networks  Linear models may not be sufficient when the underlying functions boundaries are  extremely nonlinear;  Support vector machines can construct nonlinear functions but use fixed feature  transformations, which depends on the kernel function;  Neural Networks allow the feature transformations to be learned from data.  Perceptron: a simple neural network used for binary classification;  It has only one layer with single node:  Given: input vector of dimensions and weight vector;  Given weight the perceptron linearly divides input space into two regions;  The corresponds to the two sides of the hyper-plane defined by the equation.  Perceptron Convergence Theorem  If training instances are drawn from two linearly separate sets, then the perceptron  learning rule will converge after finite iterations.  However, no guarantee for convergence of and not linearly separable.  Multi-layer Feed-forward NN  Perceptron is quite weak in what it can represent;  For complex, non-linear decision surfaces, need multi-layer network.  Sigmoid node: like a perceptron, but with the sigmoid function instead of the sign  function.  A feed forward neural network is and ANN wherein connections between units do  not form a cycle.  Multi-layer feed forward NN is also known as Multi-layer Perceptron (MLP).  Detour: Stochastic Gradient Descent(SGD)  SGD minimizes the instantaneous approximation of using only -th instance;  Update rule is SGD is cheap to perform and guaranteed to reach a local minimum in  a stochastic sense.  Training MLP: Back-propagation  Minimizing instantaneous approximation for current training sample  Summary of Reading  Deep Learning : are advanced neural networks  They have been successful in learning many real world tasks.  Deep learning architectures:   Convolution Networks(CNN / ConvNets) :  Translated in technical terms:  Sparse interactions: sparse weights within a smaller kernel instead of the whole  input, helps reduce params;  Parameters sharing: a kernel use the same set of weights while applying onto  different location.   Auroencoders : simply a neural network that tries to copy its input to its output   Input   An encoder function parameterized by   A coding representation   A decoder function parameterized by   An output, also called reconstruction   A loss function that computes a scalar to measure how good of a reconstruction  of the given input   Deep Belief Networks   Boltzmann Machines   Restricted Boltzmann Machines   Deep Boltzmann Machines   Deep Neural Networks  What assisted Deep Learning:  Larger models with new training techniques;  Large Image Net data set;  Large graphical processing units;  Self-reflection  An MLP is an artificial neural network and hence, consists of interconnected neurons  which process data through three or more layers. The basic structure of an MLP  consists of an input layer, one or more hidden layers and an output layer, an  activation function and a set of weights and biases:  The input layer is the initial layer of the network, taking input in the form of numbers.  Secondly, there is the hidden layer, which processes the information received from  the input layer. This processing is in the form of computations. There is no restriction  on the number of hidden layers, however, an MLP usually has a small number of  hidden layers. Finally, the last layer, the output layer is responsible for producing  results. The result is the output from the computations applied to the data through  the network. MLPs are usually used for data that is not linearly separable, such as  regression analysis. Alternatively, due to their simplicity, they are most suited for  complex classification tasks and predictive modeling. Additionally, MLPs have been  used for machine translation, weather forecasting, fraud detection, stock market  prediction and credit rating prediction.  A Deep Neural Network (DNN) is simply an artificial neural network with deep layers.  Deep layers in this context mean that the network has several layers stacked  together for processing and learning from data. DNNs were originally inspired by  neurobiology. Particularly, the way humans learn to perceive and recognize physical  things. Similar to the structure of an MLP, a DNN is composed of an input layer,  hidden layers, output layers, weights, biases, and activation functions. Alternatively,  in the case of a CNN, the neural network would be composed of a pooling and  convolutional layer in addition to the components already mentioned. DNNs are  powerful algorithms due to their deep layers. Hence, they are usually used for  handling complex computational tasks. Computer vision is one of these.   