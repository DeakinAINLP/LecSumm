Our brain has a highly parallel architecture and networks of interconnected neurons. The biological brain systems are the driving force behind the development of Artificial Neural Networks (ANNs).  Two groups of ANN researchers are tackling this issue in machine learning:    One group studies and models the brain using ANN.   The other group draws inspiration from the brain in order to create effective learning  machines for ANNs, however this approach might not produce a genuine model of the brain.  Because neural networks can handle high-dimensional inputs and multi-label classification, we can build them bigger and more complex.  Perceptron is a linear classifier (binary) and is a single layer neural network. A multi-layer perceptron is called a neural network.  In a feedforward neural network, connections between units do not cycle, making it an artificial neural network (ANN). In this network, information travels only in one direction— forward—from the input nodes to the output nodes, passing via any hidden nodes that may exist. The network doesn't contain any loops or cycles.  Multi-layer Perceptron (MLP) is another name for a multi-layer feed-forward neural network (NN).  The search directions used by gradient-based optimization algorithms are determined by the function's gradient at the current position.  A deep learning model is made to continuously analyse data using a logical framework that resembles the way a person would come to conclusions. Deep learning employs layered algorithms with a structure like ANNs to do this.  A Convolutional Neural network's (CNN or ConvNet) architecture is modelled after the mammalian visual cortex, the area of the brain responsible for processing visual input.  CNNs are made of three basic concepts:    Sparse interactions   Parameter sharing   Translation invariance  Every network layer in a CNN serves as a detection filter to look for certain traits or patterns in the raw data. The initial layers of a CNN find significant features that are relatively simple to identify and analyse.    A neural network that can manage numerous hidden layers is called an Autoencoder.  An Autoencoder's goal is to learn a representation (encoding) for a set of data, usually with the intention of reducing dimensionality.  