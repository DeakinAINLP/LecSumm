 Development of Artificial Neural Networks (ANNs) is motivated by biological neural systems.  Is there a necessity for neural networks in some machine learning problems? First we need to answer the question of what is different about Neural Networks? linear models may not be sufficient when the underlying functions or decision boundaries are extremely nonlinear support vector machines can construct nonlinear functions but use fixed feature transformations, which depends on the kernel function. For example, while you are using linear kernel, obviously your model output will be a linear model.  What if you are not sure about the kernels or perhaps you prefer to learn these features from the data itself? Neural Networks allow the feature transformations to be learnt from data.  Neural system basics  With the brain in mind, let’s introduce the major players in a neural network system:  a typical neural network (machine) has an input layer;      it has one or many hidden layers; it has combiners (sum functions); it has nonlinear activation functions; it has an output layer.  The following figure is shows two examples of simple neural network structures.  We can have more complex, bigger neural networks because neural networks are compatible with high dimensional inputs and multi-label classification. As you can see in the figure above, the diagram shows a complex neural network with 3 hidden layers and one high dimensional input layer with a 3- dimensional output layer.  Having a more complex neural network can result in over-fitting if you are not providing enough training data.  Perceptron Algorithm  Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. The perceptron consists of 4 parts.  Input values or One input layer  Weights and Bias Net sum Activation Function  Weights shows the strength of the particular node.  A bias value allows you to shift the activation function curve up or down.  Consider the AND and OR logical gates or functions. As you can see in the figure, we can show that both of these problems are linearly separable in 1 and 0 class labels:  Figure. Linearly separable problems.  We can easily find a line to divide based on the class labels. But what about the XOR problem? This problem is not linearly separable. As you can see in the next figure (below), it is impossible to separate the data points based on their class labels with a single line.  Figure. XOR - Nonlinear problem  These kinds of problems were a motivation to develop a new neural network, but this time with a layer in the middle to handle these cases. Later it was proven that a multilayer perceptron (MLP) can represent the XOR problem.  Figure. Multilayer perceptron representing the XOR problem  Motivation for multilayer perceptron  The multilayer perceptron (MLP) is a type of artificial neural network that consists of multiple layers of interconnected nodes, known as neurons. The motivation behind using MLPs as a machine learning model can be summarized as follows:  Non-linear relationships: MLPs are powerful models capable of capturing complex non-linear relationships in data. They are well-suited for tasks that involve non-linear mappings between input and output variables. By incorporating multiple layers with non-linear activation functions, MLPs can learn and represent intricate patterns in the data.  Universal approximation theorem: The universal approximation theorem states that MLPs with a single hidden layer can approximate any continuous function to arbitrary precision, given a sufficient number of neurons in the hidden layer. This theorem demonstrates the expressive power of MLPs and their ability to approximate a wide range of functions.  Feature extraction and abstraction: MLPs with multiple hidden layers can extract hierarchical  representations of input data. Each layer learns to abstract and transform the features learned from the previous layer, allowing the network to gradually build more complex and meaningful representations. This hierarchical feature extraction is particularly useful in tasks such as image recognition or natural language processing, where the input data has a high-dimensional structure.  Backpropagation algorithm: MLPs are trained using the backpropagation algorithm, which  efficiently updates the network's weights based on the error between the predicted output and  the desired output. This algorithm calculates the gradient of the error with respect to the weights, enabling the network to iteratively adjust its parameters and improve its performance. Backpropagation is a key factor in the success of MLPs as it enables efficient learning and optimization of the network's parameters.  Versatility and generalization: MLPs can be applied to various types of machine learning tasks,  including classification, regression, and pattern recognition. They have been successfully used in a wide range of domains, from computer vision to speech recognition, and from finance to healthcare. MLPs also have the ability to generalize well to unseen data, making them effective in real-world applications.  Multilayer perceptron  Multi-layer perceptron (MLP) is a supplement of feed forward neural network. It consists of three types of layers—the input layer, output layer and hidden layer, as shown in Fig. 3. The input layer receives the input signal to be processed. Choice of node in a multi-layer network should be continuous but it should be a continuous meaningful function such as the sigmoid function.  Feedforward neural networks  A feedforward neural network is an Artificial Neural Network (ANN) where connections between units do not form a cycle. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.  A multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer Perceptron (MLP). The term MLP is really an accurate name because the model comprises multiple layers of logistic regression like models (with continuous non-linearities) rather than multiple perceptrons (with discontinuous non- linearities).  Consider a two-layer network: input layer, hidden layer and output layer (see figure below).  Figure. Two-layer neural network.  There are some really important facts about this network you should consider:  the output is a vector there are two kinds of weights here:  input → hidden  o o  hidden → output  the input layer does no computation - it only relays the input vector it can have more than one hidden layer another interesting feature is that it does not have to be fully connected.  MLP Formulation  Multilayer Perceptron falls under the category of feedforward algorithms, because inputs are combined with the initial weights in a weighted sum and subjected to the activation function, just like in the Perceptron. But the difference is that each linear combination is propagated to the next layer.  Each layer is feeding the next one with the result of their computation, their internal representation of the data. This goes all the way through the hidden layers to the output layer.  If the algorithm only computed the weighted sums in each neuron, propagated results to the output layer, and stopped there, it wouldn’t be able to learn the weights that minimize the cost function. If the algorithm only computed one iteration, there would be no actual learning.  Backpropagation Algorithm  Backpropagation is the learning mechanism that allows the Multilayer Perceptron to iteratively adjust the weights in the network, with the goal of minimizing the cost function.  There is one hard requirement for backpropagation to work properly. The function that combines inputs and weights in a neuron, for instance the weighted sum, and the threshold function, for instance ReLU, must be differentiable. These functions must have a bounded derivative, because Gradient Descent is typically the optimization function used in MultiLayer Perceptron.  In each iteration, after the weighted sums are forwarded through all layers, the gradient of the Mean Squared Error is computed across all input and output pairs. Then, to propagate it back, the weights of the first hidden layer are updated with the value of the gradient. That’s how the weights are propagated back to the starting point of the neural network.  This process keeps going until gradient for each input-output pair has converged, meaning the newly computed gradient hasn’t changed more than a specified convergence threshold, compared to the previous iteration.  Introduction to Deep Learning  A deep learning model is designed to continually analyze data with a logic structure similar to how a human would draw conclusions. To achieve this, deep learning uses a layered structure of algorithms similar to ANNs.  Deep learning methods are advanced neural networks. They have been successful in learning many real world tasks (e.g. handwritten digit recognition, image recognition). Some of the common Deep learning architectures are:  Convolutional Networks Autoencoders Deep Belief Networks Boltzmann Machines Restricted Boltzmann Machines Deep Boltzmann Machines Deep Neural Networks  Convolutional Neural Networks  The architecture of a Convolutional Neural Network (CNN or ConvNet) is modelled after the mammalian visual cortex, the part of the brain where visual input is processed.  Within the visual cortex, specific neurons fire only when particular phenomena are in the field of vision. For example, one neuron might fire only when you are looking at a left-sloping diagonal line and another only when a horizontal line is in view. Our brains process images in layers of increasing complexity. The first layer distinguishes basic attributes like lines and curves. At higher levels, the brain recognizes that a configuration of edges and colours is, for instance, a house or a bird.  Consider the figure below. Focusing on the regions of the image is the main attribute. As you can see in the figure, this network does not appear to be fully connected (it’s not going to use all the pixels) and is dealing with local regions which result in sparsity and locality.  CNNs are made of three basic concepts:  Sparse interactions: Sparse weights within a smaller kernel (e.g., 3×3,5×5) instead of the whole input. This helps reduce the number of parameters. The term kernel in CNN generally refers to an operator applied to the entirety of the image such that it transforms the information encoded in the pixels (see the figure above).  Parameter sharing: A kernel uses the same set of weights while applying to different locations  (sliding windows).  Translation invariance: Invariance means that you can recognize an object as an object, even when its appearance varies in some way. This is generally a good thing, because it allows abstraction of an object’s identity or category from the specifics of the visual input. For example it will recognise the object even with different relative positions of the viewer/camera and the object (see figure below).  The LeNet5 architecture was fundamental. In particular the insight that image features are distributed across the entire image and that convolutions with learnable parameters are an effective way to extract similar features at multiple locations with few parameters. The following figures illustrate the structure of LeNet5.  Figure. LeNet5 structure.  LeNet5 features can be summarized as:   a convolutional neural network that uses a sequence of 3 layers: convolution, pooling, non- linearity   uses convolution to extract spatial features subsamples using spatial average of maps  nonlinearity in the form of tanh or sigmoids  basically an MLP as final classifier  uses a sparse connection matrix between layers to avoid large computational cost  Overall this network was the origin of many recent architectures and a true inspiration for many people in the field.  In a CNN every network layer acts as a detection filter for the presence of specific features or patterns present in the original data. The first layers in a CNN detect large features that can be recognized and interpreted relatively easily. As you can see the patch which has been shown in red (figure below) are found for interpreting the right side batch of images. Using other categories of images with this filter may not give proper results. In other words, each image filter patch is for finding a specific part of a pattern or texture.  You can see the same concept rules in layers 3, 4 and 5. Remember you need a large set of data for feeding these deep networks, otherwise the results can be useless.  Large ImageNet dataset  o  1.2 million training samples o  1000 categories   Using fast graphical processing units (GPU)  o  Capable of 1 trillion operations per second!  Autoencoder  An Autoencoder is a neural network which can handle many hidden layers in its structure.  The aim of an Autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.  This type of neural network is trained to attempt to copy its input to its output. Internally, it has a hidden layer Z that describes a code used to represent the input. Recently, the Autoencoder concept has become more widely used for learning generative models of data. Autoencoders are another way of feature learning. The solution is trivial, which means the solution or example are ridiculously simple and of little interest, unless there are constraints (such as sparsity) on the number of nodes in the hidden layers. 