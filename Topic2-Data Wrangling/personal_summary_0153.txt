The total number of neurons make up the ﬁrst layer of a network called the input layer. The last layer comprising of the possible outputs for the image corresponds to how well the ini?al image corresponds to the predicted output based on its ac?va?on. the layers in between a neural  network  are  the  hidden  layers.  The  ac?va?on  in  the  prior  layer  determines  the ac?va?on of the next layer. Feeding the neural network of an image ligh?ng up all the neurons of  an  input  layer  according  to  the  brightness  of  each  pixel  in  an  image,  that  paDern  of ac?va?ons  causes  a  very  speciﬁc  paDern  in  the  next  layer  to  be  ac?vated.  The  brightest neuron of the output layer is the end result of what the input represents. The hidden layers typically operate on the basis of which component of the input image corresponds well to the next input, taking by all possible combina?ons that make up the image that yield the ﬁnal predic?on accurately.  The subcomponents of the image or input is separated at each layer in order to yield all possible combina?ons of the input. The extent at which each subcomponent can accurately predict the output is denoted by its allocated weights wherein the brightness of its pixel (from nega?ve and posi?ve weights) gives a loose depic?on of the weight values within in a speciﬁc region of the image. To pick on edges, nega?ve weights can be allocated to surrounding pixels, giving darker surrounding pixels, and brighter middle pixels. The ac?va?on of  the  neuron  is  typically  how  posi?ve  the  relevant  weighted  sum  is.  The  weights  provide informa?on  on  what  pixel  paDern  the  neuron  in  the  layer  is  picking  up  on  and  the  bias indicates how high the weighted sum needs to be before the neuron becomes meaningfully ac?ve. The network is a func?on that takes in an input and yields an output.  A  complex  neural  network,  one  that  comprises  of  many  layers  can  result  in  over  ﬁHng  if enough training data is not provided. A perceptron is a linear classiﬁer and a single layer neural network. A mul?layered perceptron is called a neural network. A single layer neural network consists  of  one  layer  with  a  single  node  and  is  primarily  used  for  binary  classiﬁca?on.  A perceptron is used when the data is linearly separable. A dataset is linearly separable if there exists  a  hyperplane  that  separates  them.  The  perceptron  divides  the  input  space  into  two regions y(x,w) = 1 (above the hyperplane) and y(x,w) = -1(below the hyperplane), hence it is called a linear binary classiﬁer.  Nonlinear func?ons (also referred to as sigmoid func?on) output values from 0 to 1, displaying an s-shaped distribu?on. For MLP formula?on in order to ﬁnd the weights of the network so that the predicted values will be as close as possible, the diﬀerence between the predicted values and the true value have to be minimized. The sum is small when the network accurately predicts the output.  This is an op?miza?on problem that is achieved through minimizing the error func?on. The cost func?on input the parameters such as weights and/or biases, where a single measure is outpuDed that is known as the cost func?on. Gradient descent is used for minimizing the cost func?on. Since E(w) consists of many local minima, backpropaga?on is used. Gradient descent is a way to converge towards a local minimum of a cost func?on and ﬁnds the op?mal point. The nega?ve gradient of the cost func?on indicates how weights and biases should be changes in order to eﬃciently decrease the cost.  Backpropaga?on is an algorithm for determining how a single training example would like to nudge the weights and biases, in order to what rela?ve changes to those propor?ons yields the decrease to the cost. Data is typically divided into batches and the gradient descent is computed  for  each  batch,  resul?ng  in  the  convergence  of  a  local  minimum  to  the  cost func?on.  Deep  learning  u?lizes  a  layered  structure  of  algorithms  that  is  comprised  of  convolu?onal networks(CNN). CNN is comprised of sparse interac?ons, parameter sharing and transla?on invariance. It specializes in paDern recogni?on. It consists of convolu?onal layers wherein the amount of ﬁlters need to be speciﬁed. The ﬁlters are a matrix consis?ng of rows and columns with ini?alized values. The end product is the dot product of the matrix with the respec?ve inputs.  The ﬁlters are paDern detectors.  Autoencoders is a neural network used to encode a set of data to reduce the dimensions of a complex dataset.        