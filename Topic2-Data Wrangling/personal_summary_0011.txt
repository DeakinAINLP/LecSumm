The final topic of the Trimester focused on neural networks and machine deep learning. An artificial neural network is very much inspired by the human brain and aims to simulate the complex process that our brains go through to make observations about the world, solve problems and make decisions. An artificial neural network is made up of three main types of layers – an input layer, the hidden layers which can be one or more sets of layers, and the output layer. The input layer receives the data and passes it on to the hidden layer, which will perform computations and pass the output to the next layer, eventually the output layer receives the sum of the computations of the previous layer and produces an output depending on the purpose of the neural network. Each of the “neurons” within the neural network are known as a perceptron which performs a function on the input data. Each perceptron is connected to another in the next layer based on weights from the calculations that determine the strength of the connection. These weights are iteratively adjusted from having an activation function, and learning from the outputs of the solver function. Deep learning models are similar to artificial neural networks where the one or more hidden layers between an input and output layer. However, it is much like how a random forest is a collection of decision trees where a deep learning model is a collection of neural networks make up its’ hidden layers instead of individual perceptron’s. This allows a deep learning model to focus on certain features or aspects of a dataset separately and able to perform more complex and improved outcomes. The pass activity involved training a neural network to classify images of people’s faces and to predict if an image was a certain individual. It was an interesting exercise as it made the importance of hyperparameters much more apparent. Initially I had difficulty implementing the code as the model did not appear to be performing well and taking a very long time to process. It showed how computationally complex larger datasets can become and I modified the parameters so that there was less data being trained on. 