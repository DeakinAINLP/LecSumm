Neural networks are based on our on nervous system. They are necessary in machine learning for  problems  with  highly  nonlinear  functions  and  decision  boundaries.  They  offer  the advantage of flexible feature transformations learned from data, unlike fixed transformations in linear models or support vector machines.  The perceptron is a linear binary classifier and a single-layer neural network. It separates input space  into  two  regions  based  on  a  hyperplane  defined  by  weight  vectors.  The  perceptron algorithm  updates  weights  iteratively  to  correctly  classify  training  data,  with  convergence guaranteed  for  linearly  separable  datasets.  The  motivation  for  developing  a  multilayer perceptron (MLP) stem from the need to handle nonlinear problems that cannot be solved by a single line or linear classifier. The XOR problem serves as a classic example where data points with different class labels cannot be separated by a single line. By introducing a layer in the middle, MLPs can learn and represent nonlinear relationships, enabling them to solve complex classification tasks such as XOR. A feedforward neural network, which includes MLPs, is an artificial neural network where information flows only in one direction, from the input nodes through the hidden nodes (if any) to the output nodes. They utilize gradient-based optimization, such  as  backpropagation,  to  update  weights  and  minimize  the  error  between  predicted  and actual outputs. The  backpropagation  algorithm  is  used  to  train  multilayer  perceptron  (MLPs)  through  a stochastic gradient descent rule. It involves initializing the weights, updating them iteratively using the training data, and stopping the process when a predetermined criteria (e.g., accuracy or number of iterations) is met. The algorithm allows for efficient adjustment of weights to minimize the difference between predicted and actual outputs. Deep learning models, such as convolutional networks (CNNs), are designed to analyse data in a way that mimics human reasoning. These advanced neural networks have achieved success in  various  real-world  tasks,  including  image  recognition  and  handwritten  digit  recognition. CNNs  process  images  in  layers  of  increasing  complexity. They  utilize  sparse  interactions, parameter sharing, and translation invariance to efficiently extract features from local regions of the input. LeNet5, introduced by Yann LeCun [1], was one of the first CNN architectures, featuring convolutional, pooling, and fully-connected layers to transform the input and produce class  scores  for  classification.  LeNet5  set  the  foundation  for  many  subsequent  CNN architectures. The Autoencoder is a neural network where the input and output are the same. It consists of an encoder that compresses the input into a hidden layer code, and a decoder that reconstructs the input from  the code. The objective is  to  minimize the reconstruction error, encouraging  the network to learn meaningful representations. Autoencoders can be used for feature learning and have non-linear capabilities that make them powerful. 