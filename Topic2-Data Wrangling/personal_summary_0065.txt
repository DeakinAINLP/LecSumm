 Topic 10 – Non-linear models (neural networks and deep learning)  Basics Requirements  Input layer     1 or 1+ hidden layers   Combiners (sum functions)   Non linear activation functions   Output layer An example of a NN  Perceptron Algorithm    Linear classifier and a single layer neural network    Xi are the features   Wi are the corresponding weights   Wo is the bias term   Sum function will create value, which will be presented as the output y^  A perceptron is a simple neural network used for binary classification. It has only 1 layer with a single node    Corresponding to the 2 sides of the hyperplane defined by the equation    The distance between a point and the hyperplane is denoted as   Motivation for multilayer perceptron    XOR related problem are not linearly separable, therefore a multilayer NN is required for  solving  such task    This would require the implementation of the hidden layer; where a MLP, a multilayer  perceptron can demonstrate and solve a XOR problem  Multilayer perceptron    Selecting a more complex activation function allows the network to combine the input in more complex ways and in turn provides a richer capability in the functions they can model  Feedforward neural network    ANN, where connections between units do not form a cycle; input node → hidden layer node →  output nodes    A multi-layered feed forward Neural network is known as a MLP (contains multiple layer of  logistic regression like models)  MLP formulation    Given input xt and desired output yt t = 1 to n, the aim is to discover the network weights so that  the predicted values will be as close as possible to the real ones    Minimising error; gradient descent minimization  Gradient-based Optimization    Search direction defined by the gradient of the function at a certain point (convex function)    Where the error function is    Based on minimising function, and update rule can be implemented  Backpropagation algorithm[1]    Readjusting weights if a connection in a NN. This will be required to minimise a measure of the  difference between the actual output vector od the net and the desired output vector.    A gradient function where the derivative of function C, validates the sensitivity to change of a  function value (with respect to input x) Similarly input → hideen weights  Introduction to deep learning    Designed to continually analyse data with a logic structure similar to a human would draw  conclusions.    Layered structure of algorithms similar to ANN  Convolutional Neural Networks    Spares interaction  o  Sparse weights within a smaller kernel; instead of the whole input; reduces the number  of parameters    Parameter sharing  o  Uses the same set of weights while applying to different locations    Translation invariance  o  Recognition of an object, even when the appearance varies in some way  LeNe5    The first CNN   Consist of 5 layers  Input layer holds the pixel values of the image  o o  The convolutional layer will determine the output of neurons which are connected to local regions of the input through the calculation of scalar product between their weights.  o  Pooling layer; simply perform down sampling along the spatial dimensionality of the  given input; reducing the number of parameters within that activation  o  Full connected layers  Autoencoder    Neural network which can deal with multiple hidden layers in it’s structure   Uses dimensional reduction by encoding a set of data; this is the aim of autoencoder    Extract hidden layer Z; taking it as a ompact and meaningful vector which is able to reconstruct    the exact data It then compresses data from the input layer into a short code → then uncompresses that code into something that similarly matches the original data    Minimising error can be handled in the form of a learning function   