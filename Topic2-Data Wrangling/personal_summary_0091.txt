Perceptron Algorithm  ● A linear binary classifier that can be learned using a binary classification technique. It  was one of the first neural network models, and many current neural network structures are built on top of it.  ● Neural network = multilayer perceptron ● Target to find a linear decision that separates 2 classes in training data  What are the steps ?  1. Start by initialising the weights and bias to small randoms values / zeroes 2. Calculate the weighted sum of the input features and multiplied by the appropriate  weights for each training example, then add the bias term  3. To calculate the predicted output, apply the activation function to the weighted sum 4. Compare the data ( predicted output and true label front the training data ) 5. 6.  If prediction is correct = move on to the next training example If prediction is NOT correct = update the weights and bias ( add / subtract the input features, multiply by the learning rate ) 7. Repeat step 2- 5 or 6 until   convergence  Multilayer Perceptron (MLP)  ● Type of feedforward artificial neural network that processes input through a number  of layers of interconnected nodes to produce output. It is trained using the backpropagation algorithm, which modifies the weights of the network according the difference between expected and actual results  ● Meaning the information flow through the network in one direction starting from the  input layer all the way to the output layer without creating cycles / loops  Backpropagation Algorithm  ● An algorithm used in order to find out about the as well as correcting the errors in  neutral networks by propagating them backwards starting from the output nodes to the input nodes  ● It is a very important role in improving the accuracy of the predictions in data mining  and ML  ● Backpropagation enables the network to learn and produce more accurate  predictions over time by iteratively changing the network weights based on the estimated errors.  Deep Learning  ● Machine learning, which is simply a neural network with three or more layers, is a  subset of deep learning  ● A sub field of machine learning that focuses on building multi-layered artificial neural  networks to extract high-level representations of data.  Convolutional Neural Networks A deep learning architecture called a convolutional neural network (CNN or ConvNet) is made for learning directly from data, particularly in tasks involving image recognition, object detection, and classification. CNNs are excellent at seeing patterns in images and can also analyse audio, time-series, and signal data with efficiency. They have transformed computer vision and are frequently used in a wide range of applications that call for processing and understanding visual data.  CNNs are made from 3 basic concept  1. Sparse interactions: Instead of connecting each neuron to the entire input, CNNs use small kernels (also known as filters) with sparse weights. By lowering the number of parameters, this allows the network to concentrate on specific local pattern trends within the data.  2. Sparse interactions: The same set of weights is distributed among many different places of the input data in CNNs. This shows that the kernel collects features from various sections of the input while using the same weights across the input. Sharing parameters allows the network to learn more quickly and helps recognise common patterns.  3. Translation invariance: The ability of CNNs to recognise objects even when their appearance changes as an outcome of shifts or transformations is known as translation invariance. This is beneficial because it allows the network to separate specific variations in an object's visual representation from the identity or category of the object. For example, a CNN can identify an object in an image regardless of its orientation or position.  Autoencoder  ● Autoencoder (a type of neural network) can have numerous hidden layers in its architecture. Its target is to learn an encoding or bottleneck layer, which is a simplified version of the input data used for dimensionality reduction  ● At the output layer, the autoencoder is trained to reconstruct its input data as  accurately as possible. This is done by entering input into the network, comparing the output to the original input, and calculating the reconstruction error using a loss function. The middle hidden layer functions as a simplified form or code that highlights the most important aspects of the input  ● The network gains knowledge of the basic structure and numerical characteristics of  the input by training it to duplicate it. This makes the autoencoder useful for applications like image synthesis or data generation by allowing it to produce new samples that mirror the training data    