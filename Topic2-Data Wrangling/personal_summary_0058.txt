Topic 10 Summary Nonlinear models (Neural Networks and Deep Learning)  Neural system basics:  input layer  hidden layers  combiners (sum functions)  nonlinear activation functions  output layer  Perceptron Algorithm  linear classifier (binary) single layer neural network  Multi-layer perceptron = neural network  Has Bias  Each input connected to a processor with weight (w1, w2)  Weighted sum passed into activation function (Sum function)  Generates output  Given input vector x of M dimensions & weight vector. Perceptron produces output of y  y = sign[v(x, w)]  v(x, w) =  M ∑ i=1  wixi + w0  Simple neural network for binary classification.  1 layer, single node  Steps  1. Initialise w = 0  2. Retrieve next input x_t & desired output y_t  yt = sign[xt,w]  e = yt − yt  W IT H  Note : Learningrate =  Compute actual output  Compute output error  Update weight for all wi  $$ w{i}\leftarrow w{i}+ \triangle w_i  \triangle w{i}= \eta e{t}x{ti}  0 \leq \eta \leq 1$$  3. Repeat step 2 until convergence  Multilayer perceptron  10.4: Neural Networks: Multilayer Perceptron Part 1 - The Nature of Code  Backpropagation Algorithm  A technique used to train neural networks by adjusting the weights between neurons.  Convolutional Neural Networks  Fully-connected network: neuron sees whole picture  CNN neuron doesn't need to see whole picture, only sees 1 receptive field, reducing parameter size  Translation invariance  CNN is not invariant to scaling & rotation  Data augmentation: learn features from various orientations  Autoencoder  Unsupervised learning (Only have training dataset)  Feed input data  Reconstruct input data as output  Learns compact representation of data  Dimensionality reduction  Deep learning  Tensorflow: computational operation is static --> model deployment.  Pytorch: computational operation is dynamic --> easy to debug, R&D  Terms:  Loss function  a function that measures the performance of a model during training Perceptron  a single-layer neural network. Bias  a constant term added to the input of a neuron.  Weight matrix  a matrix that contains the weights between the input and output of a neural network.  Learning rate  a hyperparameter that controls the step size during optimization. Epoch  one pass through the entire training dataset.   