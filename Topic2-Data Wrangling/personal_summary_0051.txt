-  Linear models may not be sufficient when the underlying functions/decision boundaries are extremely nonlinear.  - Support vector machines can construct nonlinear functions but use fixed feature  transformations, which depends on the kernel function.  - Neural Networks allow the feature transformations to be learnt from data.  Historical Motivation - 2:09pm  - Our brain has networks of inter-connected neurons and has highly-parallel architecture.  - ANN (artificial neural networks) are motivated by biological neural systems.  -  Two groups of ANN researchers:  1. Group that uses ANN to study/model the brain 2. Group that uses the brain as the motivation to design ANN as an effective  learning machine, which might not be the true model of the brain.  - We are following the second group’s approach.  Perceptron - 2:20pm  Motivation & Algorithm Multi-layer Perceptron Back-propagation  - Perceptron is a simple neural network used for binary classification. -  It has only one layer with single node.  - Given: input vector 𝒙=(𝑥_1,𝑥_2,…,𝑥_𝑀 ) of 𝑀 dimensions and weight vector  -  𝒘=(𝑤_0,𝑤_1,…,𝑤_𝑀 ) The perceptron produces output: 𝑦 ̂=sign [𝑣(𝒙,𝒘)] where 𝑣(𝒙, 𝒘) is the linear combiner:  - - Better notation, let 𝑥_0=1 and 𝒙=(𝑥_0,𝑥_1,…,𝑥_𝑀 )  then and  𝑣(𝒙,𝒘)=𝒘^𝑻 𝒙 =𝒙^𝑻 𝒘 𝑦 ̂(𝒙,𝒘)=sign [𝒘^𝑻 𝒙]  What Perceptron is doing - 2:31pm  - Given weight 𝒘, the perceptron linearly divides input space into two regions:  - All 𝒙’s such that y ̂(𝒙,𝒘)=1, or 𝑣(𝒙,𝒘)≥0 - All 𝒙’s such that y ̂(𝒙,𝒘)=−1, or 𝑣(𝒙,𝒘)<0  -  This corresponds to the two sides of the hyperplane defined by the equation:  - Note that |𝑣(𝒙,𝒘)| is proportional to the distance from 𝑥 to the hyperplane.  Thus the sign of 𝑣(𝒙, 𝒘) indicates on which side of hyperplane ℋ(𝒘) is 𝒙.  - - While the magnitude |𝑣(𝒙,𝒘)| indicates how far away 𝒙 is from ℋ(𝒘).  -  Linearly separable  -  Two sets 𝒞_1and 𝒞_2 are called linearly separable if there exists a hyperplane ℋ(𝒘) that separates them.  Training (or Learning) Perceptron - 2:40pm  -  Find the weight vector 𝒘 so that the perceptron correctly classify 2 classes, given sample training data  -  -  Training data 𝐷={(𝒙_𝑡, 𝑦_𝑡)}, 𝑡=1,…,𝑛 where 𝒙_𝑡=(𝑥_𝑡1,…, 𝑥_𝑡𝑀) is the input vector at time 𝑡  -  𝑦_𝑡=±1 is the desired output  Learning Perceptron  Perceptron Learning Algorithm - 2:43pm  Initialize 𝒘=𝟎  1. 2. Retrieve next input 𝒙_𝑡 and desired output 𝑦_𝑡 Compute actual output 𝑦 ̂_𝑡=sign[𝒙_𝑡⋅𝒘] Compute output error 𝑒_𝑡=𝑦_𝑡−𝑦 ̂_𝑡 Update weight, for all 𝑖:  3. Repeat from step 2 until convergence  Example: Current weight 𝒘=(−1, 2, 1)  Current training sample 𝒙=(1/2,1), 𝑦=−1  𝑣(𝒙,𝒘)=2×1/2+1×1 −1=1 𝑦 ̂(𝒙,𝑤)=sign (1)=1 𝑒=𝑦−𝑦 ̂=−2  New 𝑤_𝑖=𝑤_𝑖−2𝜂𝑥_𝑖=𝑤_𝑖−2𝑥_𝑖 (let 𝜂=1)  𝑤_0=−1 −2 𝑥_0=−3 𝑤_1=2−2𝑥_1=1 𝑤_2=1 −2𝑥_2=−1  Perceptron Convergence Theorem - 3:03pm  -  If training instances are drawn from two linearly separate sets 𝒞_1 and 𝒞_2, then the perceptron learning rule will converge after finite iterations.  - However, no guarantee for convergence if 𝒞_1and 𝒞_2 are not linearly separable!  Some Linearly Separable Problems - 3:07pm  XOR Problem - 3:11pm  Multi-layer Feed-forward NN - 3:15pm  - Perceptron is quite weak in what it can represent.  -  For complex, non-linear decision surfaces, we need multi-layer network.  - Choice of node in multi-layer network  - Perceptron: discontinuity - Answer: sigmoid function!  - Sigmoid node: like a perceptron, but with the sigmoid  function 𝜎(𝑧)=(1+𝑒^(−𝑧) )^(−1) instead of the sign function, i.e., 𝑦=𝜎(𝒘^𝑇 𝒙).  Multi-layer Perceptron (MLP) - 3:20pm  - A feedforward neural network is an ANN wherein connections between units do not form  a cycle.  - Multi-layer feed-forward NN is also known as Multi-layer Perceptron (MLP).  -  The term “MLP” is really a misnomer.  - Why? Because the model comprises multiple layers of logistic regression like models (with continuous nonlinearities) rather than multiple Perceptrons (with discontinuous nonlinearities).  - Although a misnomer, we will continue using MLP term.  Structure of Multi-layer Perceptron - 3:23pm  Consider a two-layer network: input layer, hidden layer and output layer  Remarks:  - -  - Output now is a vector Two kinds of weights: - input → hidden hidden → output - 𝑤_𝑖𝑗^ℎ: from 𝑖^𝑡ℎ input → 𝑗^𝑡ℎ hidden - 𝑤_𝑗𝑘^𝑜: from 𝑗^𝑡ℎ hidden → 𝑘^𝑡ℎ output -  Input layer does no computation, only to relay input vector.  - Can have more than one hidden layers. - Doesn’t have to be fully connected.  MLP Formulation - 3:26pm  - Given input 𝒙_𝑡 and desired output 𝒚_𝑡, 𝑡=1,…, 𝑛, find the network weights 𝒘 such that  - Stating above as an optimization problem: find 𝑤 to minimize the error function  - We will use gradient-descent for minimization. - - We will use an algorithm called Backpropagation.  𝐸(𝒘) is not convex, but a complex function with possibly many local minima.  Detour: Gradient-based Optimization - 3:28pm  -  To minimize a functional 𝑓(𝒙), use gradient-descent:  Initialize random 𝒙_0  - - Slide down the surface of 𝑓 in the direction of steepest decrease:  - Similarly, to maximize 𝑓(𝒙), use gradient-ascent.  Detour: Stochastic Gradient Descent (SGD) - 3:33pm  -  Instead of minimizing 𝐸(𝒘), SGD minimizes the instantaneous approximation of 𝐸(𝒘) using only 𝑡-th instance, i.e.,  - Update rule (where 𝑡 denotes the current training sample) is  - SGD is cheap to perform and guaranteed to reach a local minimum in a stochastic  sense.  Training MLP: Backpropagation - 3:38pm  It is in fact a stochastic gradient-descent rule!  - - Minimizing instantaneous approximation for current training sample (𝒙_𝑡,𝒚_𝑡)  Backpropagation (SGD) - 3:52pm  Issues with Backpropagation - 3:54pm  -  Local minima.  - Possible fixes:  - Add a momentum term in the update rule, e.g.,  - Can prevent getting stuck in shallow local minimum  - Multiple restarts and choose final network with best performance  Overfitting  -  The tendency of the network to “memorize” all training samples, leading to poor generalization  - Usually happens with network of too many hidden nodes and overtrained. - Possible fixes:  - Use cross validation, e.g., stop training when validation error starts to grow. - Weight decaying: minimize also the magnitude of weights, keeping weights small  (since the sigmoid function is almost linear near 0, if weights are small, decision surfaces are less non-linear and smoother)  - Keep small number of hidden nodes!  Where to go from here?  Connections to Deep Learning - 3:59pm  - Deep Learning methods are advanced neural networks.  -  They have been successful in learning many real world tasks e.g. handwritten digit recognition, image recognition!  - Some of the common Deep Learning architectures are:  - Convolutional Networks (Due to Le Cun et al.) - Autoencoders (Due to Yoshua Bengio et al.) - Deep Belief Networks (due to Geoff Hinton et al.) - Boltzmann Machines  - Restricted Boltzmann Machines - Deep Boltzmann Machines - Deep Neural Networks  Convolutional Neural Networks - 4:01pm  - Also called CNN or ConvNets. - Motivation: - - Also (Hubel & Wiesel, 62’):  vision processing in our brain is fast  - Simple cells detect local features - Complex cells pool local features  -  Translated in technical terms:  - Sparse interactions: sparse weights within a smaller kernel (e.g., 3x3, 5x5)  instead of the whole input. This helps reduce #params.  - Parameter sharing: a kernel use the same set of weights while applying onto  different location (sliding windows). Translation invariance.  -  What helped Deep Learning? - 4:06pm  -  -  Larger models with new training techniques: - Dropout, Maxout, Maxnorm, ReLU,…  Large ImageNet dataset [Fei-Fei et al. 2012]  - -  1.2 million training samples 1000 categories  -  Fast graphical processing units (GPU)  - Capable of 1 trillion operations per second  Deep Autoencoder - 4:08pm  - Simply a neural network that tries to copy its input  to its output.  Input 𝒙=[𝑥_1,𝑥_2,…,𝑥_𝑁 ]^⊤  - - An encoder function 𝑓 parameterized by 𝜽 - A coding representation 𝒛=[𝑧_1,𝑧_2,…,𝑧_𝐾  ]^⊤=𝑓_𝜽 (𝒙)  - A decoder function 𝑔 parameterized by 𝝋 - An output, also called reconstruction  -  𝒓=[𝑟_1,𝑟_2,…,𝑟_𝑁 ]^⊤=𝑔_𝝋 (𝒛)=𝑔_𝝋 (𝑓_𝜽 (𝒙))  - A loss function 𝒥 that computes a scalar  𝒥(𝒙,𝒓) to measure how good of a reconstruction 𝒓 of the given input 𝒙, e.g., mean square error loss:  -  -  Learning objective:  argmin┬(𝜽,𝝋) 〖𝒥(𝒙,𝒓)〗  - Autoencoders are another way of feature learning.  -  -  The solution is trivial unless there are constraints (such as sparsity) on the number of nodes in the hidden layers.  Linear autoencoder with 𝐾≤𝑁 acts as PCA.  - However, it is nonlinearity that makes it powerful.    