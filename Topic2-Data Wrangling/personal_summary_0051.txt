-  Linear models may not be sufficient when the underlying functions/decision boundaries are extremely nonlinear.  - Support vector machines can construct nonlinear functions but use fixed feature  transformations, which depends on the kernel function.  - Neural Networks allow the feature transformations to be learnt from data.  Historical Motivation - 2:09pm  - Our brain has networks of inter-connected neurons and has highly-parallel architecture.  - ANN (artificial neural networks) are motivated by biological neural systems.  -  Two groups of ANN researchers:  1. Group that uses ANN to study/model the brain 2. Group that uses the brain as the motivation to design ANN as an effective  learning machine, which might not be the true model of the brain.  - We are following the second groupâ€™s approach.  Perceptron - 2:20pm  Motivation & Algorithm Multi-layer Perceptron Back-propagation  - Perceptron is a simple neural network used for binary classification. -  It has only one layer with single node.  - Given: input vector ğ’™=(ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘€ ) of ğ‘€ dimensions and weight vector  -  ğ’˜=(ğ‘¤_0,ğ‘¤_1,â€¦,ğ‘¤_ğ‘€ ) The perceptron produces output: ğ‘¦ Ì‚=sign [ğ‘£(ğ’™,ğ’˜)] where ğ‘£(ğ’™, ğ’˜) is the linear combiner:  - - Better notation, let ğ‘¥_0=1 and ğ’™=(ğ‘¥_0,ğ‘¥_1,â€¦,ğ‘¥_ğ‘€ )  then and  ğ‘£(ğ’™,ğ’˜)=ğ’˜^ğ‘» ğ’™ =ğ’™^ğ‘» ğ’˜ ğ‘¦ Ì‚(ğ’™,ğ’˜)=sign [ğ’˜^ğ‘» ğ’™]  What Perceptron is doing - 2:31pm  - Given weight ğ’˜, the perceptron linearly divides input space into two regions:  - All ğ’™â€™s such that y Ì‚(ğ’™,ğ’˜)=1, or ğ‘£(ğ’™,ğ’˜)â‰¥0 - All ğ’™â€™s such that y Ì‚(ğ’™,ğ’˜)=âˆ’1, or ğ‘£(ğ’™,ğ’˜)<0  -  This corresponds to the two sides of the hyperplane defined by the equation:  - Note that |ğ‘£(ğ’™,ğ’˜)| is proportional to the distance from ğ‘¥ to the hyperplane.  Thus the sign of ğ‘£(ğ’™, ğ’˜) indicates on which side of hyperplane â„‹(ğ’˜) is ğ’™.  - - While the magnitude |ğ‘£(ğ’™,ğ’˜)| indicates how far away ğ’™ is from â„‹(ğ’˜).  -  Linearly separable  -  Two sets ğ’_1and ğ’_2 are called linearly separable if there exists a hyperplane â„‹(ğ’˜) that separates them.  Training (or Learning) Perceptron - 2:40pm  -  Find the weight vector ğ’˜ so that the perceptron correctly classify 2 classes, given sample training data  -  -  Training data ğ·={(ğ’™_ğ‘¡, ğ‘¦_ğ‘¡)}, ğ‘¡=1,â€¦,ğ‘› where ğ’™_ğ‘¡=(ğ‘¥_ğ‘¡1,â€¦, ğ‘¥_ğ‘¡ğ‘€) is the input vector at time ğ‘¡  -  ğ‘¦_ğ‘¡=Â±1 is the desired output  Learning Perceptron  Perceptron Learning Algorithm - 2:43pm  Initialize ğ’˜=ğŸ  1. 2. Retrieve next input ğ’™_ğ‘¡ and desired output ğ‘¦_ğ‘¡ Compute actual output ğ‘¦ Ì‚_ğ‘¡=sign[ğ’™_ğ‘¡â‹…ğ’˜] Compute output error ğ‘’_ğ‘¡=ğ‘¦_ğ‘¡âˆ’ğ‘¦ Ì‚_ğ‘¡ Update weight, for all ğ‘–:  3. Repeat from step 2 until convergence  Example: Current weight ğ’˜=(âˆ’1, 2, 1)  Current training sample ğ’™=(1/2,1), ğ‘¦=âˆ’1  ğ‘£(ğ’™,ğ’˜)=2Ã—1/2+1Ã—1 âˆ’1=1 ğ‘¦ Ì‚(ğ’™,ğ‘¤)=sign (1)=1 ğ‘’=ğ‘¦âˆ’ğ‘¦ Ì‚=âˆ’2  New ğ‘¤_ğ‘–=ğ‘¤_ğ‘–âˆ’2ğœ‚ğ‘¥_ğ‘–=ğ‘¤_ğ‘–âˆ’2ğ‘¥_ğ‘– (let ğœ‚=1)  ğ‘¤_0=âˆ’1 âˆ’2 ğ‘¥_0=âˆ’3 ğ‘¤_1=2âˆ’2ğ‘¥_1=1 ğ‘¤_2=1 âˆ’2ğ‘¥_2=âˆ’1  Perceptron Convergence Theorem - 3:03pm  -  If training instances are drawn from two linearly separate sets ğ’_1 and ğ’_2, then the perceptron learning rule will converge after finite iterations.  - However, no guarantee for convergence if ğ’_1and ğ’_2 are not linearly separable!  Some Linearly Separable Problems - 3:07pm  XOR Problem - 3:11pm  Multi-layer Feed-forward NN - 3:15pm  - Perceptron is quite weak in what it can represent.  -  For complex, non-linear decision surfaces, we need multi-layer network.  - Choice of node in multi-layer network  - Perceptron: discontinuity - Answer: sigmoid function!  - Sigmoid node: like a perceptron, but with the sigmoid  function ğœ(ğ‘§)=(1+ğ‘’^(âˆ’ğ‘§) )^(âˆ’1) instead of the sign function, i.e., ğ‘¦=ğœ(ğ’˜^ğ‘‡ ğ’™).  Multi-layer Perceptron (MLP) - 3:20pm  - A feedforward neural network is an ANN wherein connections between units do not form  a cycle.  - Multi-layer feed-forward NN is also known as Multi-layer Perceptron (MLP).  -  The term â€œMLPâ€ is really a misnomer.  - Why? Because the model comprises multiple layers of logistic regression like models (with continuous nonlinearities) rather than multiple Perceptrons (with discontinuous nonlinearities).  - Although a misnomer, we will continue using MLP term.  Structure of Multi-layer Perceptron - 3:23pm  Consider a two-layer network: input layer, hidden layer and output layer  Remarks:  - -  - Output now is a vector Two kinds of weights: - input â†’ hidden hidden â†’ output - ğ‘¤_ğ‘–ğ‘—^â„: from ğ‘–^ğ‘¡â„ input â†’ ğ‘—^ğ‘¡â„ hidden - ğ‘¤_ğ‘—ğ‘˜^ğ‘œ: from ğ‘—^ğ‘¡â„ hidden â†’ ğ‘˜^ğ‘¡â„ output -  Input layer does no computation, only to relay input vector.  - Can have more than one hidden layers. - Doesnâ€™t have to be fully connected.  MLP Formulation - 3:26pm  - Given input ğ’™_ğ‘¡ and desired output ğ’š_ğ‘¡, ğ‘¡=1,â€¦, ğ‘›, find the network weights ğ’˜ such that  - Stating above as an optimization problem: find ğ‘¤ to minimize the error function  - We will use gradient-descent for minimization. - - We will use an algorithm called Backpropagation.  ğ¸(ğ’˜) is not convex, but a complex function with possibly many local minima.  Detour: Gradient-based Optimization - 3:28pm  -  To minimize a functional ğ‘“(ğ’™), use gradient-descent:  Initialize random ğ’™_0  - - Slide down the surface of ğ‘“ in the direction of steepest decrease:  - Similarly, to maximize ğ‘“(ğ’™), use gradient-ascent.  Detour: Stochastic Gradient Descent (SGD) - 3:33pm  -  Instead of minimizing ğ¸(ğ’˜), SGD minimizes the instantaneous approximation of ğ¸(ğ’˜) using only ğ‘¡-th instance, i.e.,  - Update rule (where ğ‘¡ denotes the current training sample) is  - SGD is cheap to perform and guaranteed to reach a local minimum in a stochastic  sense.  Training MLP: Backpropagation - 3:38pm  It is in fact a stochastic gradient-descent rule!  - - Minimizing instantaneous approximation for current training sample (ğ’™_ğ‘¡,ğ’š_ğ‘¡)  Backpropagation (SGD) - 3:52pm  Issues with Backpropagation - 3:54pm  -  Local minima.  - Possible fixes:  - Add a momentum term in the update rule, e.g.,  - Can prevent getting stuck in shallow local minimum  - Multiple restarts and choose final network with best performance  Overfitting  -  The tendency of the network to â€œmemorizeâ€ all training samples, leading to poor generalization  - Usually happens with network of too many hidden nodes and overtrained. - Possible fixes:  - Use cross validation, e.g., stop training when validation error starts to grow. - Weight decaying: minimize also the magnitude of weights, keeping weights small  (since the sigmoid function is almost linear near 0, if weights are small, decision surfaces are less non-linear and smoother)  - Keep small number of hidden nodes!  Where to go from here?  Connections to Deep Learning - 3:59pm  - Deep Learning methods are advanced neural networks.  -  They have been successful in learning many real world tasks e.g. handwritten digit recognition, image recognition!  - Some of the common Deep Learning architectures are:  - Convolutional Networks (Due to Le Cun et al.) - Autoencoders (Due to Yoshua Bengio et al.) - Deep Belief Networks (due to Geoff Hinton et al.) - Boltzmann Machines  - Restricted Boltzmann Machines - Deep Boltzmann Machines - Deep Neural Networks  Convolutional Neural Networks - 4:01pm  - Also called CNN or ConvNets. - Motivation: - - Also (Hubel & Wiesel, 62â€™):  vision processing in our brain is fast  - Simple cells detect local features - Complex cells pool local features  -  Translated in technical terms:  - Sparse interactions: sparse weights within a smaller kernel (e.g., 3x3, 5x5)  instead of the whole input. This helps reduce #params.  - Parameter sharing: a kernel use the same set of weights while applying onto  different location (sliding windows). Translation invariance.  -  What helped Deep Learning? - 4:06pm  -  -  Larger models with new training techniques: - Dropout, Maxout, Maxnorm, ReLU,â€¦  Large ImageNet dataset [Fei-Fei et al. 2012]  - -  1.2 million training samples 1000 categories  -  Fast graphical processing units (GPU)  - Capable of 1 trillion operations per second  Deep Autoencoder - 4:08pm  - Simply a neural network that tries to copy its input  to its output.  Input ğ’™=[ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘ ]^âŠ¤  - - An encoder function ğ‘“ parameterized by ğœ½ - A coding representation ğ’›=[ğ‘§_1,ğ‘§_2,â€¦,ğ‘§_ğ¾  ]^âŠ¤=ğ‘“_ğœ½ (ğ’™)  - A decoder function ğ‘” parameterized by ğ‹ - An output, also called reconstruction  -  ğ’“=[ğ‘Ÿ_1,ğ‘Ÿ_2,â€¦,ğ‘Ÿ_ğ‘ ]^âŠ¤=ğ‘”_ğ‹ (ğ’›)=ğ‘”_ğ‹ (ğ‘“_ğœ½ (ğ’™))  - A loss function ğ’¥ that computes a scalar  ğ’¥(ğ’™,ğ’“) to measure how good of a reconstruction ğ’“ of the given input ğ’™, e.g., mean square error loss:  -  -  Learning objective:  argminâ”¬(ğœ½,ğ‹) ã€–ğ’¥(ğ’™,ğ’“)ã€—  - Autoencoders are another way of feature learning.  -  -  The solution is trivial unless there are constraints (such as sparsity) on the number of nodes in the hidden layers.  Linear autoencoder with ğ¾â‰¤ğ‘ acts as PCA.  - However, it is nonlinearity that makes it powerful.    