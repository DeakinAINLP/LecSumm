My findings that were that tuning the hyperparameters, in particular the amount of  nodes in a given hidden layer was critical for faciliating a 16 hidden layer network. My  methodology was to to initially find a a principal value for which I did a gridsearch on  the equal node values for each hidden layer. Once I got this value and it repeatedly  converged, I then started tinkering with changing the solvers, activation methods. I  then continued generating loops to try and figure out an optimisation for each node  value rather than equal values, but this was computationally expensive and time-  consuming. I settled for emperically setting this values to induce higher performance  scores manually. Once I began to achieve diminishing returns, I started manipulating  the alpha hyperparmeter.  Overall because of the refinement in hyperparameters, the other solvers were not  optimised given equivilent conditions. They could have been similarly optimised to a  degree, but the best optimisation would have been to reduce the amount of hidden  layers to 1 or prehaps 2. High performance scores can be achieved without intensive  hyperparameter optimisation other than the nodes in hidden layers.  Overall the 16 hidden layers requirement drove extreme overfitting which had to be  tweaked by the hyperparameters.  Alternative methods could have included further scaling and pre-processing of the  data, which was deliberately not completed to test the raw capability of neural  networks.  