 Main points covered in Topic 10  1.  Why Neural Networks    Linear models are not enough when it comes to handling nonlinear functions that  are underlying.    SVM  can  construct  nonlinear  functions  but  use  fixed  feature  transformations,  which also depends on the kernel function.    Neural  Networks  comes  into  picture  in  this  case,  to  allow  the  feature  transformations to be learnt from data.  2.  Historical Motivation  The computational model is inspired by the human brain which comes from neuroscience. Being able to learn over time. The goal to develop computational models that closely resemble how the human brain functions  and  its  capacity  for  learning  and  information  processing  is  what  originally spurred the development of neural networks  3.  Examples of ANN (Artificial Neural network)  This has three layers,    Hidden  layer  –  These  are  the  labels  between  the  input  and  the  output  Input layer  layer. Can have multiple hidden layers as hidden layer 1, 2 and 3    Output layer  4.  Perceptron  It is a simple neural network used for binary classification. It is a single layer neural network. A multi-layer perceptron is called a neural network.  Perceptron divides the input space into two regions linearly.  Distance formula of Perceptron  Perceptron cannot handle XOR situations. Only a multi-layer perceptron (MLP) can handle XOR situations.  5.  Complexity – With the increase in the neurons, the complexity gets bigger. 6.  Convolutional Neural Networks –  CNN belongs to a class of deep learning models designed for processing and analyzing visual data such as media (images and videos). The spatial structure is exploited and local connectivity present in visual data. The layers used are the convolutional, pooling and fully connected layers that are designed to process images. The  final extracted output layers produce predictions and probabilities for different classes and targets. The network leverages several layers and output computer vision tasks.  Reflection on the knowledge gained.  On big, varied datasets, deep learning models are widely used. To building robust models that are good at generalizing to new cases, quality and amount of data are essential. Improving model performance  requires  a  thorough  understanding  of  data  preparation,  augmentation,  and addressing imbalances.  The significance of architectural design is a further lesson. Model performance is substantially impacted  by  selecting  the  right  network  topologies,  such  as  convolutional  neural  networks (CNNs)  for  pictures  or  recurrent  neural  networks  (RNNs)  for  sequential  data.  Regularization, dropout,  and  batch  normalization  approaches  help  with  greater  generalization  by  balancing model complexity and overfitting, which is a critical trade-off.  