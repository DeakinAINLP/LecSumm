In the previous topic, we looked into a non-linear model algorithm called Random Forest. This  algorithm uses a lot of technique and algorithm such as bootstrap, ensemble learning and bagging to combine decision tree.  Now this will be the last module in this unit, we will wrap it up with supervised learning algorithm. One of the most used and common algorithm will be neural networks and deep learning. This topic objective will be:  -  Neural Network - -  Deep learning  Perceptron and multilayer perceptron  Neural Network  An introduction to neural network is that it works like how a brain does. It consists of an input  layer, one or multiple hidden layers, resulting in an output layer. Neural networks are trained to be smart and accurate in the long run, it will perform badly at an early stage and progressively becoming better as training goes on. Neural network can handle complex and high dimensional data, but with the amount of hidden layer we might overfit the model. When the input variable has been set, the weight of each instance will be calculated determining the importance. The output is passed through an activation function which is the hidden layers.  Perceptron algorithm Continue to neural network, perceptron algorithm is binary linear classifier algorithm, multi-  layer perceptron will become neural network. Just like the neural network line, the input will be given a weight, and with the hidden layer, which is the sum function, it will then return an output. In this algorithm we also uses the hyperplane to divides the input category. The further it is from the hyperplane, the more confident it is in the prediction, and vice versa.  The process of this algorithm goes as follow:  Initialize the weight to zero  - -  Get the input t and desired output t o  Get actual output o  Finds the error between the outputs o  Update weights o  Repeat until converges  Multilayer Perceptron  Sometimes classifying the dataset, it is not always a linear type of problem that we can use an  AND OR gate for a single layer of perceptron, but when it comes to non-linear problem, we need to start using multilayer perceptron such as XOR XAND etc.. Just like the logistic function, the nonlinear function that uses the multilayer perceptron has an s shape that output a value between 0 and 1.  The input of the algorithm does not do any computation, it simply relays the input value. There  are two weights from the input to the hidden layer then to the output layer. The aim is to find the weight value that will provides you the closest predicted value to the actual value. To optimize the predicted value and minimize the error, we can use an algorithm called Gradient Based Optimization.  One of the key steps in this optimization is backpropagation. It uses derivative respect to the weight and biases to calculate the loss function. Every iteration it goes back and update the weight of the first hidden layer with value of the gradient. As it goes on, the weight will converge and give better outcome.  Deep Learning  Deep learning uses neural network to analyze data by the logical of the human brain. Here we  will look into some of the common architecture of DL.  Convolutional Neural Network act as the vision of the human eyes. This algorithm recognize  images in different layers, then outputting the result. For example, one layer looks at the line, the other looks at the color and shape, then later predicting what it is based on the experience and knowledges. CNN are made of 3 concepts. Sparse interaction help reduce the number of parameter, as it transform the information encoded in the pixel. Parameter sharing using the same hyperparameter to different location. Translation invariance translates the object depending on its appearance and how it looks, although the angle difference. One of the most famous and common CNN application in the modern world now is to detect images, such as camera. By inserting an image to the machine, it wonâ€™t be able to recognize the object, but with its feature, the machine can put piece to piece, connects them and translate them into an object that it knows.  Autoencoder is used in NN to handle multiples hidden layers. The algorithm encodes the of data  into different classifier. The purpose for this is to reduce dimensionality and compress input layer keeping only the important features.  