 ● Neural Networks (NN) have gained huge popularity in the modern world, but the inner workings of a neural network are fundamental mathematical concepts and it can quantify almost any kind of data. Just like a human brain, a NN is made up of multiple neurons, each of them connected to the other in the form of layers stacked on top of one another. Each layer takes the input from the previous layer, and then each neuron perceives the data in its own way and forwards the new output as an input for the layer ahead.  ● Neural systems consist of many moving parts, but the most common NN start with an input layer, then a number of hidden layers, and finally an output layer. The size of the input layer depends on the number of features that the network needs to consider and the output layer depends on the desired number of output params.  ● The complexity of a NN can increase rapidly with the number of neurons in each layer and needs to be considered before generating a NN. But they are also required in case of high-dimensional input data.  ● A single-layer neural network is called a perceptron which is essentially a linear classifier. On a fundamental level, it is a function f(x) that takes input x and converts it to an output y. This is called an activation function and it can vary, it could be a linear or non-linear function. Sigmoid, ReLU, tanh, etc are some examples.  ● Many perceptrons together form a multi-layer perceptron which is a neural network. The reason multi-layer perceptrons are required is not all kinds of data could be linearly classified. Hence, a NN combines many classifications from each neuron to form the final outcome. The hidden layers along with the output layer are ones that perform computation on the data, the input layer merely relays the information to the next hidden layer.  ● Backpropagation is a popular technique through which the weights of the models are adjusted on the fly. After a forward pass is performed, the weights and biases of the layer are updated through a backward pass. It aims to minimize the cost function.  ● An advanced version of Machine Learning is Deep Learning (DL). A Deep Learning algorithm aims to mimic the human brain and learn with experience over time. Some examples of DL include Convolutional networks, Autoencoders and Deep Neural Networks.  External Resources  7.2. real world datasets (no date) scikit. Available at: https://scikit-learn.org/stable/datasets/real_world.html#labeled-faces-in-the-wild-dataset (Accessed: 01 June 2023).  Daria (2021) Facial recognition using PCA and MLP in python, PDF.co. Available at: https://pdf.co/blog/facial-recognition-using-pca-and-mlp-in-python (Accessed: 01 June 2023).  Lab: Faces recognition using various learning models¶ (no date) Lab: Faces recognition using various learning models - Statistics and Machine Learning in Python 0.5 documentation. Available at: https://duchesnay.github.io/pystatsml/auto_gallery/ml_lab_face_recognition.html (Accessed: 01 June 2023).  54.5k2727 gold badges137137 silver  Rob HyndmanRob Hyndman badges186186 bronze badges and dougdoug badges2626 bronze badges (1957) How to choose the number of hidden layers and nodes in a feedforward neural network?, Cross Validated. Available at: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layer s-and-nodes-in-a-feedforward-neural-netw (Accessed: 01 June 2023).  10.3k11 gold badge2424 silver  