Topic 10: Nonlinear models (neural networks and deep learning)    Brief introduction to Neural networks in 10.2 and how they are motivated by the biological  neural systems in our brains. Neural networks allow for the learning of feature transformations from the data itself. Brain function is also occupied by different sections like some percentage for action recognition and some for-speech recognition and pronunciation for example.    The major components of a neural network system involve the input layer, one or more  hidden layers, combiners (sum functions), nonlinear activation functions and an output layer. We can have bigger and more complex neural networks by changing the number of layers.   10.4 introduces the perceptron algorithm. Perceptron is a linear classifier and is a single-layer neural network. If a perceptron has multiple layers, it is simply called a neural network. It is akin to an artificial neuron inspired by a biological neuron and can be seen as a building block of neural networks much like the neurons in our brain.    The perceptron algorithm can be summed up in a few steps, 1. Initialize weights, 2. Calculate the predicted output using weights, 3. Compare the predicted output with the actual output and compute the output error, 4. Update weights using the learning rate, 5. Repeat from step 2 until convergence.    The motivation for the multi-layer perceptron comes from the limitations of the single-layer perception in handling nonlinearly separable problems. The single-layer perceptron can handle problems such as AND and OR logical gates but cannot handle XOR gates which multi- layer perceptron’s can.    A feedforward neural network is a type of ANN (Artificial Neural Network) in which the  information only moves in one direction, being forward from the input nodes to the output nodes.    The backpropagation algorithm is a vital component in training MLPs. It is a method that is  used to adjust the weights and biases of a network based on the errors calculated during the training process.    Deep learning models are introduced in 10.9 and are designed in a way that they will  continually analyze data so that they will learn and pickup patterns to draw conclusions similar to the way a human would. Some deep learning architectures include Convolutional networks, deep belief networks and Boltzmann machines.    Convolutional Neural Networks are modelled after the mammalian visual cortex, this is  where visual input is processed in the human brain. CNN’s consist of three basic concepts which are sparse interactions, parameter sharing and translation invariance.    Application of CNN involves finding patterns in visual images, for example it has been applied to the CIFAR 10 dataset which contains 50,000 training images and 10,000 test images.   An autoencoder is a neural network which can learn a representation of input data and can  compress it to lower dimensions for dimensionality reduction.  