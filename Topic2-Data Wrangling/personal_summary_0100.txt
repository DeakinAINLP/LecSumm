 Learning Summary The Basic of the Neural System: The neural system comprise of several major parts they are:    An Input Layer   One or several hidden layers   Combiners   Non-linear activation functions   An Output Layer  Perceptron Algorithm: A perceptron is referred as a binary classifier which terms as a single layered neural network, on the other hand multi-layered perceptron are referred as a neural network.  Feedforward Neural Networks: This is an Artificial Neural Network (ANN) where connections among units don’t form a cycle, and information in such networks moves unidirectional. From the input node  it moves in a forward direction through the hidden nodes if there are to be any towards the output nodes and do not comprise of any loops or cycles within the network as such.  Multi-Layered Feedforward Neural Network: Which is also referred as Multi-Layered Perceptron (MLP), is a model which comprises multiple layers of logistic regression like models (with continuous non - linarites) instead of multiple perceptrons.  Deep Learning: A model which is designed in a manner where data is analyzed continuously with a logic structure which is very similar to that of a human being drawing conclusions, and for achieving this the model uses a layered structure of algorithms with is same to ANNs. This method comprises of advanced neural networks and some of the most common architecture that are used within this model include: Deep Belief Networks, Convolutional Networks, Auto encoders and many more…  Convolutional Neural Networks: An architecture which is modelled after mammalian visual cortex (a part of the brain where visual input processing takes place). Within the cortex particular neurons are triggered when a particular phenomenon are within the field of vision. For instance, one neuron might get triggered when one is looking at a left-sloping diagonal line or another one when a horizontal line is within a view. The brain process images in layers with increasing complexity where, the first layer is able to distinguish simple attributes like curves and lines, on a higher level the brain is able to recognize a configuration of different colors and edges like in a house or a bird.  The CNN is constructed of 3 simple concepts:    Sparse Interactions: The weight of sparse are usually in a smaller kernel like 3 x 3.5 x 5 instead of the whole input, helping in reducing the number of parameters. When referring to kernel within CNN it means that an operator get applied to the entire image which transforms the information that is encoded within the pixels.    Parameter Sharing: Within the similar set of weights get applied for different locati ons.   Translation Invariance:  This refers that one is able to recognize an object as an object even if the appearance of is varied in some or the other way. This is a positive thing as abstraction of the object’s category or identity gets allowed from the specifics of the visual input.  LeNet5: A CNN architecture which used a sequence of 3 layers, which includes pooling, convolution and non-linearity, where convolution is used for the extraction of spatial features, subsamples are used for  spatial average of maps. It is a MLP that operates as a final classifier and uses sparse connection matrix among layers for avoiding large computing costs. This architecture is known to be the origin of several recent architectures that have been developed in recent times.  Application of CNN: CIFAR 10 dataset is one of the many examples where the usage of CNN has taken place. Within a CNN, every network layer acts like a detection filter for having presence of specific patterns or features within the original data. Where the first layers within the CNN is able to detect large features which can be interpreted and recognized easily.  Auto encoder: A neural network that is able to handle several hidden layers within its structure, where the main goal  of the Auto  encoder is learning  a representation  for a set  of data for  the purpose  of dimensionality reduction. This kind of neural network gets trained in a manner where it attempts to copy the inputs to its output. It comprises of a hidden layer Z internally which describes a code that is used for representing the input. In recent times this concept has be en more popular for the purpose of learning generative data models.  