During this topic's learning, we were introduced  to the basics of neural systems and their complexity, as well as the risks associated with overÔ¨Åtting. We began by understanding the Perceptron and  how a multi-layer perceptron forms a neural network, enabling us to represent complex, non-linear decision surfaces. The concept of a feedforward neural network, where connections between units do not form cycles, was introduced.  We then delved into gradient-based optimization, which is essential for training neural networks and Backpropagation Algorithm, a fundamental technique for training neural networks by adjusting weights and biases based on error gradients.  We were also introduced to deep learning models that are designed to analyze data continuously, mimicking human reasoning processes. In particular, we studied the architecture of Convolutional Neural Networks (CNNs), and examined CNNs in the context of image processing, understanding how they can effectively extract features from images. Additionally, we explored the concept of Autoencoders, which are neural networks capable of handling multiple hidden layers.    