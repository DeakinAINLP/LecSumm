 The brain has many neurons and tasks. Artificial Neural Networks (ANNs) are inspired by the brain, can learn features from data. Two groups of study: one for brain modelling, other for learning machines.  Neural networks have input, hidden, and output layers. They can be simple or complex, but complex ones need more training data to avoid over-fitting.  Perceptron is a simple neural network for binary classification with one layer and single node. It separates input space into two regions using a hyperplane.  Multilayer perceptron helps solve problems not linearly separable, like XOR, by having middle layer in neural network.  Multilayer perceptron helps with complex, non-linear decision surfaces using a sigmoid function. Feedforward neural networks are artificial neural networks with no cycles or loops, and multi-layer perceptron is a type of feedforward neural network. Gradient-based optimization helps minimize error function in neural networks.  Backpropagation algorithm trains MLPs using stochastic gradient-descent rule. Update weights from hidden to output layer and input to hidden layer until stopping criteria met.  Deep learning models analyse data like humans using layered algorithms; common architectures include Convolutional Networks, Autoencoders, and Deep Neural Networks.  CNNs model after mammalian visual cortex, using sparse interactions, parameter sharing, and translation invariance.  Autoencoder is a neural network for data representation and dimensionality reduction. It compresses data into short code and reconstructs original data, with hidden layer being smaller and more meaningful.  