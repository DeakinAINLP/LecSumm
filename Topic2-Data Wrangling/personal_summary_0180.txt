Summarise the main points that is covered in this topic  This topic we learnt about Neural Networks, Perceptron and Mul8layer Perceptron and took an introductory peek at Deep Learning.  Motivation for Neural Networks  1.  Linear models may not be sufficient when the underlying functions or decision  boundaries are extremely non-linear  2.  Support Vector Machines can construct non-linear functions but use fixed feature  transformations which depends on the kernel which is chosen.  3.  Neural networks allow the feature transformations to be learnt from the data itself  Neural Network  1.  Components  a.  An input layer b.  One or more hidden layers c.  Combiners or sum functions d.  Nonlinear activation functions e.  An output layer  2.  Complexity  a.  Increasing the number of hidden layers can result in over-fitting, if we do not  have enough training data  3.  Single layer Perceptron - single-layer Neural Network 4.  Multi-layer Perceptron - Neural Network 5.  For input layer, we will have multiple features having different weights. There will  also be a bias term. There can be one or more output nodes  6.  Perceptron is simple Neural Network used for binary classification. It has only one  layer with a single node  7.  Steps for Perceptron  a.  Initialize bias term, w, as 0 b.  Retrieve next input and desired output c.  Compute actual output d.  Compute output error e.  Update weight for all, including the value for learning rate parameter f.  Repeat from step b until solution converges  8.  Activation functions a.  Sign function b.  Sigmoid function 9.  Feedforward Neural Networks  a.  Also called as Artificial Neural Network b.  Connections between units do not form a cycle or loop c.  Information moves only in forward direction d.  A multi-layer feedforward Neural Network is called Multi-layer  Perceptron(MLP) e.  MLP optimisation      i.  ii. iii.  Reduce the error which is the difference between the predicted value and real value Error function is not convex and can have multiple local minima Backpropagation is used to perform the minimisation here  10. Backpropagation algorithm  a.  The basic concept of training MLP is SGD(Stochastic Gradient Descent)  b.  Deep Learning  1.  Designed to continually analyse data using a layered structure of algorithms similar  to ANN  2.  Common Deep Learning architectures - a.  Convolutional Networks b.  Autoencoders c.  Deep Belief Networks d.  Boltzmann Machines e.  Restricted Boltzmann Machines f.  Deep Boltzmann Machines g.  Deep Neural Networks  Convolutional Neural Networks Made of three basic concepts - 1.  Sparse interaction 2.  Parameter sharing 3.  Translation invariance  Autoencoder  1.  Aim is to learn a representation for a set of data, typically for the purpose of  dimensionality reduction  2.  Loss function of an autoencoder should find the difference between the input and  output, and this should be as small as possible   