 What is different about neural networks?  -  Linear models may not be sufficient when the underlying functions or decision boundaries are  extremely nonlinear  -  Support vector machines can construct nonlinear functions but use fixed feature transformations, which depends on the kernel function. E.g., while you are using linear kernel, your model output will be a linear model  -  Neural networks allow the feature transformations to be learnt from data  Neural system basics  -  A typical neural network (machine) has an input layer; - - - -  It has one or many hidden layers; It has combiners (sum functions); It has nonlinear activation functions It has an output layer  Perceptron algorithm  -  Linear classifier (binary) and is a single layer neural network, a multi-layer perceptrion is called a  neural network  Motivation for multilayer perceptron  - Is it always possible to separate the data points based on their class label with a single line?  -  Can find a line to divide based on class labels -  What about XOR? This problem is not linearly separable, impossible to separate the data points  based on their class labels with a single line  These problems were a motivation to develop a new neural network, with a layer in the middle to handle these cases. A multilayer perceptron (MLP) can represent the XOR problem  Multilayer perceptron  -  Perceptron is weak in what it can be represent, for complex non-linear decision surfaces we need  multi-layer network  - Feedforward neural networks  Is an artificial neural network (ANN) where connections between units do not form a cycle In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes  -  There are no cycles or loops in the network  -  A multi-layer feed-forward neural network (NN) is also known as a multi-layer perceptron  (MLP). The model comprises multiple layers of logistic regression like models (with continuous non-linearities) rather than multiple perceptron (with discontinuous non-linearities)  Notes on the MLP  -  Two-layer network: input layer, hidden layer and output layer  - -  Important facts about this network:  o  Output is a vector o  Two kinds of weights here  ▪ Input > hidden ▪  Hidden > output  o o  The input layer does no computation – it only relays the input vector o o  Another interesting feature is that it does not have to be fully connected  It can have more than one hidden layer  MLP formulation  -  We would like to minimize the error which is the difference of the predicted value compared to the real true value of the output, we are stating the above as an optimization problem: find w to minimize the error function:  -  Detour: Gradient-based optimization  -  Gradient-based optimization methods are operated with the search directions defined by the  gradient of the function at the current point  