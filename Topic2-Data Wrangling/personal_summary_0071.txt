For our final topic in SIT307, we learnt about nonlinear models, neural networks, and deep learning. This final topic was split into these specific areas to help us understand:  -  Motivation and inspiration, and the historical approaches of developing an Artificial Neural Network (ANN). It was motivated by biological neural systems, and in history and ML, two approaches were taken to achieve this: Using ANN to study and model the brain, and using a brain to design ANN, however there is the possibility that it may not turn out the same as a real brain.  -  Neural system basics, and the introduction to major systems within a neural network, such as:  Input layer  o o  One or many hidden layers o  Combiners (sum functions) o  Nonlinear activation functions o  Output layer  We also looked at the complexity of neural networks, and how we can have bigger NN with high dimensional inputs and multi-label classifications.  -  Perceptron algorithm, which is a binary classification MLA. For it to work, it uses perceptrons, which are a linear classifier, and is generally a single layer neural network. If you were to combine multiple perceptrons to create a multi-layer perceptron, it would become a neural network.  -  Multilayer perceptron, which is a supplement of feed that is forwarded within a neural network. It utilizes three layers, input, output and hidden. We also learn how perceptrons are quite weak in what they represent, and if we want something stronger and more complex, we need to use multiple perceptrons, also known as a ‘multi-layer network’. The action of forward feeding is where connections between units don’t form a cycle.  -  Backpropagation algorithm, which is used to train multilayer perceptron (MLP), where the input is the training data, and we initialize weights at the start and then update them until the desired stopping criteria (accuracy, number of loops depending on change).  -  Deep learning, which is a model that is designed to continuously learn and analyze data with a logic structure, which is like how humans would come to conclusions. It uses a similar structure to ANN, and uses many architectures, such as:  o  Convolutional networks o  Autoencoders o  Deep Belief Networks o  Boltzmann Machines o  Restricted Boltzmann Machines o  Deep Boltzmann Machines o  Deep Neural Networks   -  Convolutional Neural Networks, which is a network that is modelled after the mammalian visual cortex, which is a part of the brain where visual input is processed. It is made up of three basic concepts:  o  Sparse interactions: weights that are in smaller kernels, rather than the whole input  (used to reduce number of parameters)  o  Parameter sharing; A kernel that uses the same set of weights while applying to  different locations.  o  Translation invariance: recognizing objects as objects, even if its appearance might vary.  -  Autoencoder, which is a type of NN that can handle many hidden layers in its structure.  