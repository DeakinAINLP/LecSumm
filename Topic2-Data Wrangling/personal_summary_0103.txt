 Overview In topic 10, we explored supervised learning further, experimenting with nonlinear supervised learning models like neural networks, perceptron and multilayer perceptron, and deep learning.  Motivation and Inspiration  Why Neural Networks?  -  Linear models may not be suﬃcient when the underlying  functions are extremely nonlinear  -  Neural networks allow the feature transformations to be learnt  from data  Major players in neural network system:    A typical neural network has an input layer   It has one or many hidden layers   It has combiners (sum functions)   It has nonlinear activation functions   It has an output layer  Must pay atention to important terms such as input layer, output layer, and hidden layer.  Example of neural network structure:  We notice the leti neural network is 3-dimensional layer while the one on the right has a 4-dimensional layers.   Perceptron Algorithm Perceptron is a linear classiﬁer and is a single layer neural network. A multi-layer perceptron is called a neural network.  Given weight w, the perceptron linearly divides input space into two regions:    All x’s such y(x, w) = 1 or v(x, w) >= 0 (the above region of the  hyperplane line)    All x’s such that y(x, w) = -1 or v(x, w) < 0 (the below region of  the hyperplane line)  In perceptron, we would like to ﬁnd the weight vector w so that the perceptron correctly classify 2 classes, given sample training data.  If appears to be a simple classiﬁcation problem as long as the data is linearly separable.  Steps of perceptron algorithm:  1. Initialize 1 = 0 2. Retrieve next input xt and desired output yt 3. Computer actual output yt = sign[xt, w] 4. Computer output error e 5. Update weight 6. Repeat from step 2 until convergence   Motivation for Multilayer Perceptron  We notice that AND and OR are linearly separable, having y = 1 as red squares for AND, and y = 0 as green circles for OR. We can draw a separating line between them in both cases. But notice the following XOR:  It’s not linearly separable. This kind of problems were a motivation to develop a new neural network, but this time with a layer in the middle to handle these cases. Multilayer perceptron (MLP) can represent the XOR problem   Multilayer Perceptron Feedforward neural network Basically, an artiﬁcial neural network (ANN) where connection between units do not form a cycle; they don’t go back to former layer, instead, they move forward only until they reach to the output layer.  Backpropagation Algorithm Rewinding up the neural network is allowed here. Proper tuning of the weights allows to reduce error rates and make the model reliable by increasing its generalization.  Introduction to Deep Learning This model is designed to continuously analyze data with a logic based structure that’s similar to how human draw conclusions. It’s basically an advanced neural network.  They have been successful in many real world tasks like handwriten digit recognition, image recognition etc.  Common deep learning architectures:    Convolutional Networks   Autoencoders   Deep Believe Machines   Restricted Boltzmann Machines   Deep Boltzmann Machines   Deep Learning Networks  Convolutional Neural Networks It’s worth mentioning that this model is based on the mammalian visual cortex, which is responsible for processing visual input.  Our brains process images in layers of increasing complexity. The ﬁrst layer distinguishes basic atributes like lines and curves. At higher   levels, the brain recognizes that a conﬁguration of edges and colours is a house or a bird.  CNNs are made of three basic concepts:    Sparse interactions: sparse weights within a smaller kernel  instead of the whole input which helps reducing the number of parameters.    Parameter sharing: a kernel uses the same set of weights while  applying to diﬀerent locations    Translation invariance: invariance means that you can recognize an object as an object, even when its appearance varies in some way.  LeNet5 The very ﬁrst CNN. The features of LeNe5 can be summarized as:    A convolutional neural network that uses a sequence of 3  layers: convolution, pooling, non-linearity   Uses convolution to extract spatial features   Subsamples using spatial average of maps   Nonlinearity in the form of tanh or sigmoid   Basically an MLP as ﬁnal classiﬁer   Uses a sparse connection matrix between layers to avoid large  computation cost  LeNet5 was the origin of many recent architectures and a true inspiration for many people in the ﬁeld.  Autoencoder A neural network which can handle many hidden layers in its structure. Its aim is to learn a representation for a set of data typically for the purpose of dimensionality reduction.  