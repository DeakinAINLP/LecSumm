In topic ten, we delved into the topic of Nonlinear models, focusing on the fundamentals of  neural  systems,  perceptron,  multilayer  perceptron  and  algorithms  in  the  context  of  deep  learning.  The topic began with  an exploration of motivation  and inspiration, discussing the  importance of these factors in driving research and advancements in the field of deep learning.  We discussed the different about Neural Networks and our lecturer emphasized the significance  of understanding the human brain and how it has inspired the development of artificial neural  systems.  Next,  we  covered  the  basics  of  neural  systems.  This  included  an  introduction  to  neurons and their interconnectedness in neural networks. We also learnt the different layers of  neural  networks,  such  as  input,  hidden,  and  output  layers,  highlighting  their  respective  functions.  After that we concentrated on the perceptron algorithm as a simple neural network model. Here  we discussed its working mechanism, algorithm's ability to learn and adapt through iterations,  perceptron convergence theorem and the importance of labeled training data. Next we explored  the  motivation  for  using  multilayer  perceptrons  (MLPs).  We  learnt  about  the  limitations  of  single-layer  perceptrons  in  handling  complex  problems,  leading  to  the  need  for  MLPs  with  multiple hidden layers. Our lecturer introduced the concept of deep learning, which involves  training deep neural networks with multiple layers, was introduced. Our focus then shifted to  understanding  multilayer  perceptrons  in  depth.  Under  this  topic,  we  covered  feedforward  neural  networks,  MLP  formulation  and  detour:  gradient-based  optimization.  We  also  went  through the backpropagation algorithm, which is a fundamental technique for training neural  networks.  This  algorithm  enables  the  adjustment  of  weights  and  biases  in  the  network  by  calculating gradients and propagating them backward from the output layer to the input layer.  We also talked about the issues with backpropagation. In the programming part of the unit, we  gained practical experience of implementing the humble perceptron.  Our lecturer provided an introduction to deep learning, outlining its principles, applications,  and some common Deep Learning architectures. Convolutional Neural Networks (CNNs) were  then introduced as a specialized type of deep learning architecture for processing structured  grid-like data, such as images. Under this topic we learned about the concepts of convolutional  stage, pooling stage, and non-linear stage in CNNs. We then explored the application of CNNs  in  various domains. Real-world  examples  were presented to  demonstrate  the versatility  and  power of CNNs in solving complex visual tasks.  We discussed about Autoencoders, a type of neural network architecture used for unsupervised  learning and dimensionality reduction. In the "Deep learning with Python" section, we were  provided  with  some  practical  examples  and  demonstrations  of  implementing  deep  learning  algorithms  using  Python.  The  topic  concluded  with  additional  resources  for  deep  learning,  where we were guided towards further readings, online tutorials, and research papers to deepen  our understanding of the subject.  Overall, topic ten provided a  comprehensive overview of motivation,  neural  system basics,  multilayer perceptrons, backpropagation, Python  programming,  and deep  learning concepts.  The  topics  covered  the  foundational  knowledge  and  practical  aspects  necessary  for  understanding and implementing deep learning algorithms.  