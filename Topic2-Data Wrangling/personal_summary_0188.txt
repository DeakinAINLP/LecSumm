Lesson Review Topic 10: Nonlinear models (neural networks and deep learning)  The feature transformations to be learnt from data is allowed by Neural Networks. The major players in a neural network system are: an input layer, one or many hidden layers, combiners, nonlinear activation functions, output layer. An input layer which is responsible for taking the input features. Neural networks are compatible with high dimensional inputs and multi-label classification. If not provided enough training data, having a more complex neural network can result in over- fitting.  Perceptron is a linear classifier (binary) and is a single layer neural network. A multi-layer perceptron is called a neural network. AND and OR logical gates or functions are linearly separable whereas XOR problem is not linearly separable. A multilayer perceptron (MLP) can represent the XOR problem.  For complex, non-linear decision surfaces, we need a multi-layer network as a perceptron is quite weak in what it can represent. A richer capability in the functions they can model can be provided can be provided by choosing a more complex activation function allows the network to combine the inputs in more complex ways. A value between 0 and 1 with an s-shaped distribution is an output by Non-linear functions like the logistic, (also called the sigmoid function). Choice of node in a multi- layer network should be continuous.  When connections between units do not form a cycle it is called a feedforward neural network is an Artificial Neural Network (ANN).  The information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes and there are no cycles or loops in the network. It is also known as a Multi-layer Perceptron (MLP). The model comprises multiple layers of logistic regression like models (with continuous non-linearities) rather than multiple perceptrons (with discontinuous non-linearities).  Gradient-descent for minimisation (E(w)) is not convex, rather it is a complex function with potentially many local minima. Methods which are operated with the search directions defined by the gradient of the function at the current point are called gradient-based optimisation. Gradient- descent finds the optimal point by finding the true direction towards the optimal point and the magnitude of the movement towards it. This is a very useful approach even in ANNs. Stochastic Gradient-Descent (SGD) is cheap to perform and guaranteed to reach a local minimum in a stochastic sense.  Backpropagation algorithm is used for training MLPs. The basic concept of training MLPs is a stochastic gradient-descent rule. Backpropagation is a principal algorithm utilized in artificial brain networks (ANNs) for preparing models in a supervised learning setting. It is fundamentally utilized to change the loads of the associations between neurons, permitting the organization to gain from input-yield coordinates and make exact predictions.The backpropagation algorithm works by propagating mistakes in reverse through the organization, thus its name. The overall thought is to figure the gradient of the misfortune capability regarding the loads of the organization, which demonstrates how delicate the result is to changes in the loads. By iteratively refreshing the loads the other way of the gradient, the algorithm means to limit the general mistake of the organization.  To continually analyze data with a logic structure similar to how a human would draw conclusions, a deep learning model is deisgned. It uses a layered structure of algorithms like ANNs and are advanced neural networks. Some of the common Deep learning architectures are: Convolutional Networks, Deep Belief Networks, Boltzmann Machine, Deep Neural Networks etc.  After the mammalian visual cortex, the part of the brain where visual input is processed, The architecture of a Convolutional Neural Network (CNN or ConvNet) is modelled. The term kernel in CNN generally refers to an operator applied to the entirety of the image such that it transforms the information encoded in the pixels. Invariance means that you can recognize an object as an object, even when its appearance varies in some way. CNNs are made of three basic concepts: Sparse interactions, Parameter sharing and Translation variance. The very first CNN was called LeNe5. It can be summarised as - a convolutional neural network that uses a sequence of 3 layers: convolution, pooling, non-linearity; uses convolution to extract spatial features; subsamples using spatial average of maps; nonlinearity in the form of tanh or sigmoids; basically an MLP as final classifier and uses a sparse connection matrix between layers to avoid large computational cost.  An Autoencoder is a neural network which can handle many hidden layers in its structure. Its aim is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. This type of neural network is trained to attempt to copy its input to its output.  