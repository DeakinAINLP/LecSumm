The final topic’s content takes us into concepts related to neural networks, perceptrons and deep learning. These are often used as buzz words when talking about machine learning, and rightfully so as they are integral components to more advanced models. Motivation and Inspiration The motivation of, and where it receives its name from, for Artificial Neural Networks comes from the desire to imitate the human brain as far as possible. As opposed to the ML models we have learnt so far, a human brain is a highly paralleled and complex system. Drawing inspiration from this allows us to design models that are not constrained to linear problems and that are able to adapt based on the data provided as opposed to being reliant on a specific data form. Neural System Basics On the most basic level, a neural network has a structure consisting of an input layer, a hidden layer and an output layer. Each of these layers contains an amount of neurons, or dimensions. Simple neural networks might only have a single dimension output such as a binary value, but more complex ones have multi-dimensional output layers and are capable of multi-label classification. Perceptron Algorithm A perceptron is a single layer neural network and is a linear or binary classifier. Perceptrons are the building blocks of neural networks as NNs are multi-layer perceptrons. To summarize simply how a perceptron works, the weights and bias of the perceptron are initialized at small, random values. The perceptron then uses each feature of the input data instance in a function where it is multiplied by the weight specific to that feature. Then the bias is added to the function result and it is finally passed through a predefined activation function. This prediction is then checked against the true value from the training data and the weights and bias are adjusted to help improve the accuracy of the perceptron. Motivation for Multilayer Perceptron The motivation for the development of multilayer perceptron lies in the need to solve classification problems that are not linear in nature. Multilayer Perceptron Multilayer perceptron (MLP) is a neural network consisting of multiple layers worth of perceptrons. There are 3 categories of layer, the input, hidden and output layers. The input layer serves to receive data points and pass them to the hidden layers. The input layer receives every feature and passes it to the hidden layers. Each input neuron is connected to each neuron within the next layer. In the hidden layers, there is a network of neurons with each neuron being connected to every neuron in the next layer, in the connection between hidden layer neurons, the weights and activation functions are applied to the feature and thus relationships in the data are learnt. Once the feature has made its way through the hidden layers, the output layer is reached where the processed data from the hidden layers is received. Here, there are different neurons to represent the different possible outcomes. In a binary problem, there would only be a single neuron whilst in a multi-class problem there would be multiple neurons with each representing a specific class. Deep Learning Introduction Another buzz word in “deep learning”, we are referring to models that are designed to continuously evaluate data it receives with a logic similar to that of a human attempting to come to a conclusion. This is doen through the use of layered structural algorithms like ANNs. Deep learning models are an advanced form of neural networks and have already proven to be successful in learning real world tasks like image recognition amongst others. These are the types of NNs that we see in self-driving cars amongst other ML powered tools in use today. Convolutional Neural Networks Convolutional Neural Networks (CNNs) are a type of NN architecture that is commonly used in tasks that require the analysis of visual data. The way in which CNNs are designed makes them effective at processing structured data that takes a grid form. Like an image, which is a grid of pixels. CNNs start with convolutional layers, at these layers a function is applied to small areas of the input data to determine the region of the data within the larger dataset. From here it is passed through an activation function like ReLu which will introduce the non-linearities required to allow the network to learn complex relationships within the input data and its features. After this it is passed through a series of pooling layers which help downsample the features to reduce the dimensionality of it. After this, the final output of the pooling layers are flattened into a one dimensional vector to be passed to the output layer which functions in essentially the same manner as an MLP. 