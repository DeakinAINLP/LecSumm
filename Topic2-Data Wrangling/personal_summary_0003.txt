The mathematical model of a neuron simulates biology where summation of multiple inputs is used to develop the output. This is a perceptron which may also have a bias term. It works best where the data are linearly separable. The first AI winter developed in part because of the realisation that some functions like XOR are not linearly separable and cannot be represented. A model with one layer can learn linear combinations of the input terms A model with two layers can learn arbitrary functions, giving complex, non-linear decision surfaces Activation functions are often differentiable (to allow back propagation), and thus avoid sharp changes like the sign function. Feed forward networks have a path from input to output without any cycles. A recurrent neural network by contrast will have cycles so that output from one layer feeds into earlier ones which form its input. This allows memory such as LTSM. Back propogation is similar to gradient descent Deep learning takes the MLP and makes it much deeper, enabling it to learn features and complex patterns. CNN was designed based on the visual cortex where neighbouring neurons subtract to sharpen the image. This gave rise to the kernel function, which are applied as sliding windows, and provide translation invariance. Autoencoders help with dimensionality reduction, and learn the important features automatically. They reduce and then increase the number of neurons in the hidden layers, forcing a compressed representation in the middle. The decompression generates the original output from the encoded form. 