This topic I learnt about neural networks, perceptron and multilayer perceptron, deep learning. Let’s talk about all these above-mentioned topics brieﬂy with other terms related to topics.  A typical neural network machine has an input layer, it has one or many hidden layers, it has combiners, it has nonlinear activation functions, it has an output layer. We can have more complex, bigger neural networks  because  neural  networks  are  compatible  with  high  dimensional  inputs  and  multi-label classiﬁcation.  Perceptron Algorithm: Perceptron is a linear classiﬁer (binary) and is a single layer neural network and multi-layer perceptron is called a neural network.  Multi-layer  perceptron:  A  multi-layer  perceptron  (MLP)  is  a  type  of  artiﬁcial  neural  network  that consists of multiple layers of interconnected nodes. It has three types of layers, input layers, hidden layers, and output layers. The information always ﬂows from input layer through the hidden layers to the  output  layer  without  forming  cycles  or  loops.  For  the  complex,  non-linear  problem  we  need  a multi-layer network.  Perceptron classiﬁer: This is a type of binary classiﬁcation algorithm based on the concept of single- layer neural network called perceptron. The perceptron classiﬁer is designed to classify input data into one or two classes based on a linear decision boundary.  Deep learning: A deep learning model is designed to continually analyse data with logic structure like how  a  human  would  draw  conclusions.  To  achieve  this,  deep  learning  uses  a  layered  structure  of algorithm like Artiﬁcial Neuron Network.  Convolution  neural  networks:    CNN  are  a  type  of  deep  learning  algorithm  speciﬁcally  designed  for processing and analysing data with grid-like structure. The key concept behind CNN is the utilisation of conventional layers, which perform localized and shared weight computations on input data.  Autoencoder:  An  autoencoder  is  a  neural  network  which  can  handle  many  hidden  layers  in  its structure. The aim of an autoencoder is to learn a representation encoding for a set of data, typically for the purpose of dimensionality reduction.  