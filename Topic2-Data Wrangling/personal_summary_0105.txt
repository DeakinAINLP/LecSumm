Main learning points:    Perceptron: A single-layered neural network (single-layered neural network)          Neural Network:    Input layer -> many hidden layers -> output layer  o  Excessive number of hidden layers is prone to overfitting.    The number of nodes in the input layer is the number of features (dimensions)   Nodes are neurons and connections are synapses.    Nodes contain an activation function.   The synapses’ parameters are:  o  A multiplier on the previous node’s output – ‘weight’. o  Then it adds another parameter – ‘biases’.    Those parameters are found through back-propagation, which works by:  1.  Starts at the output layer and moves backwards. 2.  A measurement of error is used to define the difference between the true values and  predicted values. E.g., Mean squared error or cross-entropy.  3.  Calculus is used to find the parameter values where the error is minimal. The  parameters for the synapse are updated.  ▪  Methodology varies with the ‘solver’ parameter in sklearn.  4.  The same process propagates backwards to find the optimal parameters at each  synapse.  5.  In sklearn the number repetitions of the above process is capped at ‘max_iter’. Each  repetition a sample size of ‘batch_size’ at once. o  If the training data has 1000 samples and the batch_size is set to 100, each iteration will process a batch of 100 samples, and it will take 10 iterations to process all the training data once. If max_iter is set to 5, with a batch of 100, the training process will stop when half the training data is processed.  o    ‘Dropout’ is a regularisation technique where a subset of neurons has their outputs set to zero during training.  o  This makes the model less dependent on certain neurons and helps with  generalisation.  Reading List Summary:    Learning resources 10.4   https://www.youtube.com/watch?v=CqOfi41LfDw&t=354s (StatQuest: neural networks)   https://www.youtube.com/watch?v=IN2XmBhILt4&t=325s (StatQuest: neural networks)   https://scikit-learn.org/stable/modules/neural_networks_supervised.html   https://scikit-  learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.ne ural_network.MLPClassifier  Reflection:    The pass task was done using sklearn. For further control of the model, I would need to be  familiar with keras and TensorFlow, which is my next goal.    Will also need to learn how to use neural networks for other contexts outside of MLP  classification. I plan to explore this, gaining experience for diverse usages, in the Deep Learning unit.  