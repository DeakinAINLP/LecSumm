  Artificial Neural Networks (ANNs) operates similarly to a biological neural system, a network  of inter-connected neurons in an architecture which is highly parallel.    Neural networks typically have an input layer, one or more hidden layers, combiners (sum  functions), non-linear activation functions and an output layer.    Perceptron is a linear (binary) classifier and is a single layer neural network, in other words, a  multi-layer perceptron is a neural network.    Motivation for the multi-layer perceptron (MLP) is that it can represent XOR problems.   A feedforward neutral network is an ANN in which connections between units to not form a cycle, information moves only forwards from input nodes to output nodes via any hidden nodes.    A multi-layer feedforward neural network is an MLP is comprised of multiple layers of  logistic regression rather than multiple perceptrons.    Gradient based optimisation methods are operated with search directions defined by the  current point’s function’s gradient.    A Stochastic Gradient Descent (SGD) minimises instantaneous approximation and is cheap to  perform and guaranteed stochastically speaking to reach a local minimum.    A deep learning model is designed to analyse data continually using a logic structure which is similar to the how humans come to conclusions, it is a layered structure similar to ANNs.   Some deep learning architectures include convolutional networks, autoencoders, deep belief networks, Boltzmann machines, restricted Boltzmann machines, deep Boltzmann machines and deep neural networks.    Convolutional Neural Networks (CNNs) are modelled after the visual cortex of mammals,  where the brain processes visual input.    CNNs are made up of three basic concepts, sparce interactions, parameter sharing and  translation invariance. LeNe5 was the first CNN.     An autoencoder is a neural network which can handle many hidden layers in its structure, it  encodes a representation of a dataset, for dimensionality reduction.  