Topic 10: Nonlinear models (neural networks and deep learning)  Learning objectives:  -  Analyze performance of ensemble classifiers with respect to a single model -  Construct a multi-layer neural network using a backpropagation training algorithm to  demonstrate data representation, classification and evaluation skills  Learning summary:  -  Motivation and inspiration:  Our brain has networks of inter-connected neurons and a highly-parallel architecture. Development of Artificial Neural Networks(ANNs) is motivated by biological neural systems.  Historical approaches:  ▪  In machine learning, two groups of ANN researchers are working on this problem:    one group uses ANN to study and model the brain   the other group uses the brain as the motivation to design ANNs as effective learning machines, which might not result in a true model of the brain  Neurons:  There are billions of neurons in your brain. Estimates range from 50 billion to  500 billion. (Woodford 2018)  Brain function: The brain takes physical or mental stimuli as input, processes it and, if  necessary, produces an output.  -  Neural system basics:  With the brain in mind, these are the major players in a neural network system:  a typical neural network (machine) has an input layer; ▪ ▪ ▪ ▪  it has one or many hidden layers; it has combiners (sum functions); it has nonlinear activation functions; it has an output layer.  Here is an example of a neural network with 3 hidden layers:   Figure. Diagram of a complex neural network.  -  Perceptron algorithm  Perceptron is a linear classifier (binary) and is a single layer neural network. A multi-layer  perceptron is called a neural network.  A perceptron is a simple neural network used for binary classification. It has only one  layer with a single node.  -  Multilayer perceptron  A perceptron is quite weak in what it can represent. For complex, non-linear decision  surfaces, we need a multi-layer network.  Non-linear functions like the logistic, (also called the sigmoid function), output a value  between 0 and 1 with an s-shaped distribution. Choice of node in a multi-layer network should be continuous but it should be a continuous meaningful function such as the sigmoid function.  Feedforward neural networks: is an Artificial Neural Network (ANN) where connections between units do not form a cycle. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.  A multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer Perceptron (MLP). The term MLP is really an accurate name because the model comprises multiple layers of logistic regression like models (with continuous non-linearities) rather than multiple perceptrons (with discontinuous non- linearities).  -  Convolutional Neural Networks  The architecture of a Convolutional Neural Network (CNN or ConvNet) is modelled after the mammalian visual cortex, the part of the brain where visual input is processed. Within the visual cortex, specific neurons fire only when particular phenomena are in  the field of vision. -  CNNs are made of three basic concepts:  Sparse interactions: Sparse weights within a smaller kernel (e.g., 3×3,5×5) instead of the whole input. This helps reduce the number of parameters. The term kernel in CNN generally refers to an operator applied to the entirety of the image such that it transforms the information encoded in the pixels (see the figure above).   Parameter sharing: A kernel uses the same set of weights while applying to different  locations (sliding windows).  Translation invariance: Invariance means that you can recognize an object as an object, even when its appearance varies in some way. This is generally a good thing, because it allows abstraction of an object’s identity or category from the specifics of the visual input.      