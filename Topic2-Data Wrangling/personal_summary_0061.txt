Topic10. Nonlinear models (Neural networks and deep learning)  -  What is different about Neural Networks?    Linear models may not be sufficient when the underlying functions or decision boundaries are extremely nonlinear    SVM can construct nonlinear functions but use fixed feature transformations, which  depends on the kernel function.    Neural Networks allow the feature transformations to be learnt from data   ANN(artificial neural networks) motivated by biological neural systems  -  Historical motivation    Uses ANN to study and model the brain   Uses brain as the motivation to design ANNs as effective learning machines  -  Neural system basics Input layer     One or many hidden layers   Combiners (sum functions)   Nonlinear activation functions   An output layer  -  Complexity    More complex and bigger neural networks possible as they are compatible with high  dimensional inputs and multilabel classifications.    Having a more complex neural network can result in over fitting if not providing enough  -  training data Perceptron algorithm : a linear classifier(binary) and a single layer neural network : a multi layer perceptron is called a neural network : has only one layer with a single node    Output y = sign[v(x,w)] V(x,w) = ∑wi*xi + w0  Input vector x of M dimensions and weight vector w    Better matrix notation, let x0 = 1  V(x,w) = 𝑤𝑇𝑥 = 𝑥𝑇𝑤 Y(x,w) = sign[𝑤𝑇𝑥]    Hyperplane H(w): decision surface   Given weight w, the perceptron linearly divides input space into two regisions: 1.  All x such that y(x,w) = 1 or v(x,w)>= 0 : above region of the hyperplane line 2.  All x such that y(x, w) =-1or v(x, w) <0 : the below region of the hyperplane line    This corresponds to the two sides of the hyperplane:  V(x,w) = ∑wi*xi + w0 = 0    |v(x, w)| is proportional to the distance from x to the hyperplane If it is close to the boundary hyperplane-less confident If it is far enough from the line – more fonfident  1. 2.  3.  Dist(x, H(x)) =  |𝑣(𝑥,𝑤)|  𝑀 2 √∑ 𝑤𝑖 𝑖=1    Linearly separable : two sets and are called linearly separable if there exists a hyperplane that separates them     Training (or learning) perceptron  1.  Find the weight vector so that the perceptron correctly classify 2 classes, given  sample training data  2.  Training data D=(xt, yt) where xt=xtM is the input vector at time t and yt = +/- 1 is  the desired output   Perceptron learning algorithm  Initialise w=0  1. 2.  Retrieve next input xt and desired output yt : compute actual output yt = sign[xt, w] : compute output error e= yt -yt : update weight  3.  Repeat from step 2 until convergence    Perceptron convergence theorem  1.  If training instances are drawn from two linearly separate sets and, then the perceptron learning rule will converge after finite iterations 2.  No guarantee for convergence if and are not linearly separable  -  Motivation for multilayer perceptron    AND and OR logical problems can be linearly separated in 1 and 0 class labels   XOR  problem is not linearly separable- multilayer Perceptron(MLP)  -  Multilayer Perceptron    For complex, non-linear decision surfaces, we need multi-layer network   Choice of node in multi-layer network  Perception: discontinuity, so it should be a continuous meaningful function-sigmoid function. y= 𝑠𝑖𝑔𝑛(𝑤𝑇x)-we use sigmoid function instead of the sign function    Feedforward neural networks  : A feedforward neural network – Artificial Neural Network (ANN) where connections between units and do not form a cycle.    MLP  : multi-layer feed- forward Neural Network(NN) : the model comprises multiple layers of logistic regression like models with continuous non-linearities rather than multiple perceptron  -  Structure of MLP : consider a two layer network- input layer, hidden layer and output layer (output layer and hidden layer are made of sigmoid nodes)   Output is a vector   Two kinds of weights  : input to hidden, hidden to output ℎ  𝑓𝑟𝑜𝑚 𝑖𝑡ℎ  𝑖𝑛𝑝𝑢𝑡  → 𝑗𝑡ℎ hidden 0  𝑓𝑟𝑜𝑚 𝑗𝑡ℎ ℎ𝑖𝑑𝑑𝑒𝑛  → 𝑘𝑡ℎ𝑜𝑢𝑡𝑝𝑢𝑡    𝑤𝑖𝑗  𝑤𝑖𝑗   The input layer does no computation-only relays the input vector   Can have more than one hidden layer  It does not have to be fully connected  -  MLP formulation    Given input x and desired output y- aim is to find the network weights w so that the  predicted values will be as close as possible to the real ones    Optimization problem: find to minimise the error  𝐸(𝑤) =  1 2  𝑘  𝑛 ∑ ∑(𝑦𝑡𝑘 − 𝑦𝑡𝑘)2 𝑡=1  𝑘=1    Gradient-descent is used for minimization    Backpropagation is used as an algorithm  Is not convex, but a complex function with possibly many local minima  -  Gradient-based optimization  -  -  -  Initialize random x0  : to minimize a function f(x), use gradient descent    Slide down the surface of f in the direction of the steepest decrease   Similarly use gradient ascent to maximise f Stochastic Gradient Descent(SGD)   Instead of minimizing, SGD minimizes the instantaneous approximation of using only -th instance   Update rule   SGD is cheap to perform and guaranteed to reach a local minimum in a stochastic sense Training MLP: Backpropagation   A stochastic gradient-descent rule   Minimizing instantaneous approximation for current training sample   Gradient-descent rule    This gradient rule implies   Similarly for input hidden weights   Gradient descent update Issues with Backpropagation   Let be the unsigmoided argument value at output node  Local minima : Possible fixes 1.  Add a momentum term in the update rule 2.  Can prevent getting stuck in shallow local minimum 3.  Multiple restarts and choose final network with best performance    Overfitting  1.  The tendency of the network to memorize all training samples, leading to poor  generalisation  2.  Usually happens with network of too many hidden nodes and overtrained  : possible fixes  1.  Use cross validation: stop training when validation error starts to grow 2.  Weight decaying: minimize also the magnitude of weights, keeping weights small 3.  Keep small number of hidden nodes  -  Deep learning  : designed to continually analyse data with a logic structure similar to how a human would draw conclusions : uses a layered structure of algorithms similar to ANNs(Advanced Neural Networks)  -  Convolutional Neural Networks(CNN or ConvNets)    Motivation  1.  Vision processing in our brain is fast 2.  Simple cells detect local features   3.  Complex cells pool local features    CNN concepts  1.  Sparse interactions: sparse weights within a smaller kernel instead of whole input -  this helps reduce number of params  2.  Parameter sharing: a kernel use the same set of weights while applying onto  different location(sliding windows)  3.  Translation invariance: recognise an object as an object, even when its appearance  varies in some way    Basic functionality  1.  As found in other forms of ANN, the input layer will hold the pixel values of the  image.  2.  The convolutional layer will determine the output of neurons which are connected to local regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit aims to introduce to LeNet5, an elementwise activation function such as sigmoid to the output of the activation produced by the previous layer  3.  The pooling layer will simply perform downsampling along the spatial dimensionality  of the given input, further reducing the number of parameters within that activation.  4.  The fully connected layers will perform the same duties found in standard ANNs and attempt to produce class scores from the activation to be used for classification.    LeNet5 features 1.  A convolutional neural network that uses a sequence of 3 layers: convolution,  pooling, non-linearity  2.  Use convolution to extract spatial features 3.  Subsamples using spatial average of maps 4.  Nonlinearity in the form of tanh or sigmoids 5.  Basically an MLP as final classifier 6.  Uses a sparse connection matrix between layers to avoid large computational cost  -  Autoencoder  : a neural network which can handle many hidden layers in its structure : aim is to learn a representation(encoding) for a set of data, typically for dimensionality reduction   Simply a neural network that tries to copy its input to its output   Another way of feature learning   The solution is trivial unless there are constraints (such as sparsity) on the number of  nodes in the hidden layers Linear autoencoder with acts as PCA    Extra readings  https://www.explainthatstuff.com/introduction-to-neural-networks.html  https://www.simplilearn.com/tutorials/deep-learning- tutorial/perceptron#:~:text=A%20Perceptron%20is%20a%20neural,value%20%E2%80%9Df(x).      