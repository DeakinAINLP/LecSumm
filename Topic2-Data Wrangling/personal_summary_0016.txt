For this topics module we looking at neural networks and deep learning. In general, the complexity of a neural network depends on both the number of hidden layers and the size of the hidden layers. A perceptron is a single layer neural network that is a binary classifier, it does this by finding weights for each feature that allows it to linearly separate 2 classes. Multi-layer perceptrons while similar to perceptrons (they are not made up of perceptrons) are capable of non-linear seperation, and are trained through an algorithm called back-propagation. For the problem-solving task this topic an MLP model was trained for face identification using python, so train a model that was capable of accurate prediction the complexity of the network was managed by tweaking the size of the hidden layers and the number of iterations the training took.  