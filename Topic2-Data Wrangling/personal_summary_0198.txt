Lesson Review: Neural Networks & Deep Learning Module Learning Objectives  1.  Analyse performance of ensemble classifiers with respect to a single model 2.  Construct a multi-layer neural network using a backpropagation training algorithm to  demonstrate data representation, classification, and evaluation skills.  Summarising the content: Neural Networks – is a method of computer modeling based around replicating how the human brain works.  The  brain  takes  physical  or  mental  stimuli  as  inputs,  processing  the  inputs  to  produce  an output. A Neural Network is designed to work in a similar way. A Neural network is made up of:  ▪  Has an input Layer. ▪  One or more hidden layers ▪  Combiners (sum functions) ▪  Nonlinear activation functions ▪  Has an output Layer.  Neural networks allow for more complex models that are compatible with high dimensional inputs and multi-label classification. However, complex neural networks can still result in over-fitting if they are not provided with enough training data.  Perceptron  –  A perceptron  is  a  simple neural network used for  binary  classification.  It  has  only a single layer with a single node. A Perceptron Algorithm follows these steps:  Feedforward – A feedforward neural network is an ANN where connections between units do not form  a  cycle.  In  this  network,  the  information  moves  in  only  one  direct  –  from  the  input  nodes through the hidden nodes and to the output nodes.  Multilayer-Perceptron (MLP) – A multilayer feed-forward neural network is known as a Multi-layer Perceptron.  The  model  comprises  multiple  layers  of  logistic  regression  like  models,  rather  than multiple  perceptron’s.  The  model  combines  another  layer  in  the  middle  to  represent  the  XOR problem that is not linearly separable.  Given the provided input and desired output  – the aim is to find the network weights so that the predicted value will be as  close as possible to the real ones. We would like to  minimize the error which is the difference of the predicted value compared to the real true value of the output.  Gradient-Based Optimisation – are methods with the search directions defined by the gradient of the  function  at  the  current  point.  Starting  with  initializing  a  random  point  and  sliding  down  the surface in the direction of the steepest decrease. By finding the true direction towards the optimal point and the magnitude of the movement towards it, gradient-descent finds the optimal point.  Back Propagation Algorithm - The backpropagation algorithm utilizes the chain rule of calculus to calculate  the  gradients  of  the  error  with  respect  to  the  weights  of  the  network.  By  iteratively adjusting  the  weights  based  on  these  gradients,  the  algorithm  enables  the  network  to  learn  and improve its predictions over time.  Deep Learning  (DL) – DL methods are advanced neural networks that are designed to continually analyse data with a logic structure similar to how a human would draw conclusions. These methods have  been  successful  in  many  real-world  tasks  –  such  as  handwritten  digit  recognition,  image recognition.  Convolutional Neural Network (CNN) – is modelled after the mammalian visual cortex, or part of the brain where visual input is processed. CNNs are made of three basic concepts:  ▪  Sparse interactions – sparse weights within a smaller kernel instead of the whole input. ▪  Parameter  sharing  –  A  kernel  uses  the  same  set  of  weights  while  applying  to  different  locations.  ▪  Translation  invariance  –  meaning  an  object  can  be  recognised  as  an  object  even  when  its  appearance varies in some way.  In  a  CNN  every  network  layer  acts  as  a  detection  filter  for  the  presence  of  a  specific  feature  or patterns  present  in  the  original  data.  The  first  layers  in  a  CNN  detect  large  features  that  can  be recognized and interpreted relatively easily, where further layers focus on finer details.  Autoencoder – is a neural network which can handle many hidden layers in its structure. The aim is to  learn  a  representation  (encoding)  for  a  set  of  data,  typically  for  dimensionality  reduction.  The network  is  trained  to  attempt  to  copy  its  input  to  its  output.  Autoencoders  are  a  way  of  feature learning – after learning the hidden layer can be used to reconstruct the original feature vector  – which allows autoencoder to be used for generative models.  