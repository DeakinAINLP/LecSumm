Evidence of learning Neural networks Inspired by biological neural systems.  Motivated by the need to create models for non-linear relationships.  SVMs can model non-linear relationships, but only using fixed feature transformations.  A typical neural network consists of:  -  An input layer -  One or many hidden layers -  Combiners (sum functions) -  Non-linear activation functions -  An output layer  Good introductory video: (25) MIT Introduction to Deep Learning | 6.S191 - YouTube  Perceptrons A perceptron takes a vector of features, x, and a vector of weights, w, and outputs one scalar value, ŷ.  There are two main steps to the perceptron algorithm:  1.  Calculate the sum of the products of each term in x with its corresponding term in w (i.e.  calculate the dot product of x and w)  2.  Use a non-linear function on the dot product to calculate the perceptron’s output (such as  the sigmoid function)    The perception is trained by updating the weights using this algorithm:  1.  Calculate the output of the perceptron 2.  Calculate the output error  3.  Update the weight with:  is the learning rate (a hyperparameter)  4.  Repeat until convergence  A single perceptron can be used to divide a linearly dividable dataset using a hyperplane:  However not all datasets are linearly separable such as the XOR problem:        Multi-layer perceptron/feedforward neural networks A feedforward neural network is a neural network that contains no cycles, that is, the data flows in one direction from the input layer to the hidden layers to the output layer.  It consists of multiple layers of perceptrons, where the output from a previous layer of perceptrons becomes the input for another layer of perceptrons.  Gradient descent It is computationally infeasible to try every neural network with every combination of weights. Therefore, we use a method called gradient descent to find the optimal weights for a neural network.  If we imagine the input of a neural network’s error function to be a vector of weights, and the output of the error function to be a multi-dimensional surface, then using the gradient descent algorithm we can calculate the weight vector needed to minimize the error of a neural network.  This method is not always guaranteed to find the global minima of the error function, because the error function is not guaranteed to be convex.   Backpropagation The backpropagation algorithm iteratively adjusts the weights of the neural network by propagating the errors backward from the output layer to the input layer. This process allows the network to learn from its mistakes and improve its performance over time. By updating the weights based on the error gradients, the network gradually converges towards a state where it can make accurate predictions.  The algorithm for backpropagation is as follows:  Initialize the weights of the neural network randomly.  1. 2.  For each input in the training dataset:  a.  Perform forward propagation: Compute the output of each neuron by applying the  activation function to the weighted sum of its inputs.  b.  Calculate the error: Compare the predicted output with the actual output and  calculate the difference.  3.  For each neuron in the network (starting from the output layer and moving backward):  a.  Compute the error gradient: Multiply the error of the neuron by the derivative of its  activation function.  b.  Update the neuron's weights: Adjust the weights based on the error gradient and  the learning rate.  4.  Repeat steps 2 and 3 for a specified number of epochs or until the network converges. 5.  Evaluate the performance of the trained network on a separate test dataset.  Deep learning Common deep learning architectures:  -  Convolutional Networks -  Autoencoders   -  Deep Belief Networks -  Boltzmann Machines -  Restricted Boltzmann Machines -  Deep Boltzmann Machines -  Deep Neural Networks  Convolutional neural networks CNNs are modelled after the part of the mammalian brain where visual input is processed (the visual cortex)  In the visual cortex, specific neurons fire only when particular phenomena are in the field of vision.  CNNs are a type of deep learning model commonly used for image and video processing tasks.  In CNNs, a neuron is activated when a it is exposed to a particular phenomena.  They are designed to automatically learn and extract meaningful features from input data through convolutional layers.  Key components of CNNs include convolutional layers, pooling layers, and fully connected layers.  Convolutional layers apply filters or kernels to input data, performing convolutions to extract local patterns and features.  