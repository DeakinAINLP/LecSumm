 This topic in the Machine Learning unit we learnt:  -  A neural network system consists of an input layer, one or more hidden layers,  and an output layer. Nodes in the hidden and output layer have sum functions  and nonlinear activation functions.  -  A simple neural network with one layer is called  perception which is used in  simple  linearly  separable  classification  applications.  The  output  magnitude  represents the confident of the results while the sign of the output represents  the  classification.  Multilayer  perception  is  used  for  applications  that  are  not  linearly separable.  -  Neural networks where data flows in one direction from input layer to output  layer are called  feedforward neural networks.  Multilayer feedforward neural  networks are called multilayer perception (MLP).  -  MLPs have two types of weights, first type is between the input layer and the  hidden layer and output layer. There are also weights between hidden layers.  The weights are tuned during  the training  phase with the aim to reduce the  difference between the network’s output and the actual values.  -  Gradient-based optimization is a way to tune the weights in which the weights  are updated with the opposite direction of the error function’s gradient.  -  Deep  learning  aims  to  mimic  the  structure  of  human  thinking  by  using  multilayered  neural  network  architectures  such  as  convolutional  networks  (CNN) and autoencoders.  -  CNN is a neural network architecture in which small kernels or operators are  spread out throughout the network’s input to extract certain information. It is  commonly in image processing.  -  Autoencoder  is  a  neural  network  architecture  which  aims  to  find  a  representation to the dataset with fewer dimensions. The trained network aims   to copy the input to the output going through the network. Inside the network,  a hidden layer includes a more compact representation of the input layer. The  network learns to compress and input data going deeper in the hidden layers  and then decompress as getting closer to the output layer.  