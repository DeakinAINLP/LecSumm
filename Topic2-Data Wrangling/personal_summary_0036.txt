 neural networks  perceptron and multilayer perceptron  deep learning  Neural Networks Motivation and inspiration  Our brain has networks of inter-connected neurons and a highly-parallel architecture. Development of Artificial Neural Networks(ANNs) is motivated by biological neural systems.  Is there a necessity for neural networks in some machine learning problems? First we need to answer the question of what is different about Neural Networks?    linear models may not be sufficient when the underlying functions or decision boundaries are extremely nonlinear support vector machines can construct nonlinear functions but use fixed feature transformations, which depends on the kernel function. For example, while you are using linear kernel, obviously your model output will be a linear model.   What if you are not sure about the kernels or perhaps you prefer to learn these features  from the data itself? Neural Networks allow the feature transformations to be learnt from data.  Neural system basics With the brain in mind, letâ€™s introduce the major players in a neural network system:   a typical neural network (machine) has an input layer;  it has one or many hidden layers; it has combiners (sum functions); it has nonlinear activation functions; it has an output layer.    Complexity  We can have more complex, bigger neural networks because neural networks are compatible with high dimensional inputs and multi-label classification. As you can see in the figure above, the diagram shows a complex neural network with 3 hidden layers and one high dimensional input layer with a 3 dimensional output layer.  Perceptron algorithm Perceptron is a linear classifier (binary) and is a single layer neural network. A multi-layer perceptron is called a neural network. In summary, a perceptron is a simple neural network used for binary classification. It has only one layer with a single node  Multilayer perceptron. Motivation for multilayer perceptron Is it always possible to separate the data points based on their class label with a single line? No its not These kinds of problems were a motivation to develop a new neural network, but this time with a layer in the middle to handle these cases. Later it was proven that a multilayer perceptron (MLP) can represent the XOR problem.  A perceptron is quite weak in what it can represent. For complex, non-linear decision surfaces, we need a multi-layer network. Non-linear functions like the logistic, (also called the sigmoid function), output a value between 0 and 1  with an s-shaped distribution. Choice of node in a multi-layer network should be continuous but it should be a continuous meaningful function such as the sigmoid function.  Feedforward neural networks A feedforward neural network is an Artificial Neural Network (ANN) where connections between units do not form a cycle. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.  A multi-layer feed-forward Neural Network (NN) is also known as a Multi-layer Perceptron (MLP). The term MLP is really an accurate name because the model comprises multiple layers of logistic regression like models (with continuous non-linearities) rather than multiple perceptrons (with discontinuous non-linearities).      Backpropagation Algorithm Backpropagation is an algorithm used in neural networks to train them and make them learn from data. It's a key part of how neural networks learn to make accurate predictions.  The algorithm works in two phases: the forward pass and the backward pass.  Forward Pass:  In the forward pass, you feed the input data through the network, and the activations (outputs) of each neuron are calculated based on the current weights and biases. The output layer produces a prediction.  Backward Pass:  During the backward pass, the algorithm compares the network's prediction with the actual label and calculates the error. It then calculates how much each weight and bias contributed to the error by propagating it backward through the network. This is where the name "backpropagation" comes from.  To update the weights and biases, the algorithm uses a technique called gradient descent. It calculates the gradient of the error with respect to each parameter and adjusts them in the opposite direction of the gradient. This process is repeated multiple times (epochs) until the network's predictions become more accurate.  The key idea behind backpropagation is that it distributes the error across the network, assigning more blame to the neurons that contributed the most to the error. By iteratively adjusting the weights and biases based on these errors, the network gradually learns to make better predictions.  Introduction to Deep Learning A deep learning model is designed to continually analyze data with a logic structure similar to how a human would draw conclusions. To achieve this, deep learning uses a layered structure of algorithms similar to ANNs.  Deep learning methods are advanced neural networks. They have been successful in learning many real world tasks (e.g. handwritten digit recognition, image recognition). Some of the common Deep learning architectures are:   Convolutional Networks  Autoencoders  Deep Belief Networks  Boltzmann Machines  Restricted Boltzmann Machines  Deep Boltzmann Machines  Deep Neural Networks  Convolutional Neural Networks The architecture of a Convolutional Neural Network (CNN or ConvNet) is modelled after the mammalian visual cortex, the part of the brain where visual input is processed.    Within the visual cortex, specific neurons fire only when particular phenomena are in the field of vision. For example, one neuron might fire only when you are looking at a left-sloping diagonal line and another only when a horizontal line is in view. Our brains process images in layers of increasing complexity. The first layer distinguishes basic attributes like lines and curves. At higher levels, the brain recognizes that a configuration of edges and colours is, for instance, a house or a bird.  LeNe5 The LeNet5 architecture was fundamental. In particular the insight that image features are distributed across the entire image and that convolutions with learnable parameters are an effective way to extract similar features at multiple locations with few parameters. The following figures illustrate the structure of LeNet5. The basic functionality of the example CNN above can be divided into 4 areas  1.  As found in other forms of ANN, the input layer will hold the pixel values of the image. In the  previous figure, you see a picture of a number 0 as an input to the CNN.  2.  The convolutional layer will determine the output of neurons which are connected to local  regions of the input through the calculation of the scalar product between their weights and the region connected to the input volume. The rectified linear unit (commonly shortened to ReLu) aims to introduce to LeNet5, an elementwise activation function such as sigmoid to the output of the activation produced by the previous layer.  3.  The pooling layer will then simply perform downsampling along the spatial dimensionality of  the given input, further reducing the number of parameters within that activation.  4.  The fully-connected layers will then perform the same duties found in standard ANNs and  attempt to produce class scores from the activations, to be used for classification. ReLu may be used between these layers to improve performance. Through this simple method of transformation, CNNs are able to transform the original input layer-by-layer using convolutional and downsampling techniques to produce class scores for classification and regression purposes.  Application of CNN In a CNN every network layer acts as a detection filter for the presence of specific features or patterns present in the original data. The first layers in a CNN detect large features that can be recognized and interpreted relatively easily. As you can see the patch which has been shown in red (figure below) are found for interpreting the right side batch of images. Using other categories of images with this filter may not give proper results. In other words, each image filter patch is for finding a specific part of a pattern or texture.  Autoencoder An Autoencoder is a neural network which can handle many hidden layers in its structure.  The aim of an Autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.  This type of neural network is trained to attempt to copy its input to its output. Internally, it has a hidden layer  that describes a code used to represent the input. Recently, the Autoencoder concept has become more widely used for learning generative models of data.    