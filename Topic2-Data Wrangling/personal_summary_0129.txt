 Following I have summarised the knowledge gained by the reading contents of this topic.  ➢  Neural Networks : Neural networks, also known as artificial neural networks or simply  neural networks, are computational models inspired by the structure and function of the biological brain. These are subfields of machine learning that are widely used to solve complex problems in various fields such as image and speech recognition, natural language processing, and decision-making tasks. A neural network consists of interconnected nodes called neurons that are organized into layers. Each neuron receives inputs, performs computations, and produces outputs that are passed to other neurons in the network. The strength of connections between neurons is determined by parameters called weights that are adjusted during the learning process. The basic building block of a neural network is a single neuron, the perceptron. Perceptrons can be connected layer by layer to form multilayer neural networks. Deep neural networks, in particular, have multiple hidden layers between the input and output layers, allowing them to learn complex expressions and hierarchical patterns.  ➢  Perceptron and Multilayer Perceptron :    A perceptron is a type of artificial neuron, or basic building block of a neural network. This is a simplified model inspired by how biological neurons work. A perceptron takes multiple inputs, applies weights to those inputs, computes a weighted sum, and passes that sum to an activation function to produce an output.  Mathematically, a perceptron can be represented as  Output = Activation_Function(Weighted_Sum + Bias)  Weighted_Sum is the sum of the inputs multiplied by their respective weights, where the bias is a constant term added to the weighted sum. The activation function introduces nonlinearity in the output of the perceptron. Perceptrons can be used for binary classification tasks, making decisions based on whether the weighted sum exceeds a certain threshold. It can also be extended to handle multiclass classification problems using techniques such as one-to-whole and one-to-one.  A multilayer perceptron (MLP), also known as a feedforward neural network, is an extension of the perceptron model. It consists of multiple layers of perceptrons, each layer connected to the next. The first layer is the input layer, the last layer is the output layer, and the layers in between are called hidden layers. In MLP, each neuron in a layer takes inputs from the previous layer, computes weighted sums, applies activation functions, and passes outputs to the next layer. The activation function introduces nonlinearity and allows the network to learn complex patterns and representations.  MLPs are powerful models that can learn nonlinear relationships in data. By adjusting weights and biases during training, MLPs can approximate complex functions and solve various machine learning tasks such as classification, regression, and pattern recognition.  Training an MLP involves forward propagation, where input data is passed through the network to produce predictions, the error or loss between predictions and true labels is computed, and updated weights and biases are applied to the gradients. Includes backpropagation used in Descent. An MLP with multiple hidden layers is called a deep neural network. Deep learning is a field focused on training deep neural networks and has made great strides in several areas such as computer vision, natural language processing, and speech recognition.  ➢  Deep Learning : Deep learning is a branch of machine learning focused on training  multilayer artificial neural networks, also known as deep neural networks. It is inspired by the structure and workings of the human brain, specifically the intricate network of interconnected neurons.  The key idea behind deep learning is learning hierarchical representations of data. A deep neural network consists of multiple hidden layers between an input layer and an output layer, allowing it to learn and extract increasingly complex features and patterns as data passes through each layer. This hierarchical representation helps capture complex relationships and abstractions within the data. Deep learning has received a great deal of attention and popularity in recent years due to its remarkable success in solving complex tasks, especially in areas such as computer vision, natural language processing, speech recognition, and reinforcement learning. Notable applications of deep learning include image classification, object recognition, machine translation, speech synthesis, and autonomous driving.  