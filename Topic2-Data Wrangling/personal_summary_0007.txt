A artificial Neural Network is inspired by the biological nervous system of inter connected neurons and highly parallel architecture. We use them as linear models are not sufficient for decision boundaries that are extremely nonlinear, support vector machines can construct nonlinear functions but use fixed feature transformations. A neural network system has a input later, one or more hidden layers, combiners (sum functions), nonlinear activation functions and an output layer. We can have extremely complex and big neural networks because they are compatible with high dimensional inputs and multi label classification. Perceptron is a linear classifier (binary) and is a single layer neural network. We deal with features that have corresponding weights alongside a bias too. The sum function calculates a value which is presented as the output. For multilayer perceptron’s we try to separate the data points based on their class label with a single line using the AND and OR logic gates. When using a XOR logic gate problem it is impossible to separate the data points in a single line. A perceptron is weak in what it can represent, as for complex nonlinear decisions we need a multilayer network. A Feedforward neural network is an ANN where connections between units don’t form a cycle, where the information only moves forward from input nodes to the output node. This is comprised of multiple layers of logistic regression like models rather than multiple perceptron’s. Gradient based optimisation methods are operated with the search directions defined by the gradient of the function at the current point. By finding the true direction towards the optimal point and the magnitude of the movement towards it, gradient decent finds the optimal point. Deep Learning A deep learning model is designed to continually analyse data with a logic structure. It uses layered structures of algorithms similar to ANN. Convolutional neural networks are modelled after the mammalian visual cortex, the part of the brain where visuals inputs are processed. In the visual cortex specific neurons only fire when particular phenomena are in view. The first layer distinguishes basic attributes like lines and curves but at the higher level our brains can put this together to recognise objects and animals and other things. CNNs are comprised of 3 concepts, sparse interactions, where sparse weights within smaller kernels instead of the whole input helps to reduce the number of parameters. Parameter sharing where a kernel uses the same set of weights while applying to a different location. And translation invariance, meaning that you can recognize an object as an object even when the appearance varies in some way. An autoencoder is a neural network which can handle many hidden layers in its structure. The aim is to learn a representation for a set of data for the purpose of dimensionality reduction. It is trained to attempt to copy its input to its output, having a hidden layer Z that describes a code to represent the input. It learns to compress data from the input layer into a short code and then uncompressed that code into something that closely matches the original data. This forces the autoencoder to engage in dimensionality reduction, which can help to ignore noise. The loss function finds the difference between the input and output to measure how good the reconstruction is. 