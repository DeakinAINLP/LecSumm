 During the tenth topic of my Machine Learning studies, we descended into the  enthralling area of Nonlinear Models, with a particular emphasis on Neural Networks and Deep Learning. These powerful computational models, inspired by the structure and function of the human brain, are at the heart of current artificial intelligence and have revolutionized areas ranging from computer vision to natural language processing.  We started the topic by covering Neural System Fundamentals. We discovered how the brain's network of neurones inspired Artificial Neural Networks (ANNs). We spoke about the structure of a single neurone, which is made up of inputs, weights, a bias term, an activation function, and an output, and how these components work together to process information.  Our next emphasis was on the Perceptron Algorithm, the most basic type of a neural network. It's a binary classifier that divides inputs into two categories, '1' or '0,' using a linear equation. The Perceptron Algorithm adjusts the weights repeatedly depending on the prediction errors, allowing the model to learn from its failures.  Following that, we looked into the Motivation for Multilayer Perceptron (MLP). We discussed the  limitations  of  single-layer  perceptrons,  specifically  their  inability  to  model  complex, nonlinear  relationships,  and  saw  how  adding  additional  layers  of  neurones  (hidden  layers) enables  the  network  to  learn  and  represent  such  relationships,  significantly  improving  its modelling capabilities.  We deconstructed the components and operation of MLPs, also known as Feedforward Neural Networks,  in  the  main  part  on  Multilayer  Perceptrons.  MLPs  have  a  layered  architecture, including input, hidden, and output layers. Each layer is completely linked to the next, and data flows in a single directionâ€”from input to output, with no loops.  Our investigation of MLP Formulation revealed how each neurone takes a weighted sum of its inputs, applies an activation function, and then transfers the result to neurones in the next layer.  MLPs  employ  Gradient-based  Optimisation  to  iteratively  alter  the  network's  weights, therefore minimising the discrepancy between expected and actual outputs.  To  round  up  the  topic,  we  looked  at  the  Backpropagation  Algorithm,  which  is  essential  in training  MLPs.  Backpropagation  efficiently  computes  the  gradient  of  the  loss  function  with respect  to  the  network  weights  for  a  single  input-output  case.  It  comprises  two  network passes: one forwards pass to calculate the output and an error, and one backwards pass to compute the gradient of the error with respect to the weights.  