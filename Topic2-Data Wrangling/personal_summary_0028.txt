Motivation and Inspiration The previously studied approaches to machine learning may be insufficent for a task. For instance, if a given problem is inherently non-linear, then a linear model may not be sufficient to solve it. Techniques to solve non-linear problems with linear techniques such as an SVM with non-linear kernels apply fixed transforms to data, which may not be appropriate. Neural networks allow feature transforms to be learned from the data, rather than fixed before the learning process. Artificial Neural Netwoks (ANN) have historically been used in attempts to study the brain, or taken inspiration from the brain to solve machine learning problems. The brain has a highly parallel architecture split across a number of tasks. About 15% of the brain capacity is used for each of low level image processing, image and action recognition, object detection and tracking, and speech recognition and pronunciation. A further 10% is applied to reinforcement learning. Each of these tasks has been reasonably demonstrated by neural networks. The remaining 30% performs other tasks such as motor control, complex reasoning, complex language and complex tool usage. Neural Networks A neural network is made up of a number of different elements: Input layer. atleast one hidden layer combiners, or sum functions non-linear activation functions output layer The Input, hidden and output layers consist of some number of dimensions. The hidden layer is called 'hidden' as its features are defined differently from the 'visible' input layer. Neural networks allow high dimensional input and multilabel classification, thus can be more complex then linear models. They are still vulnerable to overfitting with insufficient training data. Perceptron A perceptron is a single layer neural network. It is a binary linear classifier that has a single node. The goal of the perceptron is to find a hyperplane ( H(w) ) that divides the input space The preceptron receives a multiple weighted inputs and uses these to generate a single output classification by comparing their sum to a bias input term. 