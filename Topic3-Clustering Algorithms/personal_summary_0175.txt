Data Wrangling  A major role is played my probability in many of the machine learning algorithms. Examples of random experiments include tossing of a coin, rolling of a dice, daily temperature, counting the number of phone calls received on a mobile phone in a specific duration etc. Event is described as set of outcomes in a random experiment. Probability is the measure of likelihood that an event will occur.  A variable whose possible values are the generated outcome of a random phenomenon is known as a Random Variable. There are two types of random variables –    Discrete random variable: countable number of values. Defined using PMF.   Continuous random variable: can take values on a infinite continuum. Defined using PDF.  Probability distribution can be defined as a function that links each outcome of a statistical experiment with its probability of occurrence. Also learnt about Bernoulli distribution, Uniform distribution, Normal distribution. Central limit theorem states that if you have a population with mean μ and standard deviation σ and take sufficiently large random samples from the existing population, then the distribution of the sample means will be approximately normally distributed.  Data wrangling, otherwise called data munging, is the method involved with cleaning, changing, and sorting out a dataset to make it reasonable for examination. A few normal errands that are engaged with data wrangling include: Recognizing and remedying blunders and irregularities in the data; Taking care of absent or deficient qualities; Joining various datasets; Changing over the data into a configuration that is reasonable for examination; Recognizing and eliminating exceptions; Normalizing the data; Aggregating the data into helpful rundown measurements.  Data representation is a significant stage towards making models from huge scope data. Machine Learning expects data to be portrayed by 'features' called attributes or parameters before use. In computerized electronic or signal handling, the signal is considered as the amount that fluctuates over a boundary like space or time. f(x) = -ax^2+bx+c – represents signal.  Encoding techniques are used to convert categorical values to integer values. Eg- OrdinalEncoder, One-Hot Encodings, and LabelEncoder. Distribution refers to the way that the values in a dataset are distributed. This can be significant, as the distribution of the values can essentially affect the exhibition of a machine learning algorithm. There are many types of distributions that can happen in a dataset, including normal, uniform, and skewed distributions. The normal distribution is the most widely recognized, described by a bell-shaped curve symmetrical around the data's mean. A uniform distribution is one in which the values are equitably distributed across the scope of the data, while a skewed distribution is one in which the values are not equally distributed and are instead focused on one side of the reach.  In machine learning, scaling alludes to the most common way of changing a bunch of values over completely to another range of values. Normalization is a scaling procedure used to change the upsides of a dataset into a typical range. This is frequently finished to work on the presentation of machine learning algorithms, as numerous algorithms work better when the data is in a normalized range. One normal strategy for normalization is proportional the data to a range of 0 to 1, where 0 is the minimum worth in the dataset and 1 is the maximum worth. 