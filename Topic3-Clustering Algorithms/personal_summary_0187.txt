Topic 2 Data Wrangling  Random Experiment Random Experiments Definition Random experiments can be defined as experiments that can be performed many times under the same conditions and their outcome cannot be predicted with complete certainty. In order words, in a random experiment, all the possible outcomes are known, however, its exact outcome cannot be precisely predicted in advance. There are certain terms associated with random experiments that are given as follows:    Sample space: A sample space can be defined as the list of all possible outcomes of a random  experiment.    Outcome: An outcome is a possible result of the random experiment.    Event: An event is a possible outcome of an experiment and forms a subset of the sample space. Trial: When a random experiment is repeated many times each one is known as a trial.  Probability Probability can be defined as the likelihood of occurrence of an outcome of a random experiment. The formula for finding the probability is given as the number of favourable outcomes divided by the total number of possible outcomes of that random experiment. Suppose the probability of getting exactly two heads needs to be determined when a fair coin is tossed twice. The steps to find the probability are as follows:  Step 1: Determine the sample space of the random experiment or the total number of outcomes. The sample space of a coin tossed twice is given as {HH, HT, TH, TT}. Thus, the total number of outcomes are 4.  Step 2: Find the number of favourable outcomes. As the probability of getting exactly two heads needs to be determined the number of favourable outcomes is 1.  Step 3: Apply the probability formula. Thus, the probability of getting two heads is 1 / 4 or 0.25.  (Cuemath, 2023)  Joint Probability Probability can be defined jointly for more than one event.  Joint probability is the probability of two events happening together. The two events are usually designated event A and event B. In probability terminology, it can be written as:  Joint probability can also be described as the probability of the intersection of two (or more) events.    The intersection can be represented by a Venn diagram:  There’s a couple of different formulas, depending on if you have dependent events or independent events.  Independent Event  Formula for the probability of A and B (independent events): p(A and B) = p(A) * p(B).  If the probability of one event doesn’t affect the other, you have an independent event. All you do is multiply the probability of one by the probability of another.  Dependent Event  Formula for the probability of A and B (dependent events): p(A and B) = p(A) * p(B|A)  The formula is a little more complicated if your events are dependent, that is if the probability of one event effects another. In order to figure these probabilities out, you must find p(B|A), which is the conditional probability for the event.  Example question: You have 52 candidates for a committee. Four are persons aged 18 to 21. If you randomly select one person, and then (without replacing the first person’s name), randomly select a second person, what is the probability both people will be between 18 and 21 years old?  Solution: Step 1: Figure out the probability of choosing an 18 to 21 year old on the first draw. As there are 52 possibilities, and 4 are aged 18 to 21, you have a 4/52 = 1/13 chance.  Step 2: Figure out p(B|A), which is the probability of the next event (choosing a second person aged 18 to 21) given that the first event in Step 1 has already happened. There are 51 people left, and only 3 are aged 18 to 21 now, so the probability of choosing a young adult again is 3/51 = 1 / 17.  Step 3: Multiply your probabilities from Step 1(p(A)) and Step 2(p(B|A)) together: p(A) * p(B|A) = 1/13 * 1/17 = 1/221.  Your odds of choosing two people aged 18 to 21 are 1 out of 221.  (Glen, 2023)  Bayes Rule The essence of most of Bayesian approaches are to provide a mathematical rule explaining how you should change your existing beliefs in the light of new occurrence. Bayes rule describes the probability of an event A based on another event B that is related to A.  As an example: if cancer is related to age, using Bayes’ rule information about a person’s age can be used to more accurately assess the probability that the person has cancer.   Bayes’ rule is mathematically stated as:  There are four parts:    Posterior probability (updated probability after the evidence is considered)   Prior probability (the probability before the evidence is considered)  Likelihood (probability of the evidence, given the belief is true)   Marginal probability (probability of the evidence, under any circumstance)  Bayes' Rule tells you how to calculate a conditional probability with information you already have.  It is helpful to think in terms of two events – a hypothesis (which can be true or false) and evidence (which can be present or absent).  However, it can be applied to any type of events, with any number of discrete or continuous outcomes.  Bayes' Rule lets you calculate the posterior (or "updated") probability. This is a conditional probability. It is the probability of the hypothesis being true, if the evidence is present.  Think of the prior (or "previous") probability as your belief in the hypothesis before seeing the new evidence. If you had a strong belief in the hypothesis already, the prior probability will be large.  The prior is multiplied by a fraction. Think of this as the "strength" of the evidence. The posterior probability is greater when the top part (numerator) is big, and the bottom part (denominator) is small.  The numerator is the likelihood. This is another conditional probability. It is the probability of the evidence being present, given the hypothesis is true.  This is not the same as the posterior!  Remember, the "probability of the evidence being present given the hypothesis is true" is not the same as the "probability of the hypothesis being true given the evidence is present".  Now look at the denominator. This is the marginal probability of the evidence. That is, it is the probability of the evidence being present, whether the hypothesis is true or false. The smaller the denominator, the more "convincing" the evidence (Gleeson, 2021).  Worked example of Bayes' Rule Your neighbour is watching their favourite football (or soccer) team. You hear them cheering, and want to estimate the probability their team has scored.     Step 1 – write down the posterior probability of a goal, given cheering. Step 2 – estimate the prior probability of a goal as 2%        Step 3 – estimate the likelihood probability of cheering, given there's a goal as 90% (perhaps your neighbour won't celebrate if their team is losing badly) Step 4 – estimate the marginal probability of cheering – this could be because:      a goal has been scored (2% of the time, times 90% probability)  or any other reason, such as the other team missing a penalty or having a player sent off  (98% of the time, times perhaps 1% probability)  Now, piece everything together:  (Gleeson, 2021)  Random variable A random variable, is a variable whose possible values are the generated outcomes of a random phenomenon.  In other words, a random variable is a function that can assign probabilities to events of interest in a random experiment.  if we toss a coin the possible outcomes are head or tail. Let us define a random variable X so that X=1 means head and X=0 means tail.  The function is nothing but the mapping X=1 to head or X=0 to tail.  Continuous Random Variable: can take values on a infinite continuum (i.e. height of a person, time to failure)  Due to the above reason, the probability of a certain outcome for the continuous random variable is zero. However, there is always a non-negative probability that a certain outcome will lie within the interval between two values. (CFI Team, 2022) . Probability density function is a statistical expression that defines a probability distribution for a  Continuous random variables are defined using Probability Density Functions (PDF), denoted as continuous random variable. PDF assigns a probability to a range of values of the random variable as  integrating to 1.  Distributions of random variables A simple explanation of a probability distribution is that it is a function that links each outcome of a statistical experiment with its probability of occurrence.  Bernoulli distribution A Bernoulli distribution is a discrete probability distribution for a Bernoulli trial — a random experiment that has only two outcomes (usually called a “Success” or a “Failure”).  A Bernoulli trial is one of the simplest experiments you can conduct. It’s an experiment where you can have one of two possible outcomes. For example, “Yes” and “No” or “Heads” and “Tails.” A few examples:    Coin tosses: record how many coins land heads up and how many land tails up.   Births: how many boys are born and how many girls are born each day.   Rolling Dice: the probability of a roll of two die resulting in a double six.  An important part of every Bernoulli trial is that each action must be independent. That means the probabilities must remain the same throughout the trials; each event must be completely separate and have nothing to do with the previous event.    Rolling a single die is one example of a discrete uniform distribution; a die roll has four possible outcomes: 1,2,3,4,5, or 6. There is a 1/6 probability for each number being rolled.  For a continuous random variable:  One of the reasons for the popularity of the normal distribution is that many natural phenomena are approximately following a normal distribution.  Central limit theorem The central limit theorem states that if you have a population with mean  and standard deviation  and take sufficiently large random samples from the existing population, then the distribution of the sample means will be approximately normally distributed.  𝜎𝜎  𝜇𝜇  So, The distribution of the sum of N i.i.d. random variables becomes increasingly normal (Gaussian) as N grows.  Figure 4 illustrates N random variables, following central limit theorem.  Figure. Illustration of central limit theorem on � uniform random variable.  Data Wrangling The process of cleaning, transforming, and organizing a dataset to make it suitable for analysis. This often involves a combination of manual and automated processes, and it is a crucial step in the data science pipeline.  Data wrangling can be a complex and time-consuming process, as it often involves dealing with messy, unstructured, and incomplete data. Some common tasks that are involved in data wrangling include:  Identifying and correcting errors and inconsistencies in the data     Handling missing or incomplete values   Combining multiple datasets   Converting the data into a format that is suitable for analysis   Identifying and removing outliers     Normalizing the data   Aggregating the data into useful summary statistics  Overall, data wrangling is an essential step in the data science process, as it allows you to take raw, unstructured data and turn it into a form that is suitable for analysis and insights.  Images as datas How do we teach a machine to analyse and categorise images?  We would have to create a set of programming instructions, an algorithm, to process each picture and make a decision. The more correct answers we get, the more accurate the results, the more the machine has learned.  Computers understand numbers The first step would be finding features that can be represented with numbers.  In order to use an image as input data for this computer algorithm, it needs to be represented in a numerical vector of features. The computer’s instructions, its algorithms, will only be able to understand the image if you can feed the image in as a set of numbers.  Creating a model  We have to decide how to represent this image, and any other image, numerically.  We can begin by dividing the image into smaller 9×15=135 blocks.  For each block, we can compute features of our choice for example:          colour averaged across the block  shapes within the block - number of straight lines and curves etc  texture for example the amount of light/dark variation in the block  radiance or brightness  For these 135 blocks we can compute:    mean      variance  other statistics  So let’s say we extracted p features per block which leads to 135p features per image. As a result, for n images, the size of the Feature Matrix is 135p×n.  In this photo example there are 135 blocks, we have 1000 images to sort into indoor and outdoor and we have selected four ‘features’ to analyse. So the feature matrix is: 135×1000×4.  Having turned the image into a set of numbers, the resulting feature matrix can be fed into a proper computer algorithm for classification into indoor or outdoor.  Text data representation Words, images and ideas must be turned into numbers before you can feed them in to a computer for processing.  Data representation is an important step towards creating models from large scale data. Machine Learning requires data to be described by ‘features’ called attributes or parameters before use. Choosing the right features is important to creating a useful model.9  Bag of words A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modelling, such as with machine learning algorithms.  A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:  1.  A vocabulary of known words. 2.  A measure of the presence of known words.  It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.  The intuition is that documents are similar if they have similar content. Further, that from the content alone we can learn something about the meaning of the document.  Example of the Bag-of-Words Model Step 1: Collect Data  Below is a snippet of the first few lines of text from the book “A Tale of Two Cities” by Charles Dickens, taken from Project Gutenberg.     It was the best of times, it was the worst of times,     it was the age of wisdom, it was the age of foolishness,  Let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.  Step 2: Design the Vocabulary  Make a list of all of the words in our model vocabulary.  The unique words here (ignoring case and punctuation) are:       “it” “was” “the” “best”       “of” “times” “worst” “age”  Step 3: Create Document Vectors (Scoring)     “wisdom” “foolishness”  The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.  The vocabulary chosen has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.  The simplest scoring method is to mark the presence of words as a Boolean value, 0 for absent, 1 for present.  Using the arbitrary ordering of words listed above, we can step through the first document (“It was the best of times“) and convert it into a binary vector.  As a binary vector, this would look as follows:  1 “It was the best of times“ = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]  The other three documents would look as follows:  1  "it was the worst of times" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]  2  "it was the age of wisdom" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]  3  "it was the age of foolishness" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]  All ordering of the words is nominally discarded, and we have a consistent way of extracting features from any document in our corpus, ready for use in modelling.  Managing Vocabulary There are simple text cleaning techniques that can be used as a first step, such as:  Ignoring case Ignoring punctuation Ignoring frequent words, called stop words, like “a,” “of,” etc. Fixing misspelled words.        Reducing words to their stem (e.g. “play” from “playing”) using stemming algorithms.  the n-gram model  A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document.  In this approach, each word or token is called a “gram”. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modelled.  A vocabulary then tracks triplets of words is called a trigram model.  For example, the bigrams in the first line of text in the previous section: “It was the best of times” are as follows:      “it was” “was the” “the best”     “best of” “of times”  Often a simple bigram approach is better than a 1-gram bag-of-words model for tasks like documentation classification.  Scoring Methods    Binary Scoring. The presence or absence of words.   Counts. Count the number of times each word appears in a document.   Frequencies. Calculate the frequency that each word appears in a document out of all the words in the document.  Word Hashing We can use a hash representation of known words in the vocabulary. This addresses the problem of having a very large vocabulary for a large text corpus because we can choose the size of the hash space, which is in turn the size of the vector representation of the document.  Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word.  TF-IDF A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content” to the model as rarer but perhaps domain specific words.  One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like “the” that are also frequent across all documents are penalized.  This approach to scoring is called Term Frequency – Inverse Document Frequency, or TF-IDF for short, where:      Term Frequency: is a scoring of the frequency of the word in the current document.  Inverse Document Frequency: is a scoring of how rare the word is across documents.  The scores are a weighting where not all words are equally as important or interesting.  The scores have the effect of highlighting words that are distinct (contain useful information) in a given document.  Limitations of Bag-of-Words Vocabulary: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.  Sparsity: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.  Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modelled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”), and much more.  (Brownlee, 2019)  Data VS Signal What is Data? Everything is considered data when they are stored in the form of observation or measurement, which is represented as text, number, or media. However, not all of them are useful to certain companies or stakeholders. The meaningful representation of data can generate new insight, which can help a company to take certain decisions.  What is Signal? Signal is a form of data.  A signal is commonly referred to as a sign or gesture which conveys certain information.  However, in digital electronic or signal processing, the signal is considered as the quantity that varies over a parameter such as space or time.  The following equation represents the signal, where x is the independent variable, and f is the dependent variable.  f(x) = -ax^2+bx+c  Depending on the value of the shape of the signal will vary; if a>0, the signal will be a downward parabola, a=0, the signal will be a straight line, and if a<0, it will be an upward parabola.  Encoding and Distribution Encoding Some features contain categorical values which the machine cannot understand.  To solve this, encoding techniques are used to convert to integer values.  For example,  A dataset of student with features campus [“Burwood”, “Warun Ponds”] and school [”Architecture and Built Environment”, ”Engineering”, ”Information Technology”, ”Life and Environmental Sciences”] can be coded as [0,1] and [0,1,2,3].  [“Burwood”, “Information Technology”] could be expressed as [0,2]  [“Warun Ponds”, “Life and Environmental Sciences”] would be [1,3].  There are several techniques of encoding:    Ordinal Encoder   One-Hot Encodings   Label Encoder.  Distribution Distribution refers to the way that the values in a dataset are distributed.  There are many types of distributions that can occur in a dataset:    Normal distribution is the most common, characterized by a bell-shaped curve symmetrical around  the data's mean.    Uniform distribution is one in which the values are evenly distributed across the range of the data   Skewed distribution is one in which the values are not evenly distributed and are instead concentrated on one side of the range.  In general, the distribution of the values in a dataset can affect the performance of a machine learning algorithm in many ways. For example, if the values are not evenly distributed, this can cause the algorithm to be biased toward certain values, which can lead to poor performance. On the other hand, if the values are normally distributed, this can make it easier for the algorithm to learn and make predictions.  Imagine you are studying the heights in cm of a few students taking SIT720 course (168, 188, 167, 155, 170, 166, 179, 168, 167, 166, 164, 169, 170, 159, 163, 175, 173, 173, 177, 190, 150, 160, 181, 165). If we plot the heights as a histogram and plot the distribution curve, we can see it creates a bell-shaped curve.  Scaling  scaling refers to the process of converting a set of values to a new range of values.  In a dataset, we have several features. The raw or unscaled features can be of different ranges, which may cause problems while training a model.  For example, if we have two features, height in feet and weight in pounds, we may find that weight will dominate the model more than height. This is not ideal for getting optimal results. Therefore, we need to scale both features in a certain range.   Normalisation Normalization is a scaling technique used to transform the values of a dataset into a common range. This is often done to improve the performance of machine learning algorithms, as many algorithms operate better when the data is in a standardized range.  One common method of normalization is to scale the data to a range of 0 to 1, where 0 is the minimum value in the dataset and 1 is the maximum value.  