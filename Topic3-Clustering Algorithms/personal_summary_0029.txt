Summary This topic we focused on data wrangling and covered topics including probability and statistics, distribution, encoding, scaling, text and image representation.  Probability and statistics are a major component of machine learning. Probability is collected from the environment through random experiments. A random experiment is a situation where the outcome cannot be predicted with certainty – it is instead based on a probability.  The random phenomenon in these experiments generate possible values for random variables. A random variable acts a function that assigns probabilities to events of interest in a random experiment. Random variables can have either discrete or continuous values. A random experiment also features a sample space and events. The sample space is the set of all possible outcomes, and an event is a set of outcomes of a random experiment.  Probability is the percentage-based likelihood that a specific event will occur in a random experiment. Probability can be measured for a specific event, but also with joint and conditional events. Joint probability defines the probability of two separate events occurring:  For event A and event B the joint probability of both occurring is P(A and B) = P(A)*P(B).  Conditional probability is the probability that a specific event will happen after the occurrence of another one. For events A and B Pr(A|B) = P(A and B) / P(B), if and only if P(B) does not equal zero.  Additionally Bayes rule is used to find the probability of event A when event B is RELATED to event A and by adding new information. It can be demonstrated with the Monty Hall problem – where you pick one out of three doors for a chance at a prize, then one incorrect door is removed. Bayes’ rule suggests that you should change your choice as it is more likely to be the correct door.  Bayes Rule is defined as P(A|B) = P(B|A)*P(A) / P(B), where P(B) does not equal zero.  Probability is measured different for discrete and continuous random variables. Discrete random variables are countable, whole numbers. Continuous random variables are an infinite scale within a given range.  Discrete random variables use Probability Mass Functions (pi(x)) to assign a percentage-based probability to a specific outcome. To find the cumulative probability of a function the Cumulative Distribution Function is used, which sums the probabilities of all relevant results.  Continuous random variables instead use Probability Density Functions (f(x)), which is a function where the integral is 1. Probability can be calculated over a range of values, but not specific values as they are always zero in continuous sample spaces due to there being an infinite number of possible values.  The distribution of a dataset is called a probability distribution. There are some common forms of distributions, including uniform, normal, skewed, Bernoulli distributions. The most common form is the normal (also called Gaussian) distribution, which appears as a bell-curve and frequently appears naturally. Associated with this is the Central Limit Theorem, which states that for a sufficiently large sample sizes of random experiments the means will tend towards a normal distribution, even if the outcomes of each experiment was not normally distributed.  Uniform distribution is where each outcome value is roughly evenly distributed. Example includes dice rolls, where each side has the same chance of appearing. Skewed data is compacted to one side. Bernoulli distribution is defined for binary outcomes, such a coin flips.  Distribution can affect the performance of a machine learning algorithm either positively or negatively. Skewed data can introduce bias and cause poor results, while normalized distributions can improve predictions and simplify calculations.  Data wrangling is all about converting raw data into analysable data. The process includes data loading / saving, data exploration, encoding, distribution and scaling. This involves tasks such as finding and cleaning errors, filling or removing missing values, handling outliers, combining datasets or aggregating the data, normalizing and transforming the data, etcetera. A common issue with datasets is missing or null values. These missing entries cannot be processed by computers so need to be filled in a way that doesn’t introduce bias to the dataset. There’s different ways to fill these spaces. Methods including using a median or mean value for empty values in a feature, carrying over a previous value from that feature, etcetera.  Computers only understand numbers so working with text or image data requires feature extraction or data representation. This process involves breaking datasets into features in vector form for analysis. Images can be broken into feature vectors by breaking the image into blocks, selecting desired features such as colour average, brightness, texturing, and computing them for each block. For text analysis, documents can be separated into feature vectors based on words and the number of times they appear in a given document.  An important component of data wrangling is data encoding. Some features use categorical data that cannot be interpreted by machines, so it needs to be ENCODED into a numerical format. There are different ways to do this, such as OrdinalEncoder, One-Hot Encodings, and LabelEncoder.  Scaling is where a range of values is fitted to a new range of values. In datasets different features may require scaling to avoid some features from dominating the model. Normalisation is a dataset scaling technique that brings all features into a standardized range. Some methods include min-max scaling, where the values of each feature are converted to a decimal percentage of their relevant range. Scaling can also help with data visualization, where applicable.  