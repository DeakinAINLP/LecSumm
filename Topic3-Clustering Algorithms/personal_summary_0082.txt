In this topic we have revised the concepts of linear algebra and learnt about data wrangling.  In this topic, we have mainly covered topics such as Feature Vectors and Matrices,  Probability and its types and different types of distributions.  1.  Feature Vectors  Feature vectors are a way of representing data points in a mathematical or  numerical format. In a feature vector, each dimension represents a particular feature  or attribute of the data points, and the value of each dimension represents the  magnitude or presence of that feature.  The general purpose of creating a feature vector is to make the data into numerical  format so that it can be easily analyzed and help algorithms find patterns and make  the predictions based on the patterns found in the data-points.  2.  Feature Matrices  Feature Matrices are collection of feature vectors arranged in matrix format, typically  used in Machine Learning and Deep Learning applications. Each row in matrix  represents an individual data points and each column represents feature or attribute  of that data point.  Feature matrices are used as input data points for ML based algorithms which helps  them to learn patterns in data and make predictions or classification based on those  patterns.  Overall, it provides a convenient way and efficient way to represent complex data  sets in a format that can be easily analyzed and processed by machine learning  algorithms.  3.  Probability  Probability plays a major role in machine learning. Some of the key aspects of  probability in machine learning are as follows:  a.  Probability Distribution  It is used to model uncertainty in data, and they form the  foundation of many machine learning models. Commonly used  distributions used in machine learning are Gaussian/Normal  distribution, Bernoulli distribution, Poisson distribution and  multinomial distribution.  b.  Bayesian Inference  It is a powerful statistical framework for reasoning under  uncertainty. It uses prior knowledge and data present to update  beliefs about underlying probability distribution of the data.  Bayesians methods are widely used in machine learning task such  as classification, clustering and regression.  c.  Markov Models  Markov models are class of probabilistic models that are widely  used in machine learning models such as speech recognition,  natural language processing and image processing.  d.  Reinforcement Learning  We can say that without probability reinforcement learning is not  possible to achieve as reinforcement learning involves learning through trial-and-error bases with an environment. It uses  probability distribution to select actions based on the current state  of the environment.  e.  Generative Models  Generative models are used to generate new data points  that are  close to the given dataset. They rely high on probability  distribution to model the underlying structure of the data based on  which images and text can be generated.  4.  Bayes Rules  The main application of Bayes rule in machine learning is Bayesian  inference, which involves updating the probability of a hypothesis or  model giving some data or evidence. By applying Bayes rule we can  compute the probability of a hypothesis given the observed data, which  is proportional to the likelihood of the data given by the hypothesis  multiplied by the prior probability of the hypothesis. This allows us to  incorporate prior knowledge or beliefs about the hypothesis and update  them based on observed data.  Overall, Bayes rule is a powerful tool in machine learning that allows us  to model and reason about uncertainty, incorporate prior knowledge and  make optimal decision under uncertainty.  5.  Distribution of random variables  a.  Bernoulli Distribution – It is a distribution which is discrete and defined for a  binary variable with values X = 0(fail)| X = 1(pass).  b.  Uniform Distribution – It can be defined for both discrete as well as  continuous random variables.  c.  Normal Distribution – It is used to define continuous random variable and it is  by far the most popular distribution.  d.  Central Limit Theorem – Central Limit Theorem mainly relies on the concepts of a sampling distribution, which is the probability distribution of a statistic for a large number of samples taken from the population.  Data Wrangling  Data Wrangling also known as data munging, refers to the process of cleaning,  transforming, and organizing raw data from various places into a format suitable for  analysis or machine learning models.  Data Wrangling helps to improve the accuracy and quality of the model as models’  performance highly depends on quality of the data.  There are several ways in which data wrangling is used in machine learning.  a. Data Cleaning  b. Data Integration  c. Data Transformation  d. Feature Engineering  Overall, data wrangling is a crucial step in machine learning pipeline, as helps to make sure  the data is accurate, complete and meaningful to the problem we are solving. Data  wrangling makes the model more accurate and reliable solutions.  