Statistics: basic definitions Here is a summary of the key points:   Probability is important in machine learning algorithms.   A random experiment is an experiment or process with unpredictable outcomes.   An event is a set of outcomes from a random experiment.   Probability measures the likelihood of an event occurring, ranging from 0 to 1.   Joint probability refers to the probability of multiple events occurring together.   Conditional probability is the probability of one event occurring given the occurrence of another event.   Bayes' rule is a mathematical formula used to update beliefs or assess the probability of an event based on related information. Random variables Here is a summary of the key points:   A random variable is a variable that takes on the possible outcomes of a random event.   It is a function that assigns probabilities to events of interest in a random experiment.   Discrete random variables have a countable number of values, such as the faces of a dice or the number of emails received.   They are defined using Probability Mass Functions (PMF), which assign probabilities to each possible value of the random variable.   Cumulative Distribution Function (CDF) gives the cumulative probability associated with a discrete random variable.   Continuous random variables can take values on an infinite continuum, such as height or time to failure.   They are defined using Probability Density Functions (PDF), which assign probabilities to ranges of values.   PDF integrates to 1 over the entire range of possible values.   Probability assigned to any exact value in continuous space is zero, but probabilities can be discussed over ranges of values. Distributions of random variables Here is a summary of the key points:   A probability distribution is a function that connects the outcomes of a statistical experiment with their probabilities of occurrence.   The Bernoulli distribution is a discrete distribution for a binary random variable with values 0 and 1.   The uniform distribution can be defined for both discrete and continuous random variables,where all outcomes have equal probabilities.   The normal distribution is a continuous distribution and is the most popular distribution. It is defined by its mean and standard deviation.   The normal distribution is widely used because many natural phenomena approximately follow this distribution.   The central limit theorem states that the distribution of sample means approaches a normal distribution when random samples are taken from a population. Data Wrangling Data wrangling, also known as data munging, is the process of cleaning, transforming, and organizing a dataset to make it suitable for analysis. Data wrangling can be a complex and time-consuming process, as it often involves dealing with messy, unstructured, and incomplete data. Some common tasks that are involved in data wrangling include: Identifying and correcting errors and inconsistencies in the data.   Handling missing or incomplete values.   Combining multiple datasets.   Converting the data into a format that is suitable for analysis.   Normalizing the data.   Aggregating the data into useful summary statistics.   Identifying and removing outliers.   Feature extraction Here is a summary of the key points:   Feature extraction is an essential step in machine learning, where meaningful information is derived from raw data to build models. In order to analyse and categorize images, feature extraction techniques are used.   The example given is identifying whether an image is indoors or outdoors.   Computers understand numbers, so images need to be represented as numerical vectors of features for algorithms to process them.   Images can be divided into smaller blocks, and features such as colour, shapes, texture, and radiance can be computed for each block.   The resulting feature matrix represents the extracted features for each image.   The feature matrix can then be used as input for a classification algorithm to categorize the images into indoor or outdoor. Data VS signal Here is a summary of the key points:   Data refers to the information shared or stored in the form of observations or measurements, represented as text, numbers, or media. It includes user information, purchase history, and other relevant data used by companies for decision-making.   Not all data is useful, and meaningful representation of data can generate valuable insights.   Signals, in the context of digital electronics or signal processing, refer to quantities that vary over a parameter such as space or time.   Signals can be electrical representations of various phenomena, such as audio signals or heartbeat signals.   Signals are typically represented mathematically, such as through equations, and can have different shapes depending on their parameters.   Signals can be considered as another form of data, and they can provide information about specific phenomena when interpreted or analysed.   An example is an electrocardiogram (ECG) signal, which represents the electrical impulses of the heart and can be analysed by physicians to understand heart issues. Encoding Encoding techniques are used to convert categorical values into numerical values, allowing machine learning algorithms to process them effectively. It involves assigning unique numerical representations to different categories, enabling the algorithms to work with the data. Distribution Distribution refers to the way values are spread across a dataset. Different types of distributions exist, such as normal, uniform, and skewed distributions. Understanding the distribution of data is important as it can impact the performance of machine learning algorithms and inform appropriate analysis techniques. Scaling Scaling involves transforming a set of values into a new range to ensure that they are on a similar scale. This is done to prevent certain features from dominating the model due to their larger values, which can lead to biased results. Scaling helps in achieving better balance and optimal performance of machine learning models. Normalization Normalization is a specific type of scaling technique used to transform the values of a dataset into a common range. It typically scales the data to a range of 0 to 1, where 0 represents the minimum value in the dataset, and 1 represents the maximum value. Normalization is commonly applied to improve the performance of machine learning algorithms, as many algorithms operate more effectively when the data is standardized. 