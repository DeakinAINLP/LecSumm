Mathematics  Various mathematical concepts (vectors, matrices & regression etc.) are used in the ML process, driving every decision, iteration and output. A solid understanding of mathematical concepts such as these helps improve ML outcomes.  Regression – a specific mathematical criterion is employed (algorithm) to predict an outcome between dependant and independent variables  Vectors – provide a relationship between magnitude and direction of an object  Matrix operations – provide a means of managing related sets of various object data (via columns/rows) through addition, subtraction and multiplication  Statistics  Probability – is associated with machine learning algorithms, where the likelihood of an event outcome taking place is quantified as a number between 0 and 1.  It involves concepts such as;  Joint probability – result of multiple independent events     Conditional probability – result of an event given the result of another event   Bayes probability rule - result improvement based on adjusting input as new data emerges  Random variables – are the result of a measurable chance event. There are two types;    Discrete – have a countable number of whole values, whose probability follows a  “cumulative distribution function”, when summed is 1    Continuous – can take a non-whole value on an infinite continuum, whose probability is  based on a “probability density function”, which when integrated is 1  Distribution of Random Variables  The probability distribution of a mathematical function gives the probabilities of occurrence of different possible outcomes. Some important distributions include;    Bernoulli distribution – describes the discrete (countable) probability distribution of a  random variable with an expected Boolean outcome (1 or 0)    Uniform distribution – describes both discrete and continuous symmetric probability  distribution of a an outcome within certain bounds, whose min or max interval can vary   Normal distribution – describes continuous probability distribution of a random variable and  are typically “bell shaped” with a symmetric point of inflection    Central Limit Theorem – is where an approximated normal distribution of the average of a random variable is observed where a sufficiently large number of input variable sets are used    Data Wrangling  Data wrangling is a process through which unstructured and/or incomplete data is turned into a more usable form suitable for analysis. For example, a machine learning model cannot process null values, so data replacement with an immediate, calculated mean or median value can take place.  Also, part of this process is the removal of errors, data conversion and removal of outliers. This helps normalise the data, which then permits the joining or comparison of data sets.  Understanding Data  Images  Defining an object or basic image involves the use of labels. The labels are usually represented as “a vector of features” (images converted into numbers) which are then used in an algorithmic model. The model is then trained to recognise and understand unknown images through the use of these recognised labels.  Two dimensional image features can include such things as textures, colour and brightness or shapes. Once converted into numbers, data is placed into a matrix for algorithmic processing.  Three dimensional images however (such as facial recognition) is a more challenging to process for a machine learning model to analyse. Changes in lighting or changes to face size and shape through expression may require extra steps to help identify specific facial landmarks or features.  The choice of features used for labelling above may lead to limitations on a models effectiveness.  Signals  Signals are another form of data (which can include images) and conveys information. Signal processing is a technique used to extract feature information from this data over a period of time to make it more manageable for algorithms.  Encoding  Is a method through which qualitative features (as opposed to quantitative data) are converted into data which is more ready used by an algorithm. For example, this is where a worded feature is converted to a number.  Distribution  As noted earlier, data can be distributed across a dataset in a number of ways:    Normal distribution – most common “bell shaped” or “symmetrical”   Skewed distribution – uneven distribution favouring one side of a range   Uniform distribution – even distribution across the range  The way data is distributed can impact upon an algorithms ability to perform – for example a skewed distribution can introduce bias towards a given output.  Scaling & Normalisation  If values from one particular variable (feature) is significantly different in value within a dataset, there is a risk of one feature overwhelming that of the other – hence the use of data scaling. The scaling of both features into a common range is known as normalisation. Normalisation essentially standardises the data set and is typically between a range of 0 and 1.  Random variables, Probability Mass Function and Cumulative Distribution Function  Random variables can be discrete (countable) or continuous (values on an infinite continuum).  A probability mass function (PMF) assigns a probability to each possible value of a random variable.  The cumulative distribution function (CDF) is responsible for calculating the cumulative probability of a random variable appearing and to what range it may take. Its calculated value will be ≤ x.       