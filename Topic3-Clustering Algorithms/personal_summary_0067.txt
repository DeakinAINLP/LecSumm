 Topic 2 is focused on Mathematics and Machine Learning, with ML being based on maths, it underlies every decision, iteration, and output. To get the optimal results from ML you need to understand the mathematical concepts and processes that are the foundations of your results.  The key areas we are exploring this topic are:    Vectors and basic vector operations,   Matrices and matrix operations   Basic concept of probability   Python Programming: using various modules and packages.  Statistics  In the first part of the module we look at basic definitions:  A random experiment – an experiment that we can not predict the outcome with certainty such as tossing a coin.  Event – the set of outcomes of a random experiment.  Probability – This is defined for an event and is the measure of the likelihood of an event occurring.  Joint Probability – we can combine probability for more than one event, such as tossing two coins.  Conditional probability – It’s the probability of event A given the occurrence of even B  Bayes Rule – its about how you should change your existing beliefs based on new occurrences, so the probability of event A based on event B that is related to event A.  Random Variables  A random variable is a variable whose possible values are the generated outcomes of a random phenomenon.  This means that a random variable is a function that can assign probabilities to events of interest in a random experiment.     Note that its not like algebra, in that the value os X is not unknown, x has a whole set of values and it could take any one of those values. You can see a similar concept in Quantum computing when calculating the value of a Qbit.  Note ** - we use capital letters to represent X or Y with random variables to reduce confusion between algebra and probability.  Distributions of Random Variables  Random variables are variables whose values are determined by chance. They can be classified into different types of distributions, each with a unique pattern of values.  For example, the normal distribution is a bell-shaped curve that is symmetrical around its mean, while the binomial distribution describes the probability of a certain number of successes in a fixed number of trials. Understanding the distribution of a random variable is important in data analysis because it helps us to make predictions and draw conclusions.      Data Wrangling  Data wrangling is the process of cleaning, transforming, and preparing raw data for analysis. This includes removing missing values, handling outliers, and merging datasets. The goal of data wrangling is to create a clean and structured dataset that can be easily analysed. Some key tasks in data wrangling are:  Identifying and correcting errors and inconsistencies in the data     Handling missing or incomplete values   Combining multiple datasets   Converting the data into a format that is suitable for analysis    Normalizing the data   Aggregating the data into useful summary statistics  Identifying and removing outliers  Images as Data  Images can be represented as arrays of numbers and can be analysed using various techniques such as image segmentation, object detection, and image classification. Each pixel in an image is represented by a number that corresponds to its colour or intensity. By analysing the values of these pixels, we can extract useful information from the image.  Text Data Representation  Text data can be represented in various formats such as ASCII, Unicode, and UTF-8. Text data can also be represented using vector space models or as graphs. In vector space models, each word is represented as a vector in a high-dimensional space, while in graph-based models, words are represented as nodes in a graph, and the edges between the nodes represent the relationships between the words.  Feature Extraction  Feature extraction involves selecting and extracting relevant features from a dataset for analysis. This is often done to reduce the dimensionality of the data and improve the accuracy of machine learning models. Features can be extracted using various techniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).  Data VS Signal  Data refers to any information that can be stored and analysed, while a signal refers to a time- varying quantity that carries information. Signals can be analysed using signal processing techniques, which involve manipulating and analysing the properties of the signal such as frequency, amplitude, and phase.  Encoding and Distribution  Encoding refers to the process of converting data into a specific format or representation that can be used for analysis or storage. Some common encoding techniques include:    One-hot encoding: Representing categorical data as binary vectors. Label encoding: Assigning a numerical label to categorical data.    Binary encoding: Representing numerical data as binary vectors.  Once data has been encoded, it can be distributed in various ways, including:     Centralized distribution: Storing data in a central location, such as a database or data  warehouse.    Decentralized distribution: Storing data across multiple locations, such as in a peer-to-peer  network or in a distributed database.  Scaling and Normalisation  Scaling and normalization are techniques used to transform data so that it has a consistent scale and is easier to compare or analyse. Scaling involves converting data into a new range, such as between 0 and 1, while normalization involves converting data so that it has a mean of 0 and a standard deviation of 1.  Data Wrangling with Python:  There are several packages we can use for data wrangling in Python, Pandas, NumPy and Matplotlib are some of the main ones.  Statistics in Data Science  Statistics is a branch of mathematics that deals with the collection, analysis, and interpretation of data. Statistics plays a crucial role in data science, as it provides the foundation for many machine learning algorithms and statistical models. Some common statistical techniques used in data science include:    Descriptive statistics: Summarizing and describing the main features of a dataset. Inferential statistics: Drawing conclusions about a population based on a sample.    Hypothesis testing: Testing a hypothesis about a population using statistical methods.  Data loading and saving:  Data can be saved and loaded in a variety of formats including things like CSV and JSON.  Data Exploration  Data exploration involves examining a dataset to understand its characteristics and identify patterns or relationships. Data exploration techniques include:    Summary statistics: Describing the main features of a dataset, such as the mean and standard deviation.    Data visualization: Creating plots or charts to visualize the data and identify patterns.   Correlation analysis  