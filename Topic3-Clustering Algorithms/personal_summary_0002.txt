topic 2 covered topic 2's was data wrangling centred. Data wrangling can be defined as the process of cleaning and transforming often raw data generally with the intent to analyse. the steps involved can be condensed to discovery, structuring, cleaning, enriching, validating, and publishing. The other key topic was statistics (i guess you could say data wrangling is part of statistics), specifically basic definitions.. such as distribution measures ie mean, median, mode.. and dispersion measures such as variance, and std. Another part of statistics discussed were random variables, which can be explained using a coin, ie flipping a coin multiple times produces a random variable. We then looked at distributions of random variables as a function, which tries to describe the probability that this variable taking different values. Next was covering images as data, specifically how an image can be represented (for machine interpretation for ML . The basis of this is how digital images are comprised of pixels, which can be interpreted as cells in a vector (and often is). This then enables the development of object detection and classification in the realm of ML. Another topic focused on representing some data in a manner enabling machine interpretation was for text. A common method is the bag of words model, which assigns every word in a set of words making up some text a tag, which can exist in a multi-dimensional space. By graphing frequencies of words in this vector space, you can produce a primitive representation of the piece of text. Another approach is word-embedding, where a words meaning is actually taken into consideration, and distance between similar words (by meaning) in the vector space is closer. Next was feature extraction, which can basically be simplified into the act of filtering data that isn't thought as necessary. This enables reduction in the complexity of data which can often making analysis easier. Data vs signals was the next topic, a fairly low level concept, ie sound waves have to be converted (nearly always from analog.. cant think of an exception) to digital data for digital computers to interpret.. using codecs etc. Vice versa to hear the digitalised audio.Encoding and distribution was covered next, basically refers to representing data in a uniform structure that can be created and de-structured using pre-established algorithms. Scaling and normalisation refers to adjusting the range of data and adjusting the distribution of data. This can be useful for analysis between different datasets with different ranges/distributions rendering comparison incompatible by default. 