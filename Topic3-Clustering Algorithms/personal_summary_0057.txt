 In this module we learned about data and pre-processing of data. We also had a brief overview of probability and statistics. In probability we studied about terminology such as a random experiment, it is an experiment pr a process for the outcome cannot be predicted with certainty. An event is defined as a set of outcomes of a random experiment. Probability is defined for an event, and it is the likelihood of that event occurring. Joint probability is when we do two random experiments. Conditional probability is when some event A, gives the occurrence of another event B. In Further reading, we were introduced to few theorems and formulae such as Bayes rule and Bernoulli distribution. Then, we had a look at random variable, continuous random variable, and discrete random variable.  Then we had a look at data, everything is considered data when it is stored in the form of observation or measurement which is represented as text, number, or media.  Text data representation is what I described above; it is a crucial step in creating models from large scale data, machine learning requires data to be described by features called attributes or parameters before use. Then, we had a look at encoding, encoding is representing categorical data into integer values. Techniques used for encoding include ordinal encoder, hot encoding, and label encoding.  Distribution, it refers to the way that the values in a data set is distributed, it can have impact on the performance of machine learning algorithms. There are three types of distribution, normal distribution, uniform distribution, and skew distribution.       Scaling, it refers to the process of converting a set of values to a new set of range of values. Normalisation is a scaling technique used to transform the values of a dataset into a common range. Machine learning algorithms operate better when the data is in a standard range.  After learning these fundamental concepts of data wrangling, we were able to work with python data and test our knowledge.        