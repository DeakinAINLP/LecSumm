 ● Statistical analysis plays an essential role in any ML project. That is why it is necessary  to understand the basics of statistics and related concepts as a part of Machine Learning.  ● The statistics concepts covered in topic 2 consist of Simple Probability, Conditional Probability, Bayes Rule, Discrete and Continuous random variables and the different types of distribution such as Bernoulli distribution, Uniform distribution, and Normal distribution along with central limit theorem.  ● The other important aspect of an ML-based project is data analysis and the first step for data analysis is data wrangling, also known as data munging. This step prepares the data for further processing.  ● The data that is received for analysis is seldom in the format that we need. It needs to be rearranged/reorganised in a manner that is suitable for punching in the numbers and getting the expected results.  ● Some of the most common processes done in data wrangling are Identifying and  correcting errors in the data, handling missing data, combining multiple datasets, dealing with outliers, normalising the data and aggregating the data for use.  ● Data can arrive in multiple formats, including images. But it is not possible to feed the  image as it is to a machine and expect it to work. We need to convert these images into a language that the machines understand and what machines understand are numbers. So we need to translate each image into a set of numbers that represent the real meaning of the image.  ● One way to preprocess an image is to chop it up into tiny blocks of squares, and then learn the different values for each block such as its color, shapes in the block, light texture and brightness. We can then compute the mean, variance and other statistics for each of them.  ● Data encoding is another way to process data that is not numerical in nature. Encoding  takes care of categorical data such as types of food at a grocery store. These types have no numerical significance hence they cannot be quantified but we can still represent each category with an arbitrary number and feed it to a model. This is known as label encoding but there are other techniques such as Ordinal Encoding, One-Hot Encoding, etc.  ● When data is collected on large scales, we start observing patterns in how this data and its values are distributed. The distribution of data can have a significant impact on how the ML model performs in the real world. If the data is skewed then we risk the model  being biased and lean towards a certain outcome. A normal distribution is preferred as it caters for all kinds of outcomes.  ● Similar to distribution, if the different features in a data are represented on different  scales than each other, the ML model may not be able to learn it properly and turn out to be biased. To solve this, we can scale the data a the same range and make it easier to process and have unbiased results. Normalization is a common scaling technique where the numerical data is scaled down to a range between 0 and 1. This is known as min-max normalization.  ● Data wrangling and statistical analysis is usually performed in Python using numerous  libraries like NumPy, SciPy and Matplotlib.  