Summarise the main points that is covered in this topic.  1.  Basic concepts of probability:    Random experiment, event, probability, joint probability, conditional  probability, and Bayes' rule.    Probability measures likelihood of an event occurring, between 0 and 1. Joint probability defined for multiple events, conditional probability  given occurrence of another event.    Bayes' rule describes probability of an event based on a related event.  2.  Random variables:    Possible values generated outcomes of a random phenomenon.   Two types: discrete and continuous   Defined using probability mass and density functions, respectively.   Probability and cumulative distribution functions for each type  explained.  3.  Probability distributions:    Link statistical experiment outcomes with probability of occurrence.   Bernoulli, uniform, and normal distributions introduced.   Central limit theorem states sum of i.i.d. random variables becomes  normal as sample size grows.    Many natural phenomena follow normal distribution.  4.  Data Wrangling:    Crucial step in data science pipeline.   Cleaning, transforming, and organizing datasets for analysis.   Identifying errors, handling missing values, combining datasets, and normalizing and aggregating data.  5.  Feature extraction in machine learning:    Deriving features from raw data to enable categorization.   Dividing images into blocks and computing features such as colour,  shapes, and texture.    Feature matrix fed into computer algorithm for classification.  6.  Data representation:    Non-numerical data turned into numbers for processing.   Feature extraction important for useful models.   Watching video recommended for introducing feature vectors.  7.  Data vs. signal:    Data is information shared by users, stored as observation or  measurement.    Signal varies over a parameter such as space or time, like audio or  heartbeat signals.  8.  Encoding and distribution:    Encoding converts categorical to numerical values.   Distribution refers to how values are distributed in dataset.   Different encoding techniques and types of distributions explained.   Normal distribution example given using heights of students.  9.  Scaling and normalization techniques in machine learning:    Scaling converts set of values to new range of values.   Normalization transforms values into common range for improved performance. Min-max normalization scales data to range of 0 to 1.  10. Random variables in machine learning:    Generating random matrices and discrete random variables using  Python libraries like NumPy and SciPy.    Explaining Cumulative Distribution Function (CDF) and plotting  example using SciPy library.  11. Data wrangling steps with examples    Acquiring, analyzing, and manipulating raw data for faster processing  and evaluation.    Data loading and saving, exploration, processing, distribution, scaling,  and working with random data in Pandas dataframes.    Missing values handled by replacing with previous data point.  Categorical values encoded using LabelEncoder.    Data distribution visualized using histograms. Data scaling performed  to adjust mean and standard deviation of each feature.  