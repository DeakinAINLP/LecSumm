This topic’s content was an introduction and fairly deep dive into Data Wrangling and what it is made up of. Working in any Data Science field brings with it the need to manage the processing of, and information extrapolation from, large quantities of data (it’s called Data Science for a reason ;) ). This management of data is often referred to as “Data Wrangling”, “wrangling” being a mainly American word referring to the rounding up, herding of, or otherwise taking charge of livestock. In this case, our “livestock” is the copious amounts of data we need to deal with. Before we got into wrangling some wild integer-horses, we first need to take a look at some math. Data Science and Machine Learning rely on math and as such a good understanding is important for progression in the subject. Statistics Probability This quick math recap started with a look at statistics, and specifically probability. Probability is a very complex section of math and is an entire course in itself but having a look at some basic definitions, we learn that probability is used to express the likelihood of a specific outcome for an event. For example, an event could be throwing a dice and the outcome being what number it lands on. This is the most basic form of probability, where we are just expressing the likelihood of an outcome from an unbiased event. Probability is usually express as a fraction. For example, the likelihood of getting a 3 from a dice roll is 1/6 given that there is only 1 face for 3 on the dice and there are 6 faces. If there were 2 faces with 3, the probability would be 2/6. Joint probability is used to jointly define the probability for more than one event. For example, continuing with the dice example, say we want to determine the probability of landing two “3”s in a row. Since we always have probability of 1/6 to land on 3, we can simply multiple these two probabilities together since it is just as likely in each occurrence. Thus, 1/6 * 1/6 = 1/36. We also have conditional probability, this is used to describe the probability of an event occurring given the occurrence of a separate event. Lastly in this section, is Bayes Rule. This rule is used to describe the probability of an event based on another event that is related to the first one Random Variables The next part of statistics brings us to random variables. These are variables where the values are generated outcomes of a random event. There are two types of random variables that we mostly deal with: -  Discrete random variables which have a countable number of values. These are things such as the faces of a dice or number of phone calls in a day. -  Continuous random variables are values that have an infinite continuum such as height of a person or time it takes an object to break. Discrete Random Variables These are defined using a Probability Mass Function. The PMF will assign a probability value to each possible random variable (outcome of an event) so that they all sum to 1. Maintaining probability as a fraction. Continuous Random Variables These are defined using a Probability Density Function which unlike the PMF, assigns a probability to a range of values that the variable may fall in with all values integrating to 1. Distributions of Random Variables Probability distributions are seen as functions that link each possible outcome of an experiment with its probability of occurrence. There are a few main distributions we work with. Bernoulli Distribution Bernoulli distribution is a form of distribution that can only be applied to defined binary random variables which can have a value of 0 or 1. An example of this would be flipping a coin for heads or tails. Uniform Distribution Uniform distribution can be defined for either discrete or continuous random variables with the formula changing slightly between the two. Uniform distribution is used in a fair experiment where each outcome is just as likely as the others to occur. An example would be rolling a fair dice. Normal Distribution Normal distribution is defined only for continuous random variables and is the most popular and commonly seen distributions. The main reason for this is that there are many natural phenomena which roughly follow a normal distribution. Data Wrangling As roughly mentioned at the beginning, data wrangling is the process of taking a dataset and cleaning, transforming and organizing it to make it suitable for analysis. Data wrangling can often be complex and time-consuming as datasets are often messy, unstructured and incomplete. Data wrangling is actually a set of tasks, some commons ones are: Identifying and correcting data errors or inconsistencies - Identifying and removing outliers -  Dealing with missing or incomplete values -  Combining datasets -  Converting data into a suitable format for analysis - Feature Extraction -  Normalizing the data -  Aggregating the data into useful summaries Feature Extraction is a very important part of Machine Learning. FE refers to finding useful information within a set of data. The way in which this is done varies but an important consideration is that the data needs to be learnable by a computer. Since computers understand numbers, extracting data from an image or from word based data can be very difficult. To account for this, we can manipulate the data using a model to make a computer interpret it. For example, an image could be split into a grid of smaller parts where metrics like color average and brightness could be calculated in order for a computer to extract features from it. Text Data Representation Humans cannot interpret and analyze large volumes of text data without the use of specialist tools, often software based ones. However, as previously mentioned, computers can’t understand words. As such, there are a few different ways to extrapolate information from large volumes of text. All of these revolve around describing data to machine learning models before use so that it knows what are considered “features” and can thus work around this. One example is a Bag-of-word representation, in this representation, the ML system is given a list of important words and several different assortments of text. It can then go through these and find how many times each word occurs in the different texts. These are then stored in a table. This table can then be used to perform statistical analysis. Encoding and Distribution Encoding Machine Learning is not capable by default, of understanding categorical values. To overcome this, we make use of encoding techniques to convert these categorical values to integer values. For example, if we had School A and School B and each taught Math, Science and English. We could encode these to [0,1] and [0,1,2] and can therefore express these categorical values in integer form. For example, if we wanted to reference School A’s science program we could say [0,1]. This is just a basic example of one of the many encoding techniques out there. Distribution Distribution in this instance refers to the way in which values in a dataset are distributed. This can at times be very important, as the distribution can have a significant impact on the performance of the ML algorithm. The most common types of distribution are normal, uniform and skewed distributions: -  Normal distribution is the most common and takes the visual form of a bell-shaped curve that is symmetrical around the dataset’s average. -  Uniform distribution refers to when values are evenly distributed across the entire range of data. - Skewed distribution is when data is concentrated to one side of the range. ML algorithms can be negatively affected by skewed data specifically as it may become biased towards certain values which can produce undesirable results. Uniform data can also create challenges as the ML algorithm may struggle to learn what the desirable data is. Scaling and Normalisation Scaling Scaling in ML terms refers to the process where a set of values is converted to a new range of values. In a dataset with several features, there is a good chance that these features will be in different number ranges. For example, if we had height in meters and weight in grams, the ranges would be drastically different and will thus make it challenging for ML algorithms to work with both ranges. A solution to this is to use a scaling technique such as Normalisation. Normalisation Normalisation is a scaling technique used to transform values of features in a dataset to a common range. This helps improve ML algorithm performance. The most common form of Normalisation is to scale all data to fit into the range of 0 to 1. Thus data becomes 0.34234 and 0.5235346 for example. The formula for this is based on the minimum and maximum values of the raw data. Remainder of the Content Following the theory content, this topic introduced several different Python techniques to implement the theory we learned. 