In this topic we learnt basic definition of statistics Random experiment: This term describes an experiment or procedure where results cannot be known with confidence. Eg: Coin toss, dice roll, number of phone calls, daily temperature, etc. Event: An event is a collection of results from a random experiment. A single outcome or a collection of outcomes are both possible. Probability: Probability is a way to measure how likely something is to happen. A number in the range of 0 and 1 is used to symbolise it. P(A) stands for probability of an event occurring, while P(not A) stands for likelihood of an event not occurring. Joint Probability: For multiple events, joint probability is defined. When the two events A and B  are  independent,  their  respective  probabilities  add  together  to  form  the  joint  probability, which is expressed as P(A and B) = P(A) * P(B). Conditional Probability: Conditional probability is the likelihood that an event A will occur in the event of an existing event B. It is written as P(A|B) and is computed as P(A and B) / P(B), where P(A and B) is the probability of both A and B occurring. Bayes' Rule is a mathematical formula that enables us to revise our opinions or estimate the probabilities  of  an  event  in  light  of  fresh  data  or  evidence.  When  an  event  B  occurs,  it determines the possibility that an event A will also occur. Discrete random variables are characterized by a finite number of defined values, which are determined by the Probability Mass Function (PMF). The PMF assigns probabilities to each possible value, such as the number of emails received or rolling a dice. The Cumulative Distribution Function (CDF) is used to determine the cumulative probability associated with a random variable, providing the likelihood that the random variable will be less  than  or  equal  to  a  specific  value.  The  CDF  is  constant  between  points  where  it  is discontinuous. We  studied  about  the  Distribution  of  random  variables  and  some  of  the  most  important distributions: The Bernoulli distribution is a discrete distribution that is defined for a binary random variable with values 0 and 1. It's frequently used to simulate events with two alternative outcomes, such as pass/fail or success/failure. Uniform  Distribution:  For  both  discrete  and  continuous  random  variables,  the  uniform distribution may be defined. It assigns equal probability to each conceivable result for a discrete random  variable.  It  assigns  a  consistent  probability  density  over  a  defined  interval  to  a continuous random variable. The normal distribution is a continuous distribution and the most commonly used distribution. It has a bell-shaped curve and is described by two parameters: mean() and std(). Many natural events have a normal distribution. The  central  limit  theorem  asserts  that,  regardless  of  the  form  of  the  initial  distribution,  the distribution of the sum of a large number of independently and identically distributed random variables approaches a normal distribution. This theorem is significant because it enables us to draw conclusions about a population based on sampling. Data wrangling Data  wrangling,  also  known  as  data  munging,  is  the  process  of  cleaning,  transforming,  and organizing  a  dataset  to  make  it  suitable  for  analysis.  This  often  involves  a  combination  of manual and automated processes, and it is a crucial step in the data science pipeline. Data wrangling can be divided into the following section: ▪  Data loading and saving ▪  Data exploration ▪  Data processing, such as missing value handling, encoding categorical values ▪  Data distribution ▪  Scaling Feature extraction Feature extraction is a crucial step in machine learning, as it involves the retrieval of a set of relevant features from raw data that can be utilized to train a model effectively. These features contain  significant  information  about  the  target  variable  that  is imperative  for  successful classification. The Feature Matrix and Classification method uses machine learning to classify images based on  their  features.  Each  row  of  the  feature  matrix  represents  an  image,  while  each  column represents  a  specific  feature.  By  learning  patterns  in  these  characteristics,  the  computer  can determine if an image was taken indoors or outdoors. Text data representation Data  representation is  an  important  step  towards  creating  models  from  large  scale  data. Machine Learning requires data to be described ‘features’ called attributes or parameters before use. Choosing the right features is important to creating a useful model. Difference between data and signal Data refers to raw, unprocessed information, while a signal is a measurable variation carrying information. encoding and distribution Unlike  features  with  quantitative  value,  some  features  contain  categorical  values  which  the machine cannot understand. To solve this, encoding techniques are used to convert to integer values.There  are  several  well-known  techniques  of  encoding  such  as  Ordinal  Encoder,  One-Hot Encodings, and Label Encoder. Distribution refers to the way that the values in a dataset are distributed. This can be important, as the distribution of the values can have a significant impact on the performance of a machine learning algorithm. Scaling: In machine learning, scaling refers to the process of converting a set of values to a new range of values. Normalisation Normalization is a scaling technique used to transform the values of a dataset into a common range. This is often done to improve the performance of machine learning algorithms, as many algorithms operate better when the data is in a standardized range. One common method of normalization is to scale the data to a range of 0 to 1, where 0 is the minimum  value  in  the  dataset  and  1  is  the  maximum  value.  This  is  known  as  min-max normalization. Random variable A  random  variable  is  a  variable  whose  values  are  determined  by  the  results  of  a  random experiment. It can provide important events probability. A  random variable  is a function that can assign possibilities to event of interest in a random experiment. 