This topic's task involved a statistics refresher, in particular a probability refresher. A revisit to the definition of random variables and their distributions. The difference between data and signals, how to encode categorical data and normalisation of features within a data set.  I found the content and the tasks straight forward, especially coming off the back of completing the Data Wrangling unit last trimester. I never hurts to define these things in preparation for more complex content. Looking forward to the next few topics and getting deeper into the machine learning content.  Statistics Refresher  As statistical knowledge, especially probability, is essential to machine learning, we covered off on key definitions and formulas which included the following:    A random experiment: An experiment or process outcome that can’t be predicted.   Event: The result of a random experiment.   Joint Probability: Essentially multiple events calculated by multiplying the probability of each  even together.  𝑃(𝐴 𝑎𝑛𝑑 𝐵) = 𝑃(𝐴)𝑃(𝐵)    Conditional Probability: The probability of an event given that another event has occurred.  𝑃(𝐴|𝐵) =  𝑃(𝐴 𝑎𝑛𝑑 𝐵) 𝑃(𝐵)  provided that 𝑃(𝐵) ≠ 0    Bayes Rule: Describes the probability of and event A based on another event B that is  related to A. 𝑃(𝐴|𝐵) =  𝑃(𝐵|𝐴)𝑃(𝐴) 𝑃(𝐵)  𝑔𝑖𝑣𝑒𝑛 𝑃(𝐵) ≠ 0  Random Variables & their Distribution  A random variable is a variable whose value is a result of randomness. These usually come in two flavors:    Discrete random variables which represent whole countable values and are defined using a  Probability Mass Function.    Continuous random variables which exist on an infinite continuum and are defined using  Probability Density Functions.  Some of the most important distribution of these random variables include:    The Bernoulli distribution for discrete binary random variables.   A Uniform distribution for either discrete or continuous random variables.   A normal distribution for continuous random variables.  Recap on the central limit theorem which states that if you have a population with mean 𝜇 and standard deviation 𝜎 and take sufficiently large random samples from the existing population, then the distribution of the sample means will be approximately normally distributed.  SIT307 Machine Learning    Data Wrangling & Feature Extraction  Data wrangling is the process of cleaning, transforming, and organizing datasets ready for analysis.  Feature extraction can be a sub-process of data wrangling and involves transforming raw data into a set of meaningful and useful features that can be used to train models. The goal of feature extraction is to reduce the dimensionality of the data, while retaining as much relevant information as possible.  Data & Signals  Data is a collection of facts, statistics, or information that is used for analysis, reasoning, or reference. These days data is all around us and comes in forms such as text, numbers, images, audio, or video.  A signal on the other hand is a physical quantity that varies over time or space and conveys information. For example, an audio signal is the electrical representation of sound. A signal is represented by a quadratic equation: 𝑓(𝑥) = −𝑎𝑥2 + 𝑏𝑥 + 𝑐.  Encoding  Encoding for machine learning is typically about representing categorical variables as integers as machines don’t play nice with text data.  Scaling & Normalisation  Scaling and normalisation are pre-processing steps often performed before feeding the data to a machine learning model. In the module we focused on min-max normalisation which is motivated by the requirement that all variables contribute equally to model fitting. Variables on different scales do not contribute equally and can introduce bias in the model which we want to avoid.  