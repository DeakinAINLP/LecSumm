PROBABILITY    The study of probability is a branch of mathematics that examines the possibility that an event will occur. Joint probability is the likelihood that two or more events will occur simultaneously. The probability of an event happening given that another event has already occurred is known as conditional probability. According on prior knowledge of circumstances that might be connected to the event, Bayes' rule is a theorem in probability theory that estimates the likelihood of an event. In machine learning and data science applications, the Bayes' rule is frequently utilised for projects like prediction, classification, and recommendation systems. For the development and application of probabilistic models in numerous disciplines, it is crucial to comprehend diverse forms of probability.  DATA WRANGLING    Cleaning, converting, and putting raw data into a format ready for analysis is known as data wrangling. Data wrangling tasks frequently involve data cleaning, which involves removing or correcting inaccurate or irrelevant data, data transformation, which involves converting data into a format better suited for analysis, data integration, which involves combining data from various sources, and data reduction, which involves summarising or aggregating data to lessen its complexity. Handling missing data, addressing outliers, and developing new variables are possible additional responsibilities. The quality and accuracy of data analysis can be increased with effective data wrangling, which can also enable better decision- making.  TEXT DATA REPRESENTATION    A key component of machine learning, particularly for natural language processing (NLP) applications, is text data representation. Text data representation's main objective is to transform unstructured text data into a structured format that ML systems can understand. Word embeddings, bag-of-words, and term frequency- inverse document frequency (TF-IDF) are some popular methods for representing text data.  DATA AND SIGNAL    Two important ideas in information theory and communication systems are data and signals. A collection of facts or information that is used to depict the status of the world or a particular system is referred to as data. Data refers to the input features and output labels used to train a model in the context of machine learning. A signal, on the other hand, is a tangible or electromagnetic representation of data that is delivered via a communication channel from one location to another.    Data and signals differ primarily in that data is an abstract representation of information, whereas signals are actual representations of data that may be delivered via communication networks and processed there. Signal processing techniques can be applied to both analogue and digital signals to extract meaningful information. As opposed to this, data is frequently supplied in organised representations such as text, numbers, graphics, or other formats that may be examined and handled using data analysis tools.  DISTRIBUTION    The probability distribution of a collection of data is referred to as "distribution" in machine learning. The likelihood that a random variable will take any given value is expressed in a probability distribution. In other words, it offers data on the possibility of certain results or events. While doing tasks involving machine learning, such as classification, regression, and clustering, it's crucial to comprehend the distribution of the data. We can forecast and decide more effectively if we are aware of the distribution of the data. The normal distribution, binomial distribution, and poisson distribution are only a few examples of the many different kinds of probability distributions. Depending on the issue at hand and each type of distribution's specific characteristics, several machine learning techniques can be applied.  SCALING AND NORMALISATION   In machine learning, scaling and normalisation are crucial preprocessing methods. To increase the effectiveness of some algorithms, such gradient descent, scaling is the act of converting numerical information to the same scale. It is possible for some algorithms to converge more quickly and avoid numerical issues by normalising features, which is the process of changing data to have a mean of zero and a standard deviation of one. Standardization and min-max scaling are frequent methods for normalising, while skewed data can be scaled using log transformation. These methods can be used to increase the stability and accuracy of machine learning models.           