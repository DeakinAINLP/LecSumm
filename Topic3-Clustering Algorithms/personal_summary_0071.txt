Summary and Reflection of Topic 2: Data Wrangling  Machine learning is based on mathematics, specifically key areas of mathematics such as:    Vectors and basic vector operations   Matrices and matrix operations   Basic concepts of probability   Python programming – using different modules and packages.  A  random  experiment  is  an  experiment  or  a  process  for  which  the  outcome  cannot  be  predicted  with certainty whereas an event is a set of all possible outcomes of a random experiment. The probability of a specific event occurring is its likelihood and can be quantified between 0 and 1. Probability can be defined jointly for more than one event, i.e. two events as well as conditionally, that the probability of a specific event occurring given that another event has occurred. Bayes rule provides an approach or mathematical rule to explain how you should change your existing beliefs considering a new occurrence.  A random variable is a variable whose possible values are generated outcomes of a random phenomenon. Two types of such variable are discrete (i.e. countable) and continuous (i.e. can take values on an infinite continuum). The Probability Mass Function (PMF) assigns a function to each possible value of a random variable  and  it  sums  up  to  1.  The  Cumulative  Distribution  Function  (CDF)  gives  us  the  cumulative probability  associated  with  a  function.  Continuous  random  variables  are  defined  using  the  Probability Density  Function  (PDF).  This  is  a  statistical  expression  that  defines  a  probability  distribution  for  a continuous  random  variable,  i.e.  a  function  that  links  each  outcome  of  a  statistical  experiment  with  its probability of occurrence.  Some of the most important distributions (i.e. the way that the values in a dataset are distributed) are:    Bernoulli (discrete distribution and defined for a binary random variable)   Uniform (can be defined for both discrete and continuous random variable).  Values are evenly  distributed across the range of the data.    Normal  (defined  for  continuous  random  variable  and  characterized  by  a  bell-shaped  curve  symmetrical around the data’s mean).    Skewed – values are not evenly distributed and are instead concentrated on one side of the range.  The central limit theorem states that if you have a population with a certain mean and a certain standard deviation and take sufficiently large random samples from this population then the distribution of the sample mean will be approximately normally distributed.  Data Wrangling  A dataset is required to be cleaned, transformed and organized to ensure that it is suitable for analysis. This is referred to as data wrangling and involves common tasks such as:  Identifying and correcting errors and inconsistencies in the data     Handling missing or incomplete values   Combining multiple datasets   Converting the data into a format that is suitable for analysis    Normalizing the data   Aggregating the data into useful summary statistics  Identifying and removing outliers    Images  An image is required to be represented by a numerical vector of features to be an appropriate input for a computer algorithm. That is, the image must be converted to a numerical form. This process entails dividing the  image  into  smaller  blocks  and  finding  these  features  that  can  be  represented  numerically  (for  each block) and creating a model accordingly.  Text data  Models such as “Bag of words model” are used to convert text data into a readable and useful format for machine learning, enabling language modelling and document classification. This process broadly entails creating feature vectors for each document based on the occurrence of certain words in each document. These feature vectors are then added up into a feature matrix so that it can be used as a representation for machine  learning.  A  document  is  called  an  instance  whereas  this  matrix  is  called  a  feature  matrix  (as described below).  Data – General  Data is a collection of facts, and it can be qualitative and quantitative.  Qualitative data can be further categorized into a classification or categorization problem.  Quantitative data in turn can be discrete (able to be counted) or continuous (any value in a certain range).  Data collection can be primary (i.e. collected by researches via interviews, observations, case studies and questionnaires) and secondary (i.e. already in existence, for example in the form of statistics, web information, government reports etc.).  Concept of encoding - Features of categorical values are not understood by a machine, hence encoding techniques are used to covert these to integer values, i.e. which essentially entail assigning a number.  Concept of distribution – is important as the distribution of the values can have a significant impact on the performance of a machine learning algorithm.  Scaling – Process of converting a set of values to a new range of values. This is important as in a dataset there  may  be  several  different  features  of  different  ranges,  and  one  feature  may  dominate  the  model accordingly. Hence the results are scaled to a certain range. A technique such as normalization is used for this purpose, for example to scale the data to a range of 0 to 1, where 0 is the minimum value and 1 is the maximum value in the dataset.  Useful python libraries for statistical data analysis are NumPy, SciPy, matplotlib and Sitileanr. For example NumPy can be used to create a random variable matrix whereas matplotlib can be used to plot probabilities.  Data wrangling – continuation  Is  the  procedure  of  acquiring,  analysis,  and  manipulating  raw  data  into  a  suitable  format  for  faster processing and evaluation. It can be divided into:    Data loading and saving   Data exploration   Data processing such as missing value handling, encoding categorical values   Data distribution   Scaling  Data can be imported into a pandas DataFrame and analyzed as to its mean / standard deviation via the variable.describe() command in Python. There is a specific Python command to both find and replace null values with the previous data point. The LabelEncoder sklearn is used for categorical value encoding. Data distribution can be plotted via the use of a histogram from matplotlib and scaling can be adjusted and the mean accordingly inspected.  The  above  summary  has  been  created  using  the  full  course  material  for  Topic  2  (including  the relevant YouTube clips).       