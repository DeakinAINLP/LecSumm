Topic2 summary  This topic I had refreshed some mathematics knowledges that need to be used in ML. Such as vectors and basic operations, matrices and matrix operations, basic concept probability and python programming by using different modules and packages.  The probability and statistics played an important role in ML, it is defined for an event and is the measure of the likelihood that an event will occur. It gives us the language and tools to quantify the uncertainty of evens and reason in a principled manner. It’s useful for predicting modelling in ML. we have joint probability which happened more than one event, conditional probability, Bayes Rule. The random variables is a function that can assign probability to events of interest in a random experiment, it has two one is discrete which have a countable number of values and another is continuous which can take values on an infinite continuum.  The random variables distribution is a function tat links each outcome of a statistical experiment with its probability of occurrence. We have Bernoulli distribution for a binary random variable with values 0 and 1, Uniform distribution is for both discrete and continuous random variables, Normal distribution is for continuous random variables.  Data wrangling is an essential step to take raw and unstructured data and turn it into a form that is suitable for analysis and insights. It is the process of cleaning, transforming and organizing dataset. In ML, we can create a set of programming instructions or algorithm for machine to learn and process an image. By extracting features, it can make the machine analyse and categorise images. ML needs encoding to understand the categorial values, so it’s used to convert to integer values. Dome well-known techniques of encoding such as OrdinalEncoder, One-Hot Encodings, and LabelEncoder.  Distribution is important in the ML algorithm, it have significant impact on the performance of a ML algorithm, such as if the values are not evenly distributed it can cause the algorithm to be biased toward certain values and if it’s normally distributed it make it easier to learn and make predications. Scaling is a process of converting a set of values to a new range of values, we have normalization to transform the values of a dataset into a common range.   