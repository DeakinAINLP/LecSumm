Lesson Overview â€“ Data Wrangling  Probability is an important part of machine learning algorithms.  In topic 2, a revision on basic concept  of  probability  was  provided,  covering  joint  probability,  conditional  probability  and Bayes Rule.  Additionally, the two types of random variables, discrete and continuous, were clearly explained, a random variable is one where a random outcome is generated without any biases.  Discrete random variables are countable, such as faces of dice, whereas a continuous random variable can take any number, height of a person.  Probability distribution describes the  probability  of  all  outcomes  from  random  experiments,  some  of  the  most  popular distribution  include  Bernoulli  distribution,  uniform  distribution  and  normal  distribution.  Bernoulli distribution is a distribution defining binary random variables, for example, for a coin with values being head, or tail.  Uniform distribution is distribution defined for both discrete and  continuous  random  variables  where  occurrences  are  evenly  distributed,  for  example rolling  of  a  fair  dice.    Lastly,  normal  distribution  is  used  for  continuous  random  variables defining where majority of occurrences would fall around the mean. Data  wrangling  is  important  when  working  with  data,  data  is  required  to  be  cleaned, transformed and organised into format that is suitable for analysis and providing insights.   The process  of  data  wrangling  include  identifying  and  correcting  data  inconsistencies,  missing  values, outliers, performing normalisation, aggregations and ensuring data is in a format that is suitable for any analysis.  Some forms of data wrangling include encoding, encoding works with categorical values and turn them into values which machine is able to understand.  There are three forms of encoding, include ordinal encoding, one-hot encoding and label encoding.  Scaling and normalisation is another technique of data wrangling, where features are transformed into a common range so that all data works in a standardised range.  Common normalisation techniques include min max normalisation and z-score normalisation. Learning this topic also included plotting histograms using python, processing data by performing data filling, using sklearn to encode categorical data, and data scaling.  