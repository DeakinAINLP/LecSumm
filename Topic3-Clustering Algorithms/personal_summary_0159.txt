In topic 2 we got started with statistics where it defines about the importance of the probability in machine learning algorithms. We learnt about Probability, Joint Probability and conditional Probability and Bayes theorem which is helpful in Machine learning in decision making.  About random variable about the possible values that can be generated and also about the discrete random variables and about distribution of random variables and also about centre limit theorem .  Data wrangling Cleaning, converting, and organising a dataset to make it acceptable for analysis is the process of "data wrangling," also referred to as "data munging." Due to the fact that dealing with jumbled, unstructured, and incomplete data is frequently required, data wrangling can be a difficult and time-consuming task. This also involves in locating and fixing data mistakes and discrepancies ,handling values that are absent or incomplete ,several datasets combined transforming the data into an analysis-ready format recognising and eliminating outliers ,data normalisation combining the data to create helpful summary statistics.  Feature extraction is one of the crucial phases in machine learning. That implies that a complete representation of data is regarded as information, as we already know. Similar to this, for ML modelling, we must extract a set of features from the raw data, each of which provides details about the target variable.         Text data representation this was a really succinct overview on data representation. Humans are unable to analyse and interpret enormous amounts of data without the aid of specialised technologies. Only numbers are understood by computers. Before you can enter words, pictures, and concepts into a computer for processing, they must be converted into numbers.  We even studied about what is data vs signal we referred few YouTube videos . There was a module about encoding and decoding Certain features contain categorical values that the machine cannot interpret, in contrast to features with quantitative value. Encoding techniques are employed to convert to integer values in order to resolve this. There was also about  "distribution" describes how a dataset's values are spread out. This is crucial because the value distribution can significantly affect how well a machine learning system performs. In general, there are numerous ways in which the distribution of values in a dataset might impact how well a machine learning algorithm performs. For instance, if the values are not distributed equally, the algorithm may become biased in favour of some values, which could result in subpar performance. On the other hand, if the values are regularly distributed, learning and prediction by the algorithm may be facilitated.  Scaling and normalisation, Scaling in machine learning is the process of transforming a collection of values into a different range of values. We have a number of features in a dataset. It may be challenging to train a model when the raw or unscaled features have diverse ranges. A scaling technique    called normalisation is used to convert a dataset's values into a common range. As many machine learning algorithms perform better when the data is in a standardised range, this is frequently done to enhance their performance.  There was a module about data loading and saving which performs task like  Data loading and saving like Data exploration, Data processing, such as missing value handling, encoding categorical values Data distribution Scaling.  