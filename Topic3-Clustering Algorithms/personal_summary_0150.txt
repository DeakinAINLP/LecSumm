Statistical terms: some fundamental ones  Probability defines an event as a collection of results from a random experiment.  Probability  is  the  measure  of  the  possibility  that  an  event  will  occur  and  is  defined  for  an  event.  Its measurement is represented by a number between 0 and 1. P stands for the likelihood that event A will occur (A). P(A)'=1-P stands for the probability of an event A not occuring (A).  Joint  Probability:  Probabilities  for  many  events  can  be  defined  jointly.  Think  about  a  coin  tossing experiment  where  two  coins  are  used.  In  this  instance,  an  example  of  two  events  is  the  likelihood  of witnessing "heads for coin toss 1" and "heads for coin toss 2." If two events A and B are unrelated, then P(A and B)=P(A)P is the joint probability (B).  Predicate Probability: The likelihood of an event A given the occurrence of another event B is known as the conditional probability. The likelihood that A will happen if B happens is known as the condition probability P(A/B), and its formula is P(A and B)/P. (B) assuming that P(B) is not zero.  The Bayes rule is a mathematical formula that estimates the likelihood of an event based on knowledge of  potential  confounding  factors.  It  bears  the  name  of  the  18th-century  inventor  of  the  idea,  Reverend Thomas Bayes. The Bayes rule can be expressed in the following manner, where P(A|B) is the probability of event A given that event B has already occurred: P(A|B) = P(B|A) * P(A) / P(B).  P(B|A) is the likelihood that event B will occur in the absence of event A.   P(A) is the prior likelihood that event A will occur.   P(B) is the prior likelihood that event B will occur.  Random elements  In a random experiment, a random variable is a function that can determine the likelihood of certain events.  Typically, there are two categories of random variables:  A countable number of values can be found in discrete random variables. For instance, the number of emails received in an hour, the faces of a dice, etc.  Continuous random variables: have a continuum of values that is limitless. For instance, a person's height, the failure time, etc.  Distribution of random variables:   Bernoulli distribution   Uniform distribution.   Normal distribution.   Central limit theorem.  Data wrangling is the process of converting, purifying, and getting unstructured data ready for analysis. It requires a number of steps, such as data collection, cleansing, transformation, integration, and reduction.  The process of selecting and translating pertinent and helpful material from raw data into a set of features that  can  be  used  in  machine  learning  algorithms  is  known  as  feature  extraction.  By  lowering  the dimensionality of the data while keeping its key properties, feature extraction aims to simplify the data.  Data representation for text  This was a really succinct overview on data representation. Humans are unable to analyse and interpret enormous amounts of data without the aid of specialised technologies.  Only  numbers  are  understood  by  computers.  Before you  can input  words,  pictures,  and  concepts  into  a computer for processing, they must be converted into numbers.  Signals  and  data:  A  signal  is  a  function  that  depicts  how  a  quantity  changes  over  time  or  place. Contrarily, data is a collection of information that may be analysed and processed to yield helpful insights. It frequently takes the shape of words, numbers, or images.  Scaling and normalising:  Scaling refers to the process of rescaling the range of values of the features. This is done to ensure that all features are on the same scale and contribute equally to the learning  process. Normalizing, on the other hand, refers to the process of rescaling the values of the features to a common scale, such as between 0 and Normalization is used to ensure that all features are on the same scale and contribute equally to the learning process.  The main  difference  between  scaling  and  normalization  is that  scaling transforms the  range  of values of the features, while normalization rescales the values of the features to a common scale. Scaling is used to transform the data to a specific range, while normalization is used to transform the data to a common scale        