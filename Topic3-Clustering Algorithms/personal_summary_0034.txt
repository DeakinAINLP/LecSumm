This topic unit content was focused on Probability and Data Wrangling as these are core concepts of Machine Learning.  Probability is the understanding / measurement of a random experiment, i.e. an event where the outcome cannot be predicted with certainty. The likelihood of each of the outcomes occurring can be expressed in probability. There are multiple ways to calculate the probability, such as joint probability, conditional probability. Join probability is the probability that two independent events will occur. Conditional probability is the probability that another event will occur given the occur of a preceding event. Another key piece of probability information we covered was Bayes Rule, which states that the probability of an event changes based on the occurrence of a new event.  Random variables are variables whose possible values are generated outcomes of a random event. The values of these variables can be discrete or continuous. Discrete random variables have a countable number of values, while continuous random variables are on an infinite continuum.  Probability distributions a function that links each outcome of a statistical experiment with its probability of occurrence. There are numerous types of distributions, the main ones covered in the unit are the Bernoulli, uniform and normal distributions. The Bernoulli distribution is a binary discrete distribution, the uniform distribution is equal outcomes across all values, and the normal distribution is a bell curve. The central limit theorem states that with a large enough sample a distribution will become closer to a normal distribution.  Data wrangling is the process of cleaning, transforming, and organizing a dataset to make it suitable for analysis. This is an essential step in machine learning as the quality of the data affects the performance of machine learning algorithms. Data wrangling tasks include identifying and correcting errors and inconsistencies in the data, handling missing or invalid values, converting data sets, identifying and removing outliers, normalizing the data, and aggregating the data into useful summary statistics.  When raw data is in a format that is not suitable for analysis it needs to be transformed into a numerical vector for analysis. This is the case for images and text; images can be represented as numerical vectors by dividing the image into multiple sections and calculating the statistics for attributes in each block. Text data can be represented using the bag of words approach, which counts the occurrence of words in a document.  Encoding is used to convert categorical data or labels into numbers so that it can be processed and analyzed. Ordinal, Label, binary and one-hot are examples of encoding methods. Ordinal encoding gives assigns a numerical values where order is import to a series of labels (i.e. short, normal, tall). Label encoding assigns numerical values to numbers where order is not important. Binary encoding converts two values to a binary sequence (Yes / No â€“ 0 / 1). One-hot encoding converts the labels to a series of columns that are each binary encoded.  Scaling and normalization are techniques used to convert values of a dataset into a common range, which can improve performance. Scaling groups, a range of values into a collection of values, while normalization is a scaling technique that converts values to a common range, such as min-max normalization between 0 and 1.  Lastly the unit content includes data wrangling with Python can be done using tools such as Numpy, Scipy, Pandas, Sklearn, and Matplotlib. Each of the libraries shown contains features for managing, transforming, inspecting and visualizing datasets.  