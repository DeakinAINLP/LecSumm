  This topic lays a lot more emphasis on vectors and probability concepts    Firstly, we defined a random experiment as an experiment where we really cannot predict  what the outcome is going to be with a high degree of certainty    An event is explained as the group of outcomes emerging from a random experiment    We defined probability as how likely is an event  expected to happen where P(A) is the  probability of event A occurring    Joint probability requires both events to be happening at the same time so it usually requires  multiplication between the 2 individual events    Conditional probability is finding out the probability of some event A occurring given that  another event B has already occurred.    Bayes Rule defines the probability of an event while it bases itself on existing knowledge  of conditions which could be in relation with the event. It is defined as:    A random variable is simply a function which is used to link probabilities to events of  interest in a given randomised experiment. There is usually 2 types of random variables  namely discrete and continuous random variables        Some  crucial  distributions  of  random  variables  include  Bernoulli,  Uniform  and  Normal  distribution    When it comes to the Central Limit Theorem, we can say that the distribution sample mean  can be approximately normally distributed given large random samples    Data wrangling is a process defined as the steps which involve cleaning, transforming and  organising a given dataset to make it appropriate for analysis    Normalisation refers to a scaling method which is utilised to convert the values in a given  dataset into a common range and this is most of the times done to enhance the performance  of machine learning algorithms since they work better with this new format.    Non-numerical data encoding allows for e.g., an integer label to be produced so that one a  type could be assigned to one number and each number represents their own category. 