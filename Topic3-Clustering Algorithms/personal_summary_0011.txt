This topic focused data wrangling, how to extract features from a dataset, representing text into numerical data, how to load and export data into Jupyter notebooks, and how to scale, encode and review the distribution of data. It also touched on the basics of statistics, probability and random variables – and what scaling and normalisation of data is. From what I have learned Data Wrangling is the process of going through data sets and cleaning them up to make them better suited for analysis. This can be things like finding and removing errors, removing or correcting extreme outliers, filling in or removing null values and removing unnecessary data or fields – though with machine learning the data is unlabelled so all of the data could be relevant and would skew results if feature(s) were to be removed. The exercises for the pass activity were interesting to go through and it was a good combination of utilising Pandas, Numpy and Scikit-learn libraries. It helped me to learn how to access data files, observe the make-up of dataset, find null values and manipulate the dataset. I have been working as a Data Analyst intern for the last couple of months and data wrangling/cleaning has been a major step that is necessary to work on the data and bring about meaningful insights. I would imagine that this would be as much a major and important step to focus on in Machine Learning. One of the most significant learning experiences I had was with the example of the Monty Hall Problem through the Better Explained website. That really helped to make Bayesian probability click and the interactive examples demonstrating how the statistics and probability changed based on decisions was fantastic. 