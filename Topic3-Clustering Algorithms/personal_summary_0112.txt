 In topic 2 we learned about machine learning and its relation to mathematics. Every choice, iteration, and outcome is based on math in machine learning. So, in topic 2 we learned more about vectors and basic vector operations, matrices and matrix operations, the basic concept of probability, random variables and their distribution, and some data wrangling concepts for data preprocessing for machine learning.  Firstly,  Vectors  and  feature  matrices  are  fundamental  in  machine  learning  methods  for representing and manipulating data. Vectors are one-dimensional arrays of numerical values that  describe  the  characteristics  of  a  dataset  instance,  whereas  feature  matrices  are  two- dimensional arrays that comprise feature vectors for all instances. Building a well-organized feature matrix is critical for enhancing machine learning model performance and discovering underlying patterns and correlations in data.  Statistics are essential in machine learning because they provide a framework for assessing uncertainty  and  modeling  variable  connections.  Random  experiments,  probability,  joint probability, conditional probability, and Bayes' Rule are important ideas for comprehending data uncertainty and randomness, formulating predictions, and changing opinions based on new information.  Random variables, which can be discrete or continuous, are used to represent the outcomes of random experiments. Continuous random variables can take on an unlimited number of values  within  a  specific  range,  whereas  discrete  random  variables  can  take  on  a  finite  or countable number of values. Random variables of both types are common in machine learning tasks such as classification and regression.  Then  we  delve  into  some  probability  concepts  related  to  machine  learning.  Probability distributions describe the likelihood of various outcomes for a random variable. A uniform distribution  is  one  in  which  all  possibilities  have  an  equal  chance  of  occurring,  whereas  a normal  distribution  is a  continuous  distribution with a  bell-shaped  curve  with  a  mean  and standard  deviation.  Understanding  these  distributions  is  critical  for  hypothesis  testing, confidence intervals, and Bayesian machine learning priors.  Lastly,  we  learned  about  Data  wrangling,  which  includes  data  exploration,  processing,  and preparation, which is an important phase in the machine-learning pipeline. Data exploration uses descriptive statistics, correlations, and visualizations to assist comprehend the structure, content,  and  relationships  between  variables  in  a  dataset.  To  prepare  data  for  machine learning  algorithms,  data  preparation includes  addressing  missing  values,  encoding categorical variables, and scaling features. Missing values must be handled carefully since they might  result  in  biased  or  erroneous  models.  Missing  value  techniques  include  deletion, imputation,  prediction,  and  employing  algorithms  that  can  handle  missing  data.  Encoding categorical variables entails transforming them into numerical values via the use of techniques such as one-hot encoding and label encoding. Scaling guarantees that features are of the same size, which is critical for machine learning algorithms that are sensitive to input magnitude. Scaling strategies that are commonly used include min-max scaling and standardization.  