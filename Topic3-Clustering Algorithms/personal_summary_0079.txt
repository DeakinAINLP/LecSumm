Data Wrangling  Content Summary  This topics content began with covering some basic definitions and ideas pertaining to probability.  ● A random experiment is a process that generates an outcome based on chance. ● An event is a set of possible outcomes within a random experiment. ● Probability is a measure of the likelihood of an event occurring. ● Joint probability is the probability of two or more events occurring together. ● Conditional probability is the probability of an event occurring given that another  event has already occurred.  ● Bayes Rule is a formula for updating the probability of an event based on new  information.  ● Discrete random variables take on only a finite or countably infinite number of  values.  ● Continuous random variables can take on any value within a specified range. ● The Bernoulli distribution is a discrete probability distribution that models the  probability of success or failure of a single trial.  ● The uniform distribution is a continuous probability distribution that assigns equal  probability to all values within a specified range.  ● The normal distribution is a continuous probability distribution that is commonly  used to model real-world phenomena due to its symmetrical bell curve shape and ability to approximate many natural processes.  ● The central limit theorem states that, under certain conditions, the sum of a large number of independent and identically distributed random variables will tend towards a normal distribution.  Data wrangling, also known as data cleaning, is the process of transforming and preparing raw data into a format that can be easily analyzed. This can involve tasks such as removing duplicate entries, filling in missing values, and standardizing variable names. Effective data wrangling is a critical step in the data analysis process and can have a significant impact on the accuracy and reliability of results.  Statistics is a fundamental component of data science, providing the tools and techniques necessary to make sense of complex and large datasets. It involves analyzing, modeling, and interpreting data to gain insights and make informed decisions. Data science combines statistical analysis with machine learning, programming, and domain knowledge to extract valuable insights from data, and ultimately create meaningful solutions and predictions.  Text and images can be represented as data in machine learning by converting them into numerical form. Text data is usually preprocessed using techniques like tokenization and vectorization to convert it into a numerical representation. Images are represented as arrays of pixel values, and techniques like convolutional neural networks are used to extract relevant features and patterns from these arrays to build machine learning models. Feature extraction is the process of selecting and transforming raw data into a set of relevant and informative features. It is a critical step in machine learning that can significantly impact the accuracy and effectiveness of models.  Data refers to any form of information that can be processed, stored, or transmitted, while a signal refers specifically to a measurable quantity that varies over time or space and carries information. Data can be represented in various forms, including text, images, audio, and video, while signals are typically used in communication systems and are characterized by their frequency, amplitude, and phase. Signals are a type of data that is typically analyzed using signal processing techniques to extract meaningful information.  Encoding refers to the process of converting information into a format that can be easily stored, processed, and transmitted. This is often done through the use of specific algorithms or codes. Distribution, on the other hand, refers to the transmission or dissemination of encoded information to its intended recipients or users, which can be done through various means such as the internet, radio waves, or physical delivery.  Scaling and normalization are two common techniques used to preprocess data in machine learning. Scaling involves rescaling data features to a specified range, while normalization involves transforming data so that it has a mean of zero and a standard deviation of one, making it easier to compare and analyze.  