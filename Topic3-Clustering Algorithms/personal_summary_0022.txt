Probability =    defined for an event and is the measure of the likelihood that an event will occur. It is  quantified as a number between 0 and 1.    A random experiment is an experiment or a process for which the outcome cannot be predicted  with certainty. In probability we define an event as a set of outcomes of a random experiment.     Probability can be defined jointly for more than one event  Joint probability    is a statistical measure that calculates the likelihood of two events occurring together and at the same point in time.  Conditional probability   defined as the likelihood of an event or outcome occurring, based on the occurrence of a  previous event or outcome.  Bayes Rule    describes the probability of an event, based on prior knowledge of conditions that might be  related to the event i.e. if age is related to some health condition  A random variable, is a variable whose possible values are the generated outcomes of a random phenomenon. In other words, a random variable is a function that can assign probabilities to events of interest in a random experiment. Two types of random variables:    Discrete random variables have a countable number of values. For example, faces of a dice,  number of emails received in an hour etc.  o  A discrete random variable is one which may take on only a countable number of  distinct values such as 0,1,2,3,4,........ Discrete random variables are usually (but not necessarily) counts. If a random variable can take only a finite number of distinct values, then it must be discrete.  o  The probability distribution of a discrete random variable is a list of probabilities  associated with each of its possible values. It is also sometimes called the probability function or the probability mass function.  ▪  The cumulative distribution function (CDF) of a probability distribution contains  the probabilities that a random variable X is less than or equal to X.    Continuous random variables can take values on a infinite continuum. For example, height of a  person, time to failure etc.  o  A continuous random variable is one which takes an infinite number of possible values. Continuous random variables are usually measurements. Examples include height, weight, the amount of sugar in an orange, the time required to run a mile.  o  A continuous random variable is not defined at specific values. Instead, it is defined over  an interval of values, and is represented by the area under a curve (in advanced   mathematics, this is known as an integral). The probability of observing any single value is equal to 0, since the number of values which may be assumed by the random variable is infinite.  A simple explanation of a probability distribution is that it is a function that links each outcome of a statistical experiment with its probability of occurrence.  o  Bernoulli distribution is a discrete distribution and defined for a binary random variable with  values x = 0 and x = 1  o  Uniform distribution can be defined for both discrete and continuous random variables. In a discrete uniform distribution, outcomes are discrete and have the same probability. In a continuous uniform distribution, outcomes are continuous and infinite.  o  Normal distribution, data around the mean occur more frequently. One of the reasons for the popularity of the normal distribution is that many natural phenomena are approximately following a normal distribution. (bell curve)  o  Central limit theorem states that the distribution of a sample variable approximates a normal  distribution (i.e., a “bell curve”) as the sample size becomes larger, assuming that all samples are identical in size, and regardless of the population's actual distribution shape. Sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold. A key aspect of CLT is that the average of the sample means and standard deviations will equal the population mean and standard deviation.  Data wrangling can be a complex and time-consuming process, as it often involves dealing with messy, unstructured, and incomplete data. Some common tasks that are involved in data wrangling include:  Identifying and correcting errors and inconsistencies in the data  o o  Handling missing or incomplete values o  Combining multiple datasets o  Converting the data into a format that is suitable for analysis o o  Normalizing the data o  Aggregating the data into useful summary statistics  Identifying and removing outliers  Images as Data  The first step would be finding features that can be represented with numbers. We have to decide how to represent this image, and any other image, numerically. We can begin by dividing the image (in example) into smaller 9×15=135 blocks.  For each block, we can compute features of our choice for example:  o  color averaged across the block o  shapes within the block - number of straight lines and curves etc o o  texture for example the amount of light/dark variation in the block radiance or brightness  For these 135 blocks we can compute:  o  mean   o  variance o  other statistics  In this photo example there are 135 blocks, we have 1000 images to sort into indoor and outdoor and we have selected four ‘features’ to analyse. So the feature matrix is 135 x 1000 x 4.  Text data representation  Data representation is an important step towards creating models from large scale data. Machine Learning requires data to be described by ‘features’ called attributes or parameters before use. Choosing the right features is important to creating a useful model.  Data Vs Signal  whatever a user is sharing, everything is considered data when they are stored in the form of observation or measurement, which is represented as text, number, or media.  A signal is commonly referred to as a sign or gesture which conveys certain information. However, in digital electronic or signal processing, the signal is considered as the quantity that varies over a parameter such as space or time.  Difference?  When any data is represented in an organized manner, that becomes information.  Signal is another form of data. For example, using an ECG sensor device we can capture the electrical impulses of the heart and produce signals to represent ECG signals which can be plotted over time.  Encoding  Unlike features with quantitative value, some features contain categorical values which the machine cannot understand. To solve this, encoding techniques are used to convert to integer values.  Several well-known techniques of encoding such as OrdinalEncoder, One-Hot Encodings, and LabelEncoder.  Distribution  o  Distribution refers to the way that the values in a dataset are distributed. o  Types of distributions that can occur in a dataset, including normal, uniform, and skewed  distributions.  o  The normal distribution is the most common, characterized by a bell-shaped curve  symmetrical around the data's mean.  o  A uniform distribution is one in which the values are evenly distributed across the range  of the data  o  A skewed distribution is one in which the values are not evenly distributed and are  instead concentrated on one side of the range.  o  The distribution of the values in a dataset can affect the performance of a machine learning  algorithm in many ways.  o  if the values are not evenly distributed, this can cause the algorithm to be biased toward certain values   o  if the values are normally distributed, this can make it easier for the algorithm to learn and make predictions.  Scaling  In machine learning, scaling refers to the process of converting a set of values to a new range of values. In a dataset, we have several features. The raw or unscaled features can be of different ranges, which may cause problems while training a model. We need to scale both features in a certain range  Normalisation  Normalization is a scaling technique used to transform the values of a dataset into a common range. This is often done to improve the performance of machine learning algorithms, as many algorithms operate better when the data is in a standardized range.  One common method of normalisation is to scale the data to a range of 0 to 1, where 0 is the minimum value in the dataset and 1 is the maximum value. This is known as min-max normalization. Overall, normalization is a useful technique in machine learning, as it can improve the performance of many algorithms by standardizing the data.  Data Wrangling  Data wrangling is the procedure of acquiring, analysing, and manipulating raw data into a suitable format for faster processing and evaluation. 