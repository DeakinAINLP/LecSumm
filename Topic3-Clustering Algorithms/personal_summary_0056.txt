  Statistics: basic definitions  probability Probability is defined for an event and is a measure of the likelihood that an event will occur. It is quantified as a number between 0 and 1. The probability of event A occurring is denoted by P(A). The probability of event A not occurring is denoted by P(A). P(A)=1-P(A)  joint probability P(A and B) = P(A)P(B)  conditional probability A conditional probability is the probability of an event A, if another event occurred B. The state probability P(A/B), read as the probability that A given B is defined as P(A/B) = P(A and B)/P(B)    Bayes' law The essence of most Bayesian approaches is to provide mathematical rules that describe how existing beliefs should change in light of new events. The Bayes rule describes the probability of an event Abased on another event Bthat is related to . P(A/B)=P(B/A)P(A)/P(B) P(B) != 0   random variables random variable  A random variable is a variable whose possible values are the  generated result of a random phenomenon.  In other words, a random variable is a function that allows us to assign probabilities to events of interest in random experiments.  For example, if you toss a coin, the possible outcomes are heads  or tails. Let's define a random variable by X where X=1 means head and X=0 means tail. A function is nothing but a mapping X=1 to head or X=0  discrete random variable A discrete random variable is defined using a probability mass  function (PMF) given as π(x).  PMF assigns probabilities to each possible value of a random  variable as follows: π(x)=P(X=x) sum them up to 1   Distribution of random variables Bernoulli distribution The Bernoulli distribution is a discrete distribution, defined for  binary random variables with values X=0 and X=1.  So π(0)=P(X=0)=P and π(1)=P(X=1)=1−p In some cases, we use the following notation: π(x)=B(x∥p) and  x∼B(x∥p) where B means Bernoulli.  uniform distribution A uniform distribution can be defined for both discrete and  continuous random variables. For discrete random variables:  π(xi)=P(X=xi)=1/N,i=1..N We mean π(x)=U(x∥N) or x∼U(x∥N) where U is uniform. The same concept applies to continuous space. For continuous  random variables:  f(x)=1/,a≤x≤b We mean f(x)=U(x∥a,b) or x∼U(x∥a,b) where U is uniform. normal distribution A normal distribution is defined for continuous random  variables. This is by far the most popular distro. For continuous random variables, the normal distribution is defined as  We mean f(x)=N(x∥μ,σ^2) and x∼N(x∥μ,σ^2) where N is  normal.  central limit theorem The central limit theorem states that if we have a population of  means μ and standard deviation σ  If you take a sufficiently large random sample from an existing  population, the distribution of the sample mean will be approximately normal.   Data wrangling  Data wrangling, also known as data munging, is the process of  cleaning, transforming, and organizing datasets to make them suitable for analysis. This often involves a combination of manual and automated processes and is a critical step in any data science pipeline.  Data wrangling can be a complex and time-consuming process.  Because they often have to deal with messy, unstructured and incomplete data. Common tasks related to data wrangling include:    (cid:0) Identify and fix data errors and discrepancies   (cid:0) Handling missing or incomplete values   (cid:0) Combining multiple datasets   (cid:0) Convert data into a format suitable for analysis   (cid:0) Outlier identification and removal   (cid:0) Data normalization   (cid:0) Aggregating data into useful summary statistics   Overall, data wrangling is a key step in the data science  process as it allows us to take raw, unstructured data and transform it into a format suitable for analysis and insight.   Images as data  A series of programming instructions, algorithms, must be written to process each image and make a decision. The more correct answers, the more accurate the result, and the more the machine learned.  computers understand numbers The first step is to find features that can be expressed numerically. To use an image as input data for this computer algorithm, it must be  represented by a numeric vector of features similar to the example shown in the video in the last step. A computer's instructions, its algorithms, can only understand an image if it can be input as a series of numbers.  Create a model We need to decide how to represent this and other images numerically. You can start by splitting the image into smaller pieces 9×15=135 block. For each block you can compute the selected function. For example:    (cid:0) Color averaged across blocks   (cid:0) Shapes in blocks - number of lines and curves, etc.   (cid:0) Textures, e.g. amount of light/dark variation within blocks   (cid:0) brilliance or brightness  For these 135 blocks, we can compute:    (cid:0) Average   (cid:0) Variance   (cid:0) Other statistics  Let's say we extracted the features per block leading to p 135p features per  image. As a result, the size of the image, feature matrix is 135p×n.  In this photo example, I have 135 blocks, I have 1000 images that I want to  classify as indoors and outdoors, and I chose 4 "features" to analyze. So the feature matrix is: 135×1000×4.  Once the image has been converted to a set of numbers, the resulting feature matrix can be fed into a suitable computer algorithm to classify as indoor or outdoor.   Text data representation  Without specialized tools, analyzing and interpreting large amounts of data is  beyond human ability.  Computers can only understand numbers. Words, images and ideas need to  be converted into numbers before being put into a computer for processing.  Data representation is a key step in creating models from large-scale data.  Machine learning requires data to be described by attributes or “features” called her parameters before it can be used. Choosing the right features is critical to creating a useful model.   Data vs signal  What is data The term used a lot these days is “data”. From Facebook to Google,  companies big and small all use data from their users/customers. what are they really using? We use information you have shared, purchase history, location, or hobbies you have shared. In other words, anything a user shares is considered data if it is stored in the form of observations or measurements expressed as text, numbers, or media. However, not all of them work for a particular company or stakeholder. Meaningful representations of data can yield new insights that help companies make certain decisions.  What are signals? Signals are commonly referred to as signs or gestures that convey specific  information. However, in digital electronics or signal processing, signals are viewed as quantities that vary with parameters such as space and time. We all recognize audio signals or heartbeat signals that are very common in our daily life. An audio signal is an electrical representation of sound.  The following formula represents the signal. where x is the independent  variable and f is the dependent variable.  f(x) = -ax^2+bx+c It depends on the signal shape value. If a>0, the signal is a downward  parabola, if a=0, the signal is a straight line, and if a<0, the signal is an upward parabola.   Encoding distribution  Unlike features that have quantitative values, some features have categorical  values that machines cannot understand. To solve this, we use an encoding technique to convert it to an integer value.  Variance refers to how the values in the dataset are distributed. This is  important because the distribution of values can have a large impact on the performance of machine learning algorithms.  There are many types of distributions that can occur in datasets, such as normal, uniform, and skewed distributions. The normal distribution is the most common and is characterized by a symmetrical bell-shaped curve about the mean of the data. A uniform distribution is one in which the values are evenly distributed across the range of the data, while a skewed distribution is one in which the values are not evenly distributed, but instead concentrated on one side of the range. distribution.  In general, the distribution of values in a dataset can affect the performance of machine learning algorithms in different ways. For example, if the values are not evenly distributed, the algorithm can be biased toward certain values, resulting in poor performance. On the other hand, if the values are normally distributed, the algorithm will be easier to learn and predict.   Scaling and normalization  scaling In machine learning, scaling refers to the process of transforming a set of  values into a new range of values. Datasets have several functions. Raw or unscaled features can have different ranges, which can cause problems during model training.  Normalization Normalization is a scaling technique used to transform the values of a dataset  to a common range. This is often done to improve the performance of machine learning algorithms, as many algorithms work better when the data is within a normalized range.  One common method of normalization is to scale the data to a range of 0 to 1. 0 is the minimum value in the dataset and 1 is the maximum value. This is called min-max normalization and is calculated using the following formula:  v'=v-min(v)/max(v)-min(v) where v is the original value and v' is the normalized value  1.  Provide summary of your reading list–external resources, websites,  book chapters, code libraries, etc.  