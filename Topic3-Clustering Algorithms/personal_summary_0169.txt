Review of topic 2  We started the topic with an explanation that Machine Learning’s foundations lie in mathematics: hence the topic would largely focus on a few important mathematical concepts.  Probability  First out the gate was an overview of probability. Essentially, we must know what is possible before we know what is probable.  We first were taught some definitions:   Random experiment: a process for with the outcome cannot be predicted with certainty.   Event: the outcome of a random experiment.    Sample space: the set of all possible outcomes of a random experiment. Denoted by the Ω symbol   Probability: the likelihood that an event will take place. Expressed as a range between 0  and 1, with 0 defined as "the event won't occur" and 1 as "the event will occur". Written as P(A), with A representing the event.  Probability formulas  After the definitions, we covered some formulas for working with probabilities.   The probability that an event A won't occur is P('A) = 1 – P(A)    Joint probability: the probability of two independent events occurring P(A∩B) with A and B representing events. Note that P(A∩B)=P(B∩A)   Conditional probability: The probability that an event will occur given that another one  has. P(A|B) with P(A|B) = P(A∩B)/P(A). Note that P(A|B) != P(B|A})   Bayes rule: a way to calculate conditional probability, based on prior knowledge of  conditions that might be related to the event.  Described by the formula: In essence, one conditional probability can be calculated using another conditional property.  Random Variables  Again, we were given some basic definitions:   Random variables: a random variable is a variable whose values are the outcomes of some  random experiment. It can be seen as a function that maps probabilities to events in a random experiment.   Discrete random variables: a random variable that can only have a countable number of distinct values. Each of these values can be assigned a probability via a probability mass function (π(x)). The sum of the probabilities must be 1.   Cumulative distribution function: the function F(t) that gives the probability that a  random variable X with a given probability distribution will be found at a value less than or equal to x.   Continuous random variables: a random variable that can have an infinite number of  possible values. These are usually measurements.   Probability density function: the function f(x) that gives the probability of a continuous  variable. Of interest is that the area under the density function will be 1  Distribution Types  After the definitions, we covered types of distributions.  Bernoulli distributions  A Bernoulli distribution is a form of discrete probability distribution. It applies to events that have only two possible outcomes. For example  1. Questions with a 'yes'/'no' answers;  2. The flip of a coin: 'heads'/'tails'.  The Bernoulli distribution can be seen as a distribution that allows us to calculate the probability of each outcome.  A Bernoulli distribution is a type of binomial distribution. Bernoulli distributions have only one event, whereas Binomial distributions can have multiple.  ‘Bernoulli distribution gives you the probability of “success” (say, landing on heads) when flipping the coin just once (that’s your Bernoulli trial). If you flip the coin five times, binomial distribution will calculate the probability of success (landing on heads) across all five coin flips.’ (Stevens 2021)  Uniform distributions  A uniform distribution is a distribution in which all outcomes are equally likely. It can be envisaged as a straight horizontal line. For example, a coin flip would have a line at the y-axis drawn at 0.5.  Normal distributions  Normal distributions are also known as  Gaussian distributions. They are the most common distributions. It is defined for continuous random variables and are symmetric around the mean, and show that data closer to the mean is more frequent that data far from the mean.  In graphical form, it appears as a symmetric bell curve with an area = 1, mean = 0 and standard deviation = 1.  Central Limit Theorem (CLT)  The CLT states that averages calculated from independent, identically distributed random variables will have approximately normal distributions. In other words, if the sample size is large enough, the distribution of the samples will be normal.  Data wrangling  Data wrangling is also called "Data munging". It describes the process of cleaning, transforming and organising data so that it can be analysed.  This can be a complex and time-consuming task.  Common activities when wrangling data are:  1. Finding and correcting errors/inconsistencies in the data.  2. Handling missing or incomplete values.  3. Combining datasets.  4. Converting data into different formats (e.g. non-numeric data encoding)  5. Finding and removing outliers  6. Normalising the data  7. Aggregating the data into summary statistics.  The mean of a set of numbers is the sum of the values divided by the number of values.  The median of a set of data is the value of the middle item if an odd number of items, or the mean of the two middle items if an even number of items, when the data is arranged in increasing or decreasing order.  The immediate values are those values immediately before and after a location of interest in the data samples.  Feature extraction  After considering data wrangling, it was observed that computers work well with numbers: and will struggle to interpret other forms of data. Hence if we want to work with non-numeric data, we need to find ways of mapping it to formats that computers can deal with. It is important that our mapping exercise preserve the information that was in the original data set.  Images as data  We need to have a way of representing images with numbers. If we can’t do this then we can’t feed images into applications for processing.  One way of doing this is to divide our image up into a number of blocks. Then we can represent each blocks contents by a feature within the block. Such features can be  1. Colour average  2. Shapes detected inside the block  3. The texture of the block  4. The radiance/brightness of the block area.  This information can then be put into feature vectors and worked with.  Across all of the blocks we can also determine, for our chosen feature:   Mean   Variance   Other useful statistics.  Of interest is that the linked article to Ed Bridges suing the Police for facial recognition: Ed won his case, and the Welsh police had to halt their trial of the facial recognition software (‘Legal Challenge: Ed Bridges v South Wales Police’ n.d.).  Bag of words  A way of working with text is to take the text that we want to analyse, and place the words it contains into a ‘bag’. The we write the associated word counts of the features we are interested into vectors. This gives us our “bag of words”.  The steps followed are:  1. Tokenize (break the text up into words) 2. Build vocabulary (build up a collection of the words found by the tokenizer) 3. Count tokens (count the number of usages of the words) 4. Create vector representations. These vectors are called feature vectors.  The drawback of this method is that new words increase the size of our bags. Also, the bags will have a lot of 0 counts, as not all sentences will have all words in them. To reduce the number of words that are put into the bag, we can:   Convert all to lower case: Tower = tower  Remove stop words, such as 'the' and 'is'   Lemmatization: we use a dictionary to find the root words: words with similar meanings  Perform stemming of the words: we remove suffixes. 'playing'  'play'  ➔  are consolidated down to one word. e.g.: 'walks', 'walking', 'walked'  ➔  'walk'  Another drawback is that the technique also loses any context around the grammar and the ordering of the words.  As an example:      I love running, but I hate swimming.  I love swimming, but I hate running.  Bag of words will give both of the above sentences the same vector representation: but the sentences have very different meanings.  What is data?  The discussion of feature extraction then touched on the question: what is data?  "Data is a collection of facts, such as numbers, words, measurements, observations or just descriptions of things." (‘What is Data?’ n.d.)  Data can only be useful if you can give it a meaningful representation. As such it can be:  1. Qualitative – descriptive  2. Quantitative - numerical   Quantitative data can be:  1. Discrete - can only take on certain numbers.  2. Continuous - can take on any value within a range.  Data encoding  Features with qualitative data, such as categorical values, can't be understood by computers. We need to encode these values into integer values.  There are several well-known encoding techniques:        1. OrdinalEncoder: for use with qualitative data that groups into descriptive categories. We can assign each category an integer value to work with.  2. One-Hot Encoder: convert each value into a new column in the data, with a 1 set if this row of data has the value, or a 0 if it doesn’t. In this way we are removing any appearance of ordering or dependency between the values.  3. LabelEncoder: simply map each value in the data to a number. This encoding runs the risk that a hierarchy between the values can be inferred, when there is in fact none.  Data distribution  Distribution describes the way in which the values in a dataset are distributed. The distribution of a dataset can have a big impact on the performance of an ML system.  There are many types of distributions:  1. normal - most common, a bell shaped curve symmetric around the mean of the data.  2. uniform - the values are evenly distributed across the range.  3. skewed - the values are concentrated on one side of the range.  Scaling and normalization  Scaling refers to the process of converting a set of values to a new range of values.  Normalization is a scaling technique whereby the values of a dataset are transformed into a common range.  A common normalization technique is to scale the data into the range of 0..1. This is called 'min- max' normalization and is done by using the following formula:  Here v’ is the normalised value, and v the original value.   Normalization can improve the performance of many algorithms by standardising the data.  Python programming: modules and packages  Finally, we covered the use of the following Python libraries to demonstrate the concepts covered in this topic in Jupyter notebooks:  Pandas: a data analysis library SciPy: algorithms and data structures for scientific computing   NumPy: a numerical computing library    matplotlib: a visualisation library   scikit-learn: a machine learning package  