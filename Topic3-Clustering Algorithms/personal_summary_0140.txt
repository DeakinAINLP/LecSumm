● This topic we covered various aspects of Data Wrangling and Statistics. We also  learned how to use these mathematical concepts to extract information of value from data in python.  1) The first section of this topic's learning was regarding some basic statistical  concepts which involved a brief recap on concepts of probability:  ● joint probability (P(A and B) = P(A) * P(B|A)): Joint probability refers to the probability of two or more events occurring simultaneously. It is denoted as P(A and B), where A and B represent two events.  ● conditional probability (P(AIB) = P(A and B)/ P(B)) : Conditional  probability refers to the probability of an event occurring given that another event has already occurred. It is denoted as P(A|B), which represents the probability of event A happening given that event B has occurred.  ● Bayes Rule (P(AIB) =  𝑃(𝐵|𝐴) 𝑃(𝐴) 𝑃(𝐵)  )  where:  P(A|B) represents the conditional probability of event A given event B has occurred. P(B|A) represents the conditional probability of event B given event A has occurred. P(A) and P(B) represent the individual probabilities of events A and B, respectively.  Bayes' Rule states that the probability of event A occurring given the occurrence of event B is equal to the likelihood of event B given event A, multiplied by the prior probability of event A, divided by the prior probability of event B.  2) Further we learned about the various ways in which random variables can be  distributed:  ● Bernoulli Distribution: Models a random variable that takes two possible  outcomes, typically represented as 0 or 1, with a fixed probability for each outcome.  ● Uniform Distribution: Assumes all values in a given range are equally  likely to occur.  ● Normal (Gaussian) Distribution: Often referred to as the bell curve, it is  widely used due to the central limit theorem and describes many natural phenomena.  ● Central Limit Theorem states that if we have a large number of  independent random events occurring, their cumulative effect or sum will tend to be normally distributed, even if the individual events themselves do not follow a normal distribution.  3) Further, we learned about various data manipulation techniques which are used  to process data, some of them are:  ●  Feature extraction is used to transform raw data into a set of meaningful and representative features that can be used to build models or perform further analysis. It involves selecting and transforming the relevant information from the original data to capture its essential characteristics.  ● Scaling and normalization are two common techniques used in data preprocessing to transform the numeric features of a dataset into a standardized range. Both techniques aim to bring the features to a similar scale, which can be beneficial for various machine learning algorithms.  ● Encoding refers to the process of converting categorical or qualitative data into a numerical representation that can be used by machine learning algorithms. Categorical data represents variables that can take on discrete values or categories, such as colors, types of animals, or customer segments.  This topic's learning covered statistical concepts such as probability, joint probability, conditional probability, and Bayes' Rule. We also explored different types of distributions and their significance, including the Bernoulli, uniform, and normal distributions. Data manipulation techniques like feature extraction, scaling, normalization, and encoding were discussed to prepare data for analysis and machine learning tasks. The emphasis was on applying these concepts and techniques using Python to extract meaningful insights from data.  