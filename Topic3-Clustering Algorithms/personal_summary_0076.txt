Data wrangling  Statistics Probability is the measure of the likelihood that an event will occur, quantified as a number between 0 and 1. Bayes' rule describes the probability of an event based on another event that is related to it.  Random variables Random variables are functions that assign probabilities to events of interest in a random experiment. Rolling dice is an example of a discrete random variable, and the Cumulative Distribution Function (CDF) gives us the cumulative probability associated with a function.  Distributions of random variables A probability distribution is a function that links each outcome of a statistical experiment with its probability of occurrence.  Data wrangling Data wrangling is the process of cleaning, transforming, and organizing a dataset to make it suitable for analysis.  Data VS signal Data refers to raw facts and figures that can be processed and analyzed to derive useful information. It can be in various forms such as text, numbers, images, and videos, and is the foundation of all digital information.  A signal is commonly referred to as a sign or gesture which conveys certain information. However, in digital electronic or signal processing, the signal is considered as the quantity that varies over a parameter such as space or time.  Encoding and Distribution Encoding is the process of converting categorical values to integer values. This is important in machine learning because some features cannot be understood by machines. There are different techniques for encoding such as One-Hot Encodings and LabelEncoder.  Distribution refers to how the values in a dataset are distributed and can affect the performance of a machine learning algorithm. Normal, uniform and skewed distributions are common types of distributions.  Scaling and Normalisation  In machine learning, scaling refers to the process of converting a set of values to a new range of values. In a dataset, we have several features. The raw or unscaled features can be of different ranges, which may cause problems while training a model.  Normalization is a scaling technique used to transform the values of a dataset into a common range. This is often done to improve the performance of machine learning algorithms, as many algorithms operate better when the data is in a standardized range.  