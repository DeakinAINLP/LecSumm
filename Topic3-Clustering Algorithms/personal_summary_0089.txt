SIT307 | 2.1P Lesson review  Summary:  2.1:  Probability is important to machine learning algorithms. An event is a set of outcomes from a random experiment, this is within the sample space, which is the set of all possible outcomes from the experiment.  Probability (P) for event (A) expressed as P(A),  Joint probability: for independent events (A and B), P(A)P(B).  If the probability of A and B independently is ½, P(A and B) = P(A) * P(B) = ½ * ½ = ¼  Conditional probability: the probably of A, given the occurrence of B = P(A|B)  P(A|B) = P(A and B) / P(B) provided B != 0  Bayes rule: The probability of A is based on B that is related to A = P(A|B) = P(B|A) P(A) / P(B) where P(B) != 0  2.2:  A random variable is a variable which has possible values generated by a random phenomenon.  They can be discrete, i.e have a countable number of values (faces of a die). Or continuous, i.e infinite possibilities (height).  Discrete random variables are defined by a Probability Mass Function (PMF) denoted π(x). This function assigns a probability to each possibility of the random variable.  The Cumulative Distribution Function (CDF) gives cumulative probability associated with a function.  Continuous random variables are defined by Probability Density Functions (PDF). It assigns probability to a range of values of the random variable.  Activity: Share your understanding of random variables. Can you determine some of the random processes around you? Why do you think they are random?  A random variable is effectively self-explanatory, its potential set of values are the possible outcomes of a random event. A discrete example would be number of birds in my garden in the morning, a continuous example is the temperature on my thermometer now. They’re random because probability determines each occurrence, both are not immediately predictable.     2.3:  A probability distribution is a function which links each outcome of an experiment to a probability of occurrence.    Bernoulli distribution defines for discrete binary values.   Uniform distribution defines for cases where all occurrences have an equal  probability.    Normal distribution often occurs with natural phenomena where the average  occurrence(s) see the highest frequency.  2.5:  Feature extraction is key to ML, Computers interpret numbers so features need to be numerically represented.  For an image this may include dividing into smaller chunks, features like average colour and brightness can be computed from these.  2.6:  To create a model, we need to express data as features. Text can be represented by a bag-of-words model, using the frequency of words.  Activity: The technique described in the video is a popular model used to encode text data. What are some of its limitations?  Bag of words has a poor ability to interpret context, natural language sentences may have vastly different meanings and yet contain the same set of words. Additionally, handling large documents results in a high frequency of null values, meaning sparse vectors.  2.8:  Encoding can be used to translate categorical data into numeric, by assigning a value to a location etc.  The distribution of data can lead to biases within a model, normal distribution creates an easier basis for prediction.  2.9:  Scaling raw features ensures they do not dominate one another in the finished model. Normalisation is a way of scaling that transforms values into a common range, such as between 0 and 1 where 0 is the min value and 1 the max.  