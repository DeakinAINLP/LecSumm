In this topic module, we will be focusing on Data exploration in a csv files. But before studying  data, the topic also provide a basic core understanding of statistics.  Recap on Statistics When its about statistic, we always know its going to be about probability, mean, median, standard deviation, distribution and so on. So how are probabilities used in Machine Learning? We know that we can use probability states the likely chance of something happening, and with machine learning we can use it to predict the outcomes. For example, like weather casting, we can then predict how likely it is to rain in the next few days. Aside from probability, the statistic which is what we call data set in machine learning, is one of the most important factors. With dataset, we can test, predict, and find the accuracy of the models.  Bayes rule is an approach that show how things can change according to the light of new occurrence. For instance, probability of A happening decreases due to B happening. In a random experiment we divide those data into two variables just like the previous module: Discrete and Continuous. Where discrete can be count and continuous can take in infinite amount of value. Now let’s get to the main topic of this topic.  Data Wrangling  We all know that when we try to process a new data set, it is very complicated, unstructured,  null values and a lot of repetitive values. With our own eyes, it almost impossible to read all the values. This is where Data Wrangling comes in. So, what is Data Wrangling? Data Wrangling is an approach to process a dataset to make it structured, clean, and neat by removing complex and error data, and combining data into sets. The importance of this process is to piece together raw data into format making it easy to understand the context of data. This later allows people or machine to analyze the dataset easier and faster.  By this time, we know that computer only understands numbers (1/0), and to allow them to  read data efficiently, vector and matrix will come in handy. For each dataset, we can divide them into smaller block, then compute the feature of each block that has similarities. Then we can calculate out the mean, median and standard deviation of the data.  For the machine to easily differentiate between sets of data, it uses encoding with some integer  value. It puts them into different group then use it for distribution so that we can find it easier to analyze the dataset. There are many types of distribution, but the focus and common one is Normal Distribution.  Let’s see how the steps for Data Wrangling goes.  Data Loading and Saving  When we first load the dataset into the system, sometimes it’s going to be unstructured, and  nearly impossible for us to analyze if the data is big. We then save it into a data frame. Of course, if we were to download a dataset from a good site, they are most likely going to be organized for us to view.  With programming technique, we can use a few functions inside the data frame to describe the dataset that it receives. It will show you column-row size, the data size and type, memory usage, and its description such as the common statistic value.  Data Exploration  Data Processing  If the datasets have missing value or outliers, it is better for us to take those part out or change  it to make the value more consistent. This will help us process the data and predict the future values. Afterward, we can also group those types of data to a certain group/encode them for faster processing.  We can then find out how the distribution of the dataset looks like. Us humans does not need to self-analyze the dataset, we just need the computer to do it, and we analyze the distribution. However, sometimes the distribution is too big and can outweigh the other value, so we need to scale it down.  Data Distribution  We can pick any type of statistic value such as mean, median and standard deviation to scale  down the value. Although the look at the distribution chart does not change anything, but then the value will come down so low.  