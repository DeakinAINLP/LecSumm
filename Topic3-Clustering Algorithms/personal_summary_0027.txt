Statistics basic deﬁnitions  A random experiment is a process for which the outcome cannot be predicted with certainty.  Event is the outcome of a random experiment.  Sample space is the range of possible outcomes for a random experiment.  the probability of an event is a measure of how likely that event is to occur.  Joint probability is the probability of more than one event. For example, the probability that two die rolls will both be a 6. The probability is 1/6 * 1/6 = 1/36.  Conditional probability is the probability of an event, given the occurrence of another event. Given the ﬁrst roll of the die above is a 6, the conditional probability that both rolls will be a 6 is 1/6. P(A|B) is the probability of A given B.  Random Variables  A random variable is a function that can assign probabilities to events of interest in a random  experiment.  Discrete Random Variables  A discrete random variable is deﬁned by a Probability Mass Function (PMF). Each discrete outcome is  assigned a probability such that the probability of all possible outcomes sums to 1.  Cumulative Distribution Functions (CDF) deﬁne the probability that a random variable wll be less  than or equal to a given value. That is, the sum of the PMF for each random variable less than or  equal to the given outcome.  Continuous Random Variables  Probability Density Functions (PDF) deﬁne continuous random variables. The probability that the  random variable is between two values is deﬁned by the area under the PDF curve between those  limits.  Central Limit Theorem  The central limit theorem states that large random samples from a suﬃciently large population than  the means will be approximately normally distributed. That is, increasing the number of random  variables will see a distribution approach Normal.  Data Wrangling  Data wrangling describes the process of preparing a set of data so that it can be analysed. Its goals  are to make sure any data used for analysis is valid, and able to be compared eﬀectively. Key aspects  of data wrangling are to:  Correcting errors in data  Addressing incomplete values  Formatting the data  Normalising data and removing outliers  Data aggregation  Combination diﬀerent sets of data  Images as Data  In order for a computer to understand an image it must be converted into data.  This is done by dividing the image into a series of smaller rectangles, and storing the features of  these rectangles in a vector.  The size of the vector is given by the number of rows x number of columns.  The features may be the average color of the block, shape, brightness or diﬀerence from light to  dark.  For each of the blocks we can then compute statistics to use for modelling.  For multiple images a feature matrix with dimensions rows x columns x number of features x number  of images gives the dimensions of the feature matrix.  Text Data Representation  Text can be represented using feature vectors, where features are the number of times that speciﬁc  words are used in a document. This representation is known as 'bag of words' representation.  These feature vectors can be added to form a matrix that characterises frequency of words by  document.  Data vs Signal  Data is information that has been collected. It may be about a range of topics such as purchase  history, social networks or other information that has been gathered in a particular area.  Data may be quantative (eg. the speed of a vehicle), categorical (eg. the car is blue) or qualatitive  (the car is stylish). Quantative values can be discrete or continuous.  Data can be represented in diﬀerent ways in order to provide diﬀerent insights or useful conclusions  for a consumer. This is the crux of data analysis.  A signal is a quantity that varies with regard to another parameter such as space or time.  Signals can be represented using functions which describe how the dependent variable will change  with regard to the independent variable.  Signals can be visualised to support analysis of how the variables relate to each other.  Encoding and Distribution  Encoding. Encoding is the process of enumerating categorical values as integer values so that a  computer can understand them.  Encoding often takes the following forms:  Ordinal Encoder. An ordinal encoder converts inherently ordered categories to integers. For  instance, the award from a race of gold, silver, bronze could be encoded as 1,2,3.  Label Encoder. A label encoder applies integer values to unordered categories such as a range  of colours. When used for machine learning problems, this can create problems as the model  may determine an ordinal relationship where none exists.  One-Hot Encoder. A one hot encoder uses boolean features to indicate the category that a  data item belongs to. It has the advantage of avoiding an invalid order to data items, however  increases the amount of features that must be analysed.  Data Distribution. The distribution of data is the manner in which dataset values are spread across a  given characteristic. That is, how many of the data items have the given characteristic across the set.  Distributions tend to conform to the random variable distributions discussed earlier - frequently as a  normal distribution.  Scaling and Normalisation  Scaling is the process of changing a series of values within a range to a corresponding value in a new  range. This can be useful to allow like comparison of diﬀerent features in a data set.  Normalisation. Normalisation scales many series of values to a common range. Typically the range of  0-1 is used. This allows comparison across the diﬀerent series of values. This type of normalisation is  as follows:  =v′  v − min(v) max(v) − min(v)  Statistics in Data Science  There are a range of techniques to apply the statistical analysis discussed above to data sets.  The scipy stats library provides facilities for applying distributions such as the CDF which can then be  visualised using the matplotlib library.  The pandas library oﬀers facilities to import date and store it in frames where it can be cleaned and  analysed.  The sklearn library oﬀers encoding and scaling functions that can be applied to data frames in order  to prepare them for use in models.  