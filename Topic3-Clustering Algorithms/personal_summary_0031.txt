This topic was a high-level overview of machine learning and some of its real-world applications. The following points were covered:  Statistics recap -  Random experiments – experiments for which the outcome cannot be  predicted with any level of certainty.  Definition of an event, a sample space and the probability and joint  probability of event/s.  Conditional probability – i.e., the probability of an event occurring given that another, related event has already occurred. Bayes Rule to calculate conditional probability.  Random variables – both discrete and continuous. Probability distributions – Bernoulli distribution for discrete, binary variables. Uniform distribution for both discrete and continuous variables. Normal distribution for continuous variables.  Data Wrangling  The second step in building a machine learning model, after the sourcing  data.  Data wrangling is about getting the data into a format that can be fed  into a machine learning model.  Data wrangling may involve the following steps:  Checking for missing data and either filling the missing values or  dropping the rows/columns Combining multiple data sources Normalizing the data (features with differing scales can cause  poor performance in the machine learning model)  Encoding the data Correcting errors or inconsistencies in the data Aggregating data Plus other steps as necessary  Identifying and removing outliers   Identifying how images and text can be represented as data so that they can be fed into a machine learning model  Encoding of data – categorical data will need to be encoded for many  applications of machine learning so that the computer can make sense it.  Useful python libraries for data wrangling  Pandas Numpy Scikit Learn Matplotlib for visualization of the data  