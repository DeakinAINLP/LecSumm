Foundations of ML  The focus of this lesson was to revise knowledge of Linear Algebra and probability.  Feature Vectors and Matrices.  In Machine learning, we use data for different processes including training models. The data we get are in different forms such as signals, audio, text and images.  To apply Machine Learning we need to transform data into a form that the machine can understand. Therefore, we need to convert those data into a numbered format. That’s when Vectors and matrices can be used. Vectors can be used to store rows of data. For example, data of several feature attributes of a patient can be stored in a vector. And data on feature attributes of several patients can be stored in a matrix.  Text data can be represented as a Feature Vector. A feature Matrix is a combination of feature vectors.  Probability Concepts  Probability concepts are used to explain the tendency of an event happens. When talking about the probability concepts the following concepts are discussed in this lesson.  Random experiment  An event whose outcome cannot be predicted.  Event  Set of outcomes of a random experiment  Sample scape  All possible outcomes of a random experiment  Joint probability  Probability of happening two or more independent events  Conditional Probability  Probability of happening an event given another event happened. The event depends on  the happening of another event.  Bayes Rule  Bayes rule describes the probability of an event A based on another event B that is related  to A.   Random Variable  Random variables take on different possible values randomly with each possible value having a corresponding probability of occurrence.  Random variables are used to model random phenomena.  Random variables can be discrete or continuous. For example, discrete values are the faces of dice and for continuous values height of a person.  Discrete Random Variable  Probability Mass Function (PMF)  Mapping each value of the random variable to the probability of that value.  The sum of the probability of all the values for a random variable is 1.   Distribution of Random Variable  ● Bernoulli distribution  Discrete distribution and defined for a binary random variable with values X=0 and X=1  ● Uniform distribution  Can be defined for both discrete and continuous random variables.  ● Normal distribution  Defined for continuous random variables  It is the most popular distribution  Data wrangling  Data wrangling is the process of cleaning, transforming, and organizing a dataset to make it suitable for analysis.  Missing value replacement  This is a part of data wrangling. As Machine Learning models can’t process null values, we need to replace null values with meaningful values. This can be done in two ways.   Replace with immediate value   Replace with the mean or median value of the row  Mean value is used to replace continuous values and the median is used for discrete values. Mean can be affected by outliers and median is an effective way to replace as it is not affected by outliers.  Scaling and Normalization.  Normalization is a scaling technique used to transform the values of a dataset into a common range which is often done to improve the performance of machine learning algorithms.  Encoding  Encoding is used to give numerical values to categorical variables. There are different encoding approaches available.  One-Hot Encoding: This method creates a binary feature for each category in the categorical variable. Very common technique. Label Encoding: This method assigns a numerical label to each category in a categorical variable. Binary Encoding: This method creates binary features by encoding each category as a binary string. Ordinal Encoder. Assign a numerical value to each category based on its order.  Summary  ● Feature Vectors and Matrices ● Probability Concepts ● Random Variable ● Discrete Random Variable ● Continuous Random Variable ● Distribution of Random Variable ● Data wrangling ● Missing value replacement ● Scaling and Normalization ● Encoding  