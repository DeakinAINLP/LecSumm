Topic 2 Summary  1.1  Probability Many machine learning algorithms require the use of the concept of probability. In probability, an event is a set of results from a random experiment (an experiment or process that cannot predict results with certainty). Joint Probability The  joint  probability  of  two  independent  events  is  equal  to  multiplying  their  independent probabilities. Conditional Probability The probability of occurrence of event A under the precondition that another event B occurs. The probability that A will occur under the condition that B occurs. Bayes Rule Mathematical  rules  that  apply  observed  phenomena  to  modify  subjective  judgments  about probability distributions.  1.2  Random variables Random variables function that is usually divided into two types, discrete random variables and continuous random variables. Discrete random variables The probability mass function (PMF) will assign a probability to each possible value, with the sum of  the  probabilities  being  1.  And  the  cumulative  probability  can  be  calculated  through  the cumulative distribution function (CDF). Continuous random variables The probability density function (PDF) defines the probability distribution of continuous random variables, with an integral of 1. And the probability assigned to any exact value in continuous space is 0.  1.3  Distributions of random variables A function that associates each result of an experiment with its probability of occurrence Bernoulli distribution A discrete distribution commonly used to define binary random variables with values Uniform distribution Uniform distribution can be defined for both discrete and continuous random variables. Normal distribution Normal  distribution  can  be  used  to  define  continuous  random  variables.  And  many  natural phenomena are approximately following a normal distribution. Central limit theorem if there is a population with mean  ùúá  and standard deviation  ùúé   and take sufficiently large random samples  from  the  existing  population,  then  the  distribution  of  the  sample  means  will  be approximately normally distributed.  1.4  Data Wrangling Data  Wrangling  often  involves  dealing  with  confusing,  unstructured,  and  incomplete  data,  also known as data munging. Data Wrangling is the process of cleaning up, transforming, and organizing datasets to fit into analysis. Is an indispensable step in the process of data science.  1.5  Images as data Find features in pictures that can be represented by numbers and create a model to build a feature matrix.  1.6  Text data representation Because computers can only understand numbers, they need to first convert text to numbers for processing. Similarly, the text needs to be converted to a feature matrix or feature vector before it can be processed.  1.7  Data VS signal Data are symbols that record and identify objective events. Signals are symbols that convey specific information. Signals are another form of data. Data becomes information when presented in an organized form.  1.8  Encoding and Distribution Encoding is the conversion of classified values that the computer does not understand into integer values that can be processed. Distribution refers to the distribution of values in a dataset, which can affect the performance of machine learning algorithms.  1.9  Scaling and Normalization Scaling is the process of converting a set of values into new values within a range. Normalization is also a scaling that improves the performance of machine learning algorithms by normalizing values in a dataset.  1.10 Data wrangling with python Implement the above using Python  