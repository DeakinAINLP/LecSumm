 Firstly we learnt the basic deﬁnitions that are used in data wrangling. Examples include A random experiment, event, probability, joint probability, conditional probability and Bayes rule.   A random experiment outcome cannot be predicted with certainty, such as a coin toss, rolling dye etc.   An event is the outcome of the random experiments.   Probability is the measure in which an event is likely to happen.   Joint probability is when two events occur as well.   Conditional probability, when an event happens the likelihood of another event occurring because of the other event.   Bayes rule is simple in that It explains an event a based on another event b related to a.   We then learnt about random variable, which are just values generated randomly based on nothing. There are two types, continuous random variables are continuous to inﬁnity’s, such as time. And Discrete random variables that have countable value, such as counting cards.   The distribution of random variables is used in data wrangling, such Bernoulli, which is considered a “Discrete distribution” and used for binary values such as 0 and 1. Uniform distribution is for continuous and discrete random variables, and normal distribution is used for continuous only.  In these distribution among large data sets the central limit theorem states that each distribution will be approximately normally distributed.   Data wrangling essentially is transforming the dataset or cleansing it, to prepare for a format that is suitable for analysis.   Feature extraction involves identifying and selecting relevant features or attributes from a dataset to use in analysis. The objective of feature extraction is to decrease the dataset's dimensionality while maintaining important patterns.   Distribution describes how a dataset's values are dispersed or grouped around a core value. Means, medians, modes, standard deviations, and skewness are often used metrics to describe the distribution of a dataset. For data analysis, we may also utilise visualisations like histograms, box plots, and density plots.   To normalise the range of values across several characteristics or variables, scaling is frequently used. In order to make numerical data more comprehensible for analysis and modelling, scaling is the act of converting the data to a certain range, usually between 0 and 1.   Normalisation is a scaling technique used in order to eliminate the eﬀects of various scales of measurement and enhance the performance of particular machine learning algorithms or statistical tests, and is the process of rescaling numerical data to a common scale, often to a range between 0 and 1.  