Main points covered in Topic 2  1.  Mathematics  and  machine  learning!  Statistics  is  at  the  core  of  machine  learning  as everything  is  associated  with  numbers  in  machine  learning  models.  Especially,  the aggregation  functions  play  a  major  role  in  determining  the  nature  of  the  data.  Key functions are,  a.  Mean - Computes the average value of a set of numbers. b.  Median - Computes the middle value of a set of numbers. c.  Mode - Computes the most frequently occurring value in a set of numbers. d.  Maximum/Minimum - Computes the largest or smallest value in a set of numbers. e.  Variance - Measures the degree of spread of a set of numbers. f.  Standard  deviation  -  Measures  the  degree  of  spread  of  a  set  of  numbers,  normalized by the mean.  g.  Percentile - Computes the value below which a certain percentage of observations  fall.  h.  Quartile - Computes the value that divides a dataset into quarters. i.  Range - Computes the difference between the largest and smallest value in a set  of numbers.  j.  Skewness - Measures the degree of asymmetry in a set of numbers.  2.  Probability – On the other hand, is the fundamental concept in machine learning that is used  to  quantify  uncertainty  and  randomness  in  data.  It  provides  a  way  to  make predictions and estimates based on incomplete or noisy information. In machine learning, probability is used to model the likelihood of events and to make decisions based on those models.  In  supervised  learning,  probability  is  used  to  model  the  relationship  between  input features  and  output  labels.  Probabilistic  models  can  be  trained  using  labeled  data  to predict the probability of a certain output given a set of inputs. Similarly, in unsupervised learning, probability is used to model the underlying distribution of data. Some of the key concepts are random experiment, event-based, joint probability, conditional probability, bayes rule.  Random Variables: A random variable is a variable that takes on different values depending on the outcome of a random event. It is a mathematical function that maps the outcomes of a random process to a set of possible values. Random variables can be either discrete or continuous.  A discrete random variable takes on a countable number of possible values. For example, the number of heads obtained in a series of coin tosses is a discrete random variable, as it can only take on the values 0, 1, 2, 3, and so on. Whereas a continuous random variable can  take  on  any  value  within  a  certain  range.  For  example,  the  height  of  a  randomly selected person is a continuous random variable, as it can take on any value between 0 and infinity.  3.  Distributions –  Normal distribution - A bell-shaped distribution where skewness can lie between -0.5 to 0.5  and  is  symmetrical  around  the  mean,  with  most  data  points  clustered  around  the center. Binomial distribution: A distribution that models the probability of success or failure in a fixed number of independent trials. Poisson  distribution:  A  distribution  that  models  the  probability  of  a  certain  number  of events occurring in a fixed time or space interval. Uniform distribution: A distribution where all values have equal probability of occurring. Exponential distribution: A distribution that models the time between events occurring in a Poisson process.  4.  Data Wrangling – This involves data cleaning such as fixing erroneous data, duplicate data  and handling missing values.  5.  Images as data – Images are divided into smaller groups and aggregation functions are  applied on it.  6.  Text  data  representation  –  Similar  to  images  as  data,  text  can  also  be  represented  as  numbers in a matrix and be obtained as cases and features.  7.  Encoding – Encoding is the approach in brining the categorical data into numerical data.  There are several types of it and different types are useful in different scenarios.  8.  Scaling & Normalization – Scaling is important to bring the data in to a unit interval and normalization  is  required  to  bring  the  data  a  symmetric  format  with  which  a  machine learning model can perform better  