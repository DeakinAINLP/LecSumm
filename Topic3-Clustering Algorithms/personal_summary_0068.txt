 This topic focused on probability, along with some concepts regarding data wrangling. In terms  of probability, we are essentially trying to measure the likelihood of occurrence for an event (such as  rolling a 1 on a 6-sided dice or getting a tail when flipping a coin). Joint probability was covered which is  basically the joint probability of multiple events occurring independently. As an example, we can have  two coins and work out the probability of getting heads for both flips by multiplying the individual event  probabilities. We also have conditional probability which can be summed up as the probability of an  event occurring given another event occurring.  Also, we have Byes rule which in the seminar, was  explained through an example. Here, we have two boxes full of balls which can be either red or blue and  we have drawn a red ball. We can now use Byes rule to work out the probability of this ball being drawn  from box 1. So, in summary, my understanding of this is we are using this rule to work out probabilities  based on prior knowledge/information.  The next main topic was around random variable which can be summed up as variables that take  random values with the purpose of modelling uncertainty. We can have discrete random variable which  are essentially covering countable values and we can also have continuous random variable for values  that operate on an infinite spectrum (essentially things that would utilize floating point values such as  height, weight etc….). With discrete random variable, we use something known as the Probability Mass  Functions which maps the probability to values of the variable which will ultimately lead to a sum of 1.  Using the example from the seminar, we want to see the probability of getting a head when we flip a coin  3 times. This can result in 8 possibilities and here, we can map our variables such as x = 0 where x was  defined earlier where we get a head and 0 represents the chance of not getting a head at all which would  be 1 out of 8. In x = 1, we are seeing the probability of getting 1 head which within this space, would have  a 3 out of 8 chances of occurring. If we keep going to x = 3, all of the probabilities obtained will equate to  1.  To finish this summary, I’ll add in the information regarding data wrangling, which is a process to  clean, transform and organize a given data set so that it can be appropriately analysed. Essentially,  dealing with raw data directly can impact the model due to potential outliers and missing values. In terms  of missing values, some techniques that can be used include potentially deleting aspects of the data  (depends on the nature of the missing values) or replacing the missing value with an immediate value  such as values next to the missing value or mean/median value. Another key concept is scaling and  normalisation which involves scaling data to a common range such as 0 - 1.This can be good if we have  values will large differences in the data set and help with bias and will overall, help improve performance  of the algorithms in machine learning. Finally, we have non-numerical data encoding which involves  turning categorical values to numerical using techniques such as label encoder.   