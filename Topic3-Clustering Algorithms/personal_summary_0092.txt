Summary  Statistics:  I. Probability is essential tool in ML. And in probability we define an event as a set of outcomes of a random experiment.  The probability of an event A occurring is denoted as P(A) and P(A) = 1 – P(A) is the probability of A that is not occurring. We also have joint probability where if two events, A and B are independent, then the joint probability is P (A and B) = P(A) P(B). Adding more, conditional probability P(A|B) read as the probability of A given B is defined as:  where P(B) is not zero. Bayes Rule: describes the probability of an event A based on another event B which is related to A.  II.  Random variables: is a function that can assign probabilities to events of interests in a random experiment. And there are 2 types of random variables: discrete (countable) and continuous (infinite continuum). -  Discrete random variables are defined using Probability Mass Functions (x). In some cases, we must work with Cumulative Distribution Function (CDF) which gives us the cumulative probability associated with a function.      -  Continuous random variables are defined using Probability Density  Functions f(x).  Distributions of random variables:  III. -  Bernoulli distribution: is a discrete distribution and defined for binary  random variable with values X = 0 and X = 1.  -  Uniform distribution: can be defined for both discrete and continuous  random variables.  -  Normal distribution: is defined for continuous random variables. This is  by far the most popular distribution (since many natural phenomena are approximately following a normal distribution) and illustrated as:  -  Central limit theorem: states that if you have a population with mean  and standard deviation , and take sufficiently large random samples from the existing population, then the distribution of the sample means will be approximately normally distributed.  IV.  Data wrangling: is also known as data munging which is the process of transforming and mapping raw data into another format with the intent to make it suitable for analysis. Images as data: since computers understand numbers, the first step is to find the features that can be represented with numbers in an image. Images are represented in numerical vector of features for computer algorithms to take them as input data.  V.     VI.  Text data representation: The occurrences of words represent as feature vectors of documents. And those feature vectors are combined into feature matrices where each instance (document) is put on rows (or columns).  