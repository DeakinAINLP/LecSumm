Topic 2 for Machine Learning majorly dealt with Data Wrangling and how to handle diﬀerent issues while performing diﬀerent data cleansing ac=vi=es.  Before we could go into Data Wrangling, we went through the following topics –  1.  Feature Vectors and Matrices 2.  Probability concepts 3.  Random Variable  Then we learnt about Feature Vectors and how it is comprised of the following –   Vector space model is a representa=on of set of documents as vectors   Text data representa=on as Feature Vectors  We learnt how to calculate the Euclidean distance and cosine distance between two Feature Vectors. If we have mul=ple Feature Vectors, then we can form a Feature Matrix, where each dataset is a vector in the matrix.  We next covered the following Probability concepts  1.  Random experiment and Event  a.  An experiment or a process for which the outcome cannot be predicted with  certainty  b.  Event is set of all outcomes of a random experiment  2.  Joint probability  a.  Here we covered the concepts of dependent and independent events and  how they are related to each other mathema=cally  3.  Condi=onal probability 4.  Bayes Rule  Next, we covered the concepts for Random Variables such as –  1.  Discrete random variables – where we have a countable number of values  a.  Probability Mass Func=on/PMF – this deﬁnes probability for each random  variable  b.  Cumula=ve Distribu=on Func=on/CDF – this deﬁnes the cumula=ve or added  probability associated with a func=on  2.  Con=nuous random variables – where we have a con=nuous range of values  a.  Probability Density Func=on/PDF – the diﬀerence between Discrete and Con=nuous is that summa=on is replaced with integral sign in case of con=nuous random variable  3.  Distribu=on of random variables  a.  Bernoulli Distribu=on  i.  Only two possible outcomes  b.  Uniform Distribu=on  i.  Same value for all  c.  Normal Distribu=on  i.  Mean = Median = Mode  d.  Central Limit Theorem  i.  As the data popula=on increases, the data points will tend to aggregate towards the centre of the data  Finally, we learnt about Data wrangling and the following concepts –  1.  Missing value replacement – this is important since machine cannot understand null  values  a.  Replace with immediate value b.  Replace with mean/median value of row  2.  Scale or normalisa=on – replace the en=re dataset with values between 0 and 1  a.  Min-max normalisa=on  3.  Non-numeric data encoding – this is needed since machine cannot understand non-  numeric data  a.  Ordinal Encoder (how you deﬁne the hierarchy and assign values to each  category in the hierarchy)  b.  One-hot Encoder (posi=onal encoding) c.  Label Encoder (assign value to each category)  