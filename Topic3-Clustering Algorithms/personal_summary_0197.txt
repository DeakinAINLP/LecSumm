A large factor in the use of machine learning algorithms is probability. Within the scope of probability, random experiments are a way of describing occurrences which lead to uncertain outcomes, or ‘events’. The likelihood of these events occurring can be represented within the range between 0 and 1, and this is what probability describes in mathematical terms.  Probability  There are variations amongst the types of probability which can be calculated. Joint probability, for example, describes the probability of two independent events both occurring, and can be calculated as the product of each event’s individual probability. Conditional probability, on the other hand, describes the likelihood of one event occurring, in relation to the occurrence of another event. This can be calculated as the product of the probability of both events, divided by the probability of the initial event (as long as the initial event has a probability greater than 0).  Random Variables  The possible values that result from random occurrences define what are called random variables, which can be either discrete or continuous. These vary in how they are mathematically derived from input values, with the distribution of discrete random variables being outputs of Probability Mass Functions, and the distribution of continuous random variables being outputs of Probability Density Functions.  Data Wrangling  Data Wrangling describes the process of preparing data for analysis, which involves cleaning and arranging the data into a more useful form. There are many issues which can arise throughout the process of collecting and collating data, such as incomplete or incorrect entries, or entries missing altogether. The purpose of data wrangling is to ensure that these issues with the data do not skew or otherwise influence the analysis being done on the data.  Feature Extraction  A fundamental step in preparing data for analysis is to use the raw data to obtain useful pieces of information on a variable, known as a feature. Features will often be represented in their own ‘feature vectors’, which will often subsequently be merged together to form a ‘feature matrix’. The data being represented in this form makes it very useable by computers for analysis.  Data  Data can be described as a recorded measurement or observation of an occurrence or phenomenon. This can be recorded as numbers, text, or through many other mediums. Signals are one way of recording data as they represent the occurrence of a phenomenon (e.g., amplitude), and variations in that occurrence over another variable (e.g., time). Once data has been properly cleaned, classified and is usable, it is considered organized, and is hence considered information.  Encoding  Not all features are initially represented numerically, and therefore need to be encoded into integer values in order to be useful for computational analysis, which can be achieved through a variety of encoding techniques.  Distribution  A major factor in the performance of Machine Learning algorithms is the distribution of the values in the dataset. There are some there are some broad classifications that can be identified based on the distribution of a dataset, such as a uniform distribution, skewed distribution, or normal distribution (which is the most common). When a dataset has evenly spaced values across the data, it can be described as having a uniform distribution. Inversely, a skewed distribution would describe a dataset with values clustered in a particular section of the range. A normal distribution will have values which are symmetrical across the mean of the dataset.  Scaling & Normalisation  Scaling describes the process of mapping values from a particular range to a new range, with the intention of ensuring no values disproportionately skew the analysis of the data set, simply as a function of being represented by larger values.  When the values of a dataset are scaled into a common range, this is known as normalisation and is intended to improve the performance of machine learning algorithms.  