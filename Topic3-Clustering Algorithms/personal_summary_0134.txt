 In the topic one online class, we first revised our knowledge of linear algebra and probability.  We  discussed  feature  vectors  and  feature  matrices,  probability  concepts  such  as  random  experiments & events, joint probability, and conditional probability. Our lecturer showed us a  couple of examples of how to calculate joint probability and conditional probability in different  scenarios.  Then  we  learned  about  Bayes'  theorem,  which  is  a  fundamental  concept  in  probability  theory  that  allows  us  to  update  the  probability  of  an  event,  based  on  new  information or evidence.  During the second half of the class, we learned about random variable types of random variable  including discrete and continuous random variables.  Under discrete random variable, we go  through what Probability Mass Function and Cumulative Distribution Function are and went  through some dice experiment examples related to those functions. Next, we go through some  distributions of random variables such as Bernoulli distribution, uniform distribution, normal  distribution,  and  central  limit  theorem.  After  that,  we  concentrated  on  what  data  wrangling  means and its processes including data loading and saving, data exploration, data processing  such as missing value handling, non-numerical data-encoding methods such as ordinal encoder,  one-hot encoder, and label encoder, etc., scaling and normalization  In this topicâ€™s workshop, we acquired hands-on experience related to python matrix functions  including inversing and transposing a matrix using the Numpy library, python functions, and  related operations, linear regression, Count Vectorizer, and TF-IDF Vectorizer and how those  work.  