Data wrangling, also known as data munging, is the process of cleaning, transforming, and organizing a dataset to make it suitable for analysis. This often involves a combination of manual and automated processes, and it is a crucial step in the data science pipeline.  Data wrangling can be a complex and time-consuming process, as it often involves dealing with messy, unstructured, and incomplete data. Some common tasks that are involved in data wrangling include:  Identifying and correcting errors and inconsistencies in the data  Handling missing or incomplete values  Combining multiple datasets  Converting the data into a format that is suitable for analysis  Identifying and removing outliers  Normalizing the data  Aggregating the data into useful summary statistics  Data wrangling is the procedure of acquiring, analysing, and manipulating raw data into a suitable format for faster processing and evaluation.Data wrangling can be divided into the following section: Data loading and saving Data exploration Data processing, such as missing value handling, encoding categorical values Data distribution Scaling Random data into pandas dataframe import pandas as pd import numpy as np df = pd.DataFrame(np.random.randint(0,2000,size=(10, 6)), columns=list('ABCDEF')) df.head() The output of this code is as follows: Saving dataFrame Data wrangling is the procedure of acquiring, analysing, and manipulating raw data into a suitable format for faster processing and evaluation.Data wrangling can be divided into the following section: Data loading and saving Data exploration Data encoding , distribution and  Scaling Categorical value encoding from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() integer_encoded = label_encoder.fit_transform(df2["Types"]) df2["interger label"]=integer_encoded The output of this code is as follows: Before After Data distribution import matplotlib.pyplot as plt df2[['TV', 'Radio', 'Newspaper', 'Sales']].hist() plt.tight_layout() plt.show() The output of this code is as follows: Scaling Before scaling mean value of each feature print("--------------------------") print(df2[['TV', 'Radio', 'Newspaper', 'Sales']].mean()) print("STD:") print("--------------------------") print(df2[['TV', 'Radio', 'Newspaper', 'Sales']].std(ddof=0)) After scaling the mean value of each feature df3=df2.copy() df3[['TV', 'Radio', 'Newspaper', 'Sales']]=(df3[['TV', 'Radio', 'Newspaper', 'Sales']]-df3[['TV', 'Radio', 'Newspaper', 'Sales']].mean()) /df3[['TV', 'Radio', 'Newspaper', 'Sales']].std(ddof=0) print("--------------------------") print(df3[['TV', 'Radio', 'Newspaper', 'Sales']].mean()) print("STD:") print("--------------------------") print(df3[['TV', 'Radio', 'Newspaper', 'Sales']].std(ddof=0)) df3[['TV', 'Radio', 'Newspaper', 'Sales']].hist() plt.tight_layout() Data wrangling is the procedure of acquiring, analysing, and manipulating raw data into a suitable format for faster processing and evaluation.Data wrangling can be divided into the following section: Data loading and saving Data exploration Data processing, such as missing value handling, encoding categorical values Data distribution Scaling Data exploration import pandas as pd df=pd.read_csv("data/Advertising.csv") df.info() The output of this code is as follows: <class 'pandas.core.frame.DataFrame'> RangeIndex: 200 entries, 0 to 199 Data columns (total 5 columns):  #   Column     Non-Null Count  Dtype   ---  ------     --------------  -----    0   TV         198 non-null    float64  1   Radio      198 non-null    float64  2   Newspaper  199 non-null    float64  3   Sales      197 non-null    float64  4   Types      200 non-null    object  dtypes: float64(4), object(1) memory usage: 7.9+ KB Data descriptions such as mean, std df.describe() The output of this code is as follows: Find the missing values and replace them with the previous data point print("------------------------Before-------------------------------------------------------------") print("Check any values are null-->",df.isnull().values.any()) print("-------------------------------------------------------------------------------------") print("How many values are null in individual columns or attribute\n",df.isnull().sum()) print("-------------------------------------------------------------------------------------") df2=df.copy().bfill() print("---------------------- After ---------------------------------------------------------------") print("Check any values are null-->",df2.isnull().values.any()) print("-------------------------------------------------------------------------------------") print("How many values are null in individual columns or attribute\n",df2.isnull().sum()) print("-------------------------------------------------------------------------------------") Add small picture of type of data?  What is Data?  To most used term nowadays is “Data”. From Facebook to Google, from big to small, all the companies are using the data from their user/customers. what are they really using? They are using the information shared by the users, the purchase history, the places, or the hobbies shared by the user. In a word, whatever a user is sharing, everything is considered data when they are stored in the form of observation or measurement, which is represented as text, number, or media. However, not all of them are useful to certain companies or stakeholders. The meaningful representation of data can generate new insight, which can help a company to take certain decisions. To understand the importance of the data, check out the following video: Your data is your currency | Mozhgan Tavakolifard | TEDxArendal  What is Signal?  A signal is commonly referred to as a sign or gesture which conveys certain information. However, in digital electronic or signal processing, the signal is considered as the quantity that varies over a parameter such as space or time. We are all aware of the audio signal or heartbeat signal, which are quite common in our daily life.  The audio signal is the electrical representation of sound.   The following equation represents the signal, where x is the independent variable, and f is the dependent variable.   f(x) = -ax^2+bx+c Depending on the value of the shape of the signal will vary; if a>0, the signal will be a downward parabola, a=0, the signal will be a straight line, and if a<0, it will be an upward parabola.   Difference between data and signal?  When any data is represented in an organized manner, that becomes information. To learn more about data representation, check out the following video: What is Data?  Signal is another form of data. For example, using an ECG sensor device we can capture the electrical impulses of the heart and produce signals to represent ECG signals which can be plotted over time. This signal can be interpreted by the physician to understand the issue of the heart, if any.   © Getty Images Distributions of random variables A simple explanation of a probability distribution is that it is a function that links each outcome of a statistical experiment with its probability of occurrence. Let’s look at some of the most important distributions: Bernoulli distribution Bernoulli distribution is a discrete distribution and defined for a binary random variable with values X=0{"version":"1.1","math":"\(X=0\)"} and X=1{"version":"1.1","math":"\(X=1\)"}. So π(0)=P(X=0)=p{"version":"1.1","math":"\(\pi(0) = P(X=0) = p\)"} and π(1)=P(X=1)=1−p{"version":"1.1","math":"\(\pi(1) = P(X=1) = 1-p\)"}. Sometimes, we use notations: π(x)=B(x∥p){"version":"1.1","math":"\(\pi(x) = B(x\|p)\)"} or x∼B(x∥p){"version":"1.1","math":"\(x \sim B(x\|p)\)"} where B{"version":"1.1","math":"\(B\)"} means Bernoulli. For example we can say a distribution over the outcome of an exam is Bernouli. We may pass (x=1){"version":"1.1","math":"\((x=1)\)"} or fail (x=0){"version":"1.1","math":"\((x = 0)\)"}. Uniform distribution Uniform distribution can be defined for both discrete and continuous random variables. For a discrete random variable:  π(xi)=P(X=xi)=1N,i=1..N{"version":"1.1","math":"\pi(x_i) = P(X=x_i) = \frac{1}{N}, \quad i=1..N"} We denote π(x)=U(x∥N){"version":"1.1","math":"\(\pi(x) = U(x\|N)\)"} or x∼U(x∥N){"version":"1.1","math":"\(x\sim U(x\|N)\)"} where U{"version":"1.1","math":"\(U\)"} means uniform. The same concept is applied to continues space. For a continuous random variable: f(x)=1b−a,a≤x≤b{"version":"1.1","math":"f(x) = \frac{1}{b-a}, a \leq x \leq b"} We denote f(x)=U(x∥a,b){"version":"1.1","math":"\(f(x) = U(x\|a,b)\)"} or x∼U(x∥a,b){"version":"1.1","math":"\(x \sim U(x\|a,b)\)"} where U{"version":"1.1","math":"\(U\)"} means uniform. Rolling a fair dice follows a uniform distribution (discrete space). Normal distribution Normal distribution is defined for continuous random variables. It is by far the most popular distribution. For a continuous random variable normal distribution is defined as: N(x∥μ,σ2)=1(2πσ2)12exp{−12σ2(x−μ)2}{"version":"1.1","math":"\mathcal{N}(x\|\mu,\sigma^2) = \frac{1}{(2\pi\sigma^2)^\frac{1}{2}} exp \Big\{ -\frac{1}{2\sigma^2} (x-\mu)^2\Big\}"} We denote f(x)=N(x∥μ,σ2){"version":"1.1","math":"\(f(x) = N(x\|\mu,\sigma^2)\)"} or x∼N(x∥μ,σ2){"version":"1.1","math":"\(x \sim N(x\|\mu,\sigma^2)\)"} where N{"version":"1.1","math":"\(N\)"} means normal. Figure. Illustration of a normal distribution. One of the reasons for the popularity of the normal distribution is that many natural phenomena are approximately following a normal distribution. Central limit theorem The central limit theorem states that if you have a population with mean μ{"version":"1.1","math":"\(\mu\)"} and standard deviation σ{"version":"1.1","math":"\(\sigma\)"}  and take sufficiently large random samples from the existing population, then the distribution of the sample means will be approximately normally distributed. So, The distribution of the sum of N{"version":"1.1","math":"\(N\)"} i.i.d. random variables becomes increasingly normal (Gaussian) as N{"version":"1.1","math":"\(N\)"} grows. Figure 4 illustrates N{"version":"1.1","math":"\(N\)"} random variables, following central limit theorem. Unlike features with quantitative value, some features contain categorical values which the machine cannot understand. To solve this, encoding techniques are used to convert to integer values. For example, a dataset of student with features campus [“Burwood”, “Warun Ponds”] and school [”Architecture and Built Environment”, ”Engineering”, ”Information Technology”, ”Life and Environmental Sciences”] can be coded as [0,1] and [0,1,2,3]. For instance, [” Burwood”, ”Information Technology”] could be expressed as [0,2] while [”Warun Ponds”, ” Life and Environmental Sciences”] would be [1,3].  There are several well-known techniques of encoding such as OrdinalEncoder, One-Hot Encodings, and LabelEncoder. Distribution  Distribution refers to the way that the values in a dataset are distributed. This can be important, as the distribution of the values can have a significant impact on the performance of a machine learning algorithm.  There are many types of distributions that can occur in a dataset, including normal, uniform, and skewed distributions. The normal distribution is the most common, characterized by a bell-shaped curve symmetrical around the data's mean. A uniform distribution is one in which the values are evenly distributed across the range of the data, while a skewed distribution is one in which the values are not evenly distributed and are instead concentrated on one side of the range.  In general, the distribution of the values in a dataset can affect the performance of a machine learning algorithm in many ways. For example, if the values are not evenly distributed, this can cause the algorithm to be biased toward certain values, which can lead to poor performance. On the other hand, if the values are normally distributed, this can make it easier for the algorithm to learn and make predictions.   Imagine you are studying the heights in cm of a few students taking SIT720 course (168, 188, 167, 155, 170, 166, 179, 168, 167, 166, 164, 169, 170, 159, 163, 175, 173, 173, 177, 190, 150, 160, 181, 165). If we plot the heights as a histogram and plot the distribution curve, we can see it creates a bell-shaped curve.   Using blocks on an image to extract features Feature extraction  In machine learning, feature extraction is one of the key steps. As we already know, that means a full representation of data is considered information. Likewise, for ML modelling, we need to derive a set of features from the raw data where each feature contains information on the target variable.   Let's see an example of feature extraction techniques to make the machine analyse and categorise images.   Let’s define a simple problem. The photo above is a lovely scene. It’s obvious to our human minds that it is an outdoor scene. The problem: how would you build an ML system that is able to identify whether a given image is indoors or outdoors? What about 1000 images? Or 10,000? We would have to create a set of programming instructions, an algorithm, to process each picture and make a decision. The more correct answers we get, the more accurate the results and the more the machine has learned. Computers understand numbers The first step would be finding features that can be represented with numbers. In order to use an image as input data for this computer algorithm, it needs to be represented in a numerical vector of features similar to the example described in the video in the last step. The computer’s instructions, and its algorithms will only be able to understand the image if you can feed the image in as a set of numbers. Creating a model We have to decide how to represent this image, and any other image, numerically. We can begin by dividing the image into smaller 9×15=135{"version":"1.1","math":"\(9 \times 15 = 135\)"}  blocks. For each block, we can compute features of our choice for example: color averaged across the block shapes within the block - number of straight lines and curves etc texture for example the amount of light/dark variation in the block radiance or brightness For these 135 blocks we can compute: mean variance other statistics So let’s say we extracted p{"version":"1.1","math":"\(p\)"} features per block which leads to 135p{"version":"1.1","math":"\(135p\)"} features per image. As a result, for n{"version":"1.1","math":"\(n\)"} images, the size of the Feature Matrix is 135p×n{"version":"1.1","math":"\(135p\times n\) "}. In this photo example there are 135 blocks, we have 1000 images to sort into indoor and outdoor and we have selected four ‘features’ to analyse. So the feature matrix is: 135×1000×4{"version":"1.1","math":"\(135 \times 1000 \times 4\)"}. Having turned the image into a set of numbers, the resulting feature matrix can be fed into a proper computer algorithm for classification into indoor or outdoor. Random variables A random variable, is a variable whose possible values are the generated outcomes of a random phenomenon. In other words, a random variable is a function that can assign probabilities to events of interest in a random experiment. For example, if we toss a coin the possible outcomes are head or tail. Let us define a random variable X{"version":"1.1","math":"\(X\)"} so that X=1{"version":"1.1","math":"\(X=1\)"} means head and X=0{"version":"1.1","math":"\(X=0\)"} means tail. The function is nothing but the mapping X=1{"version":"1.1","math":"\(X=1\)"} to head or X=0{"version":"1.1","math":"\(X=0\)"}   to tail. Now let’s say P(head)=0.6{"version":"1.1","math":"\(P(head) = 0.6\)"}, P(tail)=0.4{"version":"1.1","math":"\(P(tail) = 0.4\)"}. Then we can say P(X=1)=0.6{"version":"1.1","math":"\(P(X=1) = 0.6\)"}, P(X=0)=0.4{"version":"1.1","math":"\(P(X=0) = 0.4\)"}  This way a random variable can assign a probability to all possible outcomes of a random experiment. We usually deal with two type of random variables: Discrete random variables have a countable number of values. For example, faces of a dice, number of emails received in an hour etc. Continuous random variables can take values on a infinite continuum. For example, height of a person, time to failure etc. Both of these are discussed in more detail below: Discrete random variables Discrete random variables are defined using a Probability Mass Functions(PMF), denoted as π(x){"version":"1.1","math":"\(\pi(x)\)"}.  The PMF assigns a probability to each possible value of the random variable as π(x)=P(X=x){"version":"1.1","math":"\(\pi(x) = P(X=x)\)"} summing them to 1, i.e. ∑xπ(x)=1{"version":"1.1","math":"\(\sum_{x} \pi(x) = 1\)"}. Rolling a dice is a perfect example of random variables. But what if someone asks about the probability of rolling a dice and  getting a number less than 5? Figure. Rolling a dice is an example of discrete random variable. In such cases we have to work with Cumulative Distribution Function(CDF). The cumulative distribution function gives us the cumulative probability associated with a function. it is defined as: F(X)=P(X≤x)=∑xi≤xP(X=xi){"version":"1.1","math":"\(F(X) = P(X \leq x) = \sum_{x_i \leq x} P(X = x_i)\)"}.  As you can see in the figure, it is discontinuous at points ’s and constant in between. Figure. Cumulative distribution function in problem of rolling a dice. The probability of seeing a number equal or less than five is P(X≤5)=56{"version":"1.1","math":"\(P(X\leq5) = \frac{5}{6}\)"}.  Based on the main property of probability, we can say the probability of seeing a number greater than five is P(X>5)=1−56=16{"version":"1.1","math":"\(P(X>5) = 1-\frac{5}{6} = \frac{1}{6}\)"}.  Continuous random variables Continuous random variables are defined using Probability Density Functions (PDF), denoted as f(x){"version":"1.1","math":"\(f(x)\)"}. Probability density function is a statistical expression that defines a probability distribution for a continuous random variable. PDF assigns a probability to a range of values of the random variable as f(x)d(x)=P(x≤X≤x+dx){"version":"1.1","math":"\(f(x)d(x) = P(x \leq X \leq x+dx)\)"}  integrating to 1{"version":"1.1","math":"\(1\)"}. So we can say: ∫−∞+∞f(x)dx=1{"version":"1.1","math":"\(\int_{-\infty}^{+\infty} f(x) dx = 1\)"}. Probability assigned at any exact value is zero (in the continues space). But we can talk about probabilities over a range such as P(X≥a){"version":"1.1","math":"\(P(X \geq a)\)"}, P(X<b){"version":"1.1","math":"\(P(X < b)\)"} or P(c≤X≤d){"version":"1.1","math":"\(P(c \leq X \leq d)\)"}, etc. Activity Share your understanding of random variables. Can you determine some of the random processes around you? Why do you think they are random? Discuss these ideas in the discussion forum. In machine learning, scaling refers to the process of converting a set of values to a new range of values. In a dataset, we have several features. The raw or unscaled features can be of different ranges, which may cause problems while training a model. For example, if we have two features, height in feet and weight in pounds, we may find that weight will dominate the model more than height. This is not ideal for getting optimal results. Therefore, we need to scale both features in a certain range.  Normalisation   Normalization is a scaling technique used to transform the values of a dataset into a common range. This is often done to improve the performance of machine learning algorithms, as many algorithms operate better when the data is in a standardized range.  One common method of normalization is to scale the data to a range of 0 to 1, where 0 is the minimum value in the dataset and 1 is the maximum value. This is known as min-max normalization, and it is calculated using the following formula:  v'=v-min(v)max(v)-min(v){"version":"1.1","math":"<math xmlns="http://www.w3.org/1998/Math/MathML"><mstyle mathsize="24px"><mrow><msup><mi>v</mi><mo>'</mo></msup><mo>=</mo><mfrac><mrow><mi>v</mi><mo>-</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>v</mi><mo>)</mo></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>v</mi><mo>)</mo><mo>-</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo>(</mo><mi>v</mi><mo>)</mo></mrow></mfrac></mrow></mstyle></math>"} where v{"version":"1.1","math":"<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>v</mi></math>"}  is the original value, v'{"version":"1.1","math":"<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>v</mi><mo>'</mo></msup></math>"} is the normalized value   Statistics : basic definitions Probability plays a major role in many machine learning algorithms. A detailed discussion of probability is outside the scope of this course but let’s review the basic concept of probability. Let’s start with a few basic definitions. A random experiment. A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Some common examples are: toss of a coin roll of a dice counting the number of phone calls received on a mobile phone in a given duration daily temperature how many times a specific word appears in each document of a corpus (collection or body of writings) Event In probability we define an event as a set of outcomes of a random experiment. For a coin toss experiment, sample space Ω={′head′,′tail′}{"version":"1.1","math":"\(\Omega = \{'head','tail'\}\)"}.  And event A{"version":"1.1","math":"\(A\)"} could be either {′head′}{"version":"1.1","math":"\(\{'head'\}\)"} or {′tail′}{"version":"1.1","math":"\(\{'tail'\}\)"}.  For a dice roll experiment, sample space Ω={1,2,3,4,5,6}{"version":"1.1","math":"\(\Omega = \{1,2,3,4,5,6\}\)"} and event A={{1},{2},{3},{4},{5},{6}}{"version":"1.1","math":"\(A = \{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\}\)"}. Probability Probability is defined for an event and is the measure of the likelihood that an event will occur. It is quantified as a number between 0 and 1. The probability of an event A{"version":"1.1","math":"\(A\)"} occurring is denoted as P(A){"version":"1.1","math":"\(P(A)\)"}. The probability of an event A{"version":"1.1","math":"\(A\)"} not occurring is denoted as P(A¯)=1−P(A){"version":"1.1","math":"\(P(\bar{A}) = 1 - P(A)\)"} Joint Probability Probability can be defined jointly for more than one event. Consider a random experiment where we toss two coins. In this case the probability of seeing “heads for coin toss 1” and “heads for coin toss 2” is an example of two events. If two events, A{"version":"1.1","math":"\(A\)"} and B{"version":"1.1","math":"\(B\)"} are independent then the joint probability is P(A and B)=P(A)P(B){"version":"1.1","math":"\(P(A \ and \ B) = P(A)P(B)\)"}. Let’s work on an example. Assuming fair coins with an equal probability of the toss resulting in heads or tails, the probability of heads is 12{"version":"1.1","math":"\(\frac{1}{2}\)"}. So the probability of heads for the first toss is 12{"version":"1.1","math":"\(\frac{1}{2}\)"} and the joint probability of heads for the second toss is represented by:  {"version":"1.1","math":"P(\{headsFirstToss\} \ and \{headsSecondToss\}) = P(\{headsFirstToss\}) \times P(\{headsSecondToss\}) = \frac{1}{2}\times \frac{1}{2} = \frac{1}{4}\"} P(headsFirstToss and headsSecondToss)=P(headsFirstToss)×P(headsSecondToss)=12×12=14{"version":"1.1","math":"P(headsFirstToss \ and \ headsSecondToss) = P(headsFirstToss) \times P(headsSecondToss) = \frac{1}{2}\times \frac{1}{2} = \frac{1}{4}"} Conditional Probability The conditional probability is the probability of some event A{"version":"1.1","math":"\(A\)"}, given the occurrence of another event B{"version":"1.1","math":"\(B\)"}. Condition probability P(A|B){"version":"1.1","math":"\(P(A \vert B)\)"}, read as the probability of A given B is defined as: P(A|B)=P(A and B)P(B){"version":"1.1","math":"\(P(A \vert B) = \frac{P(A\ and\ B)}{P(B)}\)"} Provided P(B){"version":"1.1","math":"\(P(B)\)"}  is not zero. Bayes Rule The essence of most of Bayesian approaches are to provide a mathematical rule explaining how you should change your existing beliefs in the light of new occurrence. Bayes rule describes the probability of an event A{"version":"1.1","math":"\(A\)"} based on another event B{"version":"1.1","math":"\(B\)"} that is related to A{"version":"1.1","math":"\(A\)"}.   As an example: if cancer is related to age, using Bayes’ rule information about a person’s age can be used to more accurately assess the probability that the person has cancer. Bayes’ rule is mathematically stated as: P(A|B)=P(B|A)P(A)P(B){"version":"1.1","math":"\(P(A \vert B) = \frac{P(B \vert A)P(A)}{P(B)}\)"} in which P(B)≠0{"version":"1.1","math":"\(P(B) \neq 0\)"}. Activity Recognise that this review of probability goes from zero to one hundred very quickly. If you’re a real beginner, try this basic tutorial or do some of your own research at the point where the topics exceed your level of knowledge. Statistics Random variables A random variable is a variable whose possible values are the generated outcomes of a random phenomenon. In other words, a random variable is a function that can assign probabilities to events of interest in a random experiment. import numpy as np B = np.random.randn(4,3) print('An example of a random matrix is:') print(B) The output of this random variable looks as follows: An example of a random matrix is: [[-0.34746929  0.90072387  1.10052698] [-1.95086028 -1.32774413  0.89592821] [-0.33078607  2.11410478 -0.12062541] [-0.44410502  0.54852488  1.3069914 ]] Discrete random variables from scipy import stats import matplotlib.pyplot as plt xk  = np.arange(1, 7)  pk = (1/6, 1/6,1/6, 1/6, 1/6, 1/6) # probabilities must sum to 1 plt.subplots(1, 1) plt.bar(xk, pk) plt.ylabel(r'$\frac{1}{6}$',fontsize=20) plt.yticks([]) plt.xlabel("X") The output of this random variable looks as follows: In such cases we have to work with Cumulative Distribution Function(CDF). The cumulative distribution function gives us the cumulative probability associated with a function.  custm = stats.rv_discrete(values=(xk, pk)) fig, ax = plt.subplots(1, 1) ax.plot(xk, custm.cdf(xk), 'ro', ms=12, mec='r') ax.vlines(xk, 0, custm.cdf(xk), colors='r', lw=4) plt.ylabel("CDF") plt.xlabel("X") plt.show() This is a very brief introduction to data representation. Without the use of specialist tools, it is beyond the capacity of humans to analyse and interpret large volumes of data. Computers only understand numbers. Words, images and ideas must be turned into numbers before you can feed them in to a computer for processing. Data representation is an important step towards creating models from large scale data. Machine Learning requires data to be described by ‘features’ called attributes or parameters before use. Choosing the right features is important to creating a useful model. An important video If you are new to ML, the video above introduces important concepts. This will help you understand how text data can be presented numerically using learning algorithms. The video also describes feature vectors, which are used in the next article and the following articles on matrices. Be sure to watch and develop your knowledge around how text becomes a table of numbers before you move on. View transcript SPEAKER 1: Welcome. In this section, we would like to talk about text data representation. In order to present a text data readable and useful for machine learning algorithms, we are performing bag-of-words model. The bag-of-words model is simple to understand and easy to implement. It has seen great successes in problems such as language modelling and document classifications. The basic idea in bag-of-words is a representation of text that describes the occurrence of words feeding a document. As you can see in here, we have two documents, document number one and document number two. Also, we have a vocabulary list or terms. Then we're going to search for these terms inside the documents. Let's say, for the word "goal," we have only one occurrence in document number one and no occurrence in document number two. For the word "data," we have one occurrence in document number one and two occurrences in document number two. For the word "information," two occurrences in document number one. Also, two occurrences in document number two. One occurrence of "insight" in document number one and no occurrence of "insight" in document number two. Also, as you can see, we have no occurrence of "you" in document number one. But we have two occurrences of "you" in document number two. We call these vectors the feature vectors of documents. So this one is the feature vector of document number one. And Doc2 is the feature vector of document two. Then we add up these feature vectors into feature matrix, so that we can later use that as representation of text data for machine learning methods. As you can see, we add up the feature vector of the first document in the first column of the feature matrix, the feature vector of the second document in the second column of the feature metrics. In machine learning linguistic, each document is called instance, example, or record. Also, this matrix is called feature matrix. Each instance is put on a row or column of this matrix. As you can see in here, each instance is put on the column of the feature matrix. Activity The technique described in the video is a popular model used to encode text data. What are some of its limitations? Extension: if you are familiar with the basic concept you might like to extend your knowledge by looking at this Lynda course which is free to Deakin students: Processing text with Python 