 Probability Probability measures the likelihood that an event will take place. The probability of an event occurring is denoted by P(A), whereas the probability of it not occurring is denoted by 1 – P(A).  Joint  probability  is  a  type  of  probability  wherein,  the  probability  of  two  events  are independent.  Therefore,  P(A  and  B)  =  P(A)P(B).  The  probability  is  jointly  for  more  than  an event. CondiFonal probability is the probability of an event occurrent given another event has taken place. It is deﬁned as P(A|B), wherein P(A) occurs, provided P(B) has taken place. Bayes rule explain the probability of an event relaFve to a diﬀerent event that is related to A.  Random variable, is the possible values that are generated from a random event. It can be categorized  into  discrete  and  conFnuous  random  variables.  Discrete  random  variables  has disFnct values that are countable, whereas for conFnuous random variables can take in values on an inﬁnite conFnuum. The main measures for a random variable with given probability distribuFon  are  its  mean  and  variance  for  a  conFnuous  distribuFon  and  is  within  the integraFon  limits.  For  a  random  variable  which  is  discrete,  the  integraFon  becomes  the weighted sum.  Examples  of  distribuFons  of  random  variables  include  Bernoulli  distribuFon,  uniform distribuFon, and normal distribuFon. Bernoulli distribuFon is a distribuFon that has a binary random  variable  as  an  outcome.  The  probability  lies  within  0  <=  p  <=  1,  whereas  the probability of taking 0 is 1 – p. In other words, the probability of an event has only two disFnct outcomes. Normal distribuFon is deﬁned for conFnuous random variables. This distribuFon can have a probability that can be signiﬁcant nonzero at tails as x can approach inﬁnity and range to negaFve inﬁnity. The total probability sFll equates to 1. Uniform distribuFon is for both discrete and conFnuous random variables. An example that follows this distribuFon is rolling  a  fair  dice.  Central  limit  theorem  is  when  you  have  a  populaFon  with  a  mean  and standard  deviaFon  and  extract  large  random  samples  from  the  distribuFon,  it  becomes normally distributed. Therefore, the distribuFon of N sample size data becomes increasingly normal.  Data Wrangling Data wrangling is the process of cleaning, transforming, and processing the dataset to make it suitable for analysis. The data is typically unstructured and incomplete, therefore common measures  are  taken  in  idenFfying  and  correcFng  inconsistencies  in  the  data,  handling  or compuFng  missing  values,  merging  datasets,  converFng  data  into  a  suitable  format, idenFfying and removing outliers, normalizing and aggregaFng data.  Data extracFon and transformaFon Images cannot be directly recognized by a machine to be analyzed. Therefore, an algorithm has  to  be  created  to  process  every  picture.  The  ﬁrst  step  is  to  ﬁnd  features  that  are represented  by  numbers  as  features  represented  in.  numerical  vector  of  features  can  be understood  by  the  algorithm.  Then  the  model  is  created  based  on  how  each  image  is represented  numerically.    Following  extracFng  important  features  and  creaFng  a  feature matrix  it  can  then  be  fed  into  a  proper  algorithm  for  classiﬁcaFon.  Data  representaFon  is important in creaFng models from large data. Feature extracFon derives a set of desired features form raw data where every feature consists of informaFon on the target variable. It is then represented by numerical vector of features to be fed as input data to the computer algorithm. For an image to be represented, it is ﬁrstly divided into smaller and each block will represent features and the blocks accumulated, mean, variance and other staFsFcs can be computed.  Data and signal are signiﬁcantly diﬀerent from one another. Data is typically using informaFon shared  by  users  such  as  purchase  history,  hobbies  etc.  Data  is  stored  in  the  form  of measurement that is represented as text, number or media. Data can generate useful insights for companies to help them come to a decision. A signal however is referred to as a sign or gesture that delivers speciﬁc informaFon.  Encoding Encoding techniques are used to convert categorical values into integer values. Examples of encoding techniques include OrdinalEncoder, One-Hot Encodings and LabelEncoder.  DistribuFon The  distribuFon  of  certain  values  can  inﬂuence  the  performance  of  a  machine  learning algorithm. If the values are not evenly distributed, this can result in the algorithm being biased toward certain values that can lead to poor performance. However, if it is normally distributed it can help the algorithm in making useful predicFons.  Scaling and NormalisaFon The process of converFng a set of values to a new range of values is called scaling. Usually, raw  or  unscaled  data  can  cause  problem  while  training  a  model.  For  example,  a  parFcular feature  can  dominate  the  model  more  than  the  other  features  if  it  carries  more  weight. NormalisaFon on the other hand is a scaling technique that is used to transform data to a common range and helps in standardizing the data.  