Basic Definition  In probability theory, an event is a group of outcomes from a random experiment, and probability is a way to measure the likelihood of an event happening. The probability of an event is a number between 0 and 1, with P(A) representing the probability of an event A occurring, and 1-P(A) representing the probability of A not occurring. Probability can also be defined for multiple events, and conditional probability is the likelihood of an event A happening given that another event B has occurred, represented by P(A|B).  Condition probability P(A|B), is defined as:  P(A|B) = (ğ‘ƒ(ğ´ ğ‘ğ‘›ğ‘‘ ğµ)) ğ‘ƒ(ğµ)  where P(B) is not zero.  Bayesian approaches involve using mathematical rules to update beliefs based on new information. Bayes rule is a mathematical formula that can be used to determine the probability of event A based on the occurrence of another related event B.  Bayesâ€™ rule is defined as:  P(A|B) = ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´) ğ‘ƒ(ğµ)  where ğ‘ƒ(ğµ) â‰  0.  Random Variables  A random variable is a variable that represents the possible outcomes of a random event. It can assign probabilities to events of interest in a random experiment.  There are two types of random variables: discrete and continuous.  Discrete random variables have a countable number of values while continuous random variables can take values on an infinite continuum. Discrete random variables are defined using Probability Mass Functions (PMF), denoted as ğœ‹(ğ‘¥) while continuous random variables are defined using Probability Density Functions (PDF), denoted as ğ‘“(ğ‘¥). The PMF assigns probabilities to each possible value of the random variable, and the PDF defines a probability distribution for a continuous random variable. The Cumulative Distribution Function (CDF) denoted as ğ¹(ğ‘‹) gives the cumulative probability associated with a function.  Distributions of random variables  The Bernoulli distribution pertains to a discrete distribution which is utilized for a binary random variable that can hold values of either X=0 or X=1.  On the other hand, the Uniform distribution can be defined for both discrete and continuous random variables.  For a discrete random variable:  ğœ‹(ğ‘¥ğ‘–) = ğ‘ƒ(ğ‘‹ = ğ‘¥ğ‘–) = 1 ğ‘ , ğ‘– = 1. . ğ‘  The same concept is applied to continues space. For a continuous random variable:  ğ‘“(ğ‘¥) = 1 ğ‘ âˆ’ ğ‘ , ğ‘ â‰¤ ğ‘¥ â‰¤ ğ‘  The most widely used distribution in statistics is the Normal distribution. It is defined for continuous random variables and provides a bell-shaped curve. The central limit theorem is a theorem in probability theory that explains the behaviour of the mean of a sufficiently large sample drawn from any distribution, provided that the mean and variance exist. According to the theorem, as the sample size increases, the sample mean tends to follow a normal distribution with mean equal to the population mean and variance equal to the population variance divided by the sample size. Therefore, the sum of N independent and identically distributed random variables becomes increasingly Gaussian as N increases.  Data Wrangling  Data wrangling, which is also referred to as data munging, encompasses a set of activities aimed at preparing a dataset for analysis by cleansing, restructuring, and consolidating it. Typically, data wrangling entails both manual and automated procedures, and it plays a pivotal role in the data science workflow.  Data wrangling can be categorized into several steps, including data loading and saving, data exploration, data processing (which may involve addressing missing values and encoding categorical variables), data distribution analysis, and scaling.  Images as data  To use an image as input data for a computer algorithm, it must be converted into a numerical vector of features. The algorithm can only interpret the image as a set of numbers. The image is divided into smaller blocks, and for each block, features such as colour and shapes are computed. Mean, variance, and other statistics can be computed for these blocks.  Suppose p features are extracted per block with w number of blocks, resulting in w*p features per image. The Feature Matrix size is then w*p*n for n images. After converting the image to numbers, the resulting feature matrix can be fed into a suitable computer algorithm for classification.  Text data representation  In order for a computer to process information, it must first be represented as numbers. This means that words, images, and concepts must be translated into numerical form before they can be utilized by a computer.  Data VS signal  The term "data" refers to information that is provided by users and can be stored in various forms, including text, numbers, and media, as observations or measurements. Companies utilize this data to produce novel insights and make decisions. In contrast, a "signal" is a quantity that fluctuates over a parameter like space or time and can be employed to communicate specific information. The distinction between data and signal is that data becomes useful information when it is organized and represented, while signal is another type of data that can be employed to capture and interpret electrical impulses, such as ECG signals.  Encoding and Distribution  To convert categorical data into numerical form, encoding is a widely used technique that employs various methods such as One-Hot Encoding, LabelEncoder, and OrdinalEncoder. The term "distribution" pertains to the pattern of how data values are spread across a dataset. Distributions can take different forms, such as uniform, normal, or skewed. The way values are distributed can have a considerable impact on the effectiveness of machine learning algorithms.  Scaling and Normalisation  Scaling is a technique used to transform a set of values into a different range of values to ensure that one feature doesn't dominate the model over another. Normalization, on the other hand, is a type of scaling technique that aims to transform the values of a dataset into a uniform range, such as 0 to 1. It is widely used to enhance the performance of machine learning models. One of the most popular methods of normalization is min-max normalization. These techniques are useful for standardizing data and ensuring that machine learning algorithms work optimally.  Reflection and summary of the reading list  All most all the content introduced in topic 2 was not new to me. Iâ€™ve previously learnt about probability, discrete and continuous random variables, distributions of random variables, data wrangling, encoding, scaling and normalisation. I learnt all these topics in a previous unit, Data Wrangling. Hence, none of this topicâ€™s content was new to me and no further reading was required to understand them.  