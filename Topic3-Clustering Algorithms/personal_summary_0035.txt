Probability is a crucial factor in machine learning algorithms. It can be defined for an event, which is a set of outcomes of a random experiment, and is the measure of the likelihood that an event will occur. Joint probability is the probability of two independent events both occurring, for example, flipping a coin two times and getting a heads on the first toss and a tails on the second toss. Conditional probability is the probability of one event occurring, given the occurrence of another event, for example, if you pick a red card out of a deck, what is the probability of it being a numbered card.  Random variables are functions that assign probabilities to events of interest in a random experiment. There are two types of random variables: discrete and continuous. Discrete random variables have countable values, like the number of cards in a deck or how many cars pass an intersection at a specific time, while continuous random variables can take values on an infinite continuum, like the height or weight of a person or the temperature. We define discrete random variables using Probability Mass Functions, while Probability Density Functions are used to define continuous random variables.  Probability distribution is a function that gives the probabilities of different outcomes of an experiment occurring. Some of the most important distributions include the Bernoulli distribution, Uniform distribution, and Normal distribution. The Bernoulli distribution is a discreet distribution and is defined for binary random variables with the values X = 0 and X = 1, for example getting a heads or a tail in a coin toss. The Uniform distribution can be defined for both discrete and continuous random variables, for example, rolling a dice. The Normal distribution is defined for continuous random variables and is the most popular distribution.  The central limit theorem is crucial in statistics and machine learning because it allows us to make inferences about the population mean based on a sample mean, and it also explains why many natural phenomena tend to follow a normal distribution. The central limit theorem implies that when we take a large enough random sample from a population with a finite mean and standard deviation, the mean of the sample will be distributed normally. The distribution will have a mean that is equal to the mean of the population, and its standard deviation will be the population standard deviation divided by the square root of the sample size.  Data wrangling is a critical process in machine learning that involves cleaning, transforming, and organizing datasets for analysis. Some common tasks involved in data wrangling include identifying and correcting errors and inconsistencies in the data, handling missing or incomplete values, combining multiple datasets, and normalizing the data.  Data represented in an unorganized manner is called information, signal is another form of data that varies over a parameter such as space or time. It is not possible for humans to be able to begin to analyse and interpret large volumes of data without the use of specialist tools, however, computers are only able to understand numbers, therefore words, images, and ideas must be turned into numbers before being fed into a computer for processing. For this purpose we use encoding techniques, they are used to convert features which contain categorical variables to integer values. Some encoding techniques include: OrdinalEncoder, One-Hot Encodings, and LabelEncoder.     Data distribution refers to the way the data has been distributed in the dataset, which can have a significant impact on the performance of machine learning algorithms, for example if the values are not evenly distributed, the machine learning algorithm will be biased towards certain values. Distribution types include Normal distribution, Uniform distribution, and Skewed distribution. Normal distribution is the most common and refers to when the distribution of the values is symmetrical around the data’s mean. Uniform distribution is when all the values are distributed evenly across the range of the data. Skewed distribution refers to when the values are concentrated on one side of the range.  Another important aspect of machine learning is scaling, which refers to converting a set of values to anew range of values in order to even the values out. For example, if we have a person’s weight in pounds and their heights in feet. Scaling the dataset will give us a normalized value and grant us better results for analysing. A common method of normalisation is called min-max normalisation which is to scale the data to a range of 0 to 1, where 0 is the minimum value in the dataset and 1 is the maximum value.     