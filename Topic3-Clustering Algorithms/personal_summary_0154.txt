 The second topic of Machine Learning SIT720 was enriching in terms of knowledge. It broadly covered the following aspects:  1.  Linear  Algebra  and  Probability:  This  section  mainly  brushed  up  on  past  concepts of linear algebra and probability such as : i.  Random Experiment: an experiment whose outcome can’t be predicted with certainty before it occurs. Eg. Tossing a coin , rolling a dice. Event and Sample space: comprises of the set of outcomes of a random experiment. Probability: a measure of the likelihood of an event to occur. Joint Probability: P(A and B) = P(A)P(B) where A and B are independent. Eg. P(head in 1st coin AND head in 2nd coin) = ½ * ½ = ¼ Conditional  Probability:  probability  of  occurrence  of  A  given  another event B. ie. P(A|B) = P(A and B) / P(B)  ii.  iii. iv.  v.  We then applied the above concepts using a red balls and blue balls problem and also learnt about the Byes rule.  2.  Random Variable: Under this section we learnt about random variables whose possible values are the generated outcomes of a random phenomenon. There are 2 types: i. ii.  Discrete random variable: these are countable Continuous random variable: these are infinite  Discrete random variables are defined using a probability mass function(PMF). The sum of this function is always equal to 1.  In  addition  to  this  we  also  learnt  about  the  Cumulative  Distribution  function (CDF) for events pertaining to a certain value range.  Next,  we  learnt  about  probability  distribution  as  a  function  that  links  each outcome of a statistical experiment with the probability of its occurrence. The different types of probability distributions that were explored were:  i. ii.  iii.  Bernoulli distribution – for binary variables of the form p and 1-p. Uniform  distribution  –  in  this  case  probability  of  getting  each  value  is same.  It  can  be  defined  for  both  discrete  and  continuous  random variables. Normal distribution – It is a probability distribution which has a symmetric bell-shape.  In addition to the above, we also learnt about the central limit theorem.  3.  Data  Wrangling:  The  last  section  of  the  lecture  introduced  us  to  Data wrangling,  its  importance  and  various  techniques  and  aspects  involved  in  it. Data  wrangling  is  the  process  of  cleaning,  transforming  and  organizing  a    dataset  to  make  it  suitable  for  analysis.  In  other  words,  it  is  the  data- preprocessing aspect of analysis. The various aspects explored were: replacement:  Deletion  of  missing  data i.  Missing  value is  not recommended, therefore we either replace it with immediate succeeding or preceding value or replace the missing values with mean or median of the entire column. Scaling and normalization: this involves the process of transforming all values to a specific range for better performance of the model. Encoding  non-numeric  values:  It  mainly  covered  3  types  of  encoding, namely- -  Ordinal: for hierarchical values based on importance of the value. -  One-hot encoding: would have a list of classes and if present would  ii.  iii.  be represented by 1, else 0.  -  Label  encoding:  the  most  widely  used  encoding  for  different  categorical data.       