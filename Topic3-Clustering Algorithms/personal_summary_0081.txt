Statistics: Basic Definitions  Since probability is vital to machine learning algorithms we will go over some relevant basic terminology.  A random experiment refers to an experiment or process for which its outcome/result cannot be certainly predicted, the most common example of this would be a coin flip.  An event is defined as the set of outcomes that result from the previously mentioned random experiment. Going with the previous example of a coin flip, event A would either be heads or tails.  Probability is the measure of the chance that an event will occur and is represented by a number between 0 and 1.  Joint probability refers to the probability for more than one event which could in the case of the coin flip example involve the flipping of two coins. Determining the probability that both coins land on heads is an example of joint probability.  Conditional probability is the probability of an event occurring based on whether a different event occurs. In other words it is the chance that event A occurs given that event B has also occurred.  Bayes rule is used to determine the chance of something occurring based on conditional probability. For example, if event A has occurred after event B then way me able to use this information to predict event A once we see event B occur.  Random Variables  Random variables refer to variables where their values are the result of random phenomenon, they are can also be referred to as a function that attributes probability to the events involved in a random experiment. These variables will usually be either discrete variables which involve countable values or continuous variables which involve values which cant be counted as they can be infinitely broken down.  We are able to use Probability Mass Functions (PMF) to define discrete random variables. While PMF allows us to assign a probability to each potential value of the random variable if we want to for example, calculate the probability of rolling a dice and getting a value equal to or less than 5 we would instead use Cumulative Distribution Function (CDF). This allows us to determine the cumulative probability associated with a function (in this case rolling >= 5).  Meanwhile we use Probability Density Functions (PDF) when defining continuous random variables, since it is a statistical expression that defines the probability distribution of said variables. This is done by assigning the probability of a range of potential results from the random variable, meaning that the probability at any fixed value will be zero meaning only the probability over the range has meaning.  Distributions of Random Variables  Bernoulli distributions is a discrete distribution defined for binary random variables with values of 0 or 1.  Uniform distributions can be defined for both discrete and continuous variables.   Normal distributions are defined for continuous variables, it’s frequent application in natural phenomena makes it quite popular,  Central limit theorem states that if you take large random samples from a populations than their means will be roughly normally distributed, the more you take the more the random variable become normally distributed.  Data Wrangling  Data wrangling refers to the making a dataset suitable for analysis through cleaning, transforming and organising the dataset, typically involving both automated and manual procedures.  Common data wrangling tasks:  Identifying and correcting inconsistencies  - -  Handling missing or incomplete values -  Combining datasets -  Converting data into an analysable form - Identifying and removing outliers -  Normalising the data -  Aggregating data into useable summaries  Data Representation  To allow a machine to analyse and categorise images we have to utilise an algorithm to process the image and make some kind of decision based on it. From there “correct” answers can be collated and accuracy can be tested.  Since fundamentally computers can only process binary data we need a method to represent the image as a vector of features.  Numerically representing an image can involve breaking the image down into smaller blocks and then calculating numerically representable information within each block such as colour averages, shape (number of lines or curves), brightness etc. From these blocks then a calculate usable statistics such as mean and variance.  To allow machines to analyse text we need to do the same thing as we do with an image, represent the data with numbers. One method of this is the bag of words method which involves a feature vector that tracks all the words present in a document and their frequency, allowing for the comparison of two text documents by a computer.  Data vs Signal  The Difference between data and signal is that data refers to information we get (when organised) while signal is a form of this information that can be interpreted or sent over a specific channel.  Encoding and Distribution  There are many categorical features that cannot be understood by the computer thus, encoding techniques are used to convert these to integer values, some of the more common encoding techniques are OrdinalEncoder, One-Hot Encodings and LabelEncoder.  Distribution relates to the way values are distributed within a dataset which can have a significant impact on how a machine learning algorithm is able to perform. For example, if values are not evenly   distributed the algorithm may perform poorly as it is biased towards certain values whereas a normal distribution would help the algorithm perform better.  Scaling and Normalisation  Scaling is the process of converting a set of values to a new range of values, we do this so that within a dataset we won’t have features that dominate the model as raw data may involve features that work within far different ranges than others.  Normalisation is a scaling technique that can be used to change the values within a dataset into a common range since algorithms are able to perform better when working with data that is within a standardised range. A common example of this is min-max normalisation which involves scaling data on a range from 0 to 1.  