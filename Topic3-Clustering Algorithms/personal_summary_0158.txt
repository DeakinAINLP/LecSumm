SUMMARY:  Statistics:  ï‚·  Statistics plays an important role in ML algorithms.  Basic definitions:  ï‚·  Random experiment:  ïƒ˜  A process whose outcome cannot be predicted with certainty is termed to be as  random experiment.  ï‚·  Event:  ïƒ˜  A set of outcomes produced from a random experiment.  ï‚·  Sample space:  ïƒ˜  set of all outcomes produced from a random experiment.  E.g.:  Random experiment: A= toss a coin.  Sample Space Î©: {â€˜headâ€™, â€˜tailâ€™}  Event: {â€˜headâ€™} or {â€™tailâ€™}  ï‚·  Probability: P(A) ïƒ˜  It is stated as the possibility of occurrence of a random event. Probability is expressed from 0 to 1.  ï‚·  Joint probability:  ïƒ˜  Probability can also be defined jointly for more than one event. ïƒ˜  Joint probability of for two independent events A and B is stated as:  P (A AND B) =P(A)P(B)  ï‚·  Conditional probability:             ïƒ˜  It is the probability of some event A, given the occurrence of event B  Provided P(B)! =0  ğ‘ƒ(ğ´|ğµ) =  ğ‘ƒ(ğ´  and B) ğ‘ƒ(ğµ)  Bayes Rule:  Bayes rule explains how you should change your existing beliefs on the arrival of new occurrence.  ğ‘ƒ(ğ´|ğµ) =  ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´) ğ‘ƒ(ğµ)  ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ğ‘ƒ(ğµ) â‰  0  Example 1:  A box I contains 4 white and 6 black balls while another box II contains 4 white and 3 black balls. One ball is drawn at random from one of the bags, and it is found to be black. Find the probability that it was drawn from box I.  Box 1  Box 2  E1: event of choosing box 1.  E2: event of choosing box 2.  A: event of chosen ball drawn from box is black  Random variable:  Code to generate random variables:  import numpy as np B = np.random.randn(4,3) print('An example of a random matrix is:') print(B)    Random variables are classified as:  ïƒ˜  Discrete random variable:  ï¶  has countable number of values. ï¶  Defined using PMF: âˆ‘ğ‘¥ ï¶  CDF is used to estimate the cumulative probability associated with a function.  ğœ‹(ğ‘¥) = 1  ïƒ˜  Continuous random variables:  ï¶  has values on infinite continuum. ï¶  Defined using PDF: âˆ«  ğ‘“(ğ‘¥)ğ‘‘ğ‘¥ = 1  +âˆ âˆ’âˆ  ï‚·  Distributions of random variables:  ïƒ˜  The probability distribution for a random variable describes how the probabilities are distributed  over the values of the random variable.  ïƒ˜  Normal distribution: ï¶  Known as bell curve. ï¶  Density curve is symmetrical. ï¶  Normal distribution is cantered about its mean with standard deviation indicating its spread.  ïƒ˜  Bernoulli distribution:  ï¶  Itâ€™s a special case of binomial distribution for n=1 (n=trial) ï¶  This distribution is a discrete probability distribution having just two outcomes (â€œsuccess â€œor  â€œfailureâ€)  ï¶  The probability of failure is labelled on x axis as 0 and success is labelled as 1 on y axis  Central limit theorem:  ï¶  This states that distribution of sample approaches a normal distribution as the sample size  increases.  ï¶  Doesnâ€™t dependent upon the shape of population. ï¶  E.g.: as the number of experiments increases of rolling a dice the distribution tends to seem like  a normal distribution.  Data Wrangling:  ï¶  The process of cleaning, transforming, and organizing dataset to make it suitable for analysis. ï¶  A crucial step in data science pipeline.  Identify,correct errors in data  handle missing values  combine multiple datasets  converting data into req format  identify and remove outliers  normalize data  prepare summary  Text data representation:  ï¶  ML models to interpret large volumes of data require the data to be represented in particular  format.ie in the form of features.  Data vs signal:  ï¶  Data is the information that we want to transmit, and signal is the waveform that carries data.  Encoding:  ï¶  ML algorithms arenâ€™t capable of understanding categorial data present in some features. ï¶  Thus, this problem is addressed by encoding the categorical values into integers using techniques  stated below:  ï‚·  Label encoding:  ï¶  This type of encoding is used when the variable present in the dataset is ordinal. ï¶  ordinal encoding converts each ordinal value into integer values. ï¶  cons: In this type of encoding the ML algorithm might compare the values based upon the  integer labels assigned.  ï‚·  One -hot encoding:  ï¶  This type of encoding is used when the variable present in the dataset is nominal. ï¶  It maps each category with a binary number (0,1)  ï‚·  Scaling:  ï¶  Scaling refers to process of converting a set of values into new range of values. ï¶  In an unscaled dataset one feature might dominate another feature. ï¶  So, we scale the features in a certain range for getting optimal results.  ï‚·  Normalization:  ï¶  ğ‘£â€² =  ğ‘£âˆ’ğ‘šğ‘–ğ‘›(ğ‘£)  ğ‘šğ‘ğ‘¥(ğ‘£)âˆ’ğ‘šğ‘–ğ‘›(ğ‘£)  ï¶  It is a scaling technique that scales the dataset in the range of (0,1). ï¶  Where 0: minimum value 1: max value  ï¶  This is also referred to as min -max scaling. ï¶  This is used to increase the performance of the algorithm by standardizing the data.  