During Topic 2 I learnt about basic definitions of statistics. 1)  Probability Probability is the branch of mathematics that deals with measuring the likelihood or chance of an event occurring. The main points of probability are: a. Probability ranges from 0 (impossible) to 1 (certain). b. The probability of an event is the ratio of the number of favorable outcomes to the total number of possible outcomes. c. The addition rule states that the probability of either of two mutually exclusive events occurring is the sum of their individual probabilities. d. The multiplication rule states that the probability of two independent events occurring together is the product of their individual probabilities. e.  The  complement  rule  states  that  the  probability  of  an  event  not  occurring  is  1  minus  the probability of it occurring. f. Conditional probability is the probability of an event given that another event has occurred. g.  Bayes'  theorem  allows  for  the  calculation  of  the  probability  of  an  event  given  some  prior knowledge or information. h. Probability distributions describe the probabilities of different outcomes for a random variable. i.  The  expected  value  of  a  random  variable  is  the  weighted  average  of  all  possible  outcomes, weighted by their respective probabilities. j. The variance and standard deviation of a random variable measure the spread or variability of its possible values around its expected value.  2)  Random variables  Random variables are mathematical abstractions used to model uncertain or random phenomena. They  are  represented  by  symbols  (usually  uppercase  letters)  and  can  take  on  numerical  values based on the outcomes of a random process. There are two main types of random variables: discrete and continuous. Discrete random variables can only take on a finite or countably infinite number of values, while continuous random variables can take on any value in a range. A  probability  distribution  is  a  function  that  assigns  probabilities  to  the  possible  outcomes  of  a random variable. For discrete random variables, this is often represented by a probability mass function  (PMF),  which  gives  the  probability  of  each  possible  value  of  the  random  variable.  For continuous  random  variables,  the  probability  distribution  is  often  represented  by  a  probability density function (PDF), which gives the probability of the random variable taking on a value within a certain range. Some common probability distributions include the binomial distribution, the normal distribution, and the Poisson distribution. The properties of these distributions, such as their mean and variance, can be used to analyze and make predictions about real-world phenomena that involve random variables.  3)  Data wrangling Data wrangling, also known as data cleaning or data preprocessing, is a crucial step in the data analysis process that involves transforming raw data into a format that is suitable for analysis. The     main points of data wrangling include: a.  Data collection: Collecting data from various sources such as databases, web services, or files. b.  Data cleaning: Cleaning data to remove errors, missing values, and inconsistencies. c.  Data  transformation:  Transforming  data  into  a  format  that  is  suitable  for  analysis,  such  as  normalizing, scaling, or encoding categorical variables.  d.  Data integration: Integrating data from multiple sources to create a single, unified dataset. e.  Data reduction: Reducing the size of the dataset by selecting relevant features or summarizing  data using aggregation techniques.  f.  Data exploration: Exploring the dataset to identify patterns, trends, and relationships among  variables.  g.  Data  visualization:  Creating  visualizations  such  as  charts,  graphs,  and  histograms  to  better  understand the data.  Overall,  data  wrangling  is  an  important  step  in  the  data  analysis  process  that  helps  ensure  the accuracy and reliability of the results obtained from the data.  4)  Scaling and Normalisation Normalization is a scaling technique used to transform the values of a dataset into a common range. This is often done to improve the performance of machine learning algorithms, as many algorithms operate better when the data is in a standardized range. Min-max normalization:  Scales  data  to  a  range  of  0  to  1,  where  0  is  the  minimum  value  in  the  dataset  and  1  is  the  maximum value.  Formula:  ùë£‚Ä≤  =  ùë£‚àímin(ùë£)max  ùë£  ‚àímin(ùë£),  where  max(ùë£)  is  maximum  and  min(ùë£)  is  minimum  value for vector ùë£.  5ÔºâNon-numerical data encoding Non-numerical data, also known as categorical data, is data that consists of labels or categories that do not have a numerical value.Machine learning algorithms typically require numerical data as input, so non-numerical data needs to be encoded into numerical form before it can be used in these algorithms. There  are  several  methods  for  encoding  non-numerical  data,  including  one-hot  encodings, labelEncoder, and ordinalEncoder. a.  One-hot encoding is a method where each category is converted into a binary vector where all elements are zero except for the element corresponding to the category, which is set to one.  b.  LabelEncoder is a method where each category is assigned a unique numerical label. c.  OrdinalEncoder is a method where each category is assigned a unique binary code, which is  then used to represent the category.  The choice of encoding method will depend on the specific problem and dataset being worked with, as well as the requirements of the machine learning algorithm being used. It is important to be careful when encoding non-numerical data, as different encoding methods can introduce different biases or affect the performance of the machine learning algorithm.  