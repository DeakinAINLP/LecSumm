  Summary  o  Linear Algebra:  ▪  Vector & their operations: Vector algebra is a field of mathematics that studies  vectors,  which  are  represented  as  directed  line  segments  with magnitude  and  direction.  Common  operations  in  this  discipline  of mathematics  include  addition,  subtraction,  scalar  multiplication,  dot product,  and  cross  product.  Vector  addition  is  the  addition  of  the respective  components  of  two  vectors,  and  vector  subtraction  is  the removal of the corresponding components. The process of multiplying a vector by a scalar quantity is known as scalar multiplication, and the dot product of two vectors is the sum of the products of their corresponding components.  Ultimately,  the  cross  product  of  two  vectors  produces  a vector  that  is  perpendicular  to  both  of  them,  with  the  magnitude  and direction  specified  by  the  magnitudes  and  angles  of  the  two  original vectors.  ▪  Matrix & their operations: Matrix algebra is an area of mathematics that deals with matrices, which are numerical arrays expressed as rectangular arrays. It includes operations like addition, subtraction, scalar and matrix multiplication,  transpose,  determinant,  and  inverse.  Subtraction  and addition  entail  adding  and  subtracting  appropriate  entries  from  two matrices. Scalar multiplication is the operation of multiplying a matrix by a scalar. Matrix multiplication is the process of multiplying two matrices, which is only feasible if the number of columns in the first matrix equals the  number  of  rows  in  the  second  matrix.  Transpose  is  the  process  of altering the rows and columns of a matrix. A square matrix's determinant is a scalar value that may be determined using its elements. The inverse of a square matrix is another matrix that, when multiplied by the original matrix, yields the identity matrix, and it is calculated using its determinant and a formula that involves its entries.  ▪  Feature vectors and matrices: A feature vector is a one-dimensional array or row vector that represents a data point or instance in machine learning and data analysis, according to the text. It has values for each feature or variable and is the same length as the number of features. A matrix, on the other hand, is a two-dimensional array of integers arranged in rows and columns that is used in machine learning to describe datasets, where each  row  represents  a  data  instance  or  observation  and  each  column represents a feature or variable. Matrices are used to conduct operations on a dataset and may also be used to express spatial transformations.  o  Probability Concepts:  ▪  Random experiment & Event: This topic we discussed probability theory ideas such as random experiments and events. A random experiment is a procedure that results in an unpredictable outcome, whereas an event is a  subset  of  a  random  experiment's  possible  possibilities.  An  event's   probability  reflects  how  probable  it  is  to  occur,  with  0  indicating impossibility  and  1  representing  certainty.  Events  can  be  mutually exclusive  or  independent,  depending  on  whether  they  can  occur concurrently or not, and if one impacts the likelihood of the other.  ▪  Joint  probability:  In  probability  theory,  the  joint  probability  of  two  or more occurrences is the likelihood that they will all occur. It is denoted as P(A  and  B)  and  is  computed  as  the  product  of  the  probability  of  each individual event, assuming they are independent. Joint probabilities are useful in computing conditional probabilities, marginal probabilities, and other related notions in many areas of probability theory.  ▪  Conditional  probability:  Conditional  probability  is  the  likelihood  of  an event  happening  given  that  another  event  has  already  occurred.  It  is represented as P(A|B) and is calculated as the probability of both events A and B occurring together divided by the probability of event B occurring. The concept of conditional probability is important in probability theory and has applications in various fields such as statistics, machine learning, and decision making.  ▪  Bayes  Rules:  Bayes'  rule  is  a  formula  for  calculating  the  conditional probability of an occurrence in the presence of additional information or evidence. The formula takes into account the chance of the occurrence as well as the probability of the evidence given the incident. Bayes' rule is utilised  in  many  domains,  including  statistics,  machine  learning,  and artificial intelligence, for things like spam filtering, medical diagnosis, and picture identification. It is named after Reverend Thomas Bayes, an 18th- century British statistician and Presbyterian pastor.  o  Random variable:  ▪  Distribution  of  random  variables:  The  value  of  a  random  variable  is decided by the outcome of a random event or experiment. Its distribution specifies  how  probabilities  are  dispersed  across  the  variable's  various values, and random variables are classified into two types: discrete and continuous. A probability mass function (PMF) describes discrete random variables, which have a finite or countable number of values, whereas a probability  density  function  describes  continuous  random  variables, which can have any value within a given range (PDF). In probability theory and  statistics,  the  distribution  of  a  random  variable  is  significant  for computing different aspects of the variable.  o  Data wrangling:  ▪  Missing  value  replacement:  The  process  of  cleaning,  processing,  and preparing data for analysis is known as data wrangling. Missing values are a typical issue in this procedure. Missing values can be replaced with a value  that  represents  the  missing  value  using  methods  such  as  mean, mode,  or  median  imputation,  as  well  as  forward/backward  fill.  The    replacement  technique  chosen  should  be  carefully  studied  because  it might have an influence on the ensuing analysis.  ▪  Scale  or  normalisation:  Normalization  or  scaling  is  the  process  of transforming  numerical  data  such  that  it  fits  within  a  given  range  or distribution.  This  procedure  is  used  to  guarantee  that  the  scales  of different  variables  are  comparable  in  order  to  avoid  concerns  with magnitude  disparities  while  analysing  data.  Scaling  or  normalising  data may  be  accomplished  using  a  variety  of  techniques,  including  min-max scaling, z-score standardisation, and log transformation. The method of scaling or normalising used is determined by the nature of the data and the individual research issue being addressed. The possible influence of any scaling or normalising procedure on the data and consequent analysis must be considered.  ▪  Non-numeric data encoding: The process of transforming categorical or qualitative data into numerical data so that it may be utilised in statistical studies  or  machine  learning  models  is  known  as  non-numeric  data encoding.  Depending  on  the  nature  of  the  data  and  the  research objective, multiple approaches such as label encoding, one-hot encoding, and binary encoding are used in this procedure. Non-numeric data must be  encoded  since  many  algorithms  require  numerical  data  and  cannot process non-numeric data directly.    Reading list: Lecture Slides, Lecture Recordings, Learning Contents.   My reflections: This topic we were introduced to various mathematical concepts. We started with linear algebra. Under linear algebra we learned about Vector, Matrix and their operations. Along with it feature vectors and matrices. Then we studied about the  probability  like  random  experiment  and  events,  joint  probability,  conditional probability and bayes rule. Then we learned about the distribution of random variable. In the end we learned about data wrangling and it’s various concepts like missing value replacement, scale or normalisation and non-numeric data encoding.  