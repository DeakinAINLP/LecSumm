Probability is a major component of machine learning as its various rules and the types of probability are used to predict outcomes and improve accuracy especially before a pattern is found. Random variables are used to represent possibilities of an outcome of an unknown event. We can then link all of these outcomes of the statistical experiment with its probability of occurrence using a probability distribution. Data wrangling or munging is the process of cleaning, transforming and organizing a dataset for analysis. To teach a machine to analyse images we need to be able to put numbers onto the image, splitting it up and finding features to be represented with numbers. Such as giving numbers to the averaged colour, number of certain shapes and light or darkness of a block. To  represent data using text the machine needs to identify features to use as parameters for which it can assign values to then be used for data representation. Data is both qualitative which is non numeric, or features of something, or quantitative data such as something that can be counted. A signal is a sign that displays or conveys information. For qualitative data we need to encode it, by assigning numeric values to categories of data so it can then be used by machines. A distribution is how the data from the dataset is distributed, different types of distribution can cause the machine to be bias or behave differently towards data. Scaling is used to change the values of the data and move them into a range, typically to make them easier to understand and more standardised. Probability is one of the fundamental components of decision making in ML. We have a random experiment which is which an actions result cannot be predicted with certainty, such as tossing a coin or dice. The event is the set of possible outcomes for this random experiment, listing all the possible outputs. We can then use various probability methods to more accurately predict outcomes, such as joint probability, which is when multiple random events occur and you can work out the total likeliness of any combination of events occurring. Conditional probability that is the probability of when one event happening given that another event also happens. Bayes rule is a mathematical rule explaining that the probability of one event A based on another event B can be better predicted as event B is related to event A. A random variable is used to represent the likeliness of an outcome from a random experiment. Discrete random variables are defined as a Probability Mass Function, summing all the values to 1, like rolling a dice and each value is 1/6. There are other cases where we use Cumulative Distribution Function, which gives us the cumulative probability associated with a function, which we use for examples like the chance of rolling a dice and getting less than 5. Continuous random variables are defined as Probability Density Functions, which is used to assign values to the range for the random variables, between 0 and 1 likelihood of happening. Distributions of random variables can be used to understand the link of the outcome with the probability of occurrence. Various distribution methods include the Bernoulli distribution, which is a discrete distribution that is binary, which is used to identify things like a pass 1 or fail 0. Uniform distribution can be for both discrete and continuous , which is typically where all outcomes are    equally as likely. Normal distribution is for continuous variables, this produces a bell-curve where most of the outcomes lay around the mode and the further away from that the less and less likely they become. Central limit theorem states that if you have a population with a mean u and standard deviation o and collect enough random data from the population then the result will be normally distributed. Data wrangling is the process of cleaning transforming and organizing a dataset for analysation. As it involves dealing with messy, unstructured and incomplete data the data needs to have a list of things done to it. Identifying and correcting errors in the data, handling missing or incomplete values, combining multiple datasets, converting data to a format that can be analysed, removing outliers, normalizing data and aggregating data into useful statistical summaries. To teach a machine to identify images we need to find features that can be represented with numbers in order to input this data into a computer algorithm. We would split the image into smaller blocks and identify the average colour for the block, shapes within it, amount of light and dark as well as brightness. We can feed this data into a machine and it can start learning. Humans do not have the capacity to analyse and interpret large amounts of data, but machines cannot understand language, they only understand numbers. So for a computer to start processing information the words, images, videos must be turned into numbers that can be categorised under different features so it can use a learning model to interpret our world. A signal is the quantity that varies over a parameter such as time or space. Such as audio signals and heartbeat signals. Data is a collection of facts, measurements and observations, it is information that has been translated into a form that is more efficient to move and process. Encoding is when we take qualitative data which is unable to be understood by machines and convert these features into integer values. Assigning numbers to categories and features to give them a meaning for the computer to interpret. Distribution is how a dataset is distributed. A normal distribution is most common with a bell shape around the dataâ€™s mean. A uniform distribution is evenly spread across the whole range. A skewed distribution is when the values are concentrated on one side of the range, distribution can affect machine learning and skewed distributions can make the algorithm biased towards certain values. Scaling is the task of changing the range of the existing dataset to a new range, this makes it easier to be interpreted by the training model and removes bias towards one of the data sets. The most common means of doing so is using normalisation which is scaling the data into the range between 0 and 1, as this is the most simple and easy way to standardise the data. 