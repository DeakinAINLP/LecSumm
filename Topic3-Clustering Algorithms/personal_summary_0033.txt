Mathematics and Machine learning ML is based on maths. Maths underlies every decision, iteration and output. To get great results, you need to understand the various mathematical concepts and processes that are the foundation of your results.  Key areas this topic:    Vectors and basic vector operations   Matrices and matrix operations   Basics of probability   Python programming  Joint Probability Probability can be defined jointly for more than one event. Eg flipping 2 coins. If 2 events A and B are independent then the probability is P(A and B) = P(A)P(B). Coin toss is Â½ * Â½ = Â¼  Conditional Probability The conditional probability is the probability of some event A, given the occurrence of another event  B. P(A|B) probability of A given B. P(A|B) = ğ‘ƒ(ğ´ ğ‘ğ‘›ğ‘‘ ğµ) ğ‘ƒ(ğµ)  provided that P(B) is nonzero.  Bayes Rule The essence of most of Bayesian approaches are to provide a mathematical rule explaining how you should change your existing beliefs in the light of new occurrence. Bayes rule describes the probability of an event A based on another event B that is related to A. As an example: if cancer is related to age, using Bayesâ€™ rule information about a personâ€™s age can be used to more accurately assess the probability that the person has cancer.  Random Values    Discrete random variables have a countable number of values. For example, faces of a dice,  number of emails received in an hour etc.    Continuous random variables can take values on a infinite continuum. For example, height  of a person, time to failure etc.  Bernoulli distribution  Bernoulli distribution is a discrete distribution and defined for a binary random variable Bernoulli distribution is a discrete probability distribution. It describes the probability of achieving a â€œsuccessâ€ or â€œfailureâ€ from a Bernoulli trial. A Bernoulli trial is an event that has only two possible outcomes (success or failure). For example, will a coin land on heads (success) or tails (failure)?  Uniform distribution Uniform distribution can be defined for both discrete and continuous random variables. Rolling a fair dice follows a uniform distribution. All outcomes are equally likely.  Normal Distribution Normal distribution is defined for continuous random variables. It is by far the most popular distribution. One of the reasons for the popularity of the normal distribution is that many natural phenomena are approximately following a normal distribution.  Central limit theorem The central limit theorem states that if you have a population with mean Î¼ and standard deviation Ïƒ and take sufficiently large random samples from the existing population, then the distribution of the sample means will be approximately normally distributed.  Identifying/correcting errors  Data Wrangling Data wrangling is the process of cleaning, transforming, and organising a dataset to make it suitable for analysis. Usually involves a combination of manual and automated processes such as:    Handling missing values   Combining datasets   Removing outliers   Normalising the data   Aggregating data into useful summaries  Feature Extraction As computers can only really understand numbers so we need to create a model to represent the data (in this case an image).  Divide the image into smaller blocks than computer features that we choose for example:    Color average   Shapes within the block   Texture   Brightness.  For all the blocks we can computer the mean, variance, and other important statistics. Turning these into a feature matrix we can fed that matrix into an algorithm to classify the image.  Encoding Some features contain categorical values which machines can not understand. Encoding must be used to transform the data into integer values. For example, a dataset of student with features campus [â€œBurwoodâ€, â€œWarun Pondsâ€] and school [â€Architecture and Built  Environmentâ€, â€Engineeringâ€, â€Information Technologyâ€, â€Life and Environmental Sciencesâ€] can be coded as [0,1] and [0,1,2,3].  Scaling Scaling referes to the process of converting a set of values to a new range of values. Given a dataset of height in feet and weight in pounds the two features would contain different ranges and can cause issues when training a model (weight could dominate more than height as the number would be much larger)  Normalisation Normalization is a scaling technique used to transform the values of a dataset into a common range . One common method of normalization is to scale the data to a range of 0 to 1, where 0 is the minimum value in the dataset and 1 is the maximum value. This is known as min-max normalization, and it is calculated using the following formula:      