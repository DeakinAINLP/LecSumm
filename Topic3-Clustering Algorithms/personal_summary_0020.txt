Statistics Random experiment: when the outcome cannot be predicted. E.g., coin toss or dice roll. The possible outcome of this kind of experiment is called an event. E.g., if you have event â€˜Aâ€™ for what card could be picked from a standard deck of cards, all 52 cards could be a possible outcome. Probability  -  Strongly related to machine learning algorithms. -  Defines the likelihood of an event occurring and measured between 0-1. -  The probability of an event (A) occurring is ğ‘ƒ(ğ´). Whereas the probability of A not occurring is ğ‘ƒ(ğ´) = 1 âˆ’ ğ‘ƒ(ğ´)  Joint probability  -  When multiple events occur simultaneously, however the outcomes are independent of one another. For example, two dice being rolled twice with each outcome noted will be independent from each other. The probability of each event separately will not change. The joint probability of these events (A & B) can be shown as ğ‘ƒ(ğ´ ğ‘ğ‘›ğ‘‘ ğµ) = ğ‘ƒ(ğ´)ğ‘ƒ(ğµ). - . Meaning there is a chance that both dice rolled will show a 6. -  When the probability of an outcome (A) is dependent on the outcome (B) occurring. note: ğ‘ƒ(ğµ) â‰  0 -  Defined as: ğ‘ƒ(ğ´|ğµ) = ğ‘ƒ(ğ´ ğ‘ğ‘›ğ‘‘ ğµ) ğ‘ƒ(ğµ)  Bayes rule  -  Used to determine conditional probability. The theory that probability of A based on event B  is related to event A. Random variables A function that can be used calculate the probability of an event for a random experiment. There are two types:  1.  Discrete random variable -  Probability mass function (PMF) defines a random variable by assigning the  probability of each random variable.  -  Cumulative distribution function (CDF): it is the cumulative probability of a random variable found less than or equal to a defined value.  2.  Continuous random variable  -  Probability dense functions (PDF) defines the probability distribution of a random variable.  Probability distribution of random variables Probability distribution refers to the way the probability of values in a dataset can be distributed. -  Bernoulli distribution: used for binary random variables, meaning there are only two possible outcomes. E.g., pass or fail, heads or tails.  -  Uniform distribution: applicable for both discrete and continuous variables. When the probability distribution is equally likely for all outcomes. E.g., dice.  -  Normal distribution: applicable for continuous variables, where the probability can be visualized as a bell curve.  Data wrangling Process of â€˜wranglingâ€™ up data to then processing it through steps which will result in clean, organised, and meaningful data sets. Common steps involved in data wrangling:    Combining multiple data sets and removing outliers.   Converting the data to a format that is easy to analyse.   Transforming the data into useful summary statistics.  Data Data is a collection of facts and can be used to give meaning. It can either be qualitative or quantitative.  Different types of data:  - Images: represented in a numerical vector of features. The image is first divided into smaller sections and each section feature (e.g., colour average, shapes) is collected. With the collected information, a statistical pattern (e.g., mean, variance) can be computed to determine classification. For example, an image of a forest would have more green across each frame compared to an image of the beach.  -  Text: can sorted using â€˜bag of wordâ€™ representation. Captures the number of times key words are present in documents.  -  Signal: a sign or gesture that conveys information. For example, a polygraph test uses signals which can read to determine whether someone is telling the truth or not.  -  Quantitative: can be captured through encoding techniques. Encoding is the practice of assigning an integer value to a feature with a quantitate value. For example, [happy, neutral, sad] could be encoded as [1,2,3]. This makes the data understandable by the machine. The assignment of integer can either be done nominally or ordinally, it is dependent on the requirement.  Data must be scaled so that data of different types can be accurately compared/visualized to optimize the results. This can be done through normalisation which is a scaling technique that transforms the dataset into a standardized range. Doing so can improve the algorithms performance.  