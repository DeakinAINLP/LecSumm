2.1:  - The Bayesian approach in AI helps a computer make decisions by using probability. It learns from data and updates what it knows as new information comes in. This way, the computer can make better guesses/more informed moves over time.  2.2:  - Discrete random variables have separate, countable outcomes, like rolling some dice.  - Continuous random variables can take any value in a range, like measuring temperature.  Examples of random processes in a computer science student's daily life:  Discrete: Number of emails/texts received per day, Wi-Fi/NBN signal strength levels.  Continuous: Time spent on a task, battery charge level of a device such as a phone/laptop.  2.5:  Outdoor:  Colours: More vibrant, natural colours like blue skies, green plants, or earth tones.  Shapes: Large structures like buildings, trees, or natural landscapes (mountains, rivers).  Textures: Rough surfaces like rocks, tree bark, or grass.  Indoor:  Colours: Softer, artificial lighting, and more variety of colours from furniture  Shapes: Room shapes, furniture, or appliances.  Textures: Smooth surfaces like walls, floors, or fabrics from curtains  2.6:  Limitations of the ‘Bag of words’ representation model:  Word order: It ignores the order of words, losing sentence structure and meaning.  Context: It cannot capture context or phrases that depend on surrounding words.  Synonyms: It treats words with similar meanings as separate entities.  Sparsity: It creates a large, sparse matrix with many zeros, consuming memory.  Frequency bias: It can give too much importance to frequent words, which may not be informative.          Reflection on the main points relating to ML:  Data wrangling: Cleaning and organizing raw data to make it suitable for analysis. It involves handling missing values, correcting data inconsistencies, and formatting data for further processing.  Feature extraction: Identifying important variables or features from the raw data that can help in building a machine learning model. This step simplifies the data while retaining its useful information.            Encoding/distribution: Converting categorical data (e.g., text or labels) into numerical values, making it easier for machine learning algorithms to process. This step can involve techniques like one-hot encoding or label encoding.  Scaling/normalization: Adjusting the range of numerical values in the dataset to bring them to a common scale. This helps improve the performance of some machine learning algorithms that are sensitive to the scale of input features.       Statistics in data science: Using statistical techniques to understand the data, find patterns, and make predictions. Data scientists use statistics to analyze data, test hypotheses, and build models.  Data exploration: Examining the dataset to understand its structure, identify trends, and gain insights. This step usually involves visualizing the data through graphs and charts, calculating summary statistics, and identifying potential relationships between variables.  Min/max scaling: A specific scaling technique that transforms features to a fixed range, typically [0, 1]. It helps to improve the performance of machine learning algorithms by bringing features to a common scale, making it easier for algorithms to learn patterns from the data.  