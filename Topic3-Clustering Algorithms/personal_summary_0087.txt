Main Points    In 2.1 basic probability is summarized only to refresh our minds, we go over random experiments, events, how to calculate probabilities of these events and even probability of events not happening or based on conditions(conditional probability).    2.2 goes over random variables which are values generated from a random experiment.  There are two types of random variables being discrete random variables and continuous. In more detail, discrete random variables are shown to be defined using PMF (Probability Mass Functions). The Cumulative Distribution Function can be used when working with certain problems like finding probability of getting a number less than 5 on a die. Continuous random variables are defined using Probability Density Functions.            2.3 goes over distributions of random variables, distributions can be explained as a function that depicts the likelihood of possible values of each outcome in an experiment. The module goes over many different distributions. Bernoulli distribution is one of them and it is shown to describe the probability of success or failure such as a heads or tails coin flip. A uniform distribution can apply to both discrete and continuous random variables, an example of one is rolling a fair dice. A normal distribution is used for continuous random variables and is the most commonly used of all distributions. Briefly explained the central limit theorem.   Brief summarization of data wrangling, which is the process of cleaning, changing, and organizing a dataset to make it appropriate for data analysis to draw conclusions from. In 2.5 we go over feature extraction which is vital in machine learning. It is a way for a machine to transform raw data into numerical features that can be deciphered and processed since computers only understand numbers. In 2.6 there is a brief introduction to text data representation, we are shown how computers take features of data and change them to numbers or feature vectors in some cases and process them in that way. In 2.7 data and signal are summarized very briefly as to what they are and what is the difference between the both of them. In 2.8 we are shown that encoding is an important technique to be done with features that contain categorical data that must be converted to integer values. Again, distribution is explained but, in more depth, it is shown that the distribution of values in a dataset it quite important and can have a huge impact on the performance of a ML algorithm. If the values of a distribution arenâ€™t normally distributed, it can lead to bias and poor performance. In 2.9 scaling is mentioned and quite important for when comparing two features in a dataset, they must be scaled appropriately to have proper ranges that can be understood easily. Normalization is a scaling technique that is used to organize values in a dataset to a more common range. As explained previously, things like this are done to improve the performance of the ML algorithm. In 2.10 we go over statistics and examples of random variables, and discrete random variables through python using SciPy and matplotlib. In 2.11 we go over one of the important sections of data wrangling which is data loading and saving. An example of a pandas data frame is shown in python. In 2.12 we go over the next section of data wrangling is summarized which is data exploration. It shows examples of using pandas to read csv files and output the contents. We also find data description of the csv files using the describe function. Lastly another example is shown where we change missing values and replace them so that there are no null data entries.            In 2.13 we go over data encoding, distribution, and scaling in a bit more depth and with some examples using python.  