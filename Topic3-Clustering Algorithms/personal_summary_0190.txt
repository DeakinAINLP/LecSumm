Summary of the main points that are covered in topic 2:    The fundamental idea behind machine learning algorithms is probability. The outcome of a random experiment or process cannot be anticipated with absolute confidence. An event is a collection of results from a random experiment. Probability measures the possibility that an event will occur and is an event in itself.    P(A) = 1 â€“ P(A)   Probability can be jointly specified for several events. The likelihood of  an event A given the occurrence of another event B is known as conditional probability.    According to the Bayes theorem, the conditional probability of an event depending on the occurrence of another event equals the likelihood of the second event given the first event times the probability of the first event.    P(A/B) = P(B/A)P(A)/P(B) in which P(B) not equal to 0.   A variable with a random value is one whose potential values are  determined by a random phenomenon. Discrete random variable (PMF) and continuous random variable are the two different forms of random variables (PDF).    It is a discrete distribution established by the Bernoulli theorem for a  binary random variable with values X = 0, 1, and 2.    Random variables, both discrete and continuous, can have a uniform distribution. Continuous variables have a defined normal distribution.   The central limit theorem asserts that the distribution of sample means will be roughly normally distributed if you collect sufficiently enough random samples from an existing population that has a mean and standard deviation.    Data representation is a crucial step in developing models from massive amounts of data. Data must be described by "features" for ML to work.    One of the crucial elements in machine language is the feature  extraction technique.     Data is structured into information through representation. While a signal is a symbol or gesture that communicates a specific message.    Scaling is the process of changing one range of values into another. The scaling technique known as normalisation is used to convert a dataset's values into a common range. The effectiveness of the machine learning algorithms is enhanced.     For normalisation, the data ranges from 0 to 1. The minimum value is 0 and the maximum value is 1. The normalisation used here is min-max.    Useful python libraries: NumPy, SciPy, matplotlib,   The cumulative probability related to the function is provided by the  cumulative distribution function.    The steps for handling data include loading and saving data, exploring  data, processing data, distributing data, and scaling.  