Statistics:  some basic definitions  Event: In probability we define an event as a set of outcomes of a random experiment.  Probability: it is defined for an event and is the measure of the likelihood that an event will occur. It is quantified as a number between 0 and 1. The probability of an event A occurring is denoted as P(A). The probability of an event A not occurring is denoted as P(A)’=1-P(A).  Joint Probability: Probability can be defined jointly for more than one event. Consider a random experiment where we toss two coins. In this case the probability of seeing “heads for coin toss 1” and “heads for coin toss 2” is an example of two events. If two events, A and B are independent then the joint probability is P(A and B)=P(A)P(B).  Conditional Probability: The conditional probability is the probability of some event A, given the occurrence of another event B. Condition probability P(A/B), read as the probability of A given B is defined as: P(A/B)=P(A and B)/P(B) Provided P(B) is not zero.  Bayes rule: Bayes rule is a mathematical formula that describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is named after the Reverend Thomas Bayes, who first formulated the concept in the 18th century. In its simplest form, Bayes' rule states: P(A|B) = P(B|A) * P(A) / P(B) where:    P(A|B) is the probability of event A given that event B has occurred.  P(B|A) is the  probability of event B given that event A has occurred.    P(A) is the prior probability of event A occurring.   P(B) is the prior probability of event B occurring.  Random variables  A random variable is a function that can assign probabilities to events of interest in a random experiment.  We usually deal with two types of random variables:    Discrete random variables: have a countable number of values. For example, faces  of a dice, number of emails received in an hour etc.    Continuous random variables: can take values on a infinite continuum. For  example, height of a person, time to failure etc.  Distribution of random variables:    Bernoulli distribution    Uniform distribution.   Normal distribution.   Central limit theorem.  Data wrangling: it refers to the process of transforming, cleaning, and preparing raw data into a format suitable for analysis. It involves several steps, including data acquisition, data cleaning, data transformation, data integration, and data reduction.  Feature extraction: it is the process of selecting and transforming relevant and useful data from raw data into a set of features that can be used in machine learning algorithms. The goal of feature extraction is to simplify the data by reducing its dimensionality while retaining its important characteristics.  Text data representation  This is a very brief introduction to data representation. Without the use of specialist tools, it is beyond the capacity of humans to analyse and interpret large volumes of data. Computers only understand numbers. Words, images and ideas must be turned into numbers before you can feed them into a computer for processing.  Data and signals:  a signal is a function that represents the variation of some quantity with respect to time or space. On the other hand, data refers to a collection of information, often in the form of numbers, words, or images, that can be analysed and processed to extract useful insights.  Encoding and distribution:  Encoding and distribution are two important concepts in data analysis and machine learning. Encoding refers to the process of representing categorical data as numerical values, which can be used as input for machine learning algorithms. On the other hand, distribution refers to the spread of data points or values in a dataset.  Scaling and normalising:  Scaling refers to the process of rescaling the range of values of the features. This is done to ensure that all features are on the same scale and contribute equally to the learning process.  Normalizing, on the other hand, refers to the process of rescaling the values of the features to a common scale, such as between 0 and 1. Normalization is used to ensure that all features are on the same scale and contribute equally to the learning process.  The main difference between scaling and normalization is that scaling transforms the range of values of the features, while normalization rescales the values of the features to a  common scale. Scaling is used to transform the data to a specific range, while normalization is used to transform the data to a common scale.    