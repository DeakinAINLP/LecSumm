  Mathematics underlies every decision, iteration and output in machine learning so it’s important to understand the concepts of vectors and vector operations, matrices and matrix operations, and probability.    Probability process of measuring the likelihood the outcome of a random experiment (e.g. whether a coin toss will result in heads or tails) will occur.   Bayes’ Theorem is a mathematical formula for determining conditional probability.  Conditional probability is the likelihood of an outcome occurring, based on a previous outcome having occurred in similar circumstances. Bayes’ Theorem allows you to update probabilities when new information is learned making it an important concept in machine learning.    Random variables can be either discrete or continuous and they are the possible outcomes of a random experiment (e.g. rolling a dice). They provide a way to quantify outcomes of random processes.    There are several types of probability distributions. Bernoulli distribution is defined for a binary random variable (e.g. pass, fail). Uniform distribution can be defined for discrete and continuous random variables (e.g. rolling a dice or estimating a person’s height). Normal distribution is defined for continuous random variables…    The central limit theorem says you will achieve a normal distribution if you take sufficiently large samples from the population and have a mean and standard deviation.    Data wrangling is the process of cleaning, transforming and organizing data to make it suitable for analysis. Common data wrangling tasks include; correcting errors, dealing with missing and incomplete values, combining datasets, converting data formats, removing outliers, and normalizing and aggregating the data.    Computers are able to process images and use them in machine learning if the images are transformed into numbers. This is done by breaking the image down into many smaller pieces and generating summary statistics describing the attributes of each image (i.e. colour, shapes, etc).    The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms, it turns text into data by extracting and encoding its features. In Python you can use the NLTK library to process and analyse text.    Categorical values need to be encoded as numbers to be understood by computers. Ordinal Encoder, One-Hot Encodings, and Label Encoder and common types of encoding methods.   Distribution refers to the way values within a data set are distributed. Normal, uniform, and skewed are common types of distributions. The way values are distributed can affect the performance of a machine learning algorithm. For example, data with a skewed distribution can cause the algorithm to be biased toward or against some values.    Scaling is the process of converting a set of values to a new range of values. This is necessary if different features in a dataset all have different ranges. Normalization is a scaling technique used to transform ranges into a common range – usually 0 to 1. This standardizes the data and improves machine learning performance.  