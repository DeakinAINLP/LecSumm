I am summarising here what I have learned In topic 2 from the Deakin resources and classes â€“  Last topic, I learned about the basics of machine learning from definition to the use of some of the mathematical concepts in Python using NumPy and pandas. This topic we used more mathematical operations in Python. The first thing that I have learn was the statistics and probability importance in machine learning. If I talk Probability, is the measure of the likelihood that an event will occur, and it is quantified as a number between 0 and 1. A joint probability is defined for more than one event, while conditional probability is the probability of an event given the occurrence of another event. Bayes Rule describes the probability of an event based on another event related to it. Additionally, we learn about random variables, that a random variable is a variable that can have different outcomes in a random experiment. It is a function that assigns probabilities to events of interest in a random experiment. There are two types of random variables - discrete and continuous. Now if I talk about Discrete random variables then they have a countable number of values such as faces of dice and are defined using a Probability Mass Function (PMF) that assigns a probability to each possible value. The cumulative Distribution Function (CDF) gives us the probability associated with a function. Continuous random variables can take values on an infinite continuum such as the height of a person and are defined using Probability Density Functions (PDF) that assign a probability to a range of values. After learning about random variables, CDF, and PMF. I learned about the concept of a probability distribution, which is a function that associates each outcome of a statistical experiment with its probability of occurrence. It then describes three important probability distributions which play a key role in understanding the probability distribution and they are the Bernoulli distribution, the Uniform distribution (for both discrete and continuous variables), and the Normal distribution (for continuous variables). It also introduces the central limit theorem, which states that the distribution of the sample means of sufficiently large random samples from a population with a known mean and standard deviation will be approximately normal. After that we come to know about Data wrangling, also called data munging, which is the process of getting data ready for analysis by cleaning, organizing, and transforming it. This involves finding and correcting errors, dealing with missing values, combining multiple datasets, and making the data easier to understand. It can be a complicated and time-consuming process, but it's essential for getting valuable insights from data. Now if I talk about feature extraction which is one of the important steps in machine learning, where a set of features is derived from raw data to contain information on the target variable. In order to make computers understand images, the first step is to find features that can be represented with numbers. For example, an image can be divided into smaller blocks, and features such as color, shape, texture, and brightness can be computed for each block. The resulting feature matrix can be fed into a proper computer algorithm for classification. We can also conclude from the above-mentioned points that Feature extraction is important to build accurate ML models because just by doing feature extraction we can get much information from the image.  Next, we learn about data representation, how the data is converted into tables in the form of numbers and used in the matrix, and how we use feature vectors to get more insights into the    data. Moreover, we come to know about the difference between data and signals and how signals are a part of useful data only. Signals like a heartbeat, voice, etc can be used as data. We also learn about two important concepts in machine learning - encoding techniques and distribution. Categorical features need to be converted to numerical values using techniques like Ordinal Encoder, One-Hot Encodings, and Label Encoder. The distribution of values in a dataset is also crucial, as it can impact the performance of machine learning algorithms. Normal, uniform, and skewed distributions are common, and a normal distribution is ideal for machine learning algorithms as it makes it easier to learn and make predictions. Furthermore, scaling and normalization are also key concepts that I learned, Scaling is the process of converting a set of values to a new range of values. Normalization is a scaling technique that transforms the values of a dataset into a common range. It is often done to improve the performance of machine learning algorithms, as many algorithms operate better when the data is in a standardized range. Also, I came to know about random variables and their importance, it is a type of variable whose values are based on the outcomes of a random event. In programming, we can use functions like np.random.randn() to generate random variables in the form of matrices or arrays. Discrete random variables are specific types of random variables that have a finite or countably infinite number of possible outcomes. We can use libraries like Scipy and matplotlib to plot the probability distribution of discrete random variables. How to use random data in the pandas data frame. Then we learned about various parts of data wrangling like data saving and exploration, data processing, data distribution, scaling and how to use them using pandas, also learned about categorical value encoding using the LabelEncoder function from the scikit-learn library.  To conclude, In this topic we explored more use of the pandas and numpy in performing data wrangling. Multiple concepts from probability to statistics, normalization. we covered and learned this topic. The quiz result is also attached below with this document.    