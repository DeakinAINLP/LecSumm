Task 2.1P  Probability plays a major role in many machine learning algorithms.  Probability: The measure of the chance that an event will occur, probability, is speciﬁed for an event. Its measurement is represented by a ﬁgure between 0 and 1.  Bayes Rule: The fundamental idea behind the majority of Bayesian approaches is to oﬀer a mathematical formula  outlining  how  you  should  adjust  your  pre-existing  views  in  the  wake  of  new information.  Random Variable: A  random  variable  is  a  variable  whose  potential  values are  produced  by an  unpredictable event.  A  random  variable,  then,  is  a  function  that  can  be  used  to  give  probabilities  to important occurrences in a random experiment. Types of random variables:  1.  Discrete random variables:  Discrete random variables are deﬁned using a Probability Mass Functions (PMF).  2.  Continuous random variables:  Continuous random variables are deﬁned using Probability Density Functions (PDF).  Distributions of random variables: A probability distribution is a function that connects each result of a statistical trial with its likelihood of occurring, which is a straigh(cid:414)orward description of what it is.  1.  Bernoulli Distribution:  Bernoulli distribution is a discrete distribution and deﬁned for a binary random variable with values X = 0 and X=1. So π(0) = P(X = 0) = p and π(1) = P(X = 1) =1 – p  2.  Uniform distribution:  Uniform  distribution  can  be  deﬁned  for  both  discrete  and  continuous  random variables.  3.  Normal Distribution:  Normal distribution is deﬁned for continuous random variables. It is by far the most popular distribution.  Data Wrangling: Cleaning, changing, and organising a dataset to make it appropriate for analysis is the process of "data wrangling," also referred to as "data munging." This is an essential stage in the data science workﬂow and frequently combines human and automated processes.  Due to the fact   that  working  with  jumbled,  unorganized,  and  missing  data  is  frequently  required,  data wrangling can be a diﬃcult and time-consuming process.  Data VS signal: When any data is represented in an organized manner, that becomes information. Signal is another form of data. For example, using an ECG sensor device we can capture the electrical impulses of the heart and produce signals to represent ECG signals which can be plotted over time. This signal can be interpreted by the physician to understand the issue of the heart, if any.  Encoding: Some features contain categorical values that the computer cannot comprehend, in contrast to features with numeric value. Encoding methods are used to transform to integer numbers in order to resolve this.  Distribution: The  term  "distribution"  describes  how  a  dataset's  numbers  are  spread  out.  This  is  crucial because the value spread can signiﬁcantly aﬀect how well a machine learning programme performs.    A  sample  may  contain  a  variety  of  distributions,  such  as  normal,  regular,  and skewed  distributions.  The  most  typical  distribution  has  a  bell-shaped  trajectory  that  is uniform around the mean of the data. A skewed distribution is one in which the values are not uniformly dispersed but are instead concentrated on one side of the range. In contrast, a uniform distribution has values that are spread equally across the data's range.  Scaling and Normalisation: Scaling  in  machine  learning  is  the  process  of  transforming  a  collection  of  values  into  a diﬀerent  spectrum  of  values.  We  have  a  few  characteristics  in  a  collection.  It  may  be challenging to train a model when the raw or unscaled features have various values.  A scaling method called normalisation is used to convert a dataset's numbers into a common range. As many machine learning methods perform better when the data is in a standardised region, this is frequently done to enhance their performance.    