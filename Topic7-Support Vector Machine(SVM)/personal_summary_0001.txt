Relevance refers to how much a feature contributes to the outcome variable, while covariance refers to how much two features vary together. Linear regression iss the relationship between a dependent variable and one or more independent variables. It involves fitting a linear equation to the data, where coefficients represent the relationship between the variables. Linear lassification can be used to predict the class of a given input based on features. It involves finding a linear boundary that separates the classes in the feature space. Generalisation and complexity is crucial in building machine learning models. A model that is too complex may overfit the training data, while a model that is too simple may underfit the data. Logistic regression is used for binary classification, modelling the probability of an event as a function of input features. Training a logistic regression model involves finding optimal coefficients that minimize the difference between predicted probabilities and actual outcomes. Regularized linear models manage complexity of linear models and avoid overfitting by adding a penalty term to the loss function. Examples include L1 and L2 models. Linear regression can be used for feature selection by identifying the subset of features that are most strongly related to the dependent variable. Techniques include examining coefficients or using forward/backward selection. 