1.  Differentiate supervised learning from unsupervised learning. 2.  Estimate the performance of different supervised learning models. 3.  Implement model selection and compute relevant evaluation measures.  Summarising the content: Linear  Regression  –  attempts  to  model  the  relationship  between  two  variables  by  fitting  a  linear equation to the observed data. The linear relationship between two variables can be measured using covariance. Which is the amount of information a specific variable can provide for another.  Pearson’s  correlation  coefficient  –  is  a  measure  of  the  linear  correlation between two variables. It has values ranging from +1 to -1, where 0 is no linear correlation.  Linear Regression formulation –  Logistic Regression – is estimating the parameters of a logistic model – the coefficients in the linear combination.  Using  a  link  function  to  make  the  conditional  probability  of  the  class  as  the  output function. The logistic function (also called sigmoid function) is an S-shaped curve that can take an real valued numbers mapped into values between 0 and 1, but not on those limits. These values approach but never reach the limits. Logistic regression models the log of odds against via linear regression.  Logistic Regression Formulation –  The model can be tested to calculate the value of xTw:  If the value of xTw is > 0 : then P(y = 1|x) > 0.5, and the point is allocated to class 1 If the value of xTw  is < 0 : then P(y = 1|x) < 0.5, and the point is allocated to class 2  ▪ ▪ ▪  Where xTw = 0 : indicates a confused model that returns both values.  Model Complexity –  Bias – Shows how accurate your hypothesis function or model is. In an accurate model with a low error rate, then bias will be a small value potentially close to 0. Variance – measures the tolerances of the model with changing data sets. Higher variance make for a more complex mode.  Variance-Bias trade off – in Machine learning the increase variance makes an increase in complexity but reduces the bias. But, reducing the complexity and the variance leads to a greater Bias. Balancing the two traits to get the right model complexity will give the minimum error.  Regularised Linear Models – uses a regulariser as an additional term in the loss function to avoid overfitting. It aims to keep the parameters more normal or regular. It works to prevenent the regression coefficients or weights from taking excessively large values. Where weights are excessively large – it can indicate a high dependence on particular features.  Regularisation (LASSO) – is the ‘Least Absolute Shrinkage and Selection Operator’. This is a regression analysis method that performs both variable selection and regularisation in order to enhance the prediction accuracy and interpretability of the statistical model.  Regularisation (Ridge) – referred to as Elastic Net. Elastic net can overcome the problems in high dimensional space since it can select a greater number of variables despite the number of data points.  