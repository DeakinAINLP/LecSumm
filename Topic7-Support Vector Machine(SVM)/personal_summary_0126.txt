To understand the connections between the characteristics or variables in a dataset, relevance and covariance are two crucial concepts in data analysis. The development of more precise prediction models and the detection of redundant characteristics can both be aided by understanding these principles.  Relevance describes how closely a specific characteristic or variable in a dataset is connected to the result or goal variable. In contrast, covariance describes how much two or more characteristics or variables in a dataset fluctuate together. (Pires, 2016)  Linear classification is a type of supervised learning algorithm in which the goal is to classify input data into one of two or more classes based on a set of features or predictors. It is called "linear" because it involves fitting a linear decision boundary that separates the input data into different classes. (Krishna, 2020)  Model flexibility refers to how well a statistical model can represent the underlying connections and patterns in the data. By varying the number of features or predictors included in the model as well as the extent of polynomial expansion of these characteristics, it is possible to alter the model complexity in linear regression. (Nanos, 2023)  In order to avoid overfitting and enhance generalisation performance, regularised linear models are a subset of linear regression models that incorporate a regularisation component in the loss function. The most popular forms of regularisation are L1 and L2, and methods like cross- validation may be used to calculate how much regularisation is best. Regularised linear models may be used for feature selection as well as high-dimensional data with several characteristics. (Anuja Nagpal, 2017)  The model's coefficients may be used to choose features using linear regression. The coefficients show how much each attribute contributed to the result variable that was predicted. When it comes to forecasting the outcome variable, factors with high coefficients are seen to be more crucial than those with low coefficients.  