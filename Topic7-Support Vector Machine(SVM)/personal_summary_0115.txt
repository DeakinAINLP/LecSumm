 Topic 6 - Linear Models - Evernote  Topic 6 - Linear Models  Covariance : It measures the amount of information a specific x can provide for y.  Cov(x,y) > 0 -> x and y are positively correlated.  Cov(x,y) < 0 -> x and y are inversely correlated.  Cov(x,y) = 0 -> x and y are independent.  Pearson's Correlation Coefficient : It is a measure of the linear correlation between two variables. It has a  value between +1 and -1. +1 indicated total positive correlation, 0 indicates no linear correlation, -1  indicates total negative correlation.  Error of value prediction : The difference between what we predicted and the true value of output of that  point is considered to be the error.  When there are only two possible values for output, its called a binary classification problem.  If there are more values, its a multi-class classification problem.  Linear regression has a closed form solution. Python implementation uses Singular Value Decomposition (SVD) to compute the Moore-Penrose inverse of matrix X.  Model complexity of Linear Regression.  It increases with the number of features. The risk of overfitting is higher for a limited set of training data. Using a limited number of features might result in under-fitting  as well.  Logistic regression formulation. It is named after the function used at the core of the method, the logistic function.  Its also called a sigmoid function. Its an S-shaped curve which can take any real valued number and map it into a value between 0 and 1.  Logistic regression models the log of odds against called logit value.  Training a logistic regression model. It means using the training data to estimate the regression coefficient vector w. This is  achieved by using Maximum Likelihood Estimation (MLE) to estimate w.  Convex optimisation : It can deal with only one optimal solution, which is globally optimal. The other is that you prove that there is no feasible solution to the problem.  Non convex optimisation : In this type, we may have multiple locally optimal points. It can take a lot of time to identify whether the problem has no solution or if the solution is global.  Hence, the time efficiency of the convex optimisation problem is much better.  Model complexity https://www.evernote.com/client/web?login=true#?fs=true&n=d1bfb8f2-a9bb-a1e8-089c-1a9e222dcdc2&  1/3   Topic 6 - Linear Models - Evernote  Over-fitting happens when we find an overly complex model based on the data.  Under-fitting is the result of an extremely simple model.  Bias Variance decomposition  Increasing the variance of a model means lowering bias as the model becomes more  complex. Low complexity for a model will result in high bias and low variance.  The best model is a model with low variance and low bias.  Models with too few parameters are inaccurate because of a large bias - under-fitting  Models with too many parameters are inaccurate because of a large variance - over-fitting.  Regularised linear models  A regulariser is an additional term in the loss function to avoid overfitting. It does not allow  regression coefficients to take excessively large values. This procedure is a way to guide the  training process to prefer certain types of weights over others.  https://www.evernote.com/client/web?login=true#?fs=true&n=d1bfb8f2-a9bb-a1e8-089c-1a9e222dcdc2&  2/3   Topic 6 - Linear Models - Evernote  https://www.evernote.com/client/web?login=true#?fs=true&n=d1bfb8f2-a9bb-a1e8-089c-1a9e222dcdc2&  3/3   