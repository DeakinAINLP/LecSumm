 Overview In topic 6, we focused on the following learning goals:    Model Assessment   Model Selection  Relevance and Covariance Among Features or Variables Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data.  We can measure the linear relationship between variable X and output Y using covariance  It’s calculated as:  Pearson Correlation Coefficient  A measure of the linear correlation between two variables, x and y. it has a value between +1 and -1, where 1 indicates total positive linear correlation, and 0 is no linear correlation, and -1 shows total negative linear correlation  Simply put,  -  the closer to 1, the stronger the positive relationship -  the closer to 0, the weaker the relationship -  the closer to -1, the stronger the negative relationship  Linear Regression Formulation The linear equation should allow us to summarize and study relationships between two continuous variables.  First we define a line:  If we have a point Xi, we can estimate the value of the line function Y(Xi).  If X is not a single dimension value, we write the linear regression as:  Where Xi0 is a dummy feature with the value of 1 and also W0 = b  How can we fit this line to data points? First we review the error of value prediction in regression. The error is the difference between the predicted value and the true value:  Linear Classification Linear regression is the appropriate regression analysis to conduct when the output values of the feature vectors are binary. Like all regression analysis, the logistic regression is a predictive analysis.  Reviewing Linear Classification When there are only two possible values for output, we call the problem a binary classification problem. For example, given an image, the task maybe to classify if it’s an image of a fruit or not a fruit. By linear classification, we mean that the separation boundary between any two classes is linear. This is just our hypothesis.  Logistic Regression Two approaches are generally available:  -  ignore non-linearity -  using link function  when using logistic link function to relate (X^T)W  to y, the linear model is called Logistic Regression. Logistic Regression is a popular model for classification.  Generalization and Complexity Linear Regression has closed form solution. Python implementation uses Singular Value Decomposition (SVD) to compute the moore- Penrose inverse of matrix X. we considered the linear regression problem as y = Xw where we used the hypothesis function:  Generalization (Prediction on unseen data) After training a linear regression model, we can start to predict the output y for a new instance x. the predicted output is computed as y = (x^T)W. given an unseen set of instances as a test set, we can  measure the error in prediction as:  Model Complexity of Linear Regression Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. This is because of the risk of over- fitting on this limited set of training data. Using a limited features may cause under-fitting.  Logistic Regression Formulation Also called the sigmoid function, it’s an S-shaped curve and it can take any real-valued number and map it into a value between 0 and 1 but never exactly at those limits.  Let x be a data instance, and y be its class label in {-1, 1}. Logistic regression does not directly model y in terms of x. Instead, it models something called logic value or log of odds via linear regression.  What are the odds?  The odds of class -1 is defined as:  Example: the odds that a randomly chosen day of the topic is a topicend are:  Logit  The log of odds is called logit. Logistic regression used the following linear model:  Training a Logistic Regression Model Training a logistic regression model means using training data to estimate the regression coefficient vector W. We can use maximum likelihood estimation (MLE) to estimate W. the likelihood function of W using data (Xi, Yi) is given as:  Logistic Loss Function  Maximizing likelihood is equivalent to maximizing the log of the likelihood function because both provide the same solution for W. remember, by taking the log of the function you are still able to find the maximum or minimum of the function since the logarithmic functions are monotone increasing functions. Thus, you can write the log of the likelihood function by taking the log of l(W) as:  Logistic Regression Example Suppose we are working with doctors, who are investigating patients who have suffered a heart attack. The dependent variable is whether the patient has had a second heart attack within 1 year (yes = 1). We have two independent variables (IV), one is whether the patient completed a treatment consistent of anger control practices (yes = 1). The other IV is a score on a trait anxiety scale (a higher score means more anxious)  After running logistic regression, we get:  Now let’s see what is the prediction rule for class 1:  What do we understand?  When both independent variables are zero, we classify no 2nd Heart Attack (Do you know why?)  because = -6.36 < 0  -  the anger treatment seem to be lowering the risk score of a 2nd  heart attack  -  the anxiety trait seems to be increasing the risk score of a 2nd  heart attack  Model Complexity Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model.  Unver-fitting occurs if the complexity of the model is lower than necessary.    Scenario-1: we may be using a linear model, when the data  requires a nonlinear model.    Scenario-2: we may be using the right hypothesis (linear or  nonlinear) but the number of variables might be falling short of what is required. For example, to predict the income of a person, age alone may not be sufficient.  Example 1: To predict a person’s income, knowing age alone is not sufficient. Assuming our dataset has information about age, sex, education; we could add them as explaining variables. Our model becomes more interesting and complex.  The new model explains the data better, but we need to add even more variables (i.e. location, profession of parents, etc.)  Our model will be even better but will probably be over-fitting now. It will probably produce poor predictions on unseen data.  Example 2: Let’s say you attend a symphony and want to record the clearest sound possible. You buy a super-sensitive microphone and audio system to pick up all the sounds in the audiotorium. Now, you have started to over-fit. You are detecting unhelpful, undesirable noise.  So fitting a perfect model is only listening to the Symphony and not to the background noise  Variance Bias Trade Off The higher the variance the more complex the model.  As you can see, the best model is with low variance and low bias.    Higher bias results in lower variance and high variance results in  lower bias.    We need to find the sweet spot where:  o  Risk = bias^2 + variance + noise    The minimum error is at the right model complexity.  Regularized Linear Models A regularizer is an additional term in the loss function to avoid over- fitting. It’s called that because it tries to keep the parameters more normal or regular; it does not allow regression coefficients to take excessively large values. If one or more weights are excessively large, it implies your model is highly dependent on that one feature.  What if this feature is noisy or highly affected by noisy observations? We don’t want to rely too much on any one thing when we are designing a model in machine learning! So this procedure is a way to guide the training process to prefer certain types of weights over others.  You can consider this term as complexity of the model.  Linear Regression for Feature Selection  Principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination are typical feature selection methods. The specific challenge and the features in the dataset determine the feature selection method to choose.  The basic idea behind using linear regression for feature selection is to evaluate the strength of the relationship between each feature and the target variable.  