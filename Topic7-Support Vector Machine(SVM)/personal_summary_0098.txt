How can we get an unbiased estimate of the accuracy of a trained model?  -  When training a model, always separate a test set  o  This is because if the test set labels influence the training model in any way, the accuracy estimate will be biased (do not use the same data points you used for training your model).  o  Data points in training set are used for learning, data in test set are utilized for assessing  the learning model in terms of accuracy.  Relevance and Covariance among features or variables  -  Linear regression attempts to model the relationship between two variables by fitting a linear  equation to the observed data.  -  Pearson’s Correlation Coefficient  -  Measure of the linear correlation between two variables; value between +1 and –1 where 1 indicates a total positive correlation, 0 = no linear correlation and –1 shows negative linear correlation.  -  Linear regression formulation  -  We want to find a line similar to h -  The linear equation should allow us to summaraise and study relationships between two continuous variables  -  - -  W = slope -  B = y intercept  Linear classification  -  Logistic regression is the appropriate regression analysiss when the output values of the feature vector are binary  -  When there are only two possible values for output we call the problem a binary classification  -  problem, if it has more values it’s a multi-class classification problem e.g., if youre given an image and the task is to classify whether if it’s picture of a fruit or not, or if you’re to decipher whether it’s a banana, apple, mango etc for a multi-class classification problem  So what is linear classification?   The separation boundary between two classes is linear  -  Logistic regression  -  o  Two approaches: o  Ignore non-linearity:  ▪  Using least squares for classification: treat binary outputs like the outputs in the  regression problem  o  Using link function:  ▪  Use the conditional probability of the class as the output in the regression  problem, I.e., fitting regression   Least squares regression can perform very badly when some pints in the training set are excessively large or small values for the dependent variable compared to the rest of the training data; this is due to least squares method being concerned with minimizing the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the data will have a disproportionately large effect on the resulting constants that are being solved for.  Generalization and complexity  Linear regression has a closed form solution, python’s implementation uses Singular Value Decomposition (SVD) to compute the Moore-Penrose inverse of matrix X  Generalization (Prediction on unseen data)  -  Model complexity of Linear regression  -  Model complexity of linear models increases with the number of features. We should be aware of model complexity if we have a limited set of training data. The reason is the risk of over- fitting on this limited set of training data. Using a limited number of features may also be problematic as it could cause under-fitting  Logistic regression formulation  -  Logistic function is also called the sigmoid function; its an S-shaped curve and it can take any real-valued number and map it into a value between 0 and 1 but never exactly at those limits. Logistic regression does not directly model y in terms of x instead it models something called logit value, or log of odds against via linear regression, so generally we are modelling log of odds based on x  -  Logit   Summary  -  Logsitc regression is like a regression problem and the only difference is in modeling the output, in linear regression we are modelling the y directly but in here we are modeling he logit(log of odds).  Training a logistic regression model  -  Means using training data to estimate the regression coefficient vector w  -  Computing the minimum  -  Difference between two types of cuntions Convex and Non-convex:  Basic differences between the two categories  -  Conex optimizations can deal with only one optimal solution, which is globally optimal. The  other possibility is that you prove that there is no feasible solution to the problem (right image on figure above) In non-convex optimizations, you may have multiple locally optimal points, it can take a lot of time to identify whether the problem has no solution or if the solution is global (left image). Hence, the time efficiency of the convex optimization problem is much better.  Sometimes we can derive a closed form formula for the minimizer (e.g., linear regression) meaning we can compute the minimizer in one step If there is no closed form formula, we must take multiple steps iteratively to reach the minimum (e..g, logistic regression and kmeans)   So:  Iterative optimizing  -  Two popular methods to compute gradient (derivatives) of the objective function are: -  Gradient Descent (uses first derivative)  -  Coordinate-wise Gradient Descent Optimization  -  Logistic regression example   Model Complexity  -  Over-fitting happens when we find an overly complex model based on the data, under-fitting is  the result of an extremely simple model   - -  We can detect under-fitting if the model fitting error on the training data is high  Example  -  To predict a persons income, age alone isn't sufficient; if our dataset had more information about age, sex, education and could add them as explaining variables and our model becomes more interesting and more complex  -  Recording a symphony and you use ultra sensitive microphone – you detect all of the  undesirable noise e.g., talking, page turning, food eating  Variance bias trade-off   Summary  Low bias implies high variance, high bias implies low variance  - -  We need to find the sweet spot where risk= bias + variance + noise is the minimum -  The minimum error is at the right model complexity  A model complexity grows with the number of features, using all data dimensions as features may fit the model on background noise as well as true patters (signal)  Regularized linear models  -  A regularizes is an additional term in the loss function to avoid overfitting – it keeps the  parameters more normal or regular; it does not allow regression coefficients to take excessively large values  -   Regularization might make sense as it greatly reduces the variance  Linear regression for feature selection  -  -  Principal component analysis (PCA), correlation-based feature selection and recursive feature elimination are typical feature selection methods, the specific challenge at hand and the features in the dataset determine the feature selection method to use. To find the most significant features in a dataset, linear regression can be sed as a feature selection strategy. The basic idea behind using linear regression for feature selection is to evaluate the strength of the relationship between each feature and the target variable. The features with the highest absolute coefficient values can be found using linear regression.   Regularized linear regression in Python  -  Model is under-fitting; a lasso penalty is .01 too high, there are only 2 non-zero coefficients  Logistic regression in python   Regularization using L1 and L2  You can perform regularized logistic regression by specifying two arguments in the function call:  Penalty: this takes values for l1 for lasso and l2 for ridge regression  - -  C: This is the inverse of regularization parameter alpha or lambda, smaller values specify  stronger regularization   