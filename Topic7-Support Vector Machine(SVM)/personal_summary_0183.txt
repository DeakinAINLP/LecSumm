Introduction  This topic we will be examining the assessment of our trained models. The goal of our models is to learn patterns in data that can then be used with new and unseen data. However, if our trained model is biased, then the predictions will be biased as well, leading to inaccurate predictions.  As an example of this occurring in the real world, look no further than the facial recognition people use on a daily basis to unlock their phones: research into these face recognition technologies shows that they are biased toward pale males, performing poorly for darker females (SITNFlash 2020). If these technologies are used by law enforcement this bias could lead to inequitable outcomes.  To ensure that our models aren’t biased, we need to devise ways of getting unbiased estimates of their accuracy once they are trained.  Relevance and Covariance  “Covariance” is the measure of the relationship between two variables. It measures how much the variables change together.   Co: together    variance: change  Covariance does not measure any form of dependency between the variables. Covariance is measured in units: the units of the two variables multiplied together. If the covariance result is:        > 0: Positive covariance, indicating that the variables are moving in the same direction  < 0: Negative covariance, indicating that the variables are moving in opposite directions  0: There is no relationship between the two variables.  The formula used to calculate covariance is:  with x being the mean of xi and y being the mean of yi.  Covariance is not the same as Correlation!  Covariance measures the variance of the variables from their expected value.  Correlation measures the strength of a linear relationship between the variables. It does not have units. Its values range from -1 through to 1. -1 indicates that as the value of one increases, the other will decrease. A value of 1 indicates that there is a direct relationship between the variables. A value close to 0 indicates that there is no relationship.  The most common correlation coefficient is called the Pearson correlation coefficient: it measures the both the strength and the direction of the linear relationship between the two variables. The Pearson correlation can’t asses nonlinear relationships between variables, and can’t differentiate between dependent and independent variables.  Linear regression  Example  As discussed in last topics lectures linear regressions have a closed form solution: [wo,w1] = ([1, x1]T[1, x1])-1[1, x1]Ty  Hence if we have two vectors, xi = [1, 2, 3, 4] and yi = [ 1, 2, 2, 4] and we want to create a model that will, when given some input x predict the y, then we can see from the above solution that we need to create a dummy feature containing 1’s  and append it to the x:  1 4)  y=(1 X =(1 1 4)  1 2 1 3  2 2  Plugging this into the closed form solution:  w=( X T X )−1 X T y =Y =(w 0  w 1)=(0.5 0.6)  Having the value for w means that we now make predictions based on it.   For example, given an x of 2.5, what could we expect the y value to be?  y=(1 2.5)(0.5  0.6)=1×0.5+2.5×0.6=0.5+1.5=2  Formulation  In our worked example, we were trying to fit a line that best fits through our data points:  The formula for this line is: y = wx + b, with:   w being the slope    b being the y intercept (the value of y when x = 0)  There are two ways in which we can do this:  1. To find the line that minimises the total absolute error between the line and our data points. This is doable, but computationally expensive, as there is no analytical solution, meaning we have to iterate until we find the best fit line.  2. To find the line that minimises the total squared error between the line and our data points. This way does have an analytical solution, so it is the preferred way of solving for this formula.   Linear classification  If there are only two possible values for our output target, the problem is termed a binary classification problem. For example, given an image, does it contain a light bulb?  If there are more than two possible values for our output target, the problem is termed a multi-class classification problem. For example, given an image, does it contain an incandescent, a fluorescent or an LED light bulb?  Linear classification is when we try to find a linear relationship between the classes we are investigating. This relationship might not exist!  Having decided on a linear relationship, how do we classify our class instances? We invoke a classification function that measures the distance between the class instance and our dividing line. This function can either be a simple one, such as a least squares measure, or a more complex one that uses probability to fit the regression line.  This more complex function is termed a “logistic link function”, and it will output a value between 0 and 1, representing the probability of the relationship. If using a logistic link function, the linear classification is termed a “Logistic Regression”.  Other functions, such as tanh can also be used.  Generalisation and complexity  Model complexity will grow as the number of features in the data grows. This is a problem we should be aware of, especially if the training data is a small set. This is because a small set of data with a large number of features can lead to over-fitting of our model. Conversely, if we have a small number of features, and a small data set, we risk having our model under-fit the data.  Logistic regression  As mentioned earlier in this document, when we perform linear classification using a logistic link function, the linear classification is termed a “Logistic Regression”.  When using standard linear regression we are modelling the output y directly. However, when using logistic regression, we are modelling the log of odds (logit) rather.  Model complexity  The lecture notes have a wonderful visual example of under-fitting and over-fitting:   Under-fitting can occur if we:  1. are using a linear model: and the data actually requires a different model.  2. do not have the right number variables that is required to make accurate predictions.  If we discover that we have the second scenario in play, and so we start to add more variables to our model, we then run the risk that we might add too many variables, so start to over-fit. Which takes us to the next scenario that we need to be aware of.  Over-fitting can occur if we have selected too many variables: we could be introducing noise into our model that won’t be present in the unseen data.  Regularised linear models  A regulariser is an additional function that we can add to the loss function to avoid over-fitting. It works by preventing regression coefficients from taking on large values.  A regulariser that is proportional to the L1 norm is termed a LASSO (Least Absolute Shrinkage and Selection Operator). LASSO effectively penalizes the model for using too many features. This encourages the model to select a subset of the most important features. The strength of the regularization can be controlled using a hyperparameter, which can be tuned to find the optimal trade-off between model complexity and performance.  A regulariser that is proportional to the L2 norm is termed a Ridge reguliser. One difference between LASSO and Ridge regularisers is that Ridge tends to shrink the co-efficients towards 0, without them ever reaching 0, whereas the LASSO regularizer can force them to 0. This means that LASSO can identify the most important predictor variables, and discard the less important ones: something that Ridge doesn’t do.  Linear regression for feature selection  Linear regression can be used as the a feature selection strategy by evaluate the coefficients of the the regression model. This is possible because in linear regression the the goal is to predict an output target based on one or more input features. The linear regression model represents the relationship between the input features and the output target as a linear function. The coefficients of this linear function represent the weights assigned to each input feature, which then determine the impact of the input feature on the output that is predicted.   So a strategy would be to fit a linear regression model using all of the input features, then evaluate the resultant coefficients of the model. As the coefficients represent the importance of each feature in predicting the output, the features with the largest coefficients can be see as the most important and selected for further analysis.  Regularised and Logistic linear regression in Python The Scikit-learn (Pedregosa et al. 2011) library has the following classes for performing linear regression tasks:          LinearRegression squares between the observed targets and the predicted targets.  : A class which fits a linear model that minimises the sum of the  Ridge  : A class wich fits a linear least squares model with l2 regularization.  Lasso  : A Linear Model trained with L1 prior as the regularizer.  LogisticRegression applied by default.  : A class that fits a logistic regression. Note that l2 regularization is  This topics problem solving activity goes through implementing both regularized and logistic linear regression in depth. For more details please see the code in that.  