  Summary:  o  Linear  Regression:  Linear  regression  is  a  statistical  approach  for  modelling connections  between  one  or  more  independent  variables  and  a  dependent variable.  It  is  often  used  for  prediction  in  supervised  learning.  Simple  linear regression employs a single independent variable and a straight line to represent the connection. The aim is to select the best-fitting line that minimises the sum of squared discrepancies between the predicted and actual values. Two or more independent variables are used in multiple linear regression. Linear regression models may be trained using a variety of approaches, but they have constraints such as susceptibility to outliers and the linearity assumption. More complicated models may be required if the connection is not linear.  o  Logistic Regression: A statistical approach for modelling connections between a binary  dependent  variable  and  one  or  more  independent  variables  is  logistic regression. It is commonly used in supervised learning for categorisation tasks. To represent the relationship between variables, logistic regression employs the logistic  function,  which  produces  a  probability  between  0  and  1.  For  binary classification,  a  threshold,  commonly  0.5,  is  utilised.  Maximum  likelihood estimation (MLE) is used to calculate coefficients. Logistic regression is simple to use,  interpretable,  and  can  handle  a  variety  of  factors.  However,  it  has disadvantages, such as susceptibility to multicollinearity, linearity assumptions in  the  log-odds  space,  and  the  inability  to  properly  describe  complicated interactions. If the connection is not linear, more complex procedures may be required.  o  Model Complexity:  ▪  Bias: Model complexity relates to a machine learning model's capacity to detect patterns in data, which influences its ability to generalise to new data. One of the two major types of mistake is bias, which results from employing  a  simplified  model  that  does  not  adequately  reflect  the underlying connection of the data, resulting in underfitting. Bias normally reduces  as  model  complexity  grows,  however  lowering  bias  too  much might result in overfitting. The objective is to achieve the best balance of bias and variance, which will result in a model with minimal generalisation error and excellent performance on unknown data. Regularisation, cross- validation,  and  suitable  model  selection  all  aid in  achieving  this equilibrium.  ▪  Variance: Variance, along with bias, is one of the two primary sources of mistake  in  model  complexity.  It  refers  to  the  sensitivity  of  a  model  to variations in training data, with excessive variance resulting in overfitting. Bias  diminishes  as  model  complexity  grows,  although  variance  may increase. Machine learning seeks to manage bias and variance in order to achieve minimal generalisation error and strong performance on unseen data.  Regularisation,  cross-validation,  model  selection,  ensemble approaches,  and  feature  selection  are  among  techniques  for  achieving   this  balance.  These  strategies  aid  in  the  development  of  models  that balance bias and variance, hence increasing generalisation to new data.  o  Regulariser:  ▪  L1: L1 regularisation, also known as Lasso regularisation, is a method that adds a penalty term to the objective function to prevent overfitting and enhance  model  generalisation.  The  penalty  is  calculated  by  adding  the absolute  values  of  the  model's  coefficients.  L1  regularisation  promotes sparse  weights,  resulting  in  a  more  interpretable  model  with  fewer features,  which  essentially  does  automated  feature  selection.  It  is beneficial for high-dimensional data or data that has a large number of unnecessary  or  redundant  attributes.  However,  with  strongly  linked characteristics,  L1  regularisation  may  be  less  stable,  and  in  such circumstances,  L2  regularisation  or  a  mix  of  L1  and  L2  regularisation (Elastic Nett) may be more suited.  ▪  L2: L2 regularisation, also known as Ridge regularisation, is a method that reduces  overfitting  and  enhances  model  generalisation  by  adding  a penalty term to the objective function based on the squared values of the model's coefficients. Small weights are encouraged by L2 regularisation, resulting  in  a  more  stable  and  less  complicated  model.  It  is  especially helpful  in  dealing  with  multicollinearity  because  it  offers  stability  by dispersing weights across associated characteristics. However, unlike L1 regularisation, L2 regularisation does not undertake automated feature selection,  resulting  in  a  less  interpretable  model.  When  both  feature selection  and  stability  are  needed,  Elastic  Nett,  a  mix  of  L1  and  L2 regularisation, can be utilised.    Reading list: Lecture Slides, Lecture Recordings, Learning Contents.   My  reflections:  In  supervised  learning,  linear  regression  models  the  connections between independent and dependent variables for prediction. A single independent variable is used in simple linear regression, whereas two or more are used in multiple linear regression. However, it has disadvantages such as being susceptible to outliers and assuming linearity. For  classification  problems,  logistic  regression  models  the  associations  between  a binary dependent variable and independent factors. It employs the logistic function, which provides probabilities ranging from 0 to 1. It offers benefits such as ease of use and interpretability, but it also has drawbacks such as sensitivity to multicollinearity and the inability to model complicated interactions. The capacity of a model to generalise to new data is affected by model complexity. The objective is to obtain minimal generalisation error and high performance on unknown data  by  balancing  bias  and  variance.  Regularisation,  cross-validation,  and  model selection are all techniques that can assist attain this equilibrium. L1 (Lasso) and L2 (Ridge) regularisation methods add penalty terms to the objective function to prevent overfitting and increase generalisation. L1 favours sparse weights and  automated  feature  selection,  whereas  L2  encourages  tiny  weights  and  gives    stability in multicollinearity instances. When both feature selection and stability are needed, Elastic Nett, a mix of L1 and L2 regularisation, can be utilised.  