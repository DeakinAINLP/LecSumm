 Summarise the main points that were covered. Main points covered in topic 6 of the unit content:   Linear regression  Logistic regression   Model complexity   Regulariser  Reflection My reflection on the knowledge gained this topic from reading the unit contents for this topic with respect to machine learning.  This is a reflection on the knowledge that I gained during topic 6 in regard to machine learning.  This topic I learnt about linear regression. The goal is to find the relationship between the dependant variable Y (class label), and the independent variable X (feature). The formula for linear regression is y = wx + b. Within this formula, X is the independent variable, Y is the dependant variable, w is the slope of the line, and b is the intercept on the y-axis. X can be one or more values, so if the number of X (features) increases and therefore increasing the dimensionality, then the formular for multi-dimensional data can be used.  I also was introduced to Logistic Regression. It uses a logistic function that can take a value in the data set and map it to a value between 0 and 1. The logistic function uses the Maximum likelihood (MLE) to help find a solution for w within the equation. The function fits an S shaped curve from 0 to 1. From this curve it can be known which category a data point belongs to. For an example where there are two categories with a data point towards the middle of the S line, the chance that the data point belongs to either category is around 50%. Whereas if the data point is at the end of the S line, the stronger likelihood it belongs to one category over the other. The aim is to categorise something, rather than to predict a continuous value like in linear regression.  I gained more knowledge on model complexity. This topic overfitting and underfitting was discussed again, however, I was also introduced to the new concepts of Bias and Variance. With high bias, a model is not complex enough to accurately predict solutions. Whereas a high variance model potentially contains too much noise, where it might perform well on training data but is not general enough to accurately predict solutions from new data. The bias variance trade off arises when fixing these issues within the model. Increasing the variance within the model results in lowering the bias in the model, and higher bias in the model can result in lower variance. Ideally the model works best with low variance and low bias.  I also learnt about using a regulariser in linear models. Within the loss function, another item is added to help reduce the chance of overfitting. It achieves this by discouraging the weights from having values that are too large. Using regularisation reduces the variance in the model and may then increase the bias in the model, similar to what was discussed previously. Two examples of techniques for regularisation are L1 (LASSO â€“ Lease Absolute Shrinkage and Selection Operator) and L2 (Ridge).   