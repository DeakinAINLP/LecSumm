In this topic we learnt about Linear Regression, Logistic Regression, Model Complexity (Bias & Variance) and Regularizer (L1 and L2). Regression is a statistical techniques which tries to find relationship  between  dependent  variable  and  one  or  more  independent  variables.  Linear Regression uses linear equations to model the relationship between two variables. To minimize the empirical risk squared loss function can be used and thus we can find the best fitting line for linear  regression.  Logistic  regression  is  another  form  of  regression  for  predicting  categorical values.  Maximum  likelihood  estimation  is  used  instead  of  squared  loss  function  in  Logistic regression. Mostly used in binary categorical data. Likelihood function can be modified by taking the log to form the Logistic log function L(w). For Logistic regression we use logistic loss function:  W  can  be  learned  iteratively  since  closed  solution  is  not  there.  To  find  derivative  of  objective function Gradient descent or Newton’s method can be used. Gradient descent used first derivative and  maximizes  a  function  using  knowledge  of  its  derivative.  Newton’s  method  use  second derivative  and  a  root  finding  algorithm  maximizes  a  function  using  knowledge  of  its  second derivative.  We are considering  overfitting and underfitting  with  regards to  model complexity. Overfitting occurs  due  to  extremely  complex  model  on  data  due  to  over  training,  whereas  underfitting  is originates from extremely simple model. Increasing the the variance of a model indicates lower bias and the model becomes complex. On the flipside, low complexity for a model will result in high bias and low variance. A best model is a product of low variance and low bias. Best model should not be having too few parameters or too many parameters. In general we can conclude we need to find a spot where risk is minimum for better model. Regulariser is an additional term in the loss function to avoid overfitting.  Regularizer functions can be of two types. One uses l1 norm and other one uses l2 norm in the regularization function. L1 norm is also called as LASSO(Least Absolute Srinkage and Selection Operator. L2 regularizer are called Ridge and Elastic net.Also the risk of having sparse sparsity in the l1 norm is more.   