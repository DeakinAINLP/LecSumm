Summary: Through my study of Topic 6 I learnt by reading the content, watching the videos, taking notes and working through the activities. Evidence can be seen below.  Activity 6.1:  Assessing a trained model  Activity 6.2:  1.  Relevance and Covariance among features or variables 2.  Pearsonâ€™s Correlation Coefficient  Activity 6.3:  1.  Example of linear regression  Activity 6.4:  1.  Linear regression formulation 2.  Binary or Multi-class classification 3.  Logistic regression  Activity 6.5:  1.  Linear classification.  Activity 6.6:  1.  Generalisation and complexity  Activity 6.7:  1.  Logistic regression formulation 2.   Logistic/sigmoid function  Activity 6.8:  1.  Training logistic regression model 2.  Logistic Loss Function 3.  Iterative optimising  Activity 6.9:  1.  An example of Logistic Regression.  Activity 6.10: Model Complexity Under and overfitting. Bias Variance Decomposition Variance bias trade off  Activity 6.11: Regularised linear models Two popular regularising functions Regularisation impact Lasso (Least Absolute Shrinkage and Selection Operator) Ridge Regularisation Regularisation increases bias in our model  Activity 6.12: Linear regression for feature selection.  Activity 6.13:  MSE decreases with increasing model complexity Size of coefficients, in general, increasing model complexity Objective_fn = Regression_Loss_Function + lambda * (sum of square of coefficients) In general literature, the L2 penalty is called lambda, but in Ridge regression, the parameter is called alpha. Objective_fn = Regression_Loss_Function + alpha * (sum of absolute value of coefficients)  