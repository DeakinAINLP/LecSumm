Evidence of Learning  Module 6: Linear Model  Module Learning Outcomes –  I certify that I have successfully learned and understood the following topics:  1.  Relevance and covariance among features or variables 2.  Linear regression formulation 3.  Linear classification 4.  Generalization 5.  Complexity 6.  Logistic regression formulation 7.  Training a logistic regression model 8.  Model Complexity 9.  Regularized linear models.  Summary and reflection –  The  pages  below  contain  the  handwritten  summary  referring  to  the  given  learning resources. It includes all the important points of the topic 6 module.  In  this  module,  we  first  learnt  about  the  relevance  and  covariance  among  features  or variables. Then we got to learn about linear regression formulation and linear classification. Then  we  learnt  generalization  and  complexity.  Then  we  were  introduced  to  logistic regression formulation and how to train a logistic regression model with examples. Then we learnt  about  model  complexity  and  how  to  regularize  linear  models.  Then  we  learnt  the concept of linear regression for feature selection.  When we started topic 5, I was not familiar with any of these concepts at all. So it was very interesting and exciting to learn about a new side of machine learning which affects every and all aspects of it. And the practical part done with Python was a little bit confusing at first but  was  able  to  understand  all  the  necessary  commands  and  ways  of  implementation  by referring to both given resources and online resources.   Why do you think many believe you should run the Gradient descent with many different random initialisations?  Many  people  believe  that  running  gradient  descent  with  many  different  random initializations is beneficial because it can help to mitigate the issue of getting stuck in local minima.  When  running  gradient  descent,  the  goal  is  to  find  the  global  minimum  of  a  cost function. However, in many cases, the cost function may have multiple local minima, which can  make  it  difficult  to  find  the  global  minimum.  We  increase  the  chances  of  finding  the global  minimum  by  running  a  gradient  descent  with  many  different  random  initializations. This is because each initialization will result in a different path the algorithm takes, and some of these paths may lead to the global minimum. Additionally, running gradient descent with multiple  initializations  can  help  improve  the  model’s  robustness.  By  evaluating  the performance of the model across multiple initializations, we can get a better understanding of how well it is likely to perform in practice, and we can identify any areas where it may be particularly  sensitive  to  initialization.  Overall,  while  running  gradient  descent  with  many different  random  initializations  can  be  computationally  expensive,  it  is  often  seen  as  a worthwhile investment to improve the performance and robustness of a model.  Activity 6.9  In  this  example,  we  are  using  logistic  regression  to  predict  whether  a  patient  who  has suffered  a  heart  attack  will  have  a  second  heart  attack  within  a  year.  We  have  two independent  variables:  whether  the  patient  completed  a  treatment  consisting  of  anger control  practices  and  the  patient's  score  on  a  trait  anxiety  scale.  Logistic  regression  is  a statistical method used to analyze the relationship between a set of independent variables and a binary dependent variable. Logistic regression aims to create a predictive model that can  classify new  data  points  into  one of  two  possible  categories  (in  this case,  whether the patient  will  have  a  second  heart  attack  within  a  year).  In  this  example,  logistic  regression helps  us  to  identify  the  relationship  between  the  independent  variables  (anger  treatment and anxiety trait) and the dependent variable (second heart attack). By analyzing the data, we can identify the impact that each independent variable has on the likelihood of a second   heart  attack.  The  benefit  of  logistic  regression  is  that  it  allows  us  to  create  a  predictive model  that  can  be  used  to  identify  patients  who  are  at  high  risk  of  having  a  second  heart attack.  This  can help  doctors  to  develop targeted  prevention  strategies and treatments for those  patients  who  are  most  at  risk.  In  this  example,  we  see  that  the  logistic  regression model  indicates  that  completing  the  anger  treatment  appears  to  lower  the  risk  score  of  a second heart attack, while a higher score on the anxiety trait scale appears to increase the risk  score  of  a  second  heart  attack.  This  information  can  be  used  to  identify  patients  who may benefit from targeted interventions to reduce their risk of a second heart attack.  