In this topic we were mainly focused on concepts focusing on model assessment and selection.  When training a model, we should always separate a test set. Pretend, that we do not have access to the test set yet. Because we are not allowed to use the same data points you used for training your model. Therefore, we need to divide the data set in to two parts to as training and testing.  Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data, in other words an equation that graphs as a straight line. The important fact is that linear regression assumes that there is a linear relationship between the input features and the output variable. output can be represented by a straight line. If the relationship is not linear, then linear regression may not be the best model to use.  Linear classification is a type of supervised learning algorithm used to classify data into one of two or more possible categories based on a set of input features. It assumes that the decision boundary that separates the different classes is a linear function of the input features.  Then we learnt how to do a logistic regression and its examples as real world practices.  Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model.  Under-fitting occurs if the complexity of the model is lower than necessary.    Scenario-1: We may be using a linear model, when the data requires a nonlinear model.   Scenario-2: We may be using the right hypothesis (linear or nonlinear) but the number of variables might be falling short of what is required. For example, to predict the income of a person, age alone may not be sufficient.  We can detect under-fitting by checking if the model fitting error on the training data is high.    The feature selection using linear regression is to use the coefficients of the linear regression model. The coefficients indicate the magnitude and direction of the relationship between each input feature and the output variable. Features with larger coefficients are more important and contribute more to the prediction of the output variable.  Finally, we used the python to implement this.     