This topic’s content starts with some further insight into linear regression, and specifically evaluating models and understanding relationships of features or variables. After this, the module starts an intro to logistic regression. Relevance and Covariance Covariance is an important metric in linear regression as it measures the degree and direction of the linear relationship between two values. In other words, it is able to give a quantifiable indication of the effect that a change in one variable has on another variable. From this value we can determine the effect to be expected as a positive covariance value indicates a positive correlation meaning that if the first variable is increasing, so is the second. A negative covariance value indicates that the variables are inversely correlated meaning that if one is increasing, the other is decreasing and lastly, a covariance of 0 indicates that the two variables are independent of one another. Pearson’s Correlation Coefficient is a measurement that is very similar to covariance but is standardized by dividing the covariance value by the product of the standard deviations of the two variables. Linear Classification Moving on from linear regression we come to linear classification and logistic regression. This is an analysis that is used to classify data points into different discrete categories or classes. This is done by use of a linear decision boundary. In linear classification, if there are only two possible values for an output then this is called a binary classification problem. If there are more, it is a multi-class classification problem. Linear classification is good for problems that can be effectively split into a few discrete classes but struggles when the relationship is nonlinear. Logistic regression is a modelling technique that is used for binary classification problems. Logistic regression allows for classification of outputs based on features or variables that are continuous or categorical. A logistic regression model works to categorize the output value somewhere between 0 and 1, true and false. It will assign the prediction to whichever it is closest to. Generalisation and Complexity Generalisation refers to a model’s performance on unseen or new data. A model that generalizes well has learned the underlying relationships and patterns within the training dataset allowing it to make accurate predictions on unseen data. Model complexity is a measurement of the flexibility of a model based on its ability to capture more intricate relationships or patterns in the training data. To allow a model to have a higher complexity, the model needs to work on more features. Thus there is a trade off in performance as there are more dimensions of data as well as a risk of over-fitting when using a limited training set. On the other hand, using too few features could result in under-fitting. As such it is important to evaluate the model to find a good middle ground. Model Complexity To combat over-fitting, the Bias Variance Decomposition of a model can be calculated using the bias, variance and noise. This will provide insight into the trade-off between high and low variance within the model as well as other metrics allowing for an improvement in the model’s performance by helping determine the optimal number and selection of features on which to operate on. This is used to calculate the minimum error which is the best all round model complexity. Regularised Linear Models A regulaised linear model is one in which a regulariser has been applied. A regulariser is a loss function that is used to avoid over-fitting. It works by keeping the parameter in a more normal or regular range. In regression models, features that are very large will be implied to be very important even if they are not. This leads the model to be more dependent on that feature. There are two commonly used regularisers that are known as L1 (LASSO) and L2 (Ridge). Regularisation increases the bias of the model and reduces the variance which could lead to more accurate predictions from the model. Linear Regression for feature selection Linear regression can be used to help with feature selection by evaluating the strength of the relationship between each feature and target pair to determine the features with the highest absolute coefficient value which can then be used as the principal components. 