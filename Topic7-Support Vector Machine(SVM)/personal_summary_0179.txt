Main points summary  -  Relevance and Covariance among features or variables. Linear regression  attempts to model the relationship between two variables by fitting a linear equation to the observed data. We can measure the linear relationship between the variable x and output y, using covariance.  -  Pearsonâ€™s Correlation Coefficient. Ranges between -1 and 1, where 0 is weak and  -  the further from 0 the stronger the relationship. Linear classification, when the separation boundary between two classes is linear, can be done with Logistic regression. Not sensitive to outliers. Logistic regression is named after the function used at the core of the method, the logistic function, which is also called the sigmoid function.  -  Model complexity. Overfitting happens when we use an overly complex model for the data. Underfitting happens when the model is too simple to capture the patterns of the data. Bias-variance decomposition. Too simple models (models with too few parameters) can be inaccurate because of a large bias (not enough flexibility): under-fitting. Too complex models (models with too many parameters) can be inaccurate because of a large variance (too much sensitivity to the sample): over-fitting. There is therefore a bias-variance tradeoff: We need to find the sweet spot where bias + variance + noise is the minimum. The minimum error is at the right model complexity.  -  Regularised linear models. A regulariser is an additional term in the loss function  to avoid overfitting. Examples: ridge- and lasso-regularisation. Linear regression for feature selection. Linear regression may be used as a feature selection method to identify the most important characteristics in a dataset. Linear Regression in Python.  