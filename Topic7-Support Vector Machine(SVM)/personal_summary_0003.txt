Assessing a trained model requires determining its ability to generalise to unseen data. Linear regression fits a line to the training data, producing estimates of the target variable yi from the independent variable xi. These can be vectors, in which case the target is a linear combination of the components of the independent variable. Covariance measures the amount of information a specific x provides for y. Cov(x, y) = (Σ(x-xi)(y-yi))/(n-1) It shows how much a change in x from the mean alters y, on average. This is expected to be 0 when the data are independent, otherwise positive or negative depending on the slope of relationship. Pearson Correlation Coefficient r = Cov(x, y)/sqrt(var(x)var(y)) This will be between -1 to +1 and measures the strength of the linear relationship between variables.0 when there is no linear relationship, and +1 or -1 when there is a strong relationship (positive or negative depending on slope) Minimising the error provides the best fitting line. Using squared error prevents positive and negative errors cancelling out, so that errors from all points contribute to the total error.Using the given values of x and y, w can be determined representing the best fit linear combination using sum of squares error. Linear Classification involves setting boundaries between group(s). Least squares error is sensitive to outliers. Using a link function eg logistic p(y=1|x), probit, tanh helps to manage non-linearity. Note that creating extra features of x do not need to be linear functions, just the ombination will be linear. Logistic regression is used as a classifier, as the sigmoid function is essentially either low or high (apart from the middle section, and ignoring that it never actually reaches -1 or 1). It reflects the log of the odds, and using a threshold becomes a classifier. It is trained using MLE.  Methods like gradient descent are often used in solvers, but depending on the step size can miss global extrema. Model complexity is important as an overly complex model will be able to fit exactly to the training data, learning the noise. This will prevent generalisation to unseen data where the random noise produces errors. However if the model is too simple it will not be able to represent the underlying distribution. Using bias-variance decomposition allows assessment of the fit of the model and likelihood of underfitting or overfitting. Similar to accuracy/precision target taught in Medicine. A model which underfits will have high bias as it cannot represent the distribution. As the complexity of the model increases, the bias will reduce. Variance will likely track similarly with bias, until overfitting occurs where the variance will increase. Regularisation adds an extra term to the loss function, which ensures that when it is minimised it is encouraged to reduce the sum of the weight of features (L1/Lasso)  or squared sum (L2/Ridge) L1 – drives small weights to 0, performing feature selection L2 – results in smaller, more distributed weights, reducing effects of correlation between features ElasticNet overcomes a limitation of L1, as it is not saturated when d >> n. Regularisation increases bias, but reduces variance. Linear regression can be used as feature selection, as the weights determine the relative contribution each feature makes, and selecting the features with the top n weights is an alternative to PCA. The difference is that linear regression does not redefine the axes, but it also means that dropped  features do not need to be measured in the test subjects (unlike PCA where all features are still used  to derive the transformed feature set) 