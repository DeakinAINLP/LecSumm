In topic 6 , we learnt about linear and logistic models, and evaluation for these models. This topic was split into several areas that covered specific areas to help us understand these concepts, which entailed:  -  Relevance and covariance among features or variables, which entailed further information regarding linear regression, Pearson’s correlation coefficient, which is a measure of linear correlation between two variables. There were also illustrations to demonstrate the correlation of the variable r, and their relationship types, such as linear, curvilinear, strong, weak or none.  -  -  The formulation of linear regression, and how to find a line like a denoted variable. This topic also touched base on linear hypothesis, which is the mean value of a random observation, which can be written in a linear combination from predictor variables. We also learnt how to fit these lines to data points so they can be visualized.  Linear classification, which is a model that is used to decide to categories a set of data points into a combination of explanatory variables, and further explained logistic regression, which helps to make a classification decision.  -  Generalization and complexity, which explains how generalization can adapt and fit properly to new or even unseen data. It also details the model complexity on linear regression, which is when the linear model increases with the number and features of sets within the data.  -  -  Logistic regression formulation, which helped explain further on logistic function, and how it is also referred to as the sigmoid function. We also learnt the use of logging odd values within a data set, which is known as ‘Logit’, and how to test that model.  Training logistic regression models, which essentially means you use training data to estimate the regression coefficient vector. We also learn about logistic loss functions, and iterative optimization methods, such as gradient descent and coordinate -wise gradient descent optimization.  -  Model complexity, and the issues that come with it, such as over-fitting (overly complex model based off of the data), or underfitting (extremely simple model). We also learnt about bias and variance in data, and the different tradeoffs that each of them give.  -  Regularized linear models, and how we can add regulariser to linear data, which essentially is an extra term that is located within the loss function in order to avoid overfitting data. We also learnt how these regularisers work in linear models, and the impact that incorporating regularisers have, such as greatly reducing variance in the data. In this section, we also learnt about two methods of regularlisation; LASSO (Least absolute shrinkage and selection operator, performs both variable selection and regularization to enhance prediction accuracy and Lachlan Connor Patrick Geary, 221260728  interprebility) and Ridge, or ‘Elastic Net’, which also helps with large datasets to help the issue of high correlation among data.  