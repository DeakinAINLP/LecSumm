Summary:  6.2:  Linear regression models the relationship between two variables by fitting a linear equation to the data. That is, it seeks the relationship between a feature and an output in the data.  This can be measured with covariance. The result of applying this to a specific feature variable and output results in a value above, below or equal to 0, which respectively indicates that the variables are positively, inversely or not correlated.  Pearson’s correlation coefficient results in similar, although it is bounded to +/-1 respectively indicating total positive and negative correlation.  6.4:  The goal is to find the line, to do this, b the y intercept and w the slope of the line are used. Across more than 1 dimensional features this can be applied also. We need to minimise the total absolute error with our line, this involves either trying to find the total squared error.  6.5 / 6.6:  If the output variables for some problem only have two possible values, the problem is considered binary, else, they’re considered multi-class classification. Logistic regression is applied when the output values of the features vectors is binary.  A linear decision boundary implies a linear separation in a classification problem, this is a starting point for building a model.  Higher volumes of features will lead to higher model complexity, this is important as complexity may result in overfitting, the opposite problem also exists. Regression can be conducted on non-linear features, and in higher dimension spaces.  6.7:  Logistic regression uses the logistic (sigmoid) function, which is an S shaped curve. It can accept any real value number and map is to a value between 1 and 0. It does not model directly, but rather on a logit value, the log of odds based on a data instance. This is actually a linear model.  6.8:  Training a logistic regression model entails using data to estimate the regression coefficient vector.  Expand!     6.10:  High model complexity leads to overfitting, underfitting is the result of a model too simple. Underfitting can occur when constructing a linear model, while the data requires a non-linear model. The hypothesis may be correct, but the volume of variables is too low, i.e. not enough features for a reliable prediction.  Increasing variance entails minimising bias as complexity increases. Low complexity entails high bias and low variance. Thus as one increases, the other decreases and vice versa. A low value of both is ideal.  6.11:  A regulariser can be employed to avoid overfitting to data. It is an additional term in a loss function. It attempts to keep parameters more normal hence the name. It disallows a high weight value for features, which would imply dependence on that feature. Dependence on a feature is considered bad, as it could lead to error.  Two popular function exist for regularising. They encourage 0 weights and penalise large weights respectively. This may be desirable as irrelevant features exists which need to be eliminated. It increases bias but reduces variance massively.  