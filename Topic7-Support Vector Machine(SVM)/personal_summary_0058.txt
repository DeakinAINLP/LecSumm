 Always assess a model's ability to accurately anticipate the target based on current and upcoming data. Holding up a sample of data that has been labelled with the target (ground truth) from the training data-points allows you to properly assess a model.  Relevance and Covariance among features or variables  By attempting to fit a linear equation‚Äîthat is, an equation that graphs as a straight line‚Äîto the observed data, linear regression seeks to model the relationship between two variables.  Pearson's Correlation Coefficient  Pearson's Correlation Coefficient, which assesses the linear relationship between two variables, is another intriguing measurement.  Its value falls between and, indicating total linear positive correlation, total linear zero correlation, and complete linear negative correlation, respectively.  Linear Regression Formulation  We are looking for a line that resembles h in linear regression.  The linear equation ought to make it possible for us to analyse and examine the connections between two continuous (quantitative) variables.  The slope and intercept of the line are the two parameters that make up the line. We can utilise these two parameters to find our straight line if we have them.  The regression model's inaccuracy in predicting value. The error is the discrepancy between our prediction and the actual value or result at that point.  Linear Classification  When the feature vectors' output values are binary, logistic regression is the appropriate regression analysis to perform. The logistic regression is a predictive analysis, as are other regression studies. Let's cover linear classification first, then we'll talk about logistic regression.  When we talk about linear classification, we mean that the boundary separating any two classes is linear. This is merely our theory. It might not be accurate!  Linear Regression  There is a closed form solution for linear regression. The Moore-Penrose inverse of matrix X is calculated using the Python implementation of Singular Value Decomposition (SVD).  Generalisation  We can begin predicting the outcome for a new instance after training a linear regression model. The mean square error (MSE) refers to the error. A performance measure like this is an example. We may also calculate a explained variance and mean absolute error (MAE).  Complexity  The number of features raises the model complexity of linear models. Particularly if we have a small amount of training data, we should be aware of model complexity. The possibility of over-fitting on this modest collection of training data is the cause. Utilising a small number of features might also be problematic because it might result in under-fitting.  Logistic Regression Formulation  The logistic function, which is at the heart of the technique, inspired the name of logistic regression. Another name for the logistic function is the sigmoid function. It is an S-shaped curve that can transfer any real-valued number into a value between 0 and 1, but never precisely at those ranges (see the above graphic). The value comes close to but never reaches zero or one.  The only difference between logistic regression and a regression problem, as we have seen, is how the output is modelled. While in linear regression we model the y directly, in this case we model the logit (log of odds). We'll look at training a logistic regression issue in the lecture after this one.  Training Logistic Regression  We can use maximum likelihood estimation (MLE) to estimate w when training a logistic regression model.  Because both approaches yield the same answer for w, maximisation of likelihood is equivalent to maximisation of the likelihood function's logarithmic component. Since logarithmic functions are monotone increasing functions, keep in mind that by taking the log of the function you can still find its maximum or minimum.  It's crucial to keep in mind that, in some cases, we can derive a closed form formula for the minimiser (as with linear regression), which allows us to compute the minimiser in a single step. If it can't be done, we iteratively perform multiple stages (such as Kmeans and logistic regression) to get to the minimum. As a result, we must in this situation do coordinate-wise gradient descent optimisation. The following is where the two categories fundamentally diverge:    Only one globally optimal solution can be handled by convex optimisations. The alternative possibility is that you demonstrate that the problem lacks a workable solution.    You may have numerous locally optimal points in non-convex optimisations. Finding out if a problem has a universal answer or not can take a long time. The convex optimisation problem is therefore significantly more time efficient.  Iterative Optimising  Returning to iterative optimisation now. Theoretically, there are numerous iterative optimisation techniques, as we already stated. There are two well-liked approaches of calculating the gradient (derivatives) of the objective function:    Gradient Descent: Gradient Descent maximises a function by leveraging knowledge  of its first derivative. As opposed to this, Newton's method, a root-finding algorithm, maximises a function using knowledge of its second derivative. When the second derivative is well-known and simple to compute, this process can be faster. However, the analytical expression for the second derivative is frequently difficult or unsolvable, necessitating a significant amount of computation.    Coordinate-wise Gradient Descent Optimisation : Until the objective function stops changing, keep going. Regardless of initialisation, the solution is distinct (owing to the objective function's convexity).Keep in mind that there is always a potential of stalling out at local minimums as opposed to the global minimum. When working with convex functions, that isn't a problem, but when working with non-convex functions, it could be a major issue.  Model Complexity  When a model is too complex to match the data, this is known as over-fitting. An incredibly simplistic model leads to under-fitting. In earlier classes, we have already encountered both over- and under-fitting.  Over-fitting occurs when, during creating the model, certain irrelevant noise points in the data begin to be captured rather than the entire pattern. An incredibly simplistic model leads to under-fitting. If the model's complexity is less than required, under-fitting will happen.    In scenario 1, we can be employing a linear model even though the data calls for a nonlinear one.    Scenario 2: Although we may be employing the correct hypothesis‚Äîeither linear or nonlinear‚Äîthe quantity of variables may not be sufficient. For instance, age alone might not be adequate to predict a person's income.  When the model fitting error on the training data is large, under-fitting can be identified.  As a model gets more complex, it becomes clear that raising variance also means lowering bias. On the other side, you can see that a model with a low level of complexity will have a large bias and a low level of variation. High variance leads to lesser bias, and lower bias leads to larger variance. In essence, since noise is not a concern at this time, we can simply limit bias and variance. Noise is connected to the observations you made using the function.  Considering the bias-variance trade-off information presented above, we can conclude that:    great bias suggests low variance, while low bias implies great variance.   The sweet spot is where Risk = bias + variance + noise is at its lowest.   At the proper level of model complexity, the error is minimal.  Here, another intriguing question comes up: Can we still overfit with linear models? In some cases! depends on the intricacy of our model. With linear models, the complexity of the model rises as the number of features does. All data dimensions can be used as features to fit the model on both background noise and real patterns (signal).  Regularised Linear model  We know that even with linear models, fitting the model to all data dimensions as features allows you to account for both background noise and real patterns (signals). An extra term called a regularizer is added to the loss function to prevent overfitting. Since it aims to maintain more regular or normal parameters, it is known as a regulariser. To put it another way, it prevents regression coefficients (or weights) from taking extremely high values. What will happen if a weight or weights are too large? It implies that your model is very reliant on that particular feature.    ùêø1Regularisation(Lasso)   ùêø2Regularisation(Ridge)  A regression analysis technique called Lasso (Least Absolute Shrinkage and Selection Operator) uses both variable regularisation and selection to improve the predictability and understandability of the statistical model it generates.  LASSO chooses most variables before it saturates when given few samples in large dimension spaces (cases like bioinformatics datasets). But Elastic Nett can get around this issue because it can pick more variables despite having more data points.  Regularisation makes our model more biased. Only a small portion of our training data is being considered.  Linear Regression for feature selection  The most common methods for feature selection include principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination. The feature selection approach you utilise depends on the specific challenge at hand and the features in the dataset. Linear regression is a feature selection technique that can be used to identify the most important elements in a dataset. Evaluation of the strength of the link between each feature and the target variable is the fundamental tenet of utilising linear regression for feature selection. Linear regression can be used to identify the attributes with the highest absolute coefficient values.  