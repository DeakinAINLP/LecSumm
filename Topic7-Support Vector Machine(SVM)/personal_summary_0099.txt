 Learning Summary Linear Regression is a model which attempts of modelling a relationship among two variable by fitting a  linear equation  over the  data that  is being  observed, or  we can  say an  equation  which form  of graphs  like  a  straight  line.  The  training  data  in  this  regression  is  in  the  is there that can be said to  be  any  sort  of  real-valued  number.  Hence,  the  relation  among  feature  and  output  is  being observed in this regression algorithm.  where for each feature vector  an output  The measurement of the linear relation among variable  x and y can be done using  covariance.  Where  it  measures  the  amount  of  information  a  specific  has  the  ability  of  providing  . This is calculated as:  Pearson’s Correlation Coefficient: This is a measure of linear correlation among two variables x and y comprising of values among +1 and  -1 where 1 indicates total +ve linear correlation,  0 where it has no linear correlation and -1 indicates total –ve linear correlation.  When the output of feature vectors are said to be in binary then the appropriate analysis for such a thing is Logistic Regression. Similar to various other regression analysis, logistic regression is a predictive analysis  Linear Classification: In this classification it is meant that the separation boundary among any two classes is said to be linear, but this might only be a hypothesis and cannot be said to be true.  There are two ways by which we can come up with a decision for classification:  Ignore  non-linearity:  Where  in  this  approach,  the  usage  of  least  squares  take  place  for classification (treating binary outputs as outputs for regression problem) not said to be the best method but it’s easy.  Using  link-function:  On  the  other  hand,  the  usage  of  conditional  probability  can  take  place, where the  class  can  be  the output  for  the  regression  problem which  is  fitting  regression  on  instead of  y.  In other  words  the usage  of  link function  will  take for  transforming  the  output to the classification scenario.  On the  other  hand,  least  square  regression  has  the  probability  of  performing  very  badly  when some points within the training data have excessive number of small or large values for dependent variables when there are compared against the rest of  the training data, this is due to the  least square method which  only is  concerned about  minimizing  sums of  the squared  error, and  any training point which comprises of a dependent value differing with a lot of difference from the rest of the  data available  will certainly  have  a large  effect  over  the  resulting constant  for which  it  is being solved.  Logistic Function which is also known as sigmoid function, is a S-shaped curve which has the ability of taking any real-valued number and map it, on a value that is in the range of 0 to 1, but never exactly at those limits. The value does approach but never reaches at 0 or 1.  