Linear  models  in  machine  learning  are  models  that  assume  a  linear  relationship between  the  input  features  and  output  label.  In  topic  6,  we  delved  deeper  into understanding these linear models and how they work.  Linear Regression  Regression is a statistical method to find relation between the independent variables (x) and dependent variable y. Linear Regression in particular refers to the process of finding a line that can predict the y value. It can be represented by the equation:  Y = h(x) = wx + b (where w is the regression coefficient)  We adapt the same equation with multiple values of w and x when we have multiple dimensions. We fit this line to datapoints by minimising the empirical risk. The error is the difference between predicted and true value.  During the training time of the model, we are actually trying to minimise the error and find the best line by taking the derivative of the error function wrt w and equating it to 0.  Logistic Regression  Logistic Regression  is a statistical method mainly used to  predict binary/categorical values.  Unlike  linear  regression  which  uses  mean  squared  error,  here  we  use  the maximum likelihood function to predict the value of w.  Since the solution in logistic regression does not have a closed form  when derivative is taken, we need to solve the problem iteratively.  Convex solution  They have only one minimum solution. Eg. Objective functions of linear and logistic regression.  Non-convex solution  They may have 2 minimum solutions – local minima and global minima. Eg. K-means objective function  Computing the minimum        We compute the minimum by calculating the slopes via the derivative. 2 methods to compute the gradient (derivative) of the objective functions:  1.  Gradient descent (uses 1st derivative) 2.  Newton’s method (uses 2nd derivative)  Of  the  two,  Newton’s  method  is  faster,  but  also  more  complex  and  computationally expensive.  Coordinate-wise gradient descent optimisation randomly initialises w and then tries to iteratively solve and determine its value.  Model Complexity  There are two terms involved:  1.  Overfitting: Happens when we are fitting an overly complex model on the data. 2.  Underfitting: Happens when the model is extremely simple.  Bias and Variance Decomposition  We can prevent overfitting using 3 components, namely – bias, variance and noise.    Bias: shows the accuracy of the hypotheses function h(x)   Variance: measures the tolerance of the calculated model when changing the  dataset. Higher variance implies increase in complexity.  Increase in variance means lower bias and the model becomes more complex. Vice versa, low complexity will result in high bias and low variance. This is another trade- off problem in machine learning.  The best model, however, must ideally have low bias and low variance, whereas the worst model would have high bias and high variance. The balance or sweet spot is where the risk is minimum.  Regularised Linear Models  The process of regularisation tries to keep parameters more normal or regular, and is represented by lambda. It prevents the regression coefficient from taking excessively large values by penalising such large weights and encouraging small/zero weights.  2 popular regularisation functions are:  1.  L1 norm: Encourages 0 weights. Implies closed form function of a square. 2.  L2  norm:  Penalizes  large  weights. This  function  implies  the  closed form  of  a  circle.  L1 works on LASSO whereas L2 works on ridge and elastic net.         