 Summary  This topic, we have gone through evaluation methods, and how they can be used for  supervised machine learning methods like linear regression common linear classification, and logistic classification. So linear regression basically attempts to model the relationship between the features and the labels of a data set by fitting them into a linear equation. In other words, we're basically fitting an equation that graphs as a straight line for the entire data set. We can measure the linear relationship of this linear model with the help of a concept called covariance. Covariance is defined as the amount of specific input data that is provided for the corresponding output data. If the value of covariance is greater than 0 (or a positive value), then important output values are positively correlated. Similarly, if X&Y are inversely correlated, they will have a covariance of a value less than 0 (or a negative value). If the value of covariance is exactly 0, both important outputs are independent. Another measure we can apply for the linear correlation of the linear model, is the Pearson's Correlation Coefficient. It has the value between a + 1 and a - 1, where the former indicates a positive linear correlation, 0 showing no linear correlation, and the letter showing a total negative linear correlation.  We also learn about the linear regression formulation, which introduced concepts like finding the two parameters of a linear equation, namely the slope (weights vector W) and the dummy weight (called a Bias). Before we can find any of the two things, we can also find the error of value prediction (considering a linear regression model). The linear model seeks to minimise empirical risk with the help of the formulation of the square loss of all instances. The overall formulation we will consider is basically finding the product of the matrix call the ‘Moore-Penrose pseudo-inverse’. Since linear regression is a closed form solution, our Sci-kit libraries uses singular value decomposition to compute this matrix. When it comes to the generalisation process (prediction of unseen data), we can you compute either the mean squared error (MSE) or the mean absolute error (MAE).  For a linear model, we learned that there are three special cases of classification: Binary Classification, Multi-class Classification, and Logistic Regression. When it comes to logistic regression, in this topic, we have learned about its formulation. The logistic function happens to use a ‘Sigmoid Function’, which is an S-shaped curve mapping between values of 0 and 1. This model does not directly model the output in terms of inputs, but it uses something called a logic value or log of odds based on the Input. Training a logistic regression model has the same concept in mind as training a linear regression model, where we used to estimate the weight of the function. Maximum likelihood estimation is used to train a logistic regression model. Maximising likelihood is equivalent to maximising the log of the likelihood function given both provide the same solution for the weights. Coordinate- wise Gradient Descent Optimization is also involved in finding the minimum of the function. In this topic, we also learned about Model Complexity. With the increasing number of features for linear models, Model Complexity usually increases. With model complexity, we would experience two types of problems: overfitting and underfitting. The former occurs when a linear model starts to capture some irrelevant noise points in the data while building the model, other than the whole pattern. The letter occurs due to an extremely simple model. In fact, underfitting occurs if the complexity of the model is lower than necessary, as    if we are using a linear model for data which requires not a non-linear counterpart. While computing the risk, we also discovered then mathematically it is a sum of Bias and Variance. An interesting trend occurs for both types of linear models (w.r.t Model Complexity) as earlier mentioned. Models with low complexity or those which experience underfitting will result in a high bias and low variance (and vice versa in the latter case). Hence, we must come with a way to balance the trade-off between bias and variance, to get a suitable model for a particular dataset.  We also learned about different ways to control the model complexity. Two methods  of controlling Model Complexity are namely L1 and L2 Regularization (collectively called Regularization Methods).  