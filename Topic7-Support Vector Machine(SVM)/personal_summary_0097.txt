 Relevance and Covariance among features or variables: Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data, in other words an equation that graphs as a straight line.  Pearson’s Correlation Coefficient:  The relationship of the input and output can be illustrated as graphs. And they can have either simple linear relationship or curvilinear relationship. The relationship strength is demonstrated how strictly the data points follow these lines. Linear regression formulation: Linear hypothesis: is to find the line y = h(x) = wx +b. We must determine the slope w, b, and y intercept of the line.  After finding the line, we determine a way to fit this line to the data points, and this is conducted based on the error of value prediction in regression. The linear model seeks to minimise the empirical risk R(w) via the square loss  And for the line to fit on the feature vectors:  Linear classification: Logistic regression is the appropriate regression analysis to conduct when the output values of the feature vectors are binary. We handle the non-linearity as h(x) = xTw. which basically project x to the new line h(x). Relying on decision function (h(x)) which uses a fixed non-linear link function and projects the value of h(x) into the [0,1] interval. There are two approaches: Ignore non-linearity and use link function.   Generalisation and complexity:  After training a linear regression model, we can start to predict the output y for a new instance x by using y = xTw.  Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. The reason is the risk of over-fitting on this limited set of training data. Using a limited number of features may also be problematic as it could cause under-fitting.   Logistic regression formulation: This formulation is named after logistic function which is also known as sigmoid function. It’s basically S-shaped curve and takes any real-valued number and map it into the range from 0 to 1 but the mapped values never reach 0 or  Let x be a data instance, and y be its class label in {−1,1}.  Logistic regression does not directly model y in terms x. Instead, it models something called logit value or log of odds against via linear regression. So generally, we are modelling log of odds based on  The Log of odds is called logit.   Training a logistic regression model: Training a logistic regression model means using training data to estimate the regression coefficient vector w.  Computing the minimum of the Logistic Loss Function:  We must know the basic difference between the two categories is that: convex optimisation can deal with only one optimal solution, which is globally optimal. In non-convex optimisations, you may have multiple locally optimal points. Derivatives offer us the solution to these problems.        Model complexity:  Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model. Bias Variance Decomposition:    Variance bias trade off:  The best model is a model with low variance and low bias. It means the model is not too complex but is properly accurate. The worst model would have high bias, which means it’s not accurate based on the training data, and high variance which means it’s far too complex.  VIII.  Regularised linear models:  A regulariser is an additional term in the loss function to avoid overfitting. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values.    Regularisation impact:    Linear regression for feature selection:  IX. Linear regression is a statistical method used to identify the relationship between a dependent variable and one or more independent variables. It is often used for feature selection, which is the process of identifying the most relevant features or variables that affect the target variable.  In linear regression for feature selection, the goal is to identify the subset of independent variables that are most strongly related to the dependent variable. This can be achieved by fitting a linear regression model to the data and examining the coefficients of the independent variables.  One approach is to use a technique called forward selection, where we start with no variables and add one variable at a time, selecting the variable that produces the best improvement in the model fit. Alternatively, we can use backward elimination, where we start with all variables and remove one variable at a time, selecting the variable whose removal produces the least degradation in the model fit. Once we have identified the subset of features with the strongest relationship to the dependent variable, we can use them to build a more efficient and effective predictive model. However, it's important to keep in mind that linear regression assumes a linear relationship between the dependent and independent variables and may not work well with non-linear relationships. Additionally, linear regression assumes that the independent variables are not highly correlated with each other, so it may not be effective when dealing with multicollinearity.  