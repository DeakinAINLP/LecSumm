 Linear Regression Overview   Linear regression is a statistical method that allows us to model the relationship between  two continuous variables by fitting a linear equation to the observed data.   The linear hypothesis is the assumption that the true relationship between the variables  is linear, or can be approximated(modelled) by a linear function.   The linear model can be fitted to the data by minimizing the empirical risk, which is the  average squared loss between the observed and predicted values.   The coefficients of the linear equation can be estimated by using the Moore-Penrose  pseudo inverse of the matrix X, which contains the values of x and a column of ones for the intercept term. The pseudo inverse can be calculated using singular value decomposition (SVD), which decomposes X into three matrices U, S, and V such that X = U*S*VT.   Generalization is the ability of the linear model to make accurate predictions on unseen  data, after training the model on a sample of observed data.   The error on prediction is a measure of how well the model fits the data. It can be  quantified using different metrics, such as mean squared error (MSE), mean absolute error (MAE), or explained variance (R2).  Measuring Linear Regression   Covariance: The linear relationship between 2 variables can be measured by the covariance COV(X,Y) which is a measure of how much two variables change together. It can be positive, negative or zero depending on the direction and strength of the relationship. 3 Possible values for Cov(X,Y):  Cov(X,Y) > 0 -> x and y are positively correlated; if x is increasing, y is increasing.  Cov(X,Y) > 0 -> x and y are negatively correlated; if x is increasing, y is decreasing.  Cov(X,Y) = 0 -> x and y are independent.   Pearson correlation coefficient is a normalized version of covariance that  ranges from -1 to 1, measuring how well a linear model fits the data and whether the relationship between 2 variables is positive or negative.  Closer to 1, the stronger the positive relationship  Closer to O, the weaker the relationship.  Closer to -1, the stronger the negative relationship.  Linear Classification   Linear classification can use linear regression by  fitting a linear model to the data and then using a threshold or a decision rule to assign labels.   Linear classification thus finds a linear boundary  between different classes of data.  Logistic Regression   Logisitic regression is names after the link function used at the core of the method, the  logistic function or sigmoid function which outputs in the range (0, 1), for binary classification problems where we need to find the conditional probability ( Log of odds) of the data (categorical dependent variable e.g binary) belonging to a particular binary class with a bernoulli distribution.   Training a logistic regression model involves the following steps:   Using maximum likelihood estimation to estimate the regression coefficient that best fits the data  Defining a logistic loss function that measures the discrepancy between the predicted and  observed outcomes   Applying an iterative optimization algorithm to minimize the logistic loss function and find the  optimal regression coefficient. 2 methods are:  Gradient descent: This method updates the model parameters by moving in the opposite direction of the gradient of the loss function with respect to the parameters. The size of the update is controlled by a learning rate parameter. Gradient descent may get stuck in local minima.   Newton's method using the Hessian matrix: The Hessian matrix is a matrix of second-order partial derivatives that captures the curvature of the loss function. The minimum of the loss function is computed using the gradient and the Hessian matrix of the loss function with respect to the model parameters.   Other link functions that can be used to model binary outcomes, such as the probit link  function and the tanh function.  Model Complexity   Model complexity refers to how well a model can capture the patterns and relationships in the data.   Overfitting is finding an overly complex model that fits the training data very well, but fails to generalize to new and  unseen data.   Underfitting is finding an extremely simple model that does not capture the essential features of the data, and performs  poorly on both training and test data.   Model complexity expected loss (or risk) is the sum of three components:      (Bias)2 : Bias shows how accurate the hypothesis function models the true function. Therefore an accurate model has low bias. A high bias model is inaccurate. Variance: Variance measures the tolerance of your calculated model depending on the specific training data used. A high variance model is inconsistent and has a large error for different data sets.   Noise: Comes from the randomness and uncertainty in the data and is an irreducible error that cannot be reduced by any  model.   Models have a Bias-Variance tradeoff :   Higher bias results in lower variance ( Low complexity/too few parameters model-underfitting).  High variance results in lower bias ( Highly complex Model/Too many parameters - overfitting)       The model complexity grows with the number of features. The best model is the one that minimizes the expected loss (or risk) by balancing bias and variance. The worst model is the one that maximizes the expected loss by having both high bias and high variance. The optimum model is when Risk = bias + variance + noise is the minimum.  Regularisation   Controls model complexity, by not allowing regression coefficients (or weights) to take excessively  large values.   Large regression coefficients means a small change in a feature would result in a large change in the  prediction.   An additional term that ensures the model is not highly dependent on certain features to avoid  overfitting.  2 approaches:    Lasso(Least Absolute Shrinkage and Selection Operator) / L1 Regularization: Encourages small/zero regression coefficients/weights by using an absolute value penalty. It can perform feature selection by setting some coefficients to zero. It has a closed form solution that resembles a square and is non- differentiable at the origin.   Ridge & Elastic Net /  L2 Regularization : It penalizes large coefficients/weights by using a squared value penalty. Penalize large coefficients/weights. It has a closed form solution that resembles a circle and differentiable everywhere.   For regularized logistic regression, need to perform iterative optimization, such as gradient descent  or Newton's method, because there is no closed form solution for the loss function.   By imposing more regularization, you will get smaller values for weights, reducing the variance of the  model and prevent overfitting, but we also risk increasing the bias and underfitting.   Therefore, we need to find the optimal level of regularization that balances the bias-variance trade-  off.  