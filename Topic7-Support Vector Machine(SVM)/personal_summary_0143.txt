 In the topic six online class we mainly learnt about Linear Regression, Logistic Regression  Model Complexity (Bias & Variance) and L1 and L2 Regularizers.  We began by briefly discussing the importance of relevance and covariance among features or  variables. We also talked about the important features of the Pearson’s Correlation Coefficient,  which is a measure of the linear correlation between two variables. Next, we explored linear  regression, which is a technique used for predicting continuous numeric values based on input  features. We  examined an example of linear regression, gaining insights  into how it can be  applied in practice. Then we covered the mathematical formulation of linear regression. We  discussed the hypothesis function and the process of obtaining the best-fit line by fitting the  data points.  Moving on to linear classification, we talked about what we mean by linear classification, the  concept  of  linear  decision  boundaries  and  their  applications.  We  concentrated  on  Logistic  regression, a widely used algorithm for binary classification. We examined the formulation of  logistic regression using the logistic function and discussed its application in solving  binary  classification problems. Then we explored the process of training a logistic regression model,  including  the  logistic  loss  function,  computing  the  minimum  using  gradient  descent  and  Newton’s  method,  and  coordinate-wise  gradient  descent  optimization  algorithm  used  to  optimize  the  model's  parameters.  To  reinforce  our  understanding,  we  analyzed  a  logistic  regression  example.  This  practical  illustration  helped  us  see  how  logistic  regression  can  be  applied to real-world classification tasks.  Model complexity was another crucial aspect that we covered during this topic. We examined  the  impact  of  model  complexity  on  generalization  and  learned  about  the  bias  variance  decomposition and variance bias trade off, aiming to strike a balance between under-fitting and  overfitting.  Then,  we  went  through  regularized  linear  models,  where  we  discussed  how  regularisers work in linear models, regularisation impact and two regularized linear models:  L1 Regularisation  (LASSO) and  L2 Regularisation (Ridge).  Finally, we  explored the  linear  regression  for  feature  selection  as  a  means  of  identifying  the  most  relevant  features  for  a  predictive  model.  We  learned  how  Principal  component  analysis  (PCA),  correlation-based  feature selection, and recursive feature elimination can assist in feature selection by assessing  feature importance.   In the programming part of the unit, we focused on implementing regularized linear regression  and  logistic  regression  in  Python.  Using  libraries  like  scikit-learn,  we  gained  practical  experience in training and evaluating these models.  