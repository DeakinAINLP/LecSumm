In topic I learnt about linear regression, logistic regression and model complexity and regularities.  Linear regression is considered to be the easiest and most efficient algorithm. It is s statistical method which is used for predictive analysis. Linear regression makes prediction for real or numeric variables such as sales, salary etc. It shows a linear relation =ship between a dependent (y) and one or more independent variables, which is why it is known as linear regression. This model often represents a sloped straight line to show the relationship between the variables. There are 2 types of linear regression:    Simple linear regression   Multiple linear regression.  To find the best model from many models, we do optimization. Which can be achieved by many different methods, which include, R-squared method, which measures the strength of relation in dependent and independent variable of a scale of zero to hundred.  Logistic regression is another statistical approach in machine learning which is used for classification task, where the task is to predict categorical outcomes which are mostly binary and are based on one or more independent variables. It is basically an extension of linear regression w=but in cased where dependent variable are categorical.it produce value from 0 or 1 the model is :  P(y=1|x) + 1/ (1+exp(-(b0 +b1* x1 + b2 * x2+â€¦. +Bn * xn)))  Where:    P(y=1|x) shows the probability of the outcome   X1,x2 are independent variable   B0 is the bias term   B1,b2 are coefficient  The model coefficient is estimated by a process called maximum likelihood estimation. It is used in many different applications which include spam detection, credit risk analysis and many other.  Regularization is a process which is used to prevent overfitting and improve the generalization of the model to unseen data. It occurs when a model learn to noise or random fluctuations in the training data, which results in poor performance of data. There are two common types of regularization, L1 and L2. In L1, we add the absolute values of the model to the loss function, which is then multiplied by the regularization. Loss function with l1 regularization  Loss = original loss + (lambda * sum of absolute values of the parameter)  L2 add the quare value of model to the loss function, which is multiplied by a regularization.  Loss function with l2:  Loss = original loss +( lambda * sum of squared values of parameter )  Multiple Linear Regression in Machine learning - Javatpoint                