TOPIC 6 General overview of assessing a trained model was the main objective of the topic , By fitting a linear equation to the observed data, or an equation that graphs as a straight line, linear regression makes an attempt to predict the relationship between two variables. We went through linear regression and we studied few example and also went through linear regression formula .  We studied about how to fit a line into data points we took few reference from YouTube for these topics and understood it better . linear classification When the feature vectors' output values are binary, it is best to run a logistic regression analysis. Logistic reasoning There are so often two methods:  Non-linearity should be ignored when using least squares for classification; binary outputs should be treated similarly to those from a regression problem. Although it might not be the ideal approach, it is simple! Linking technique: Another strategy is to fit the regression on P ( y = 1 | x ) instead of y, using the conditional probability of the class as the output in the regression problem.  In other words, transforming the output to the classification scenario via a link function. In this straightforward scenario, if P ( y = 1 | x ) > 0.5, we may choose to choose class label 1 for the data point x, unless it makes more sense to choose label 0.  Generalisation and complexity , A closed form solution exists for linear regression. The Moore-Penrose inverse of matrix X is computed using Singular Value Decomposition (SVD) in the Python code. If the size of the matrix X is n d, then the computations are of the order O (n d 2), assuming n d. We went through Training a logistic regression model means using training data to estimate the regression coefficient vector.  Logistic Loss Function is Because both approaches yield the same answer for w, maximum likelihood is identical to maximising the log of the likelihood function. Since logarithmic functions are monotone increasing functions, keep in mind that by taking the log of the function you can still determine its maximum or minimum.  Difference between convex and non-convex function There is only one globally optimal solution that convex optimisations can handle. The other scenario is that you demonstrate that the issue lacks a workable solution (see picture above, right image). There may be more than one locally optimal location in non-convex optimisations. Determining whether a problem has a worldwide answer or no solution at all might take a long time (left picture on figure). As a result, the convex optimisation problem has a significantly improved time efficiency. So:  Sometimes we can develop a closed form formula for the minimiser, allowing us to compute the minimiser in a single step (for example, linear regression). If there is no closed-form solution, we must iteratively perform several steps to get to the minimum. We even studied logistic regression example .              Model Complexity, When we find an excessively complex model based on the data, this is known as over-fitting. A very straightforward model leads to under-fitting. Regularised linear models , In order to prevent overfitting, a Regularised is an additional term in the loss function. Since it works to maintain the parameters more regular or normal, it is known as a Regularised In other words, it prevents regression coefficients (or weights) from having values that are too high. What will happen if one or more of the weights are too heavy? It means that one feature in your model is really important.  What happens if this feature is noise or is heavily influenced by noise in the observations? When creating a machine learning model, we do not want to rely too much on any one factor! In order to direct the training process to favour some types of weights over others, this approach is used. Linear regression for feature selection Common feature selection techniques include principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination. The feature selection technique to employ is determined by the particular challenge at hand and the features in the dataset. Linear regression can be used as a feature selection method to identify the most important features in a dataset. To determine the strength of the correlation between each feature and the target variable, linear regression is primarily used for feature selection. With the aid of linear regression, the features with the highest absolute coefficient values can be identified. If you want to learn more about the idea of utilising linear regression in the feature selection process, please do more research.  We even practised linear regression and Logistic regression in python .  