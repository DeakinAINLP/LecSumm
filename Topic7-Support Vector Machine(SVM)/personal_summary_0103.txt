  This topic in the Machine Learning unit we discussed linear and logistic regression:  -  Linear regression is the process of finding a linear relationship between different features and  certain output. Linear hypothesis is a candidate function for the linear relationship. The linear  relationship  constitutes  from  the  linear  summation of the  features, each multiplied  with  a  weight w. The factors are chosen to minimize a loss function, such as mean square error (MSE).  -  When  the  output  has  only  limited  distinct  values,  the  process  is  then  called  Linear  classification or Logistic regression. The process is then interested in finding linear boundaries  to separate the features. This is done by adding another decision function to the process.  -  Model complexity increases with the number of features included. Increased complexity risks  overfitting, which results in accurate results in the training data but poor results in the testing  data.  -  Logistic regression uses a logistic function such as the sigmodal function as a decision function.  The  logistic  function  includes  w  weights  as  well  that  need  to  be  chosen  to  maximize  the  likelihood function (equivalent to the lose function). Iterative searching methods are used to  find the optimal w factors such as the Coordinate-wise Gradient Descent Optimization.  -  The loss function used for fitting a hypothesis function has three components; bias, variance,  and  noise  which  is  an  irreducible  error.  The  bias  reflects  the  accuracy  of  the  hypothesis  function, while the variance reflects how robust is the hypothesis function to the change in  the data. The  tradeoff  here  is to have  a hypothesis function that  is accurate enough while  robust to the change in data (training and testing data).  -  Regularised linear models are models that add a penalty parameter in a was to control having  huge weights. Two types of regularised models are L1 regularised model and L2 regularised  model. The L1 model encourages small weights (close to 0) while L2 penalises large weights.  -  Principal component analysis (PCA) could be used to select the most influential features that  would be used in the regression process.  