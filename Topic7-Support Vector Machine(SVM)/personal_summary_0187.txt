   Linear regression models the relationship between two variables by fitting a straight line to the observed data. Covariance measures the linear relationship between a feature and output, while Pearson's Correlation Coefficient measures the linear correlation between two variables, with a value ranging from -1 to 1, where -1 indicates total negative linear correlation, 0 is no linear correlation, and 1 shows total positive linear correlation.    Linear regression aims to find a line to summarise and study the relationships  between two continuous variables. The line has two parameters, the slope and the intercept, and the linear regression equation can be written in dimensions using vector notation, with the goal of minimizing the square loss error function by finding the proper values of parameters.    Linear classification assumes that the separation boundary between two classes is  linear. However, in cases where there is non-linearity, logistic regression can be used as an alternative method to classify data points, by using a fixed non-linear link function to project the value into an interval for classification decisions.    After training a linear regression model, the predicted output for a new instance can be computed and the error can be measured using performance measures such as Mean Square Error or Mean Absolute Error. Model complexity increases with the number of features, and over-fitting can occur with a limited set of training data, while under-fitting can occur with a limited number of features.    The logistic regression formulation models the logit value or log of odds against the class label via linear regression, using the sigmoid function to map any real-valued number into a value between 0 and 1. Logistic regression estimates the probability of a data point belonging to a particular class and makes classification decisions based on a threshold value of 0.5.    To train a logistic regression model, we use maximum likelihood estimation (MLE) to estimate the regression coefficient vector, and the logistic loss function to minimize the error. Coordinate-wise gradient descent optimization is used when no closed form formula for the minimizer is available, to iteratively find the minimum of the function. The solution is unique, irrespective of the initialization of the weights, and is globally optimal in the case of convex functions.    Model complexity refers to the balance between under-fitting (an overly simple  model) and over-fitting (an overly complex model). The trade-off between bias and variance is important in finding the right level of complexity, and regularization is a technique used to control model complexity.    Regularised linear models use regularisers to prevent overfitting in linear models by penalising large weights and keeping the model's parameters more normal or regular. Regularisation techniques like LASSO and Ridge improve the prediction accuracy and interpretability of the statistical model and reduce variance while increasing bias. Regularisation can also be used for feature selection by shrinking the weights of noisy dimensions to zero.  