 Relevance and covariance   Pearson’s Correlations Coefficient- measures the linear correlation between two variables  The closer to 1, the stronger the positive relationship The closer to 0, the weaker the relationship The closer to −1, the stronger the negative relationship  Linear classification- when there are two possible values for an output we call the problem a binary classification problem Logistic regression is conducted when the output values of the feature vectors are binary Python implementation uses singular value decomposition SVD to compute the moore penrose inverse of matrix  -  Generalisation  -  Model complexity increases with the number of feature in linear models. -  Training a logistic regression uses training data to estimate the regression coefficient vector Logistic loss function  -   -  Convex and non convex -  Convex optimisation can only deal with one optimal solution -  Non convex optimisation can have multiple locally optimal points  Example-- If there is no closed form formula, we must take multiple steps iteratively to reach to the minimum. (eg. logistic regression and Kmeans) Imagine you’re blindfolded, but you can see out of the bottom of the blindfold to the ground at your feet. I drop you off somewhere and tell you that you’re in a convex shaped valley and escape is at the bottom/minimum. How do you get out?  The simplest way is to look for steepest slope down! Basically you start walking and you look for slopes going down, preferably the steepest slopes. In maths, we call the slopes  derivatives!  The slope of a secant line (line connecting two points on a graph) approaches the derivative when the interval between the points shrinks down to zero. That is the basic idea for optimising these scenarios.  -  Overfitting and underfitting, happens when the model based on the data is overly  complex and when the model is too simple respectfully.       