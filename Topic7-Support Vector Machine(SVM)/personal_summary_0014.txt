  Covariance is used to measure the linear relationship between the variable x (could be single or multi-dimensional) and the output y (just one dimension). This shows the relationship between a  feature and an output.    Covariance can be > 0 which means x and y are positively correlated, or < 0 meaning x and y are negatively correlated, or 0 meaning x and y have no relationship.    Pearsonâ€™s correlation coefficient falls between -1 and +1. -1 is a perfect negative correlation, +1 is a perfect positive correlation, and 0 means no relationship between variables. The closer to -1/+1 the correlation coefficient is the stronger the negative/positive correlation.   When a relationship has a curved or nonlinear shape it is called a curvilinear relationship. Curvilinear relationships are very variable, more complex and less easily identified than simple linear relationships.    To perform linear regression you fit a straight line to the data points and calculate the slope and intercept of the line. The slope is the rate of change in one variable for a unit change in the other variable, the intercept represents the value of the dependent variable when the independent variable is zero.    Unlike linear regression where the response variable is continuous in logistic regression the response variable is binary (or categorical coded as binary). Logistic regression models the probability of the response variable being in one category versus the other as a result of the predictor variables.    After a linear model is trained it can be used to predict the output of y on unseen data.   Useful performance measures in linear regression models are mean square error (MSE) and mean absolute error (MAE) Linear models become more complex when the number of features is increased. If the training data is limited model complexity can lead to overfitting. Likewise using too few features can cause under-fitting. Logistic regression uses a function that maps any real-valued number to a value between 0 and 1. This is called the logistic function, and it has an S-shaped curve. To predict the response variable the logistic function the values of the predictor variables and their corresponding coefficients are used. These coefficients are estimated using maximum likelihood estimation, which finds the set of coefficients that best explain the relationship between the predictor variables and the response variable.    Then values of the predictor variables and their corresponding coefficients are plugged into the logistic function and this outputs the predicted probability.    You can train a logistic regression model by using training data to estimate the regression   coefficient vector. Logistic loss function is used to measure the error between the predicted probabilities and the actual binary labels of a dataset. It measures how well a logistic regression model is able to predict whether an observation belongs to one category or another.    Over-fitting happens when we find an overly complex model based on the data. Under- fitting is the result of an extremely simple model.    The variance-bias trade off refers to the relationship between the complexity of a model and its ability to make accurate predictions. A model with high bias makes strong assumptions about the data and a model with high variance is more flexible and can fit the data more closely. A high-bias model is simple and has fewer parameters, while a high-variance model is more complex and has more parameters. You should aim to find a model that has a good balance between bias and variance.    To find balance between bias and variance you can use cross-validation, regularization,  feature selection, ensemble methods, and hyperparameter tuning.    Using all data dimensions can fit the model to true patterns (signal) but also to background noise. A regularizer is an additional term in the loss function to avoid overfitting. It is called a regularizer since it tries to keep the parameters more normal or regular. L1 regularization (Lasso regularization) and L2 regularization (Ridge regularization) are methods to perform regularization.   The main difference between L1 and L2 regularization is the way they penalize the parameters of a machine learning model. L1 regularization adds a penalty term proportional to the absolute value of the model's parameters. This shrinks the less important parameters to zero creating a sparse model with just the most important features. L2 regularization adds a penalty term proportional to the square of the model's parameters. This shrinks parameters towards zero smoothing the model and reducing the impact of outliers and noisy features on the prediction.    Principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination are typical feature selection methods. Linear regression can also be used as a feature selection strategy by evaluating the strength of the relationship between each feature and the target variable and only retaining those with a strong relationship.  