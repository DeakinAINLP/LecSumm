Linear Model Linear regression tries to model the relationship between 2 variables by fitting a linear equation to the observed data. For each training data point of x there is an output of y which can be any real-valued number. We search for a particular relationship between the a feature and the output. Covariance is used to measure the relationship, covariance measures the amount of information a specific x can provide for y. For Cov(x,y) we can have Cov(x,y) > 0 meaning x and y are positively correlated if both are increasing. Cov(x,y) < 0 meaning x and y are negative correlated is x is increasing and y is decreasing. Or Cov(x,y) = 0 meaning x and y are independent. We can use Pearson’s correlation coefficient which measures the linear correlation between 2 variables between 1 and -1. The closer to 0 the less the correlation and if negative then a negative correlation. In linear regression we want to find a line similar to h. The linear equation should allow us to summarise and study the relationship between 2 variables. A line is defined as y = h(x) = wx + b. If we have 2 parameters we can find the line by finding the slope w and b the y intercept. Then we can use an x value to estimate the value of y. If we have multiple dimensions the equation is almost the same except we handle the multiplication in all d dimensions. The error of the value predicated in regression compared to the true value is shown as ei = yi – y(predicted), and the goal is to minimise the total error of the line. When classifying a linear line, if all values have a binary output where there are only 2 possible values, 0/1, -1/1, etc. This is a binary classification such as if something is or is not something. A multi-class is when there are more values, such as when classifying the specific type of the thing. Linear classification is when the separation boundary between 2 classes is linear. To make a classification decision we use logistic regression, to do this we need a decision function o(h(x)) which uses a fixed non-linear link function and projects the value of h(x) into 0,1 interval. The 2 approaches are to either ignore non-linearity, using least squares for classification, treating binary outputs like the outputs in the regression problem. Or using link function which fits a regression on P(y =1|x) instead of y. Linear regression has a closed form solution. Python uses SVD to compute the Moore-Penrose inverse of matrix X. If matrix X of size n x d the computation of the order art O(nd^2) if n > d. if d > n then the complexity would be O(dn^2). After training a linear regression model we can generalise and start to predict the output of y for new instances of x. The predicted outcome is computed as y = x^T w. Logistic regression is named after the function used at the core of the method, the logistic function. It is also a sigmoid function, an S-shaped curve and can take any real-valued number and map it between 0 and 1,  the value approaches but never reaches the values. Logic regression doesn’t directly model y in terms x, instead it models the logit value which is the log of odds against via linear regression. To train a logistic regression model we use training data to estimate the regression coefficient vector w. We use the maximum, likelihood estimation (MLE) to estimate w. Maximising the likelihood is equivalent to minimising the log of the likelihood function as both provide the same solution for w. To find the minimum we can derive a closed form formula for the minimiser. Or if there is no closed form formula we can take multiple steps iteratively to reach the minimum. A popular method for iterative optimising is the gradient descent, which uses first derivative. Or the coordinate-wise gradient decent optimisation which randomly initialises w. Over-fitting happens when we find an overly complex model based on the data, under-fitting is the result of an extremely simple model. Bias Variance decomposition when finding the true relation involves the expected loss or risk. Which is comprised of the bias, the variance and the noise. If we increase the variance of the model means lowering the bias as it becomes more complex, or lower the complexity and the bias will be higher with low variance. This is a trade-off problem in machine learning. We need to find the sweet spot that is both accurate and precision, of has both low variance and low bias. A regulariser is an additional term in the loss function that avoids overfitting. It tried to keep the parameters more normal or regular and does not allow the regression weights to take large values. We can use LASSO, least absolute shrinkage, and selection operator, that performs both variable selection and regularisation in order to enhance the prediction accuracy and interpretability of the statistical model it produces. Or we can use the Ridge, which is known as Elastic net LASSO. Regularisation increases the bias of our model. Principal component analysis (PCA),  correlation based feature selection and recursive feature elimination are feature selection methods. The challenge is to identify the best feature selection to use for a given dataset. The idea behind using linear regressions for feature selection is to evaluate the strength of the relationship between each feature and the target variable. The feature with the highest absolute confection values can be found using linear regression. 