Linear regression A method used for a continuous target variable. It finds the relationship between the independent variables and the target variable by fitting a straight line to the data. The model estimates the regression coefficients using a method called least squares, which minimizes the sum of the squared errors between the predicted values and the actual values. To measure the performance of the model, we use MSE, RMSE, MAE, and R- squared.  Linear classification A method used to classify data into discrete categories based on input features. It involves finding a linear boundary to separate classes in the feature space. Linear classification is used in a variety of applications, such as image recognition, spam filtering, and sentiment analysis.  Logistic regression A method used to deal with the categorical target variable. used to model the probability of the dependent variable taking on the value 1 (in case of dealing with binary label) given the input variables. The logistic regression model estimates the parameters of the logistic function by maximizing the likelihood of the observed data. To measure the performance of the model, we use confusion matric, ROC curve, and F1 score.  Model complexity  Over-fitting happens when the model captures some irrelevant noise points in the data while building the model, rather than the whole pattern. Under-fitting is the result of an extremely simple model when the number of variables is not enough, or we choose a linear model when the data does not have a linear relationship. Variance bias trade-off A model with low variance and low bias is the ideal model. This indicates that while the model is accurate, it is not overly complex. The worst model would have high variance, indicating it is very complex, and high bias, indicating it is not correct based on the training data.  Regularization – a technique used to control the model complexity L1 (Lasso) tends to perform better when only a small number of predictor variables are significant, as it can remove insignificant variables completely from the model by turning some of the weights exactly zero. L2 (Ridge) encourages the model to have smaller weights overall, but it does not necessarily force any of the weights to be exactly zero in case all the features are important for prediction. The strength of the regularization is controlled by a hyperparameter, denoted as λ. A larger value of λ leads to stronger regularization and a simpler model, while a smaller value of λ leads to weaker regularization and a more complex model.  