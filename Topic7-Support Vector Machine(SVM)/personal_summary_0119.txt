Following the previous topic, this topic focused on how to assess and select model by separating training set and test set to prevent getting biased estimate of the accuracy of a trained model.  Linear regression  As a basic and commonly used analysis and it basically examines two aspects:  (1)  does a set of predictor variables do a good job in predicting an outcome (dependent)  variable?  (2)  Which variables in particular are significant predictors of the outcome variable, and in what  way do they–indicated by the magnitude and sign of the beta estimates–impact the outcome variable? (What is Linear Regression?, 2023)  Covariance shows us how the two variables differ, whereas correlation shows how the two variables are related. Relevance refers to the degree of influence or importance of a particular feature or variable and focuses on the importance of a feature in predicting the target variable, while covariance examines the relationship between features themselves.  Linear regression formulation  The goal of linear regression is to minimize the difference between the predicted values and the true values, which is measured using the square loss function. And this minimization problem can be solved by taking the derivative of the error function with respect to the parameters and equating it to zero.  Linear classification  Linear classification refers to categorizing a set of data points into a discrete class based on a linear combination of its explanatory variables and non-Linear classification refers to categorizing those instances that are not linearly separable. It is possible to classify data with a straight line (Gill, 2022)  Logistic regression is an example of supervised learning and used to calculate or predict the probability of a binary(yes/no) event occurring. For example, it can determine if a patient is covid positive or negative.  Linear regression has a closed form solution, using Singular Value Decomposition(SVD) to compute the Moore-Penrose inverse of the matrix, with the larger dimension resulting in linear complexity and the smaller dimension resulting in squared complexity.  Generalisation allows additional features to be derived from the original features, even non-linear ones. By adding these new features, the problem remains a linear regression.  When dealing with a limited set of training data, increasing the number of features can increase the complexity of the model, but using too few features may lead to under-fitting. Over-fitting on limited training data is a potential risk to be aware of.      Logistic regression formulation  Logistic function (sigmoid function) maps real-valued numbers into a range between 0 and 1. Logistic regression models the logit value, or the log of odds, using linear regression instead of directly modelling the class label. The odds of a class are defined, and the logit is described as the log of these odds.  Logistic regression and linear regression are similar in terms of both involving linear equation modelling, however, logistic regression models the logit(log of odds) of data point instead of directly predicting the class label  Also, simply running gradient descent multiple times with different initial positions and selecting the lowest point can be quicker than sampling every point individually to find optimal solution especially dealing with non-convex functions  (Convex function:only one minimum point  Non-convex function: many minimum points)  Benefit of using logistic regression  It aims to measure the relationship between a categorial dependent variable and one or more independent variables and allows us to evaluate the relationship between the variables.  The main advantage of logistic regression is it’s easier to set up and train, implement, interpret, allowing us to easily update with new data using stochastic gradient descent. It also fits the line values to the sigmoid curve.  Over-fitting vs under-fitting  Over-fitting occurs when a model becomes too complex and captures irrelevant noise in data  Under-fitting happens when a model is overly simplistic.  Bias-variance decomposition: Analyses the components of expected loss in a model  Linear regression feature selection  Feature selection methods such as PCA, correlation-based feature selection, and recursive feature elimination are commonly used, depending on the specific challenge and dataset. In the case of linear regression, the goal is to assess the strength of the relationship between each feature and the target variable. By examining the absolute coefficient values, the most significant features can be identified.  