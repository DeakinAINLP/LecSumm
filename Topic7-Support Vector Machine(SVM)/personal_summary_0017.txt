Linear Regression  Linear Model    Linear regression is a method of modeling the relationship between two variables by ﬁBng a linear equation to the observed data. In linear regression, training data is of the form of x, y, where x is the feature vector and y is the output.    We can measure the linear relationship between the variable x and output y using covariance.    Covariance measures the amount of information a speciﬁc x can provide for y.    Pearson’s Correlation Coeﬃcient is another measurement that measures the linear correlation between two variables x and y.   It has a value between +1 and -1 where 1 indicates total positive linear correlation, 0 is no linear correlation, and -1 shows total negative linear orrelation.    The most important features of this measurement are that it measures the linear relationship between two variables and that it is not suitable for non-linear relationships.  Linear classiﬁcation  o  A technique used to classify data into two or more classes. o  Binary classiﬁcation  § Only two possible values for output.  o  Multi-class classiﬁcation  § More than two possible values for output.  o  Linear classiﬁcation  § Separation boundary between any two classes is linear. § Separation boundary is just a hypothesis and may not be true.   Non-linear ambiguity in forms of regression  If we think in terms of regression in which outputs are continuous diﬀerent numbers, classiﬁcation is intrinsically non-linear because two instances and their outputs may be diﬀerent but may belong to the same class.  o To handle this non-linear ambiguity in forms of regression, assume it is just a  regression case and handle the non-linearity as h(x) = x’w.  Generalisation and Complexity Linear regression has a closed form solution  Generalisa4on (Predic4on on unseen data) A[er training a linear regression model, we can start to predict the output x for a new instance y  Model complexity of Linear Regression Model complexity of linear models increases with the number of features.  Logistic Regression Logistic Regression Overview:    Supervised machine learning algorithm used for classiﬁcation problems    Particularly suitable for binary classiﬁcation (e.g., spam vs. non-spam emails)    Predicts probabilities of class membership rather than exact class labels  Logistic Regression Formulation:    Core function: logistic function (also called sigmoid function)    S-shaped curve maps real-valued numbers to values between 0 and 1 (never exactly 0 or 1)    Useful for transforming linear regression output into probability estimates    Data instance: xti class label: y in {0, 1}    Goal: learn a model that estimates the probability of class 1 given input features x    Models logit value or log-odds via linear regression (not directly modeling y in terms Logit: log(odds) of data point for class 1  of x)    Odds: ratio of the probability of class 1 to the probability of class 0   Linear model: y = w0 + w1x1 + … + wdxd   w: weights to be learned during model training  x: input features    Probability estimation: need to calculate P(y=1|x)    Equation for probability estimation: P(y=1|x) = 1ti(1+exp(-x'w))   Logistic function transforms log-odds (linear model output) into probability estimates  Training and Evaluation:    Model training typically involves optimizing the weights (w) using techniques like  gradient descent    Cross-entropy loss or log loss is commonly used to measure model performance  during training    Performance metrics for evaluation include accuracy, precision, recall, and F1 score  Training a logistic regression model Training a logis4c regression model means using training data to estimate the regression coeﬃcient vector w  The training process involves ﬁnding the optimal weights for the linear model to minimize prediction errors  Training process:    Prepare data: split dataset into training and test sets, preprocess features (e.g., scaling, encoding) Initialize weights: assign initial values to model weights (e.g., zeros, small random numbers)    Deﬁne cost function: typically cross-entropy loss or log loss, measures the diﬀerence  between true labels and predictions    Optimize weights: use techniques like gradient descent or stochastic gradient descent  to iteratively update weights and minimize the cost function    Compute gradients: calculate the partial derivatives of the cost function with  respect to each weight    Update weights: adjust weights in the direction of the negative gradient,   proportional to the learning rate Iterate: repeat the optimization steps until convergence or a predeﬁned number of iterations    Regularization (optional): add penalty terms (e.g., L1, L2 regularization) to the cost  function to prevent overﬁBng and improve model generalization    Model evaluation: assess model performance on the test set using metrics like  accuracy, precision, recall, and F1 score    Hyperparameter tuning (optional): optimize hyperparameters (e.g., learning rate,  regularization strength) to further improve model performance  Ac4vity Why do you think many believe you should run the Gradient descent with many diﬀerent random initialisations?  Running gradient descent with diﬀerent random initializations is beneﬁcial because it:  1.  Reduces chances of geBng stuck in poor local minima 2.  Helps escape problematic points, like saddle points 3.  Increases the robustness of the ﬁnal model 4.  Allows creation of ensemble models for beker performance  Model complexity  Variance bias trade oﬀ  Regularised linear models A regulariser is an additional term in the loss function to avoid overﬁ3ng. It is called a regulariser since it tries to keep the parameters more normal or regular.  Linear regression for feature selection  Using linear regression for feature selection:  1.  Fit the linear regression model on the dataset  2.  Analyze the coeﬃcients and signiﬁcance of each feature  Larger absolute coeﬃcients indicate more important features   Statistical tests (e.g., t-test) can help determine the signiﬁcance of each feature  3.  Apply feature selection methods:    Filter methods: Rank features based on correlation with target variable or  other metrics (e.g., mutual information) and select top features   Wrapper methods: Utilize search algorithms (e.g., forward selection, backward elimination) to ﬁnd the best subset of features    Embedded methods: Incorporate feature selection within the model training process (e.g., Lasso regression, Ridge regression)  By selecting the most relevant features, the model's performance can be improved, overﬁBng can be reduced, and interpretability can be enhanced.  