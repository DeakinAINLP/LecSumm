This module’s focus was on regression and classification. Linear and Logistic Regression, Model Complexity, Generalization, Regularization, Feature Selection.  LINEAR REGRESSION  Linear regression models the relationship between two variables by fitting a linear equation to the observed data. We want to find a model that can summarize and study relationships between two quantitative (continuous) variables. I.e. map a function to a feature vector to predict the result of new data instances.  Fitting the line involves minimising the empirical risk. The difference between the prediction and true value of a data instance is the error. The linear model minimises empirical risk R(w) using the squared loss (yi – y- hati)^2, where yi – y-hati = ei (empirical risk). We want to minimise the average of all loss functions for each data instance.  Goal is to map two variables together with the hypothesis function: y = h(x) = wx + b. We need w, the slope of the line, and b, the y intercept, to find the line.  For multi-dimensional X, y(xi) = w0xi0 + w1xi1 + …. + wdxid = b + w1x1i  In other words w0xi0 = b. This is true for xi0 = 1, where xi0 is a dummy feature (all values = 1). In vector notation we can determine this as yi = xTi*w, for d+1 dimensional vectors. Or y = Xw, collectively.  Fitting the line requires minimising empirical risk. This is done using the mean of the square loss error function. This is a closed function so the derivative with respect to w can be used to find the global minimum (d(w) = 0) to determine that w = (xT*X)^-1*xT*y. This matrix is called the Moore-Penrose pseudo-inverse.  Linear classification problems come in two forms, binary and multi-class problems. Binary classification problems have just two possible values (true/false) for the output variable, while multi-class classification problems have multiple outputs. “Is this data instance of class X or not” vs “what class does this data instance fit into?”  Linear Regression can be used as an alternative to PCA for feature selection. Linear Regression can find the relationship between each feature and target variable, and those with the highest coefficients can be selected.  LOGISTIC REGRESSION  Logistic regression is commonly used for prediction and classification problems. utilizes the logistic function, or sigmoid function. This is used to find the log of odds against X, where X is a given data instance. Hence the output is mapped between 0 and 1. The log of odds is also called logit. The odds are defined as P(y = 1|x) / (1 – P(y = 1|x)), where x is the data instance and y is the output variable. This function is equivalent to w0 + w1*x.  The model is trained to estimate the regression coefficient vector, w using the maximum likelihood estimation (MLE). Doing so means maximising log l (w), or minimising -log l (w)m the Logistic Loss Function. Logistic regression functions are convex, having a global optimal solution. Logistic regression functions do not have a closed formula so require an iterative approach to minimise – the Gradient Descent method.  Non-convex lines contain local minimums and maximums, and a global minimum/maximum. Convex solutions only have a singular minimum or maximum. Linear and logistic regression deal with convex functions. The way to find the minimum in convex functions is to search for the steepest slopes – derivatives. Gradient descent uses the first derivative.  REGULARIZATION  Linear models can be interfered with from outliers, or noise. A regularizer can be added to the loss function to defend against overfitting. It does so by forcing regression coefficients to be somewhat regular as highly weighted features will dominate the dataset. Regularizing the feature weights ensures that any noise- impacted feature isn’t dominating the dataset and introducing bias. Regularizers can also set weights to zero to remove irrelevant features.  Regularizers come in two forms, L1 and L2. L1 Regularization (LASSO) encourages sparsity by prioritizing 0 or small weights.  L2 Regularization (Ridge) instead penalizes large weights.  Regularization increases bias in the model but aims to reduce (bias^2 + variance).. I.e. removing noise.  