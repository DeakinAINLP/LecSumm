This topic we also focused on several related model assessment and selection. What we need to do that to be sure that the model is predicting the target good with the new and future data; in fact, we expect it top work good with unseen data. We should hold out a sample of data that has been labelled with the target (ground truth) from the training data-points. We train a model and then measure the accuracy and after that we need to assess the model. Note that we need to separate the test set before any pre-processing. Relevance and Covariance among features or variables: We learned that the Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data, in other words an equation that graphs as a straight line. We use Covariance to measure the linear relationship between the variable x and output y. This is another indicator to measure the relationships between the x and y which is between -1 and 1. Linear Regression Formulation: In linear regression we want to find the best suitable line siting between datapoints to study and summarise the relationships between continuous variables. But we need to fit the line to data points. So we need to minimise the errors and the error of value prediction in regression. To show the error we use the following formula which is the mean of square error function which has already been introduced: Linear Classification: Binary classification problem is when the output has only two possible values and a multi-class classification problem is when the output has multi values. When the output is binary classification, we can use Logistic Regression. Logistic regression: In the logistic regression the output is in the [0,1] interval. If the score is close to 0, it would be in one class and if it`s closer to 1, then it would be in another class. However, there are two approaches available to make the decision: Ignore non-linearity: we use least squares for classification. This method is not the best method and especially when we have some noise and outlier, it will not work well enough. Using link function: at this method we use the conditional probability of the class at the output to decide which class we should put the output in. Generalisation and Complexity: Linear regression has a closed form solution. Python implementation uses Singular Value Decomposition (SVD) to compute the Moore-Penrose inverse of matrix X. after creating a model based on the training data, we need to test it and measure the errors in prediction for test dataset. We need to know that the model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. It can cause over-fitting. However, using a limited number of features may also be problematic as under-fitting. Logistic regression Formulation: The logistic regression is also called the sigmoid function. It`s an S-shaped curve and it can take any real-valued number and map it into a value between 0 and 1 but never exactly those numbers. Logistic regression does not directly model y in terms of x. in fact, it models something called logit valuer or log of odds against via linear regression; so in fact we are modelling log of odds based on X. Logistic Loss Function: By taking the log of the function, we are still able to find the maximum or minimum of the function since the logarithmic functions are monotone increasing functions. ConVex and Non-Convex: Covex optimisations can deal with only one optimal solution, which is globally optimal. The other possibility is that you prove that there is no feasible solution to the problem. In non-covex optimisations, we may have multiple locally optimal points. It can take a lot of time to identify whether the problem has no solution or if the solution is global. Iterative optimising: As we learnt before, optimisation theory has many methods for iterative optimisation. One of the popular methods to compute gradient (derivatives) of the objective function is Gradient Descent. Gradient Descent maximises a function using knowledge of its derivative. We use it to find the minimum point step by step, however there is always a chance of getting stuck in local minimums rather that the global minimum. When we are dealing with covex functions, that`s not a problem but with non-covex functions, it could be a serious problem. Model Complexity: Over-fitting happens when we find an overlay complex model based on the data. Under-fitting is the result of an extremely simple model. The figure below shows the differences between them: In fact, under-fitting occurs if the complexity of the model is lower than necessary. For example if the data needs a non-linear model and we used a linear model to simplify that; or we may use the right hypothesis (linear or nonlinear) but the number of variables might be falling short of what is required. We can detect under-fitting by checking if the model fitting error on the training data is high. In other hand, over-fitting can happen when we make more that required complex models or using too many features that required. Bias Variance Decomposition: If we are fitting an hypothesis function, the expected loss (or risk) has three components: Bias shows how accurate your hypothesis function is and how accurate your design model is. Variance just relies on the model to put it simply. This model measures the tolerance of your calculated model while changing just the data set. If it varies too much that would be a problem. The E or the expectation of this term measures the complexity of the model. The higher the variance the more complex the model. Increasing the variance of a model means lowering bias as the model becomes more complex on the other hand we can see that the low complexity for a model will result in high bias and low variance. Also, we need to note that models with too few parameters are inaccurate because of a large bias (not enough flexibility); so under-fitting will happen. Models with too many parameters are inaccurate because of a large variance (too much sensitivity to the sanmple); so over-fitting will happen. So as a result: Low bias implies high variance; and high bias implies low variance. We need to find the sweet spot where Risk=Bias^2 + variance + noise is the minimum. The minimum error is at the right model complexity. Regularised Linear Models: Regularisation is an additional term in the loss function to avoid over-fitting. It`s called a regularisation since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values. If one or more weights are excessively large, then it implies that the model is highly dependent on that one feature. This procedure is a way to guide the training process to prefer certain types of weights over others. Regularisation Impact: The l1-norm forms a square shape, assuming the loss function is in the form of ellpises in the plot. Since we are minimising the loss function which actually has l1-norm regularisation inside it, we need to find a sweet spot which is the intersection of these two regions. If we keep drawing the ellipses, we can find the intersection. For l2-norm we have the same process and definition, however instead of square, we have circle. In the circle form, there is an increased chance of intersection. Also, there is less chance of having 0 weights for w1 and w2. Two methods of regularisation: 1 L1 Regularisation (LASSO): Lasso (Least Absolute Shrinkage and Selection Operator) is a regression analysis method that performs both variable selection and regularisation in order to enhance the prediction accuracy and interpretability of the statistical model it produces. 2 L2 Regularisation (Ridge): This is known as Elastic Net.  LASSO and Ridge regularisation are then special cases if Elastic Net for  landa2=0 and landa1=0. Linear regression for feature selection: There are some techniques such as PCA, correlation-based feature selection, and recursive feature elimination for feature selection. To find the most significant features in a dataset, linear regression can be used as a feature selection strategy. 