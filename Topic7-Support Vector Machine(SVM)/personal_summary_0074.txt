 Main points of this module:    Pearson’s correlation coefficient is a measure of linear correlation and is given by a value by    between 1 and negative 1, where 0 is no correlation. Linear classification involves determining whether the separation boundary between any two classes is linear.    Binary classification is when there are only two possible values for output, if there are more,  then it is a multi-class classification problem.  Logistic regression approaches involve either ignoring non-linearity or using link function.   Generalisation, or prediction on unseen data, can be measured for performance evaluation using mean square error (MSE) or mean absolute error (MAE), or explained variance (R2).   Maximum likelihood estimation (MLE) is used in training a logistic regression model to make  likelihood predictions.    Convex optimisations can deal with only one optimal solution, non-convex optimisations  may have multiple locally optimal points.    Over-fitting is the result of using an overly complex model based on the data, under-fitting is       the result of an overly simple model. Low bias implies high variance and high bias implies low variance. L1 (LASSO – Least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularisation. L2 (Ridge or Elastic Net) can overcomes saturation since it can select a greater number of variables despite the data point count.  Provide a summary of your reading list:  I just followed the given study material (6.1 – 6.16), reading everything until I understood it confidently, as well as viewing any linked videos provided. As usual, I did spend a fair amount of time on the Python section and problem-solving activity, I had difficulty in the last two questions with overfitting the data. In the end, I decided to keep it consistent with the previous questions and explained the reason for it.  