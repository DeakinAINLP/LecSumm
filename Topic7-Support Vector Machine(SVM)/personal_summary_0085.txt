Relevance and Covariance among features or variables  Linear Regression tries to model the relationship between variables through the fitting of a linear equation. In linear regression for each input (feature, x) there is an output (y), we are looking for the relationship between this feature and output, this relationship can be measured using covariance. Covariance measures the amount of information that a particular x/feature can provide for the output/y.  Covariance above zero indicate a positive relationship, below zero indicates a negative relationship and if it equals zero it indicates no relationship meaning that that x and y and independent.  Pearson’s correlation coefficient is a measure of the linear correlation between variables, its most important features are:  -  Only indicates and works with linear relationships -  Ranges from -1 to 1 -  Closer to 1 means stronger positive relationship -  Closer to -1 means stronger negative relationship -  Closer to 0 the weaker the relationship  Linear Regression Formulation  A linear equation allows us to study the relationship between two continuous variables. The line that represents this relationship has two parameters w which represents the slop of the line and b which represents the y intercept. With these parameters we can fine the straight line and with this line we can estimate the value of y given a certain x input.  If x is not a single dimension value due to having multiple features then the problem will have to be represented in d dimensions. The dimensioned formula is almost the same as the regular formula with the only difference being that w and x are multiplied in all d dimensions. Error is the difference between what we predicted to be the output and the true output.  Linear Classification  When the output values of the feature vectors are binary the appropriate regression analysis to conduct is logistic regression, a predictive analysis like all regression analyses. Considering data where for each x input there is a y output and for these outputs there are only two possible values they can be, this data represents what is considered to be a binary classification problem. Whereas, in the same situation if there are more than two possible y outputs then it is considered a multi-class classification problem.  Linear classification means that the separation boundary between two classes is linear or at least that we are currently assuming as such. This kind of classification can be seen as not linear as two instances of x can have different outputs while still belonging to the same class, making them in the same class by not linear in respect to one another. In a regression case we handle this non-linearity by projecting x to a new line that can be represented as h(x).  But to actually make a classification decision we need another function which projects the value of h(x) into an 0,1 interval through a fixed non-linear link function. Overall two approaches are generally available for a logistic regression approach:    -  Ignore non-linearity: Least squares is used for classification which may not be the best fit but is an easier method.  -  Using link function: Use a link function to transform the output of the classification scenario.  Generalisation and Complexity  Linear regression has a closed form solution, python implementation utilises SVD (Singular Value Decomposition) to compute the Moore-Penrose inverse of matrix X. When n >= d and if X is of size n x d then the complexity will be O(nd^2) whereas, if d >= n then the complexity is O(dn^2).  When considering linear regression, we can use derived features of the original x instead of x even if these features are not linear, as long as we are using a linear formulation the problem is still linear regression.  After training a linear regression model and making predictions of y based on x the error in this prediction can be measured as the mean square error (MSE). MSE is a performance measure of our linear regression model just like Mean Absolute Error (MAE) and explained variance (R^2).  Logistic Regression Formulation  Logistic regression is named as such since the function at the core of the method is the logistic function. The logistic function also know as the sigmoid function is an S-shaped curve that can take any value between 0 and 1 and will approach 0 and 1 without ever reaching them. Logistic regression doesn’t directly model y in terms of x as it is instead modelling the logit value which is the log of odds based on x.  The log of odds is known as the logit, during logistic regression we are modelling the logit of a data point y rather than x to be labelled as class 1, compared to linear regression which is modelling y directly.  Training a Logistic Regression Model  Training a logistic regression model involves using the training data to estimate the regression of w which is the coefficient vector. We can use maximum likelihood estimation (MLE) to estimate w, maximising likelihood is equivalent to maximising the log of the likelihood function as both provide the same solution for w.  Before discussing how the minimum is computed we need to address convex and non-convex functions. Convex optimisations deal with only one optimal solution while non-convex optimisations can have multiple locally optimal points. Meaning that sometimes a closed form formula can be derived for the minimiser meaning it can be computed in one step. If there is no closed form formula then multiple steps will be required to reach the minimum (logistics regression and kmeans).  Iterative optimising has many methods one of these methods is gradient descent which maximises a function using knowledge of its derivative.  Model Complexity  Overfitting occurs when a model that is too complex for a certain set of data is used on said set of data while underfitting is the opposite.  Bias variance decomposition, increasing the variance of a model means that lowering the bias of that model becomes increasing complex. Low complexity results in high bias and low variance meaning that higher bias results in lower variance and high variance results in lower bias. A model with low variance and low bias will both be not too complex and properly accurate, this is a good model.   Meanwhile, a model with high bias means it is inaccurate and with high variance means its too complex, this would be a bad model.  Regularised Linear Models  A regulariser is an additional term in the loss function added to avoid overfitting, it attempts to keep the parameters “normal”. It doesn’t allow regression coefficients to take values that are too large, if these weights are excessively large it indicates that the model is highly dependant on that one feature. We do not want to rely overly on any one feature when designing a model for machine learning so using a regulariser helps guide the training process.  In linear models we also do not want huge weights (don’t want to overly rely on any one feature), huge weights would mean that a small change in a feature would cause a large change in the predictions. There are two popular regulariser functions the first one encourages 0 weights while the second one penalises large weights.  LASSO (Least Absolute Shrinkage and Selection Operator) is a regression analysis method that does both variable selection and regularisation, this enhances the predication accuracy and interpretability of the statistical model it produces.  Data that is highly dimensional but with few samples (d > n) LASSO can only select up to n variables before it saturates, another method known as Elastic Net can overcome this as it can select a greater number of variables regardless of the number of data points.  Regularisation will increase the bias in our model as we are only partially listening to the training data.  Linear Regression for feature selection  Typical feature selection methods involve PCA, correlation-based selection and recursive feature elimination, one challenge can be determining which to use. To find the most significant features in data linear regression can be used as the basic idea behind it is to evaluate the strength of the relationship between each feature and the target variable.  