1. Summarize the main points that is covered in this topic.  A. Covariance  a) When measuring the linear relationship between the variable x and output y, use covariance, which measures the amount of information a specific xi provides for yi. If covariance > 0, then x and y are positively correlated, and y increases as x increases. If covariance < 0, then x and y are negatively correlated, and y decreases as x increases. If covariance = 0, then x and y are independent.  b)  c) Pearson’s Correlation Coefficient: a measure of the linear correlation between  two variables, x and y.  d) Ranges between -1 to 1. The closer to 1, the stronger the positive relationship;  The closer to 0, the weaker the relationship; The closer to -1, the stronger the negative relationship  B. Linear Regression Formulation  a) Error: The difference between predicted value and the true value or output of that point. The process is to write down the error function, take its derivative, and find the values that minimize the function. In linear regression, we model the output, whereas in logistic regression, we model log of odds.  b)  C. Linear Classification  a) Separation boundary between any two classes is linear b) Logistic regression is the appropriate regression analysis to conduct when the output values of the feature vectors are binary. (Binary classification problem)  c) Least squares regression can perform very badly when some points in the  training data have excessively large or small values. The reason for this is that since the least squares method is concerned with minimizing the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the data will have a disproportionately large effect on the resulting constants that are being solved for.  D. Generalization and Complexity  a) Generalized form of linear regression: when we adopt our own list of features  and try to fit a line based on the new features.  b) Generalization: Prediction on unseen data, we can measure the error by using  MSE.  c) Complexity: Model complexity of linear models increases with the number of  features, which might cause over-fitting.  E. Logistic Regression Formulation  a) Logistic function also called sigmoid function; it models logit value (log of odds)  b) When xT > 0, P(y=1 | x) > 0.5, we decide in favour of class 1; When xT < 0,  P(y=1 | x) < 0.5, we decide in favour of class -1; xT = 0, P(y=1 | x) = 0.5, both classes are equally possible.  F. Training a Logistic Regression Model  a) Use use maximum likelihood estimation (MLE) to estimate w  b) Joint likelihood function while having n independent samples using training  data is the multiplication of the likelihood of each point.  c) Logistic loss function: Maximizing likelihood is equivalent to maximizing the  log of the likelihood function  d) We sometimes can derive a closed form formula for the minimizer (e.g. linear regression) meaning we can compute the minimizer in one step. If that’s not possible, we take multiple steps iteratively to reach to the minimum (e.g. logistic regression and Kmeans)  e) Convex optimizations can deal with only one optimal solution (global optimal).  f) g)  The other possibility is no feasible solution to the problem. In non-convex optimizations, you may have multiple locally optimal points. Iterative Optimizing: Gradient Descent (uses first derivative) and Coordinate- wise Gradient Descent Optimization  G. Model Complexity  a) Trade-off problem: Increasing the variance of a model means lowering bias as the model becomes more complex, and low complexity for a model will result in high bias and low variance. So higher bias results in lower variance and high variance results in lower bias.  b) We need to find the sweet spot where Risk = bias2 + variance + noise  H. Regularized Linear Models  a) A regulariser is an additional term in the loss function to avoid over-fitting; it tries to keep the parameters more normal or regular; and it does not allow regression coefficients (or weights) to take excessively large values.  b) c) There are two types of regulariser functions: The first one encourages 0  weights (sparsity). This function implies the closed form function of a square. The second one penalizes large weights. This function implies the closed form function of a circle.  d)  e) Two Regularisation Methods  i.  LASSO (Least Absolute Shrinkage and Selection Operator): a regression analysis method that performs both variable selection and regularisation in order to enhance the prediction accuracy and interpretability of the statistical model it produces. It also selects features: Only those dimensions that have non-zero weights participate in the prediction. Therefore, LASSO is also used to select predictive features among all dimensions.  ii. Ridge  I.  Linear Regression for Feature Selection a) Principal component analysis (PCA) b) Correlation-based feature selection c) Recursive feature elimination  i.  The basic idea: Evaluate the strength of the relationship between each feature and the target. The features with the highest |absolute| coefficient values can be found using linear regression.  