In this topic we studied Linear regression and its formulation, linear hypothesis, model complexity, Logistic regression and its training process, and regularization process.  Linear regression is used to model the relationship between a dependent variable and one or more independent variables. Itâ€™s main goal is to estimate the parameters of the model, which include the intercept and coefficients, that minimize the difference between the predicted values and the actual values of the dependent variable. This method can be used for both simple and multiple linear regression, and can be extended to handle non-linear relationships between the variables using techniques such as polynomial regression. Linear regression is a widely used tool in machine learning, and can be used for tasks such as predicting a numerical outcome, estimating the effect of different variables on an outcome, and understanding the relationship between different variables.  The model complexity is defined as flexibility and rigidity of model in fitting the training data. A model with high complexity can fit the training data very well but may not generalize well to new data, which is overfitting and results in high variance and if the model with low complexity may not fit the training data well and results in high bias. Finding the right balance between bias and variance is crucial to create a model that generalizes well to new data. This can be achieved through techniques such as regularization, cross-validation, and tuning hyperparameters.  Logistic regression is used for binary classification tasks, where the goal is to predict the binary outcome of a target variable based on one or more input features. The algorithm models the relationship between the input features and the binary target variable using a logistic function, which outputs a probability value between 0 and 1. The logistic function maps the linear combination of the input features to the probability of the positive class. During training, the algorithm learns the optimal weights for the input features that minimize the error between the predicted probability and the actual target value. The algorithm is efficient and interpretable, and its output can be easily converted to a binary classification decision by setting a threshold on the predicted probability.  Regularization helps preventing overfitting by limiting the magnitude of the model weights and promoting simpler models and improves the generalization of a model. The hyperparameter controlling the strength of the regularization can be tuned through cross-validation to find the optimal balance between bias and variance.  L1 regularization adds a penalty term to the cost function of a model that is proportional to the absolute value of the model weights. This results in a sparse model, where some of the weights are forced to zero, effectively performing feature selection. L2 regularization adds a penalty term to the cost function that is proportional to the square of the model weights. This results in a model where all the weights are shrunk towards zero but never exactly zero, leading to a less sparse model than L1 regularization.    