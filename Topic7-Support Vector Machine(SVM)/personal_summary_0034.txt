This topic was all about Linear Machine Learning models. The following points were covered:  Relevance and Covariance among features or variables:  Linear regression attempts to model the relationship between two  variables by fitting a straight line through them.  Relationships between variables can measured by covariance, which  describes the amount of information one variable gives about another.  Covariance can be either positive, negative or zero. If  it is zero, then  there is no correlation between the variables. If it is positive, it means that when one variable is increasing, so is the other, with the reverse being true for negative covariance.  Pearson’s Correlation Coefficient measures the linear correlation between two variables. So if the data is a perfect linear correlation, then the coefficient is equal to 1. Logistic regression:   used when the classification is binary (i.e., there are only two possible outputs). Logistic regression, despite the name, is a classification exercise and the purpose is not to predict a continuous variable. Logistic regression is based on the logistic, or sigmoid, function. This function maps any number to a value between 0 and 1. The category that a data point then depends on the value of the output once is has been passed through the logistic function.  Overfitting occurs when the model is very complex, and underfitting  occurs when the model is too simple.  Bias vs. Variance Trade-off:  Bias is the difference between the predicted and actual output. It can be considered the accuracy of the model. A high bias means that the model is underfitting. In order to reduce the bias, more complexity can be added to the model.  Variance is the amount of variability that the function has to change in the training data. Models with too much variance are too responsive to the data and this leads to overfitting.  Variance can be reduced by reducing the model complexity.  Complexity can be added to a model by:  ▪  Adding features ▪  Using higher degree algorithms ▪  Using hyperparameter tuning ▪  Decreasing regularization ▪  Increasing training iterations  As bias increases, variance decreases and vice versa. A balance must be  found between the two to produce a model that is capable of generalizing well to unseen data.  The error in the model accounts for noise, and should be random.  Regularised Linear Models:  Additional term in the loss function used to avoid overfitting. Does not allow regression coefficients to take excessively large values, i.e., prevents the model from relying on any one feature too heavily.  Regularisation increases bias and reduces variance in a model. 