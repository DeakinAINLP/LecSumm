Linear model  Relevance and covariance among features or variables  Linear regression is a statistical technique used to establish a connection between two variables by creating a linear equation that can be plotted as a straight line on a graph. This equation is derived by analysing the observed data and finding the best fit line that can predict the relationship between the variables.  In linear regression, the training dataset consists of n pairs of observations (xi, yi), where xi is a feature vector and yi is a real-valued output for that feature. The goal is to find a relationship between the feature vector and the output values. Specifically, we seek to identify a linear equation that best describes the relationship between the inputs and outputs.  We can measure the linear relationship between the variable x (could be of many dimensions) and output y (for now suppose y is only one dimensional), using covariance.  Pearson correlation coefficient:  The Pearson correlation coefficient, also known as the Pearson's r, is a statistical measure that quantifies the strength and direction of the linear relationship between two continuous variables. It ranges from -1 to +1, where a value of -1 indicates a perfect negative linear relationship, 0 indicates no linear relationship, and +1 indicates a perfect positive linear relationship. The Pearson correlation coefficient is calculated by dividing the covariance between the two variables by the product of their standard deviations. It is widely used in statistical analysis and research to assess the degree of association between two variables.  r = cov (x, y)/(var(x)var(y)) ^1/2  Linear regression formulation:  The general formulation of linear regression involves finding a linear equation that can describe the relationship between a dependent variable y and one or more independent variables x1, x2, ..., xn. The equation can be written as:  y = β0x0 + β1x1 + β2x2 + ... + βn*xn  where β0 is the intercept or constant term, β1, β2, ..., βn are the regression coefficients or parameters associated with each independent variable, xn is the value of the nth independent variable. The goal of linear regression is to estimate the values of the regression coefficients that minimize the sum of the squared residuals, which is known as the least squares criterion. This can be achieved through various methods such as ordinary least squares (OLS), gradient descent, or maximum likelihood estimation. Once the    coefficients are estimated, the equation can be used to make predictions for new values of the independent variables.  Logistic regression:  Logistic regression is a statistical method for analysing a dataset in which there are one or more independent variables that determine an outcome. It is a classification algorithm used to predict the probability of a binary outcome (e.g., success or failure, win or lose) based on one or more predictor variables.  Logistic regression is widely used in various applications such as credit scoring, medical diagnosis, and marketing analytics. It is a simple and interpretable method that can handle both categorical and continuous predictor variables.  Logistic regression formulation  Logistic regression is named after the function used at the core of the method, the logistic function.  The logistic function is also called the sigmoid function. It’s an S-shaped curve and it can take any real-valued number and map it into a value between 0 and 1 but never exactly at those limits. The value approaches but never reaches 0 or 1.  Let x be a data instance, and y be its class label in {-1,1}.  Logistic regression does not directly model y in terms x. Instead, it models something called logit value or log of odds against via linear regression. So generally, we are modelling log of odds based on x.  The odds of class -1 are defined as  Odds= P(y=1|x)/(1-P(y=1|x))  Logit:  This Log of odds is called logit. Logistic regression uses the following linear model:  log(P(y=1|x)/(1-P(y=1|x))) = Wo+W1x  then we have trained the logistic regression model with example  regularised linear models: A regulariser is an additional term in the loss function to avoid overfitting. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values.  We have two regularisations.        L1 regularisation (LASSO): Lasso (Least Absolute Shrinkage and Selection Operator) (also seen in uppercase: LASSO) is a regression analysis method that performs both variable selection and regularisation to enhance the prediction accuracy and interpretability of the statistical model it produces.    L2 regularisation: Another method of regularisation is called Ridge.  We have also learnt how we can use linear and logistic regression in python.      