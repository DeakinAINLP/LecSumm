Relevance and Covariance among features or variables Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data. The training data is of the form of a feature vector and an output value, and the goal is to find the relationship between a feature and the output. Covariance measures the amount of information a specific feature can provide for the output. Pearson's Correlation Coefficient measures the linear correlation between two variables and ranges between -1 to 1, with 1 indicating total positive linear correlation, 0 indicating no linear correlation, and -1 showing total negative linear correlation. Linear regression formulation For multidimensional problems, the equation includes a dummy feature and a weight vector. The goal is to minimize the error function, which is the difference between the predicted value and the true value. To find the optimal parameters, we can take the derivative of the error function and equate it to zero. The solution involves the Moore-Penrose pseudo-inverse of the matrix. Generalisation and complexity Linear regression has a closed-form solution that can be implemented in Python using Singular Value Decomposition (SVD). The computational complexity of this solution depends on the size of the matrix used, and if the matrix is of size m x n, the computational complexity is O(mn^2). Linear regression can be used with any derived features, including non-linear ones. Generalized linear regression is used to fit a line based on new features. After training a linear regression model, the predicted output for a new instance can be computed, and the error in prediction can be measured using performance measures such as mean square error (MSE) or mean absolute error (MAE). Model complexity of linear regression increases with the number of features, which can cause over-fitting or under-fitting on limited training data. Logistic regression formulation Logistic regression is a classification algorithm that models the logit (log of odds) of a data point to be labeled as a class. The logistic function or sigmoid function is used to map any real-valued number into a value between 0 and 1. The odds of a class are defined as the probability of that class divided by the probability of the other class. The model leads to classification rules based on the value of the logit. Testing the model involves calculating the logit value for a test point and allocating it to a class based on its value. Logistic regression is similar to linear regression, but instead of directly modeling the output, it models the logit value. Training a logistic regression model MLE is used to estimate the regression coefficient vector. The likelihood function is derived assuming a Bernoulli distribution on the binary output of the model.. Model complexity This passage discusses the concepts of over-fitting and under-fitting in machine learning models. Over-fitting occurs when a model is too complex and captures irrelevant noise in the data, while under-fitting occurs when a model is too simple and does not capture the underlying patterns in the data. The passage also introduces bias-variance decomposition, which helps to balance model complexity and accuracy. Finally, the passage discusses the trade-off between bias and variance and how regularisation can be used to control model complexity. Regularised linear models Regularisation helps to avoid overfitting by adding an additional term to the loss function that prevents the regression coefficients or weights from taking excessively large values. This term can be considered as the complexity of the model. Two popular regularisation functions are the L1-norm (encourages sparsity) and L2-norm (penalises large weights). Regularisation can increase bias in the model but greatly reduces variance. Lasso and Ridge are two common types of regularisations, and Elastic Net is a special case of both. Lasso performs variable selection and regularisation, while Ridge can select a greater number of variables despite a small number of data points. Regularisation can be used for both linear and logistic regression. The goal is to find an optimum model complexity that balances bias and variance. Linear regression for feature selection In the context of feature selection, linear regression can be used to evaluate the strength of the relationship between each feature and the target variable. Specifically, the magnitude of the coefficient associated with each feature in the linear regression model can be used as a measure of the feature's importance. There are several methods for selecting features using linear regression. One approach is to simply use the absolute value of the coefficient associated with each feature as a measure of its importance. This is known as the "magnitude-based" approach. Another approach is to use statistical tests, such as the t-test or the F-test, to evaluate the significance of each feature's coefficient. Features with coefficients that are statistically significant can be considered important. It's worth noting that linear regression is just one of many methods for feature selection, and it may not be the most appropriate method for all datasets. Other methods, such as principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination, may be more effective for certain types of data. The choice of feature selection method ultimately depends on the specific problem at hand and the characteristics of the data. 