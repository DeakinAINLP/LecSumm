Linear Regression  Simple linear regression attempts to create an equation that graphs a straight line in order to model the relationship between two variables – feature & output. The linear equation created will permit summarisation and study of the relationships between these two variables as the other one changes. As the number of independent variables increases, it becomes known as multiple linear regression.  Covariance  Covariance provides a measure of the strength of the correlation between two or more sets of random variates. The covariance for two random variables of certain sample size is defined by the “expectation value”.  Pearson’s Coefficient  Measures the linear correlation between two variables: which can range between -1 and +1.    +1 indicates total positive linear correlation (strong positive relationship)   0 is no linear correlation (weak relationship)   -1 shows total negative linear correlation (strong negative relationship)  Linear Classification  Identification of which class a data point belongs to through the use of its features is called linear classification. This is where a classification decision is made based on the value of a linear combination of its feature characteristics, where data is separated through the formulation of a boundary line (2D), plane or hyper-plane (3D or more).  Training of a model  A cost function and an optimizer both need to be defined in order to train a linear regression model. The cost function is used to measure how well our model fits the data, whereas the optimizer decides which direction to move in order to improve this fit.  Prediction & Model performance  After training a linear regression model, we can start to predict the output for new instances. The error (or difference) between the predicted versus test data set is able to be observed through measures such as:    mean squared error   mean absolute error   r-squared error (coefficient of determination)    Logistic Regression  Logistic regression typically requires a large dataset, where predictive analysis estimates the probability of an event occurring based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1.  Regularisation  Logistic regression is susceptible to overfitting, particularly when there is a high number of predictor variables present. Regularization is typically used to help keep parameters normal and regular. It penalises parameters when one or more weights is large or the model is dimensionally high.  Two regularisation methods can be employed: L1 LASSO (reduces noisy features to zero) and L2 Ridge (features with strong correlations). Both of these methods help reduce output variance.  Bias and Variance  Bias is the difference between the models average prediction and the correct prediction value. A high bias model poorly references the training data and oversimplifies the model, resulting in a high error on both training and test data.  Variance is a value which tells us spread of our data, or the variability of model prediction for a given data point. Model with high variance don’t generalize on the data which it hasn’t seen before and solidly references its training data. As a result, such models perform very well on training data but has high error rates on test data.    