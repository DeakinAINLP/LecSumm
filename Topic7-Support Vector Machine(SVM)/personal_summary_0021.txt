Covariance: In linear regression, training data is 𝑥𝑖, 𝑦𝑖, where x is the input and y is the output. The relationship between x and y can be calculated by measuring the covariance. There are three possible outcomes when calculating covariance:  1.  𝑪𝒐𝒗(𝒙, 𝒚) > 𝟎 –  positive correlation, thus as x increases, so will y 2.  𝑪𝒐𝒗(𝒙, 𝒚) < 𝟎 – inverse correlation, thus as x increases, y decreases 3.  𝑪𝒐𝒗(𝒙, 𝒚) = 𝟎 – no correlation, x and y are independent.  Pearson’s correlation coefficient: another method that can be used to measure the linear correlation between two variables. The outcome ranges between -1 to 1 where outcomes closest to…  -  1 have a stronger positive relationship. -  0 have a weaker relationship.  -1 have a stronger negative relationship.  Curvilinear relationship: shows the relationship between two or more variables and where the graph is depicted as anything but a straight line.  Linear regression formulation Used to find the best fitting linear trend line for the linear regression model. The linear regression line has two parameters:  1.  w – the slope 2.  b – the y intercept  Hypothesis: The two parameters above can be used to find the line. Error of value: The difference between what has been predicted and the true output. ^ 𝑒𝑖 = 𝑦𝑖 − 𝑦 There are two ways to find the best fitting line:  1.  Find a line that minimizes the total absolute error. This can be done by trying different w and b combinations.  2.  Find a line that minimizes the total squared error.  Binary classification: when there are only two possible values for output. E.g., given an image of a vehicle, the task may classify it as a car or not a car. Multi-class classification: when there are more than two possible values for an output. E.g., given an image of a vehicle, the task may identify it by brands such as BMW, Mercedes, Toyota etc. Linear classification: the hypothesis that separation between two classes is linear. Despite outputs having the same classification, it does not necessarily mean they are linear to each other. Complexity: If the number of features captured is increased, so will the model’s complexity. Training a complex model on limited training data may cause over fitting. Generalisation: prediction of unseen data. MSE can be used to measure the error of the prediction made against the unseen data.  Logistic regression A decision function used to determine the classification of an output. This function uses a fixed non-linear link function to find a score. The score can be anywhere between 0 to 1, with closeness to 1 indicating one label and closeness to 0 indicating the other label. If the decision is somewhere in the middle of 0 to 1, then it is unclear what the label/classification should be. Logistic regression formulation: the function is also known as a sigmoid function. The graph produced from this function is an S-shaped curve that takes any real number and maps it between 0 to 1. Unlike the linear regression function, logistic regression does not directly model y (output) in terms of x (input). Instead, logit, the log of odds based on x is modelled. Odds:  𝑃(𝑦=1|𝑥) 1−𝑃(𝑦=1|𝑥) 