Because the system works on having the two separate training and test sets. If they are not separated properly, the test set can have a direct influence on the trained model. This will result in inaccuracies with the training data which will endanger the system’s fairness. The system will obviously make false accusations and likely biased ones as it is being directly influenced rather than taught.  Go through this report about Predicting heart disease diagnoses with machine learning and share your thoughts  I find that this article is quite interesting in explaining how heart disease can be detected using machine learning, and also how it attempts to reduce false positives and negatives in diagnosing a patient. 84 percent accuracy is relatively accurate but I was thinking at what could be done to increase that number. Perhaps changing some hyper parameters to tweak the model’s accuracy. Or perhaps introducing additional data, such as a patient’s dietary information, whether they smoke or not and many other variables which could influence heart conditions.  Relevance and Covariance among features or variables  Linear regression is used to find the relationship between two variables X and Y, by fitting a linear equation which can be used to observe the positive or negative relationship between the two data sets. Pearson’s correlation coefficient is used to measure this strength with a value between -1 and 1. -1 demonstrates a negative linear correlation, 0 shows no correlation and 1 shows a totally positive correlation. Covariance can also be used to measure the linear relationship between the variable features and output. There are also multiple kinds of relationships to describe the data such as, simple linear, curvilinear, strong, weak, and no relationships.  Linear classification  Linear classification is a method in which we use to depict whether a feature value’s output in binary ore not. There are two kinds of classification problems which are binary, and multi class classification problems. Binary being the model has to either distinguish whether something is or is not and multi being the model has to pick the correct value out of multiple decisions. Logistic regression is a predictive analysis used for when the out put of the regression analysis features are binary.  6.6 Generalization and complexity  Generalization is used to predict and extrapolate future data values based new instances. The mean square error (MSE) is used to measure the average of squared errors to differentiate between actual and predicted values, it is used to estimate the expected squared error lost. Complexity is increased alongside the number of features as it defines the number of coefficients used within the model. The model complexity must be considered when we have limited training data as there is a risk of overfitting data on the limited training set.  Why do you think many believe you should run the Gradient descent with many different random initializations?  I believe its important to run with multiple random initializations to avoid the algorithm having difficulty optimizing the model, as it will struggle to find a global optimization. Not having random initializations will make the algorithm get stuck in a local optimum. Local optimum is not suitable for complex and non-convex solutions which require larger scale optimizations for better accuracy. I believe it’s also important to not that random initializations can also eliminate any potential initialization bias, which could influence the entire algorithm performance on a single starting point. An averaged array of results from different starting points is a much better way to increase the accuracy and optimal solution.  Logistic regression example:  Can you explain what’s described above in your own words? What’s the aim and benefit of logistic regression? Share in the discussion forum.  The example displays a dataset which uses logistic regression to demonstrate the number of heart attack patients who have suffered from a second heart attack within a year of the first or haven’t suffered from a second one. The two variables explaining the aim of the data are the Independent variable (The treatment of anger management and anxiety score) used to explain the dependent variable (Whether the patient suffered a second heart attack). The correlation between these two variables is what is used to generate predictions of possible further heart attacks. After running a logistics regression, we receive the weights of the IV, and compute the formula:  𝑤0 − 𝑤1 ∗ (𝐷𝑉1 (𝑡𝑟𝑒𝑎𝑡𝑚𝑒𝑛𝑡 𝑜𝑓 𝑎𝑛𝑔𝑒𝑟)) + 𝑤2 (𝐷𝑉2(𝑎𝑛𝑥𝑖𝑒𝑡𝑦 𝑡𝑟𝑎𝑖𝑡)) > 0  we can use the formula to classify the potentiality of a second heart attack And come up with the two results:    The Anger treatment seems to be lowering (negative effect) the risk score of a 2nd  Heart attack.    The Anxiety trait seems to be increasing (positive effect) the risk score of a 2nd Heart  attack.  Model complexity  Model complexity incorporates the over and under fitting of data, which occurs when the model is either overly complex or extremely simple respectively. Therefore, model complexity must be determined to provide the model with the correct complexity. Bias-variance trade is what is used to determine these complexities. It is used to measure the accuracy of a model and the model’s sensitivity when interacting with the training data. 