In the last topic, we had dived into Supervised Learning, and in there we know that it divides into regression and classical problem. But what had these two in commons is that supervised learning train model and evaluate by checking its accuracy. In this module, we will go in-depth with linear regression and linear classical.  In this topic around, we will focus on:  - -  Linear regression and Linear classical Training linear regression model and testing performance via python  Covariation  Linear model creates a function that has a relationship between 2 variables, so for each input  there will be an output. These relationships are called covariance, it shows how x can affect y in that linear regression equation. A quick formula for linear function is : f(x): y = mx + b, where m is the covariance of the function, b is the y intercept.  Regression One theory that we know from previous units is the Pearson’s Correlation Coefficient. This concept measures the correlation between the two variables and has the value -1 to 1, where -1 is 𝑐𝑜𝑣(𝑥,𝑦)  negative correlation and +1 is positive correlation. This is the formulation for this is:  𝑟 =  √𝑣𝑎𝑟(𝑥)𝑣𝑎𝑟(𝑦) In supervised learning though, the linear regression line does not mean that the output will come out perfectly, the goal is to train the model to make it as accurate as possible.  .  Classification  We classified two types of data type, binary and multi-class. “When there are only two possible  values for output, we call the problem a binary classification problem. If there are more values it’s a multi-class classification problem.” To decide which class the output belongs to, we can use two options:  Ignore non-linearity: where we just use the regression line as the border between two binaries.  - -  Using link function: then we can note down the output even more  Generalization and Complexity  The generalization method in machine learning is that once you have trained your models until a  certain level, we can use it to predict an outcome from a new instance. However, with how accurate your performance is, the outcome might differ from the regression function, this error is called Mean Absolute Error.  The complexity of the linear function increases as the number of features increases. The two  problems is that, with limited training dataset, we could be overfitting the model, vice versa by limiting the features, we could be under fitting.  Logistic function  Instead of predicting and finding the outcome value y depending on x, logistic function find the  odd of getting y instead. These probabilities modules are called logits using log. Which makes the regression line becomes a s-shaped curve. In linear regression, we use MAE to find y, with logistic, we can use Maximum Likelihood Estimation. Likewise, we can also use the loss function to maximize the estimation. We can use a closed form formula or iterative of logistic regression to find the minimum of the model. The two common iterative technique is:  -  Gradient Descent make use of first derivative, which creates the curve for us to find the  minimum and maximum of the model.  -  Coordinate-wise Gradient Descent Optimization is to initialize the likelihood function w and fix  the variable value. Iterative optimize the function until the value stops changing.  Noted that this works differently on model that are convex and non-convex. Whereas convex  can easily spot the global minimum, and non-convex are continuous type.  Inside a loss function, we will be expecting three component that could cause it.  Bias Variance Decomposition  -  Bias: is the value that defined the difference between the actual value and the expected value. A high bias value shows that it has the correlation between the two, but it is not accurate as there are errors between the two. Usually, it is due to underfitting the model.  -  Variance: is the dispersion value of the data. It shows how accurate your model is. With high  variance, it shows how close the outcomes are to the target. Usually happens when overfitting the model.  -  Noise error: incontrollable model problem.  Solution  We now know that when modelling in machine learning, it is very common to underfit or overfit the function. One solution we have is Regularized in linear models, this helps prevents these problems in the loss function. Stopping the regression coefficient from taking in large value, selecting only the important features, you will reduce the amount of noise and unwanted data. Since noisy data, will cause the data to fluctuate and hence the chance of underfitting and overfitting the model. There are two options we can use for regularizing the model:  -  Lasso: Perform variable selections in order to increase the prediction accuracy. Uses sum of the absolute values of coefficients.  -  Ridge: Compared to lasso, it adds a penalty to the sum of squared of the magnitude of the  coefficient.  Besides Lasso and Ridge, there are also PCA which we have learned in the previous task, as it will help with dimensionality reduction. With the help of Lasso and Ridge on top of that, it will increase the accuracy of the model.  