Introduction to Topic 6  ● Assessing a trained model  6.2 Relevance and Covariance among features or variables  Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data, in other words an equation that graphs as a straight line.  The next figure, shows a simple linear relationship vs. a curvilinear relationship. A curvilinear relationship is a relationship between two or more variables which is depicted graphically by anything other than a straight line.  Curvilinear relationships are very variable, more complex and less easily identified than simple linear relationships.  Figure. Illustration of linear relationship and curvilinear relationship.  The next figure, depicts a strong relationship vs. a weak one. As you can see in strong relationships, the points are highly correlated in the direction of a line. So  increasing x in most cases will result in increasing y too. Whereas in weak relationships, you can not see a strong relationship among variables.  Figure. Strong relationship vs. weak relationship.  And finally the last figure shows no relationship among data points. You can see that increasing x results in no discernible patterns of differences in y  6.3 Example of Linear Regression  6.4 Linear regression formulation  https://www.youtube.com/watch?v=w9mVr7yJN6I&feature=youtu.be  6.5 Linear classification  Linear classification Logistic regression is the appropriate regression analysis to conduct when the output values of the feature vectors are binary.  Like all regression analyses, the logistic regression is a predictive analysis. Before talking about logistic regression, lets first review linear classification.  6.6 Generalisation and complexity  https://www.youtube.com/watch?v=py8QrZPT48s  6.7 Logistic regression formulation  Summary  To conclude, we saw that logistic regression is like a regression problem and the only difference is in modelling the output. In linear regression we are modelling the y directly but in here we are modelling the logit(log of odds). In the next lesson, we are going to see how to train a logistic regression problem.  6.8 Training a logistic regression model  Computing the minimum Before answering this question, lets recap the difference of two types of functions Convex and Non-convex. Consider the following figure as illustration of two types of functions.  Convex optimisations can deal with only one optimal solution, which is globally optimal. The other possibility is that you prove that there is no feasible solution to the problem (right image on the figure above) In non-convex optimisations, you may have multiple locally optimal points. It can take a lot of time to identify whether the problem has no solution or if the solution is global (left image on figure). Hence, the time efficiency of the convex optimisation problem is much better. So:  Sometimes, we can derive a closed form formula for the minimiser (e.g. linear regression) meaning we can compute the minimiser in one step. If there is no closed form formula, we must take multiple steps iteratively to reach to the minimum. (eg. logistic regression and Kmeans)  Strategies for finding your way forward. © Getty Images (2018)  Imagine you’re blindfolded, but you can see out of the bottom of the blindfold to the ground at your feet. I drop you off somewhere and tell you that you’re in a convex shaped valley and escape is at the bottom/minimum. How do you get out?  The simplest way is to look for steepest slope down! Basically you start walking and you look for slopes going down, preferably the steepest slopes. In maths, we call the slopes derivatives!  The slope of a secant line (line connecting two points on a graph) approaches the derivative when the interval between the points shrinks down to zero. That is the basic idea for optimising these scenarios.  6.10 Model complexity  Model complexity Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model.  We have already encountered over-fitting and under-fitting in previous lessons. The figure below illustrates some of these concepts. Over-fitting will happen when your model starts to capture some irrelevant noise points in the data while building the model, rather than the whole pattern (right image on the figure). Under-fitting is the result of an extremely simple model (left image on the figure).  Figure. Over-fitting and Under-fitting. Under-fitting occurs if the complexity of the model is lower than necessary.  Scenario-1: We may be using a linear model, when the data requires a nonlinear model. Scenario-2: We may be using the right hypothesis (linear or nonlinear) but the number of variables might be falling short of what is required. For example, to predict the income of a person, age alone may not be sufficient. We can detect under-fitting by checking if the model fitting error on the training data is high.  Example 1: To predict a person’s income, knowing age alone is not sufficient. Assuming our dataset has information about age, sex, education; we could add them as explaining variables. Our model becomes more interesting and more complex.  The new model explains the data better but is still not good enough. We need to add even more variables (i.e. location, profession of parents, social background, number of children, weight, preferred colour, best meal, last holiday destination).  Our model will be even better but will probably be over-fitting now. It will probably produce poor predictions on unseen data. It has learnt too many specifics of the training data and will probably have learnt the unhelpful background noise.  Example 2 (Overfitting): Let’s say you attend a symphony and want to record the clearest sound possible. You buy a super-sensitive microphone and audio system to pick up all the sounds in the auditorium. Now, you have started to over-fit. You are detecting unhelpful, undesirable noise such as:  you hear your neighbours shuffling in their seats the musicians turning their pages even the swishing of the conductor’s coat jacket. So fitting a perfect model is only listening to the Symphony (signal) and not to the background noise.  Variance bias trade off  To better illustrate the variance-bias trade-off examine the following figure. As you can see, the best model is a model with low variance and low bias. It means the model is not too complex but is properly accurate. The worst model would have high bias, which means it’s not accurate based on the training data, and high variance which means it’s far too complex.  Figure. Bias Variance Trade-off  As another example lets consider these two situations:  ■  Models with too few parameters are inaccurate because of a large bias (not enough flexibility): under-fitting.  ■  Models with too many parameters are inaccurate because of a large variance (too much sensitivity to the sample): over-fitting.  Figure. Under-fitting with a large bias.  Figure. Over-fitting with large variance  Summary  Based on the above information on the bias-variance trade-off, we know that:  ■  Low bias implies high variance, and high bias implies low variance  ■ We need to find the sweet spot where Risk = bias^2 + variance + noise is the minimum.  ■  The minimum error is at the right model complexity.  Another interesting question arises here is when using linear models, can we still over-fit? It depends!  Depends on our model complexity. In linear models, the model complexity grows with the number of features. Using all data dimensions as features may fit the model on background noise as well as true patterns (signal).  In the next section we are going to introduce Regularisation as a technique used to control the model complexity.  6.11 Regularised linear models  You have learned that even in linear models, using all data dimensions as features may fit the model to true patterns (signal) but also to background noise.  A regulariser is an additional term in the loss function to avoid overfitting. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values. What will happen if one or more weights are excessively large? It implies your model is highly dependent on that one feature.  What if this feature is noise or highly affected by noisy observations? We do not want to rely too much on any one thing when we are designing a model in machine learning! So, this procedure is a way to guide the training process to prefer certain types of weights over others.  There is one more interesting fact about LASSO. Because L_1 regularisation shrinks the weights of noisy dimensions to zero, these dimensions do not participate in the prediction model. Only those dimensions that have non-zero weights participate in the prediction. Therefore, LASSO is also used to select predictive features among all dimensions. This is the feature selection property of LASSO.  6.12 Linear regression for feature selection  Principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination are typical feature selection methods. The specific challenge at hand and the features in the dataset determine the feature selection method to use. To find the most significant features in a dataset, linear regression can be used as a feature selection strategy. The basic idea behind using linear regression for feature selection is to evaluate the strength of the relationship between each feature and the target variable. The features with the highest absolute coefficient values can be found using linear regression. Please explore further for further knowledge regarding this idea of using linear regression in the feature selection process.  References: Chen, Xiaojun, et al. "Semi-supervised Feature Selection via Rescaled Linear Regression." IJCAI. Vol. 2017. 2017. Sethi, Jasleen Kaur, and Mamta Mittal. "A new feature selection method based on machine learning technique for air quality dataset." Journal of Statistics and Management Systems 22.4 (2019): 697-705.  