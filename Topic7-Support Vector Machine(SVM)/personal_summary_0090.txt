Relevance and Covariance among features and Variables Relevance: A measurement of how feature informations are provided for target Covariance: Measurement of how much 2 variables change simultaneously and it also shows the positive / negative relationship between them.  Linear Regression:  ● Used in order to model the relationship between dependent data and independent data (variables) and find a linear relationship between them. The idea is to find the best fit straight line that displays the relationship between the variables.  ● Predicting the value of y using the value of x ● Dependent = Y ● Independent = x1, x2,x3,....., xk  Pearson Correlation Coefficient  ● Measure relationship between two variables (linear regression) ● Range:  ○  -1 (perfect negative correlation): Both variables increase / decrease at the same time meaning stronger relationship between each other  ○ 1 (perfect positive correlation ): One variable increase as the other decrease  meaning no relationship between each other  ○ 0: No correlation between both variables  Linear Hypothesis: Assumption that a linear equation can accurately predict the relationship between the dependent and independent variables  Linear classification:  ● The Goal is to find the prediction of the class based on the features. ● Draw a line in the middle of the graph in order to divide / separate the different class in the  features  Logistic regression (most popular linear classification)  ● Aim is to predict the probability of an events happening base on the variables (one or two) ● Score input will be between 0 and 1 ○ 1 = instance label ○ 0 = label zero ○ Between 1 and 2  ● 2 approach  ○ Ignore non-linearity: Using least squares, binary outputs should be treated similarly  to the outputs of a regression issue  ○ Using link function: Without transforming the independent variables, we can more effectively and simply model non-linear relationships among the independent and dependent variable  Model complexity of Linear Regression  ● Model flexibility in terms of fitting into the data ● As featured increase the complexity increase as well resulting fitting a more complex curves  which will also lead to overfitting  ● Variance bias tradeoff = model become complex = ability to fit the training data increase =  ability to produce new unseen data decrease  ○ Aim to discover model with low bias and variance which can help to generalize  properly with new data  Training a logistic regression model  ● Estimate the regression coefficient vector w using training data. ● Goals is to discover the w value that minimise the difference between the predicted  probability of model plus the actual outcome discovered in the training data  Logistic Loss Function: Calculate the error between the predicted probabilities of the model and the actual binary target values.  Regularised linear model:  ● Works by preventing overfitting the model to the training set of data by decreasing  parameter estimation to zero. This may enhance the model's ability to generalise to new data  ● L1 Regularisation (LASSO): Increases the cost function's cost by the absolute value of the  parameter values. Also a regression analysis technique that selects variables and regularises them in order to improve the accuracy and accessibility of the statistical model it creates. ● L2 Regularisation (Ridge): The squared parameter values are to be added to the cost function    