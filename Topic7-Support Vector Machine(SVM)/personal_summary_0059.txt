Summary   To ensure that we do not get a biased evaluation of our model, it seems logical to not use  the test data to re-evaluate the performance of the model   Covariance measures the volume of information a given Xi is able to provide for yi and is  hence derived as:   Pearsonâ€™s correlation coefficient ranges between -1 and 1 and the closer it is to 1 the  stronger the positive relationship and the closer it is to -1, the stronger it is to a negative  relationship. We also note a value close to zero indicates a weak relationship.   A linear equation should permeate us to summarise and reflect relationships between two  quantitative variables   From the concept of linear classification, we mean finding a separation boundary which  is linear but it may also not be the case   To implement the Logistic Regression approach, we can ignore non-linearity and/or use  the link function   We also note that least squares regression can work very poorly in the case that some  points in the training data have obsessively big or small values for the dependent variable  as opposed to the remaining training data as shown below:     After a given linear regression model has been trained, we can begin to predict the output  yi for a given new instance x (prediction on unseen data)   To calculate the mean square error, we can use the following:   The model complexity of linear models keeps on increasing as the number of features  does. We should hence be aware of the risk of over-fitting and under-fitting which may  arise   The logistic function is also named as the sigmoid function, is an S-shaped curve which  can be given any real-valued number and it would map it to a value ranging 0 to 1  excluding the limits   When we train a logistic regression model, it signifies that we are utilising training data  to predict the regression coefficient w as shown below:   To demonstrate Bias Variance trade-off, the following diagram summarises everything:   