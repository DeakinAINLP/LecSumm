1.  Linear classification  Linear classification is used as a fundamental technique to categorize the data points into different classes. The primary aim is to categorize new data based on the trained data under this model. In this classification, The primary objective of linear classification is to learn a set of parameters that can assign correct  class  labels  to  new,  unseen  data  points  based  on  their  features.  In  linear classification, each data point is represented as a vector of feature values, and a weight vector is multiplied with the feature vector to obtain a scalar value. One of the popular linear classification algorithm is logistic regression. This uses a logistic function that maps linear combination of features and weight to a value between 0 to 1, representing probability.  The  advantages  of  linear  categorisation  are  numerous.  Given  that  a  linear  equation  is used  to  represent  the  decision  boundary,  it  is  computationally  efficient  and  easy  to understand.  Both  huge  datasets  and  feature  spaces  with  many  dimensions  can  be handled by it. Additionally, linear classifiers are resistant to data noise and outliers.  2.  Linear hypothesis  Linear hypothesis is the assumption or mode, that assumes a linear relationship between inputs and outputs.  For linear regression, linear hypothesis can be expressed as:  y = w0 + w1x1 + w2x2 + ... + wn*xn where, y = Predicted output, x1, x2, ..., xn are input features w0, w1, w2, ..., wn are corresponding coefficients.  The hypothesis helps with an efficient computation and easy model training.  3.  Performance metrics  To evaluate the usability of the model, the performance metrics play a huge role. Each model require different performance metrics that considers different scenarios.  For Linear Regression:  Mean Squared Error (MSE), Root Mean Squared Error (RMSE) and R-squared (R2) Score are among the top performance metrics that are more suited for these models.  Mean  Squared  Error  measures  the  average  squared  difference  between  the  predicted outputs and actual values in regression problems. It provides a comprehensive metric on accuracy. The lower the values better is the performance.  The average size of the prediction mistakes is indicated by the Root Mean Squared Error, which  is  the  square  root  of  MSE.  Lower  RMSE  values  represent  higher  model performance, just like MSE.  The  proportion  of  the  dependent  variable's  variation  that  can  be  predicted  from  the independent variables is measured by the R-squared (R2) Score. Higher values (closer to 1) suggest a better fit, with the measure showing how well the linear regression model fits the data.  Logistic Regression:  Accuracy,  precision,  recall  and  F1  Score  are  among  the  top  performance  metrics  for logistic regression.  Accuracy is the computation of the percentage of rightly classified instances out of the total  number  of  data  points.  It  offers  a  general  gauge  of  model  performance  and  is frequently applied to binary classification issues.  Precision  measures  the  percentage  of  accurate  positive  predictions  versus  all  positive forecasts. It is especially helpful in situations when the goal is to reduce false positives, such as in spam detection, where it would be bad to mistakenly classify real emails as spam.  From  the  actual  positive  examples,  recall  determines  the  percentage  of  real  positive forecasts. It is useful when the goal is to reduce false negatives, like in medical diagnosis, when it is vital to correctly identify positive instances.  Finally the F1 score is the harmonic mean of precision and recall. When there is an uneven distribution  of  positive  and  negative  examples,  it  is  appropriate  since  it  provides  a balanced measure that takes both precision and recall into account.  4.  Generalization and model complexity  A  machine  learning  model's  generalization  refers  to  its  capacity  to  make  precise predictions on novel, untried data. Any machine learning model's ultimate objective is to generalize well, which means it must successfully identify and learn patterns from the training data and then use those patterns to generate precise predictions on data that has not yet been observed. Generalization is essential because it enables the model to be effective in situations outside of the training set.  Similarly,  model  complexity,  refers  to  a  model's  ability  to  capture  complicated relationships  in  the  training  data.  A  more  complicated  model  can  potentially  achieve greater  accuracy  on  the  training  data  and  has  a  larger  capacity  to  represent  complex patterns.  5.  L1 and L2 regularization  Regularization techniques are used to alleviate the overfitting problem and improve the generalization performance of linear regression models.  The  objective  function  is  modified  by  the  L1  regularization  by  adding  the  total  of  the absolute  values  of  the  coefficients  times  a  regularization  parameter  (lambda).  By encouraging sparsity in the coefficient values, this regularization effectively causes some coefficients to become zero. L1  regularization can therefore  perform  feature  selection because  it  makes  less  important  features  have  little  to  no  influence  on  the  final predictions.The  objective  function  is  increased  by  the  sum  of  the  squared  coefficient values multiplied by a regularization parameter (lambda) during L2 regularization. Large coefficient  values  are  regularization  by  this  regularization,  whereas  small  coefficient values  are  encouraged.  It  can  lessen  the  effect  of  collinearity  between  features  and prevent the model from depending too much on any one feature.  However,  to  determine  which  regularization  is  better,  the  context  of  the  problem matters.  Reflection on the knowledge gained.  The MSE, RMSE, and  R2 scores among other linear regression performance indicators offer a quantitative  assessment  of  the  model's  correctness  and  capacity  to  fit  the  data.  These measurements enable for model comparisons and hyperparameter adjustment.  For  the  purpose  of  assessing  model  performance,  it  is  common  practise  to  divide  data  into training  and  test  datasets.  We  can  replicate  the  model's  ability  to  generalise  to  new  data  by training it on the training dataset and assessing it on the test dataset. This procedure offers an objective  assessment  of  the  model's  performance  on  new  data  and  aids  in  the  detection  of  overfitting.  Our  comprehension  of  a  linear  regression  model's  capabilities  and  its  practical applicability  is  improved  when  proper  performance  indicators  and  dataset  partitioning  are combined.  