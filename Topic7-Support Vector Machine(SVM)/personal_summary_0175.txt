Machine learning (Pass Task- Topic 6)  After a model is trained; in order to measure its accuracy, The model needs to be accessed. This topic is about how to get an unbiased estimate of the accuracy of a trained model.  Relevance and Covariance among Features  Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data.  Pearson correlation coefficient  The Pearson correlation coefficient (r) is the most common way of measuring a linear correlation. It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables.  Linear Regression- Fitting the line to data points.       There can be more than one single feature in the problem, the problem can be in d dimensions, the linear regression can be written as,  by taking the derivative of the error function and equating it to 0, we will find that:        Model complexity of Linear Regression  Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. The reason is the risk of over-fitting on this limited set of training data. Using a limited number of features may also be problematic as it could cause under-fitting.  Logistic Regression  Logistic Regression was used in the biological sciences in early twentieth century. It was then used in many social science applications. Logistic Regression is used when the dependent variable(target) is categorical.  For example,    To predict whether an email is spam (1) or (0)    Whether the tumor is malignant (1) or not (0)  Consider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.  From this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.  The logistic function is also called the sigmoid function. It’s an S-shaped curve (see the above figure) and it can take any real-valued number and map it into a value between 0 and 1 but never exactly at those limits. The value approaches but never reaches 0 or 1.  Logistic regression does not directly model ‘y’ in terms ‘x’. Instead, it models something called logit value or log of odds against  via linear regression. So generally, we are modelling log of odds based on x.   The odds of class −1 is defined as:  Training a logistic regression model  Training a logistic regression model means using training data to estimate the regression coefficient vector w.  Model Complexity  Over-fitting  happens when we find an overly complex model based on the data. Under-fitting  is the result of an extremely simple model.        Bias Variance Decomposition  Hence, you can see that increasing the variance of a model means lowering bias as the model becomes more complex. On the other hand, you can see that the low complexity  for a model will result in high bias and low variance. So higher bias results in lower variance and high variance results in lower bias.  As you can see, the best model is a model with low variance and low bias. It means the model is not too complex but is properly accurate. The worst model would have high bias, which means it’s not accurate based on the training  data, and high variance which means it’s far too complex.  ▪  We need to find the sweet spot where Risk = bias2 + variance + noise is the minimum. ▪  The minimum  error is at the right model complexity.     Regularized  linear models.  A regulariser  is an additional  term in the loss function  to avoid overfitting.  It is called a regulariser since it tries to keep the parameters more normal or regular.  Regularization  greatly reduces the variance.  It is useful when the net effect (i.e., bias2  +variance) reduces. The following  figure illustrates  the effects of bias and variance on model complexity.  There is a trade-off or optimum  model complexity.  