During this lesson Linear Regression, Logistic Regression, Model Complexity (Bias and Variance) and Regulariser (L1 and L2) are discussed.  Linear Regression  Linear regression is for one dependent variable and one or many independent variables. Linear regression models the relationship between two variables by fitting linear equations to the observed data.  y = h(x) = wx+b  We can find many lines with different w and b values. But we need to find the best line for the problem.  When we want to expand this to many dimensions(many features) we can use the following formula.  ≈∑ (xi ) = w0 xi 0 + w1 xi 1 +w2 xi 2 + ‚Ä¶ + wd xi d  = b + w1 xi 1 +w2 xi 2 + ‚Ä¶ + wd xi d  Value of xi 0 =1 and w0 =b  How to find the best line  To find the best line or to fit the line we need to minimise the Empirical Risk. Minimising the Empirical Risk means finding the almost accurate line. Empirical Risk works on the training dataset.  minw  1 ùëõ  ùëõ 2 ‚àë (ùë¶ùëñ  ‚àí ≈∑ùëñ ) ùëñ=1  is a constant. It doesn‚Äôt affect the minimum value. We need to find the derivative of the rest of the  1 ùëõ function and make it equal to zero to get the minimum value.  Loss function: L(yi , xT  i w) = ( yi - xT  i w )2  Logistic Regression  Logistic regression is used for categorical variables. Can be used for binary classification.  When finding the best fit we use Maximum likelihood estimation. (in linear regression we used Squared error function)  Instead of using the Maximum Likelihood function we are using the Maximum Log of the likelihood function. This is because the Maximum Likelihood function is a product of many functions. It‚Äôs hard to use. But the Maximum Log of the likelihood function is common and easier as it is the sum of logarithms.  Lost Function:  Maximizing log l(w) is equivalent to minimizing -log l(w). Therefore,  the Derivative of an exponential function is exponential.  We need to solve the problem step by step to get the minimum. There are two types of functions. Convex and non-convex.  Non-convex : Have many different solutions  Convex: Have only one minimal solution.  We are looking for the steepest slope.  Two popular methods to compute the gradient (derivatives) of the objective function:  ‚óè Gradient descent (uses first derivative) ‚óè Newton‚Äôs method (uses second derivative)  Model Complexity  Overfitting  happens when we try to find over complex model on data.  The model is over-trained or over learned  Model is complex  REach all the data points in the training set  When testing gets poor predictions.  When we are increasing the number of variables in the model, the model gets complex and the model doesn‚Äôt perform well with the test data.  Underfitting  Happens when we try to find extremely simple model on data.  Bias Variance Decomposition  Getting higher variance means the model is getting more complex. Lower complexity means high bias and lo variance.  Higher complexity means lower bias higher variance. We have to create a balance/ tradeoff between bais and variance.  Best model : Low bias(accurate) and low variance( less complex).  The bias is too high, Variance is low. Underfitting  Bias is low and high variance. Overfitting  Variance controls the complexity of the model.  Regularised Linear Model  Regulariser is an additional term in the loss function to avoid overfitting.  Can make the model more regular.  Adding a regularizer to the loss function to control the complexity of the model to avoid overfitting.  Weight high for relevant features. Zero or low value for irrelevant features.  There are two popular regulariser functions:  Option 1:  This encourages 0 weights (sparsity). This function implies the closed-form function of a square.  Option 2:  This penalizes large weights. This function implies the closed-form function of a circle.  L1 - norm  Square shape.  The absolute value in every direction. It has a chance to get zero weights for features. We can discard that feature.  L1-norm is good to remove features.  L2-norm  Circle shape.  Possible to get weights for all the features. It has less chance of removing features.  All the models try to remove the complexity.  L1 Regularisation (LASSO)  Perform  1. variable selection 2. Regularization 3. Reduce the variance of the model  L2 Regularization  1. Ridge regularisation  Has L2-norm  2. Elastic net  Has both L1 and L2 norms  