  Relevance and Covariance among features or variables Covariance is calculated as the average of the product of the deviations of each variable from its  mean. A  positive  covariance  indicates  that  the  two  variables  tend  to  move  in  the  same direction, while a negative covariance indicates that they tend to move in opposite directions. However, the magnitude of the covariance does not provide information about the strength of the relationship between the variables. Correlation is calculated as the covariance divided by the product of the standard deviations of the two variables. Correlation ranges from -1 to 1, with a value of 1 indicating a perfect positive linear relationship, a value of -1 indicating a perfect negative linear relationship, and a value of 0 indicating no linear relationship.  Linear Regression Linear regression is a statistical method used to model the relationship between two variables by fitting a linear equation to the observed data. One variable is the dependent variable, while the other is the independent variable. The goal of linear regression is to find the line of best fit that  minimizes  the  sum  of  the  squared  distances  between  the  observed  data  points  and  the predicted values on the line.  Linear classification A linear classifier is a model that makes a decision to categories a set of data points to a discrete class based on a linear combination of its explanatory variables. As an example, combining details about a dog such as weight, height, colour and other features would be used by a model to  decide  its  species.  The  effectiveness  of  these  models  lie  in  their  ability  to  find  this mathematical combination of features that groups data points together when they have the same class and separate them when they have different classes, providing us with clear boundaries for how to classify.  f(x,W,b)=∑j(Wjxj)+b  x: input vector W: Weight matrix b: Bias vector   Logistic regression Logistic  regression  is  a  statistical  model  used  for  classification  and  prediction  problems.  It models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In  logistic  regression,  the  dependent  variable  is  categorical,  and  the  goal  is  to  predict  the probability of an observation belonging to a certain category. For example, it can be used for fraud detection, where certain behaviors or characteristics may have a higher association with fraudulent activities. The logistic function, also known as the sigmoid function, is used in logistic regression to map any real input to a value between zero and one. The output of the logistic function is interpreted as the probability of the event taking place.  Linear Regression vs Logistic Regression  Linear Regression  Logistic Regression  Linear Regression is a supervised regression model. Equation of linear regression: y = a0 + a1x1 + a2x2 + … + aixi Here, y = response variable xi = ith predictor variable ai = average effect on y as xi increases by 1  we predict the value by an integer number. we calculate Root Mean Square Error(RMSE) to predict the next weight value. Linear regression is used to estimate the dependent variable in case of a change in independent variables.  Logistic Regression is a supervised classification model. Equation of logistic regression y(x) = e(a0 + a1x1 + a2x2 + … + aixi) / (1 + e(a0 + a1x1 + a2x2 + … + aixi)) Here, y = response variable xi = ith predictor variable ai = average effect on y as xi increases by 1 we predict the value by 1 or 0.  we use precision to predict the next weight value.  logistic regression is used to calculate the probability of an event.   Model complexity it refers to the number of predictor or independent variables or features that a model needs to take into account in order to make accurate predictions. For example, a linear regression model with just one independent variable is relatively simple, while the model with multiple variables or non-linear relationships is more complex. A model with a high degree of complexity may be able to capture more variations in the data, but it will also be more difficult to train and may be more prone to overfitting. On the other hand, a model with a low degree of complexity may be easier to train but may not be able to capture all the relevant information in the data. Finding the  right  balance  between  model  complexity  and  predictive  power  is  crucial  for  successful machine learning. The picture below represents a complex model (extreme right) vis-a-vis a simple  model  (extreme  left).  Note  the  aspect  of  a  number  of  parameters  vis-a-vis  model complexity.  The following are key factors that govern the model complexity and impact the model accuracy with unseen data:    The  number  of  parameters:  When  there  is  a  large  number  of  tunable  parameters, which  is  also  sometimes  called  the  degrees  of  freedom,  the  models  tend  to  be  more susceptible to overfitting.    The range of values taken by the parameters: When the parameters can take a wider  range of values, models can become more susceptible to overfitting.    The  number  of  training  examples: With  a  fewer  or  smaller  number  of  datasets,  it becomes easier for models to overfit a dataset even if the model is simpler. Overfitting a dataset with millions of training examples requires an extremely complex model.  