 In this module we learned further about linear regression and logistic regression. Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data. We can measure the linear relationship between the variables x and output y by using covariance.  Covariance measures the amount of information xi can provide for y1. There is a mathematical formula for it which is given to us in the module. If covariance is > 0 this signifies x and y are positively corelated as x and increases so does y. if covariance is < 0 it signifies x and y are inversely corelated as x increases y decreases and vice versa. If covariance is = 0 it signifies x and y are independent.  Then we learned how Pearson's correlation coefficient can also measure the linear relationship between two variables. Then we learned about the mathematical formulation for linear regression. Then we learned about linear classification aka logistic regression. This type of regression is most suitable when the values of feature vector are binary. If the values of a problem are binary, we call it a binary classification problem. When there are more than two values, we call it a multi- class problem. Examples are between classifying if an apple or not, the answer is binary, this is an example of binary classification. Another example, classifying if an object is an apple, orange, or pineapple is a multiclass problem.  Then we learned about generalization and complexity. Generalization is prediction on unseen data. Model complexity is the same as the previous module where underfitting means it a simple model and not complex enough, overfitting means the complexity is extremely high. We need to find the sweet spot between the two. Then we learned about bias variance decomposition. It is the estimate of error in a model based on its bias and variance. Low bias implies high variance and high variance implies low bias we need to find a sweet spot between the two.  Then we learned about regularization, in linear models, using all data dimensions as features may fit to true patterns but also to noise, so a regularize is an additional term in the loss function to avoid over fitting. It does not let the model be over   dependent on one feature. One thing to note regularization increases bias in our model. There are two techniques LASSO or l1 and l2 which is ridge used to regularize data.  We can use linear regression for feature selection apart from PCA and correlation- based feature selection. In it we evaluate the strength of each feature and target variable. Then in the last part of this module we learned about applying these theoretical concepts to the practical use of Python.  