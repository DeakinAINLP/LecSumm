Topic6. Linear model  Relevance and Covariance among features or variables  -  Covariance: measures the amount of information a specific x can provide for y  -  If cov(x,y) > 0 -positive correlated, cov(x,y) <0 inversely correlated, if 0, independent Pearson`s correlation Coefficient : measure of the linear correlation between two variables x and y. â–ª  Only about linear relationship â–ª  Ranges between -1 to 1 â–ª  Closer to 1, the stronger the positive relationship â–ª  Closer to 0, weaker relationship â–ª  Closer to -1, stronger the negative relationship  -  Curvilinear relationship: relationship between two or more variables which is depicted  graphically by anything other than straight line. Very variable, more complex and less easily identified. linear hypothesis Fitting the line to data point ð‘¤  =   (ð‘‹ð‘‡ð‘‹)âˆ’1ð‘‹ð‘‡ð‘¦ Linear classification  - -  -    For only two possible values for output, binary classification, and if there are more,  then multi-class classification    Meaning that the separation boundary between any two classes is linear, but this is  just hypothesis    Very effective when the classes are well separated in the feature space  -  Logistic regression    Model the probability of a binary outcome   Based on the logistic function, maps any real-valued input to a value between o & 1  1.  Ignore non-linearity : using least squares for classification : treat binary outputs like the outputs in the regression problem  2.  Using link function  : to use the conditional probability of the class as the output in the regression problem : using a link function to transform the output to the classification scenario  -  Generalisation (prediction on unseen data)    Measure the error in prediction: Mean Square Error(MSE)-performance measure  : or Mean Absolute Error(MAE), explained variance(R^2)  -  Model complexity of linear regression  Increases with the number of features     Risk of overfitting on limited set of training data   Risk of underfitting on limited number of features  -  Logistic regression formulation : logistic function(sigmoid function)    S shaped curve   Odds   -    Logit(log of odds) Training a logistic regression model : means using training data to estimate the regression coefficient vector w : logistic loss function(maximizing the log of the likelihood function)    Computing the minimum  a.  Convex optimization can deal with only one optimal solution, which is globally  b.  optimal. Or there is no feasible solution In non convex optimization, there may be multiple locally optimal points. Can take long time to identify where the problem has solution or if it is global. Time efficiency of the convex is better    Iterative optimizing a.  Gradient Descent(uses first derivative):maximises a function using knowledge of  its derivative  -  Model complexity    Bias variance decomposition  : increasing the variance of model => lowering bias as the model becomes more complex    Variance bias trade off  : best model is with low variance and bias. : high bias-not accurate(under fitting), and high variance(over fitting) too complex    Risk = bias^2 + variance + noise  -  Regularised linear models    Regulariser: an additional term in the loss function to avoid overfitting   Does not allow regression coefficients (weights) to take excessively large  value(highly dependent on that one feature) a.  Encourages 0 weights(sparsity), implies the closed form function of square b.  Penalizes large weights, implies the closed form function of a circle    P <= 1 tends to create sparse wights.   Higher values of p tends to like similar and smaller weights  1.  L1 (Lasso)-Least Absolute Shrinkage and Selection Operator  : a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability : encourages sparsity in the weights and can perform feature selection  2.  L2(Ridge)  : add penalty term proportional to the squared magnitude of the model weights : encourages the weights to be small and can handle correlated predictors    Limitations of Lasso : struggle with high dimensional data where there are many correlated predictors, then Lasso tends to select only one predictor 1.  Elastic Net  : regularization technique combines Lasso and Ridge to overcome limitation. : can be useful in high dimensional data with correlated predictors   Regularization increases bias in model (under-fitting), and greatly reduces the  variance  -  Linear regression for feature selection    Principal Component Analysis(PCA)  : reduce dimensionality of a dataset by identifying the most important features that explain the majority of the variability in the data. : done by transforming the original features into a new set of uncorrelated variables (principal components)    Correlation-Based Feature Selection  : selects features based on their correlation with the target variable. Calculates the correlation coefficient between each feature and the target variable. : assumes that highly correlated features are redundant and can be replaced by a smaller subset of highly correlated features    Recursive Feature Elimination(RFE)  : an iterative method that starts with all features, fits as model, eliminates the least important feature based on a feature importance metric  