This topic’s module expanded on our understanding of Linear Regression and prediction of continuous metrics to Logistic Regression and prediction of discrete or categorical values. I especially found the analysis of the results interesting, in particular, extracting the true positives, true negatives, false positives, and false negatives from the performance results to gain detailed insight into how the model performed.  We also applied Lasso and Ridge regularisation to the logistic models to prevent overfitting and fine tune the results by adjusting the penalty costs for the loss functions for the models. This had an impressive improvement on the results.  Covariance & Correlation  Covariance and correlation are two terms often used interchangeably, they’re related, but they are not the same.  Covariance signifies the direction of the linear relationship between two variables. Direction refers to whether the variables are directly proportional or inversely proportional to each other. Essentially, how the variables change together, not the dependency of one on the other. It’s also important to note that covariance is only useful to find the direction of the relationship, not the magnitude. In the case of Linear regression, we want to know what information a particular variable or feature (𝑥) can provide for a target output (𝑦) and is calculated as:  𝐶𝑜𝑣(𝑥, 𝑦) =  n (𝑥𝑖−𝑥)(𝑦𝑖−𝑦) Σ𝑖=1 𝑛−1  The result of which gives us three possible outcomes:    𝐶𝑜𝑣(𝑥, 𝑦) > 0: 𝑥 and 𝑦 are positively correlated. An increase in 𝑥 = an increase in 𝑦.   𝐶𝑜𝑣(𝑥, 𝑦) < 0: 𝑥 and 𝑦 are inversely correlated. An increase in 𝑥 = a decrease in 𝑦.   𝐶𝑜𝑣(𝑥, 𝑦) = 0: 𝑥 and 𝑦 are independent.  Correlation on the other hand is a statistical measure of the strength of a relationship between two numerical continuous variables. It not only tells us the kind of relationship in terms of direction, but also the strength of the relationship because the correlation values are standardized ranging between -1 and 1. Correlation is calculated as:  𝐶𝑜𝑟𝑟(𝑥, 𝑦) =  𝐶𝑜𝑣(𝑥,𝑦)  𝜎𝑥∗𝜎𝑦  𝑤ℎ𝑒𝑟𝑒 𝜎 𝑖𝑠 𝑡ℎ𝑒 𝑠𝑡𝑎𝑛𝑑𝑎𝑟𝑑 𝑑𝑒𝑣𝑖𝑎𝑡𝑖𝑜𝑛.  Similar to covariance, correlation is interpreted as:    𝐶𝑜𝑟𝑟(𝑥, 𝑦) = 1:   Perfect positive relationship.   C𝑜𝑟𝑟(𝑥, 𝑦) = 0:  No relationship.   𝐶𝑜𝑟𝑟(𝑥, 𝑦) = −1: Perfect negative relationship.  Linear Classification & Logistic Regression  As we’re aware, Linear regression is a commonly used supervised machine learning algorithm that predict continuous values. Linear regression assumes that a linear relationship exists between dependent and independent variables.  Linear classification is initially an extension of a linear regression model, but used for binary classification problems, essentially fitting a line to some data that categorizes those data into one of the two categories. Linear classification’s strength is that it is computationally easy, its weakness is that it’s highly sensitive to outliers.  Logistic regression fits an “S” shaped logistic function line to the data rather than a straight line using conditional probability as the output of the regression problem. This is done using a linking function typically a Sigmoid function, to calculate the probability ranging from 0 to 1. If the output is greater than 0.5 then it’s categorised as a 1, if the output is less, then 0. The strength of logical regression is in its ability to classify samples whether continuous or discrete.  Essentially, instead of modeling 𝑦 in terms of 𝑥, Logistical regression models 𝑦 in terms of the log of odds which is calculated as:  𝑂𝑑𝑑𝑠 =  𝑃(𝑦=  1 ) 𝑥 1 1−𝑃(𝑦= ) 𝑥  Another key difference between Linear Regression and Logistic Regression is the type of dependent variable we’re aiming to predict. With linear regression we’re aiming to predict a metric, with logistic regression we’re aiming to predict a binary value, 0 or 1, essentially categorizing the output into one of two categories.  Model Complexity & Variance Bias Tradeoff  Model complexity refers to the level of complexity or flexibility of a machine learning model in representing a relationship between independent and dependent variables. A model with high complexity can represent more complex relationships but may suffer from overfitting, where a model with low complexity may not be able to capture complex relationships in the data, leading to underfitting.  The bias-variance tradeoff describes the relationship between model complexity, bias, and variance. Bias is the difference between the predicted values of a model and the true values, while variance refers to the variability of model predictions for different training datasets.  In general, as model complexity increases, the bias of the model decreases, while the variance increases. Conversely, as model complexity decreases, the bias increases, while the variance decreases. The goal is to find the optimal balance between bias and variance that minimizes the overall error of the model.  Regularised Linear Models  Regularisation is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function of the model. The penalty term controls the complexity of the model by adding a cost for large parameter values. Two common regularisation techniques are 𝐿1 and 𝐿2 which we covered in this topic’s module.  𝐿1 regularisation, which is also known as Lasso regularisation, adds a penalty term that is proportional to the absolute value of the coefficients. This has the effect of shrinking some of the coefficient’s values to zero, effectively selecting a subset of features for the model.  𝐿2 regularisation, also known as Ridge regularisation, adds a penalty term that is proportional to the square of the coefficients. This also has the effect of shrinking the coefficient values towards zero but not necessarily zero. 𝐿2 is useful for preventing overfitting and improving model performance on unseen data.  𝐿1𝑎𝑛𝑑 𝐿2 regularisation can be used independently but are often used together to combine the benefits of each. This application is referred to as Elastic Net regularisation.  