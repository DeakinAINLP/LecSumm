Learning objective  Learning Report  -  Differentiate supervised learning from unsupervised learning - -  Estimate the performance of different supervised learning models Implement model selection and compute relevant evaluation measures  Learning summary  -  Relevance and Covariance among features or variables  Linear regression attempts tmodel the relationship between twvariables by fitting a linear equation tthe observed data, in other words an equation that graphs as a straight line. In linear regression, training data is of the form of xi, yi. For each data point (feature vector) xi, there is an output yi, which can be any real-valued number. We are looking for a particular relationship between a feature and the output.   We can measure the linear relationship between the variable and output using  covariance.  There is a different measurement called Pearson’s Correlation Coefficient, which is a measure of the linear correlation between twvariables, x and y.  It has a value between +1 and −1 where 1 indicates total positive linear correlation, 0 is nlinear correlation, and −1 shows total negative linear correlation.  -  Linear regression formulation  The linear equation should allow us tsummarise and study relationships between tw continuous (quantitative) variables.  Defining a line: y = h(x) = wx + b  -  Linear classification  Logistic regression is the appropriate regression analysis tconduct when the output  values of the feature vectors are binary.    A linear classifier is a model that makes a decision tcategories a set of data points ta  discrete class based on a linear combination of its explanatory variables.  There are only twpossible values for output, we call the problem a binary classification  problem. If there are more values, it’s a multi-class classification problem.  -  Generalization and complexity: Linear regression has a closed form solution. Python  implementation uses Singular Value Decomposition (SVD) tcompute the Moore-Penrose inverse of matrix X.  Given an unseen set of instances as a test set, we can measure the error in prediction as:  ▪  The error is called the mean square error (MSE). This is an example of a performance measure. We can alscompute many other measures such as Mean Absolute Error (MAE) or explained variance.  Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. The reason is the risk of over-fitting on this limited set of training data. Using a limited number of features may alsbe problematic as it could cause under-fitting. Training a logistic regression model: Training a logistic regression model means using training data testimate the regression coefficient vector w.  -   In training a logistic regression model, we can use maximum likelihood estimation (MLE) testimate w.  Logistic Loss Function: Maximising likelihood is equivalent tmaximising the log of the  likelihood function because both provide the same solution for w.  Logistic regression models generate probabilities. Log Loss is the loss function for logistic   regression. Iterative optimizing: An iterative optimization algorithm is a recursion, which generates successive approximations for the global minimum of J(ρ).  -  Regularised linear models  A regulariser is an additional term in the loss function tavoid overfitting. It is called a  regulariser since it tries tkeep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) ttake excessively large values.  Regularisation impact: significantly reduces the variance of the model, without  substantial increase in its bias  There are twregularly used models of regularization: L1 Regularisation (LASSO) and L2  Regularisation (Ridge)  -  Linear regression for feature selection: Principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination are typical feature selection methods. Tfind the most significant features in a dataset, linear regression can be used as a feature selection strategy. The basic idea behind using linear regression for feature selection is tevaluate the strength of the relationship between each feature and the target variable. The features with the highest absolute coefficient values can be found using linear regression.   Topicly quiz:    