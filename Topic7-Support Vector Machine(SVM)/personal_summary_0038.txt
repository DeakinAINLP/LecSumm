Linear regression is an equation that graphs as a straight line by fitting a linear equation to the observed data to model the relationship between two variables. The relationship between a variable and the output is measured by using covariance, which is the amount of information a specific data point can provide about the output. Pearson’s Coefficient Correlation is also used to measure the correlation between two variables, outputting values between -1 and 1 to show the strength of correlation. The closer to 1, the stronger the positive relationship is, the closer to 0, the weaker the relationship and the closer to -1, the stronger the negative relationship.  In linear regression, the goal is to find an equation that will allow us to summarise and study the relationships between quantitative variables. In order to find this line, we need two parameters,  the slope of line as well as its y-intercept.  When the output only has two possible outputs, the problem is called a binary classification problem and the proper regression analysis to conduct would be logistic regression. Linear classification refers to the hypothesis that the separation boundary between any two classes is linear. In some instances, we can have outputs which although are in the same class as one another, they are also non-linear with respect to one another. To handle these non-linear ambiguity, we have two approaches we can generally use, either we use least squares for classification, or we use the conditional probability of the class as the output in the regression problem. However, since the least squares aims to minimise the sum of the squared error, any training data with excessively large or small values compared to the rest of the data will have a disproportionately large effect on the resulting constants that are being solved for.  Once a linear regression model has been trained, it can be used to predict the outputs of new instances and can then measure the error in prediction. Linear models become more complex as the number of features increase. If we have limited training data, it's important to be aware of model complexity since it can lead to over-fitting. On the other hand, having too few features can lead to under-fitting.  Logistic regression is named after the function used at the core of the method, the logistic function, which is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1. Logistic regression is similar to linear regression, except that we model the output differently. In linear regression, we model the output directly, but in logistic regression, we model the log of odds.  Training a logistic regression model means using training data to estimate the regression coefficient vector. While training a logistic regression model, we can use the maximum likelihood estimation in order to estimate the regression coefficient vector. Maximising likelihood is equivalent to maximising the log of the likelihood function because both provide the same solution for the regression coefficient vector. In some cases, we can easily find the answer to a problem in one step. But other times, we have to take multiple steps to find the answer. This is the case with problems like logistic regression and K-means clustering. In these cases, we use a method called Coordinate-wise Gradient Descent Optimisation to iteratively find the answer.  Under-fitting occurs if the complexity of the model is lower than necessary. For example, if a linear model is being used when a nonlinear model is more appropriate for the data, or the correct hypothesis is being used, but there aren't enough variables to make accurate predictions. The concept of bias-variance decomposition is an essential aspect of machine learning. It helps us understand the generalization error of a model, which is the difference between the performance of the model on the training data and the performance on new, unseen data. This difference can be  broken down into two main components: bias and variance. Bias refers to the difference between the expected predictions of the model and the true values, while variance is the amount of variability of the model's predictions for different training sets. Balancing bias and variance is critical in developing a model that can generalize well to unseen data, which is the ultimate goal of any machine learning algorithm.  Another term in the loss function is the regulariser, which tries to keep the parameters more regular, meaning it doesn’t allow regression coefficients to take excessively large values. The term encourages the model to choose some weight types over others, for example, by penalising heavy weights or encouraging sparsity. LASSO which promotes sparsity, and Ridge, which penalises heavy weights, are the two most widely used regularisation functions. : LASSO is a regression analysis method that performs both variable selection and regularisation in order to enhance the prediction accuracy and interpretability of the statistical model it produces. Regularisation can increase bias in the model, but it greatly reduces variance and can lead to a better overall performance. The choice of regularisation method and strength depends on the specific problem and dataset, and finding the right balance between bias and variance is crucial for optimal performance.  As there are multiple feature selection methods to choose from, the challenge being faced and the features in the dataset are used to determine which method to use. To find the most significant features in a dataset, we can use linear regression, as we can use it to evaluate the strength of the relationship between each feature and the target variable.     