Relevance and Covariance among features or variables   Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data, in other words an equation that graphs as a straight line.    We can measure the linear relationship between the variable x (could be of many dimensions) and output y (for now suppose y is only one dimensional), using covariance - a measure of the relationship between two random variables.  Pearson’s Correlation Coefficient    A measure of the linear correlation between two variables   Pearson’s Correlation Coefficient is all about linear relationship.  o  Ranges between −1 to 1 o  The closer to 1, the stronger the positive relationship o  The closer to 0, the weaker the relationship o  The closer to −1, the stronger the negative relationship  A curvilinear relationship is a relationship between two or more variables which is depicted graphically by anything other than a straight line.  Curvilinear relationships are very variable, more complex and less easily identified than simple linear relationships.  Relationships  o  Strong relationships, the points are highly correlated in the direction of a line. o in weak relationships, you can not see a strong relationship among variables. o  no relationship among data points. increasing x results in no discernible patterns of  differences in y  Linear regression formulation  In linear regression we want to find a line similar to ℎ. The linear equation should allow us to summarise and study relationships between two continuous (quantitative) variables.  Linear hypothesis  So how can we find the line? The line has two parameters. w, the slope of the line, and b, the y intercept of the line. If we have these two parameters, we can use them to find our straight line.  if x is not a single dimension value? Because we may have more than one single feature in the problem, the problem can be in d dimensions. The only difference is regarding the multiplication of w and x which should be handled in all d dimensions in the introduced formulation.  What is the error of value prediction in regression? The difference between what we predicted and the true value or output of that point, is considered to be the error.  Linear classification  Task 6.1P  Logistic regression is the appropriate regression analysis to conduct when the output values of the feature vectors are binary. Like all regression analyses, logistic regression is a predictive analysis.  When there are only two possible values for output, we call the problem a binary classification problem. If there are more values it’s a multi-class classification problem.  o  Example 1: binary  ▪  Given an image, the task maybe to classify if it is an image of a fruit or not fruit.  o  Example 2: multi-class  ▪  Given an image of a fruit, the task may be to classify whether it is an orange, an  apple or a banana.  By linear classification, we mean that the separation boundary between any two classes is linear. This is just our hypothesis. It may not be true!  Logistic regression  Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables.  So two approaches are generally available:  o Ignore non-linearity: Using least squares for classification: treat binary outputs like the outputs in the regression problem. This may not be the best method but it is easy!  o  Using link function:  Another approach is to use the conditional probability of the class as the output in the regression problem.  In other words using a link function to transform the output to the classification scenario. In here, in a simple case, we may want to choose class label 1 for the data point x unless, it seems more logical to select label 0 for this data point.  Least squares regression can perform very badly when some points in the training data have excessively large or small values for the dependent variable compared to the rest of the training data. The reason for this is that since the least squares method is concerned with minimising the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the data will have a disproportionately large effect on the resulting constants that are being solved for.  Generalisation and complexity  It is more flexible than linear regression because: GLM works when the output variables are not continuous or unbounded. GLM allows changes in unconstrained inputs to affect the output variable on an appropriately constrained scale.  As long as we are using a linear formulation, the problem remains a linear regression. This is called the generalised form of linear regression in which we are adopting our own list of features and try to fit a line based on the new features.  Generalisation (Prediction on unseen data)  After training a linear regression model, we can start to predict the output y^ for a new instance x. Given an unseen set of instances as a test set, we can measure the error in prediction  Model complexity of Linear Regression  Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. The reason is the risk of over-fitting on this limited set of training data. Using a limited number of features may also be problematic as it could cause under-fitting.  Logistic regression formulation  The logistic function is also called the sigmoid function. It’s an S-shaped curve (see the above figure) and it can take any real-valued number and map it into a value between 0 and 1 but never exactly at those limits. The value approaches but never reaches 0 or 1.  Logit = This Log of odds.  To conclude, we saw that logistic regression is like a regression problem and the only difference is in modelling the output. In linear regression we are modelling the y directly but in here we are modelling the logit(log of odds).  Training a logistic regression model  Training a logistic regression model means using training data to estimate the regression coefficient vector w.  In training a logistic regression model, we can use maximum likelihood estimation (MLE) to estimate  Logistic Loss Function  Log loss is a logistic regression cost function and a classification metric based on probabilities.  Maximising likelihood is equivalent to maximising the log of the likelihood function because both provide the same solution for w. by taking the log of the function you are still able to find the maximum or minimum of the function since the logarithmic functions are monotone increasing functions.  Computing the minimum  The basic difference between the two categories is that:   o  Convex optimisations can deal with only one optimal solution, which is globally optimal. The other possibility is that you prove that there is no feasible solution to the problem (right image on the figure above) In non-convex optimisations, you may have multiple locally optimal points. It can take a lot of time to identify whether the problem has no solution or if the solution is global (left image on figure). Hence, the time efficiency of the convex optimisation problem is much better.  So:  o  Sometimes, we can derive a closed form formula for the minimiser (e.g. linear  o  regression) meaning we can compute the minimiser in one step. If there is no closed form formula, we must take multiple steps iteratively to reach to the minimum. (eg. logistic regression and Kmeans)  The slope of a secant line (line connecting two points on a graph) approaches the derivative when the interval between the points shrinks down to zero. That is the basic idea for optimising these scenarios.  Iterative optimising  Optimisation theory has many methods for iterative optimisation. Two popular methods to compute gradient (derivatives) of the objective function  o  Gradient Descent (uses first derivative)  ▪  Gradient Descent maximises a function using knowledge of its derivative. While  Newton’s method, a root finding algorithm, maximises a function using knowledge of its second derivative. This can be faster when the second derivative is known and easy to compute. However, the analytic expression for the second derivative is often complicated or intractable, requiring a lot of computation.  o  Coordinate-wise Gradient Descent Optimisation = is an optimization algorithm that  successively minimizes along coordinate directions to find the minimum of a function. there is always a chance of getting stuck in local minimums rather than the global minimum. When you’re dealing with convex functions, that’s not a problem but with non-convex functions, it could be a serious problem.  Model complexity Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model.  Over-fitting will happen when your model starts to capture some irrelevant noise points in the data while building the model, rather than the whole pattern (right image on the figure). Under-fitting is the result of an extremely simple model (left image on the figure).  Task 6.1P  Under-fitting occurs if the complexity of the model is lower than necessary.  o  Scenario-1: We may be using a linear model, when the data requires a nonlinear model. o  Scenario-2: We may be using the right hypothesis (linear or nonlinear) but the number of variables might be falling short of what is required. For example, to predict the income of a person, age alone may not be sufficient.  We can detect under-fitting by checking if the model fitting error on the training data is high.  Variance bias trade off  the best model is a model with low variance and low bias. It means the model is not too complex but is properly accurate. The worst model would have high bias, which means it’s not accurate based on the training data, and high variance which means it’s far too complex.  Based on the above information on the bias-variance trade-off, we know that:  Low bias implies high variance, and high bias implies low variance  ▪ ▪  We need to find the sweet spot where Risk = bias2 + variance + noise is the minimum. ▪  The minimum error is at the right model complexity.   In linear models, the model complexity grows with the number of features. Using all data dimensions as features may fit the model on background noise as well as true patterns (signal).  Regularised linear models  A regulariser is an additional term in the loss function to avoid overfitting. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values. What will happen if one or more weights are excessively large? It implies your model is highly dependent on that one feature.  We do not want huge weights (i.e. do not want to over-rely on any one feature). If weights are huge, a small change in a feature would result in a large change in the prediction! In fact, since we may even have irrelevant features, we want some of the weights to be zero so we can discard some features.  How do we create a regulariser that penalises large weights or encourages small/zero weights ? What should our regulariser function be?  There are two popular regulariser functions: Ridge Regularization and Lasso Regularization.  Ridge Regularization / Regression, it modifies the over-fitted or under fitted models by adding the penalty equivalent to the sum of the squares of the magnitude of coefficients. This means that the mathematical function representing our machine learning model is minimized and coefficients are calculated. The magnitude of coefficients is squared and added. Ridge Regression performs regularization by shrinking the coefficients present.  Lasso regression also performs coefficient minimization, but instead of squaring the magnitudes of the coefficients, it takes the true values of coefficients. It modifies the over-fitted or under-fitted models by adding the penalty equivalent to the sum of the absolute values of coefficients. This means that the coefficient sum can also be 0, because of the presence of negative coefficients  What are the effects of Regularisation on bias and variance? = We know that regularisation increases bias in our model.  Why regularisation might make sense? = Because it greatly reduces the variance.  Linear regression for feature selection  Principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination are typical feature selection methods.  The specific challenge at hand and the features in the dataset determine the feature selection method to use. To find the most significant features in a dataset, linear regression can be used as a feature selection strategy. The basic idea behind using linear regression for feature selection is to evaluate the strength of the relationship between each feature and the target variable. The features with the highest absolute coefficient values can be found using linear regression.  