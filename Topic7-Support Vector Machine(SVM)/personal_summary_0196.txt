Linear regression is a tool used to map the correlation between two variables by plotting a straight line along the main axis of the data. When correlation between variables is present, data points for x will increase or decrease as x increases. This results in data plotted on a graph to grow or shrink, creating a direction of travel. Linear regression aims to map this line as accurately as possible. The closer the data points are along this line the higher the correlation present between variables, with weaker correlation being represented by looser grouping of data points while still moving on this line and a completely horizontal line indicating no correlation.  Linear classification involves graphing a line which best estimates the point of separation between two classes. This is used in ‘binary classification’ problems where each datapoint must fit into one of two classes. In the case of linear classification, the regression measurements of these datapoints becomes irrelevant but rather the Boolean response of whether the point is above or below the classification line is the focus.  After training a model using one of the models we can then use it to predict the outcome of other data passed through the model. We test the accuracy by performing this prediction on a test set which we can compare the results from the model with the real targets. The more accurate our tests are, the more we can rely on our model to make useful predictions on real data. This process of prediction on unkown data is lnown as generalization.  Logistic regression is a model similar to linear regression, but based on a logistic function rather than a linear one. The figure below shows a visualisation of a logistic function.  Logistic regression, rather than predicting y based directly on a value of x, models a ‘logit value’ which is a ‘log of odds’ via linear regression.  Similarly to linear regression, model complexity forms a key consideration when training a model. We must avoid overfitting a model by providing randomised and varying training data without being too biased on a particular outcome while also avoiding underfitting a model by providing a broad spread of training data.  