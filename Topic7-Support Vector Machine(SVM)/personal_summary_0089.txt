 Linear regression is used to model a linear relationship between variables, in which a linear equation fits the data and a straight line can be graphed. It aims to find the best fitting line to describe the relationship between the dependent and independent variables. Linear regression is used widely and can be applied to problems relating to sales analysis, scientific research or any prediction based on historical data.  Linear classification deals with classification problems, and aims to fit a straight line boundary to classify data into different categories. Logistic regression can be used in these binary classification problems by mapping the linear regression output to a probability, which will aid the classification process. It uses a logistic function to do so, which maps the output to a probability value between 0 and 1.  Logistic regression models can be trained using MLE (maximum likelihood estimation) to predict the regression coefficient vector. The likelihood function, which is based on a Bernoulli distribution for binary outputs, aims to minimize the logistic loss function and establish the best coefficients for the model. Gradient descent can be used to iteratively find the best fit, and can be optimised by updating only one parameter per iteration in coordinate-wise gradient descent optimisation.  Underfitting and overfitting are two issues in machine learning models that occur when a model does not fit the data optimally. The model needs to capture enough information to be accurate and useful, while avoiding potentially unnecessary points. Bias variance decomposition is a way of predicting a model’s error by decomposing its bias, the model’s error due to simplifications or assumptions, and its variance, the variability of its predictions. The bias variance trade off refers to the balance between a model’s bias and variance errors. It help to find a model with low error overall, which can be determined with the sum of the bias and variance.  Regularised linear models work by adding a regulariser or penalty to the loss function during training. This makes the model preference simpler models by preventing unnecessary reliance on specific data points, therefore avoiding overfitting. L1 (Lasso) and L2 (Ridge) are the two most common types of regularisation for linear models. L1 uses the absolute values of the coefficients as the regulariser, while L2 adds the sum of squares of the coefficients.  Linear regression can also be used for feature selection, by assessing the relationship between features and their target. This can help to determine the most significant features in a dataset.  