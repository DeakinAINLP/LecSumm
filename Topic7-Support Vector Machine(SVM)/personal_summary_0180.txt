 This topic we learnt about Linear Regression, Logis5c Regression, Model Complexity and Regulariser.  Linear Regression  1.  Here we attempt to fit a linear equation between two variables in the observed data 2.  Hence we are looking for a relationship between input/feature and output/prediction 3.  The linear relationship can be measured by covariance  a. b. c.  If covariance > 0 , then x and y are positively correlated If covariance < 0 , then x and y are negatively correlated If covariance  = 0, then x and y are uncorrelated  4.  Pearson's correlation coefficient  a.  Has a value between -1 to +1 b.  The closer the value is to 1, the stronger the positive relationship c.  The closer the value is to -1, the stronger the negative relationship d.  The closer the value to 0, the weaker the relationship  5.  The difference between prediction and actual value is the error in a linear regression model 6.  Moore-Penrose pseudo-inverse of the matrix X is (ğ‘‹!ğ‘‹)"#ğ‘‹!ğ‘¦  Logistic Regression  1.  Logistic regression is the appropriate regression analysis when the output values of the  feature vectors are binary in nature If there are only 2 possible values for the output, then it is a binary classification problem If there are more than 2 values for the output, then it is a multi-class classification problem  2. 3. 4.  Linear classification means that we can draw a straight line between the two classes 5.  We need a non-linear function to project the values into a [0, 1] interval.  a. b. c.  If the output value is closer to 1, then we label them as 1 If the output value is closer to 0, then we label them as 0 If the output value is in the middle of 0 and 1, then it is not so clear which side it should be classified to  Generalisation  1.  prediction on unseen data. 2.  Performance measure can be -  a.  Mean Square Error or MSE b.  Mean Absolute Error or MAE c.  Explained Variance or ğ‘…$  Model complexity of Linear Regression  1.  Model complexity increases with the number of features 2. 3.  If we have a limited set of training data, we can risk over-fitting the model If we have a limited number of features, we can risk under-fitting the model  Logistic Regression Formulation  1.  A Sigmoid function can take any value and map it into a value in (0, 1). It never reaches 0 or  1  2.  The Sigmoid function models logit value or log of odds 3.  We estimate the probability by using the function the following function -         a.  b.  When ğ‘‹!ğ‘¤ > 0, P(y = 1|x) > 0.5, we decide in favour of class 1 c.  When ğ‘‹!ğ‘¤ < 0, P(y = 1|x) < 0.5, we decide in favour of class -1 d.  When ğ‘‹!ğ‘¤ = 0, P(y = 1|x) = 0.5, we cannot decide, i.e. both classes are possible  here  4.  Maximum Likelihood Estimate is given as -  a.  5.  Logistic Loss function is given as. -  a.  6.  Here the solution does not have a closed form, hence we need to solve the problem  iteratively. Here we need to perform Coordinate-wise Gradient Descent Optimisation  7.  Convex solutions have only 1 optimum solution of global minimum. Time efficiency of  convex optimisation is better than non-convex optimisation.  8.  Non-convex solutions can have multiple local minimum and 1 global minimum 9.  Iterative optimising -  a.  Gradient Descent - uses first derivative. Gradient Descent maximises a function  using knowledge of its derivative.  10. Coordinate-wise Gradient Descent Optimisation  a.  First randomly initialise w b.  Fix all the variables except for one. For each j, optimise wj by fixing w1, w2, â€¦wj-1,  wj+1, â€¦wd  c.  The minimise the objective function wrt wj using Gradient Descent -  i.  d.  Similarly continue for other wj's and continue till the objective function stops  changing.  e.  For convex functions, we can find the global minimum this way. However, for non-  convex functions, we cannot.  Model Complexity  1.  Over-fitting happens when we have a overly complex model 2.  Under-fitting happens when we have a very simple model 3.  Bias Variance Decomposition  a.  True relation for a dataset - ğ‘¦ = ğ‘“(ğ‘¥) + ğœ€	where Îµ is a measurement of noise in y  with mean 0 and variance ğœ2  b.  Hypothesis function to be fitted to the dataset D - hD(x) c.  Expected loss or risk is -  i.  ii.  Bias is - 1.          2.  This shows how accurate the model is 3.  Bias value needs to be minimised for a good model  iii.  Variance is -  1.  2.  This shows how complex the model is 3.  Variance value needs to be minimised for a good model  Regulariser  1.  This is an additional term in the loss function to avoid overfitting 2.  This does not allow the regression coefficients to take excessively large values. If a  regression coefficient takes a very high value, this means that the model is highly dependent on that particular feature 3.  Loss function updated as -  a.  4.  Regulariser options -  a.  This option encourages small weights(L1 norm or L1)  i.  ii.  This function implies the closed form function of a square  b.  This option penalises large weights(L2 norm or L2)  i.  ii. 5.  L1 regularisation or LASSO  This function implies the closed form function of a circle  a.  LASSO(Least Absolute Shrinkage and Selection Operator) is a regression analysis  method that performs both variable selection and regularisation  i.  6.  L2 regularisation or Ridge  a.  This is also known as Elastic Net  i.  7.  For high dimensional data(d > n), LASSO selects at most n variables before it saturates. However, Elastic Net can overcome this problem since it can select a greater number of variables despite the number of data points  8.  For Linear Regression, the optimisation equation is -  9.  For Logistic Regression, the optimisation equation is -   10. Regularisation increases the Bias for a model and at the same time reduces the variance 11. Only those dimensions that have non-zero weights participate in the prediction. Therefore, LASSO is also used to select predictive features among all dimensions. This is the feature selection property of LASSO.  