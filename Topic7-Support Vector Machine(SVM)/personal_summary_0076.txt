Summary and Reflection of Topic 6:  The labelled dataset is divided into two separate sets, training set (used for the learning process) and test set (assessing the learned model), usually in terms of accuracy.  Liner regression: Attempt to model a relationship between two variables by fitting a linear equation to the observed data.  Covariance measures the amount of information a specific x can provide for the output, y. If it is greater than 0, then the variables are positively correlated, inversely correlated, negatively correlated and finally if it is 0, then the two variables are independent.  Pearson’s  Correlation  Coefficient  –  another  measure  of  the  liner  correlation,  which  has  a  value between +1 and −1 where 1 indicates  total  positive  linear  correlation, 0 is  no  linear  correlation, and −1 shows total negative linear correlation  To find the best linear trend line you can use two methods:  1)  Find the line that minimizes the total absolute error (by effectively trying different combinations  as there is no analytical solution).  2)  Find the line that minimizes the total squared error (there is an analytical solution that proves that you can exactly calculate m and b that minimizes the squared error, hence this is the preferred method).  Logistic regression – appropriate regression analysis to conduct when the output values of the feature vectors are binary.  When there are only two possible values for output, we call the problem a binary classification problem. If there are more values, it’s a multi-class classification problem.  By linear classification, we mean that the separation boundary between any two classes is linear. This is the hypothesis.  Two approaches are available:    Ignore-nonlinearity by using the least squares for classification (can perform very badly when some points  in  the  training  data  have  excessively  large  or  small  values  for  the  dependent  variable compared to the rest of the training data).    Another approach is to use the conditional probability of the class as the output in the regression  problem (logistic regression).  The logistic function is an S-shaped curve as it can take any real-valued number and map it into a value between 0 and 1 but never exactly at those limits.  Logistic regression does not directly model in terms x. Instead, it models something called logit value or log of odds against via linear regression. So generally, we are modelling log of odds based on x.  Logistic regression is like a regression problem and the only difference is in modelling the output. In linear regression we are modelling the y directly but in here we are modelling the logit(log of odds).        Overfitting and complexity  More complex models (by increasing the polynomial order) will always fit the training data better, but they may ‘overfit’ the training data, learning complex relationships that are not really present.  Underfit = high bias  Overfit = high variance  Higher degree polynomials much more complex functions being fitted.  As we increase the degree of the polynomial, we are going to fit the training data better and better.  Useful diagram:  Cross validation being high – either the model is way too simple or way too complex  High basis case – both the cross validation and the training error are high. Cross validation either close or slightly higher than the training error.  Even in linear models, using all data dimensions as features may fit the model to true patterns (signal) but also to background noise.  A regulariser (tries to keep the parameters more normal or regular) is an additional term in the loss function to avoid overfitting.  It does not allow regression coefficients (or weights) to take excessively large values = issue as it implies that your model is highly dependent on that one feature.  We do not want huge weights (i.e. do not want to over-rely on any one feature). If weights are huge, a small change in a feature would result in a large change in the prediction.  Since we may even have irrelevant features, we want some of the weights to be zero so we can discard some features.   Two common methods of regularization methods are LASSO and Ridge.  Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.  Interesting fact about LASSO. Because L1 regularization shrinks the weights of noisy dimensions to zero, these dimensions do not participate in the prediction model. Only those dimensions that have non-zero weights participate in the prediction. Therefore, LASSO is also used to select predictive features among all dimensions. This is the feature selection property of LASSO.  Principal  component  analysis  (PCA),  correlation-based  feature  selection,  and  recursive  feature elimination are typical feature selection methods.  The specific challenge at hand and the features in the dataset determine the feature selection method to use. To find the most significant features in a dataset, linear regression can be used as a feature selection strategy. The basic idea behind using linear regression for feature selection is to evaluate the strength of the  relationship  between  each  feature  and  the  target  variable.  The  features  with  the  highest  absolute coefficient values can be found using linear regression  The  above  summary  has  been  created  using  the  full  course  material  for  Topic  4  (including  the relevant YouTube clips)        