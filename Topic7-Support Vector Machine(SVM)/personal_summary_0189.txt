Lesson Review Topic 6: Linear Model  By fitting a linear equation to the observed data, Linear Regression attempts to model the relationship between two variables, an equation that drafts a straight line. Covariance is used to measure the linear relationship between the variable x and output y, which means the amount of information a specific xi can provide for yi. The measure of the linear correlation between two variables x and y is Pearsonâ€™s Correlation Coefficient. It has a value between +1 and -1. A relationship between two or more variables which is depicted graphically by anything other than a straight line is a curvilinear relationship. These relationships are very variable, more complex and less easily identified than simple linear relationships.  When the output values of the feature vectors are binary, Logistic Regression is the appropriate regression analysis to be conducted. It is also a predictive analysis. Binary Classification problem is when there are only two possible values for output and its Multi-class Classification problem when more values. When separation boundary between any two classes is linear, its linear classification. To compute the Moore-Penrose inverse of matrix X, Python implementation uses Singular Value Decomposition (SVD). With the number of features, model complexity of linear model increases.  Logistic function (also called Sigmoid Function) was used in the naming of Logistic Regression. It is a S-shaped curve that can take any real valued number and map it into a value between 0 and 1 but never exactly at those limits. Logit is the log of odds. Using training data to estimate the regression coefficient vector w means training a logistic regression model. We can use MLE (Maximum Likelihood Estimation) to estimate w. This method maximises l(w) with respect to w. Maximising the log of the likelihood is the same as maximising likelihood because both provide the same solution for w. The usual approach for Logistic Loss Function is to take the derivative and equate it to zero to solve for w.  Convex optimisation deal with only one optimal solution, which is globally optimal whereas, non- convex optimisations may have multiple locally optimal points. Gradient Descent maximises a function using knowledge of its derivative. When we find an overlay complex model based on the data, over-fitting happens. The result of an extremely simple model is under-fitting. To avoid overfitting, a regulariser is an additional term in the loss function. It tries to keep the parameters more normal or regular.  Typical feature selection methods are Principal Component Analysis (PCA), correlation-based feature selection, and recursive feature elimination. Linear regression can be used to find the most significant features in a dataset. The basic idea is to evaluate the strength of the relationship between each feature and the target variable.  