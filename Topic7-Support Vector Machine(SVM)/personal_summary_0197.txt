The split training and test data is used for the purpose of learning and assessing the model separately. The training data set is used for the process of training and then the trained model is applied to the test dataset for assessing the accuracy of learned model. It is not recommended to use the same training dataset for the purpose of testing as that can result in biased output. The method of linear regression involves fitting a linear equation to the observed data to determine the relationship between two variables. Covariance measures how much data a feature can provide to the outcome. An indicator of the linear correlation between two variables is the Pearson’s correlation coefficient. The value of Pearson’s correlation coefficient is usually between 1 and -1. Stronger the Positive linear correlation when the value of Pearson’s coefficient is closer to positive one and weaker the negative linear correlation when the same value is closer to negative one. There is said to be no linear correlation when the coefficient value is equal to zero. For the formulation of linear regression, it is important  to  identify  the best-fitting  linear  relationship  between  the  input  features  and  the  target  variable. Defining a line with w as the slope of line and b as the intercept is done. But for high dimensional data this equation is altered accordingly. Once we have the line for any point say x it is easy to find the output function value. Equation when there are multiple features are involved is studied. It is crucial that this line fits the data points well. For this the error which is the difference between the predicted and actual output value is aimed to be kept minimum. Using the square loss, the Empirical risk is altered to keep minimum. This problem of minimisation can be solved by using a closed form function and finding the value of w for which the value of error is minimum. Logistic regression is used for binary classification, that is when there are two possible outcomes. For logistic regression where the data is classified into two classes, a decision function is used which projects the values of hypothesis function in the interval 0 to 1. The values whose value is closer to 0 is labelled as zero and if the values are closer to one then the final instance is labelled as one. Two methods of ignoring non- linearity  and  using  link  function  are  used  for  classification.  First  approach  is  to  use  least  squares  for classification and other approach is using conditional probability as output, that is if the probability value is greater than 0.5 then the label 1 is used and when the probability value is less than 0.5 then the label 0 is used. Singular Value Decomposition (SVD) is a closed-form solution used in linear regression to calculate a matrix's Moore-Penrose inverse. It can be applied to develop derived features, including non-linear ones, or to solve the issue using linear formulations. When features are expressive, linear regression can be effective. Model complexity of linear regression increases with the number of features, so it is important to be aware of it if there is a limited set of training data. This could lead to over-fitting and under-fitting. Generalisation is nothing but making predictions on unseen data. The performance of model can be measured by calculating various metrics like MSE, MAE, RMSE and other metrics. Logistic regression is like any other regression problems where  output  modelling  is  done  using  logit.  Like  any  other  regression  model,  training  model  is  used  for estimating the regression coefficient vector w using maximum likelihood estimation (MLE) which is obtained by multiplying the likelihood of each datapoints. Log of likelihood function that is obtained by taking log of function  is  maximised  to  obtain  maximised  likelihood  of  a  datapoint  since  both  values  are  equal.  The minimisation problem in logistic regression that is to obtain the minimum value for Logistic Loss function by taking  derivative  of  the  function  and  equating  the  function  to  zero  to  solve  for  the  value  of  w.  The unavailability  of  closed  form  leads  to  solving  the  problems  iteratively  using  Coordinate-wise-  Gradient Descent Optimisation. It can be said that a convex model is much time efficient with the availability of one optimal  point  when  compared  to  a  non-convex  optimisation  where  there  be  multiple  possible  solutions. Therefore, opting for a multiple step iterative way when there is no closed form is the efficient way. The two popular  iterative  optimisation  approaches  are  Gradient  Descent  and  Coordinate-wise  Gradient  Descent Optimisation. Gradient Descent optimisation operates by repeatedly updating the parameters in the direction of the function's inverse gradient. The algorithm estimates the gradient of the function with respect to each iteration and simultaneously changes each parameter and assuming that the function is convex and learning rate  is  adjusted  then  the  global  minimum  can  be  obtained.  Newtons  method  on  other  hand  uses  second derivative while Gradient Descent method uses first derivative. Using second derivative on one hand can be faster  but  on  other  hand  it  is  a  complicated  method  and  requires  lots  of  computations.  Coordinate-wise Gradient Descent is a variant of Gradient Descent where during each iteration one parameter and updates it  while the other parameters are fixed, and Gradient descent approach is applied to the function. Having little information or having too much information both are not good for a model’s performance. One can cause under-fitting while other can lead to over-fitting problem. Noise interferences can also be the cause of over- fitting problem. It is important to avoid these and fit data to the model perfectly for an accurate model. While fitting a hypothesis function to the given dataset the loss that is observed is called the risk or expected loss and this risk has three components which are bias, variance and noise. Bias helps identify how accurate the model obtained is, the smaller the bias value and the closer it is to zero the better the models performance. Variance assesses how well the estimated model can handle the changes that are done to dataset. The increase in  variance  leads  to  model  complexity  and  reduce  in  value  of  bias.  While  high  bias  and  low  variance  is observed with low complex models. The variance-bias trade-off can be displayed when the model is perfectly fit and the model has low variance and low bias, which gives us the best model while the opposite to this that is high variance and high bias gives worst performing model. Regularisation is the technique used to prevent the problem of over-fitting the model and to improve the generalization of model. In order to avoid these the approach that a regulariser follows is the minimise or eliminate any large weights that might influence the results and keep as many zero weights that is removal of unwanted features to improve the performance of model. In other words, the parameter values are altered to be kept simple and suitable for model evaluation resulting in better model performance. The commonly used regularization techniques are L1 Regularization (Lasso Regularization) and L2 Regularization (Ridge Regularization). L1 Regularization promotes sparsity in model  by  encouraging  more coefficients  to  have zero  weights  and  helps  in  feature  selection  by  removing pointless  features  from  the  model.  L2  Regularization  penalises  large  weights  and  helps  reduce multicollinearity and thus improve model stability. Finally feature selection which is important in selecting the  most  relevant  features  to  get  an  effective  model.  The  commonly  used  techniques  for  this  are  PCA, correlation-based feature selection and recursive feature elimination. Linear regression can be used for feature selection to identify the most significant features in dataset. The result of this gives relationship between each feature and target variable. Based on this the significance of each coefficient determined can be used to identify most relevant features. The above discussed approaches are practiced using python.  