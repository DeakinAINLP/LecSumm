Relevance and Covariance among features or variables Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data, in other words an equation that graphs as a straight line. In linear regression, training data is of the form of xi,yi,i=1,...,n{"version":"1.1","math":"\({\textbf{x}_i,y_i}, \quad i=1,...,n\)"}. For each data point (feature vector) xi{"version":"1.1","math":"\(\textbf{x}_i\)"}, there is an output yi{"version":"1.1","math":"\(y_i\)"}, which can be any real-valued number. We are looking for particular relationship between a feature and the output. We can measure the linear relationship between the variable x{"version":"1.1","math":"\(\textbf{x}\)"} (could be of many dimensions) and output y{"version":"1.1","math":"\(y\)"} (for now suppose y{"version":"1.1","math":"\(y\)"} is only one dimensional), using covariance. To put it simply, covariance measures the amount of information a specific xi{"version":"1.1","math":"\(\textbf{x}_i\)"} can provide for yi{"version":"1.1","math":"\(y_i\)"}. Cov(x,y){"version":"1.1","math":"\(Cov(\textbf{x},y)\)"}.  It is calculated as: Cov(x,y)=∑i=1n(xi−x¯)(yi−y¯)n−1{"version":"1.1","math":"\(Cov(\textbf{x},y) = \frac{\sum_{i=1}^{n} (\textbf{x}_i - \bar{x})(y_i - \bar{y})}{n-1}\)"} Which x¯{"version":"1.1","math":"\(\bar{\textbf{x}}\)"} shows the mean of xi{"version":"1.1","math":"\(\textbf{x}_i\)"} and y¯{"version":"1.1","math":"\(\bar{y}\)"} s the mean of yi{"version":"1.1","math":"\(y_i\)"}. Now we are faced with three possible values for Cov(x,y){"version":"1.1","math":"\(Cov(\textbf{x},y)\)"}: Cov(x,y)>0   →x{"version":"1.1","math":"\(Cov(\textbf{x},y) > 0 \ \ \ \rightarrow \quad \textbf{x}\)"}  and y{"version":"1.1","math":"\(y\)"} are positively correlated; if x{"version":"1.1","math":"\(\textbf{x}\)"} is increasing, y{"version":"1.1","math":"\(y\)"} is increasing. Cov(x,y)<0   →x{"version":"1.1","math":"\(Cov(\textbf{x},y) < 0 \ \ \ \rightarrow \quad \textbf{x}\)"}  and y{"version":"1.1","math":"\(y\)"} are inversely correlated; if  x{"version":"1.1","math":"\(\textbf{x}\)"} is increasing, y{"version":"1.1","math":"\(y\)"} is decreasing. Cov(x,y)=0   →x{"version":"1.1","math":"\(Cov(\textbf{x},y) = 0 \ \ \ \rightarrow \quad \textbf{x}\)"}  and y{"version":"1.1","math":"\(y\)"} are independent. Pearson’s Correlation Coefficient Another interesting measurement is Pearson’s Correlation Coefficient, which is a measure of the linear correlation between two variables, x{"version":"1.1","math":"\(x\)"} and y{"version":"1.1","math":"\(y\)"}.  It has a value between +1{"version":"1.1","math":"\(+1\)"} and −1{"version":"1.1","math":"\(-1\)"} where 1{"version":"1.1","math":"\(1\)"} indicates total positive linear correlation, 0{"version":"1.1","math":"\(0\)"} is no linear correlation, and −1{"version":"1.1","math":"\(-1\)"} shows total negative linear correlation. r=cov(x,y)var(x)var(y){"version":"1.1","math":"r = \frac{cov(x,y)}{\sqrt{var(x)var(y)}}"} To summarise, the most important features of this measurement: measures the linear relationship between two variables. Lets say y=x2{"version":"1.1","math":"\(y=\textbf{x}^2\)"} should we expect high values of Pearson’s Correlation Coefficient? No! Because they are not linearly related - it’s not a linear equation. Note: Pearson’s Correlation Coefficient is all about linear relationship. Ranges between −1{"version":"1.1","math":"\(-1\)"} to 1{"version":"1.1","math":"\(1\)"} The closer to 1{"version":"1.1","math":"\(1\)"}, the stronger the positive relationship The closer to 0{"version":"1.1","math":"\(0\)"}, the weaker the relationship The closer to −1{"version":"1.1","math":"\(-1\)"}, the stronger the negative relationship The following figure, illustrates different values of r{"version":"1.1","math":"\(r\)"}, for different correlations in data points. As you can see, r=0{"version":"1.1","math":"\(r = 0\)"} means weak relationship and r=1{"version":"1.1","math":"\(r = 1\)"} means the strong positive relationship while r=−1{"version":"1.1","math":"\(r = -1\)"} shows the strong negative relationship. Notice in the case where r=−0.6{"version":"1.1","math":"\(r = -0.6\)"},  we can discern a linear relationship among the points, but unlike the r=−1{"version":"1.1","math":"\(r = -1\)"} case, we are not sure how strong the relationship is.  Figure. Illustration of correlation in terms of r{"version":"1.1","math":"\(r\)"}. The next figure, shows a simple linear relationship vs. a curvilinear relationship. A curvilinear relationship is a relationship between two or more variables which is depicted graphically by anything other than a straight line. Curvilinear relationships are very variable, more complex and less easily identified than simple linear relationships. Figure. Illustration of linear relationship and curvilinear relationship. The next figure, depicts a strong relationship vs. a weak one. As you can see in strong relationships, the points are highly correlated in the direction of a line. So increasing x{"version":"1.1","math":"\(\textbf{x}\)"} in most cases will result in increasing y{"version":"1.1","math":"\(y\)"} too. Whereas in weak relationships, you can not see a strong relationship among variables. Figure. Strong relationship vs. weak relationship. And finally the last figure shows no relationship among data points. You can see that increasing x{"version":"1.1","math":"\(\textbf{x}\)"} results in no discernible patterns of differences in y{"version":"1.1","math":"\(y\)"}.  Logistic regression in python Logistic regression is much simpler. Don’t forget to clear the last Python program and restart the kernel so your old code doesn’t get mixed up with your new code. ‘Solver error’ note: Coders don’t necessarily commit rules to memory. Most coders will look up the documentation or do an internet search looking for solutions to problems. Different versions of code will use different syntax or parameters - that’s normal. Sometimes a new version of a language will ‘break’ old code. That’s when you need to resort to reading the free manual (rtfm) online. If you get a ‘solver’ error (or 500 solver errors) please read the scikitdocumentation on Logistic Regression. It’s always good practice to read the documentation if your code is throwing errors. In this case, go to the documentation and look up the ‘solver’ parameter. Read it carefully. The answer is there. Hints You can try different options and evaluate, e.g. using cross-validation, to see which option is good for a particular dataset. There is no clear answer but reading the manual may help. As parameters of the model are estimated via many iterations (runs) (not just one run like the linear regression), the parameters can’t be exactly the same for different runs, therefore, we can’t expect to get exactly the same result from different runs. Regularised Logistic Regression Download the train_wbcd.csv and test_wbcd.csv data to use in the code. Add it to your data store and rename it if need be. Let’s read in the data from our data file and preview it. Code example 1 from sklearn.model_selection import train_test_split import pandas as pd train_data=pd.read_csv("train_wbcd.csv") test_data=pd.read_csv("test_wbcd.csv") train_data.head() Outputs: We will  separate trainin and test datasets into feature and class/target: Code example 2 from sklearn import preprocessing le = preprocessing.LabelEncoder() train_data=train_data.dropna() test_data=test_data.dropna() # Encode “diagnosis” to numerical values train_data['Diagnosis'] = le.fit_transform(train_data['Diagnosis'].values)#[1 if each == 'M' else 0 for each in train_data['Diagnosis']] test_data['Diagnosis'] = le.fit_transform(test_data['Diagnosis'].values) X_train=train_data.iloc[:,2:] y_train=train_data.iloc[:,1] X_test=test_data.iloc[:,2:] y_test=test_data.iloc[:,1] print("Train: ",train_data.shape) print("Test: ",test_data.shape) Output: Train:  (98, 32) Test:  (19, 32) Regularisation using L1{"version":"1.1","math":"\(L_1\)"} and L2{"version":"1.1","math":"\(L_2\)"} You can perform regularised logistic regression by specifying two arguments in the function call: penalty: this takes values l1 for lasso and l2 for ridge regression C: this is the inverse of regularisation parameter alpha or lambda. smaller values specify stronger regularisation. You can refer to the documentation for more detailed information Let’s try with lambda = 0.1. So we have to set C = 1/0.1 Code example 4 from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score lambda_val = 0.1 #Initialize the Logitic regression model with l2 penalty lr = LogisticRegression(C=1/lambda_val, penalty='l2') lr.fit(X_train, y_train) y_predict = lr.predict(X_test) #Evaluate our model model_acc = accuracy_score(y_predict, y_test) print("Model Accuracy is: {}".format(model_acc)) print("Model Coeff: {}".format(np.append(lr.intercept_, lr.coef_))) Outputs : Model Accuracy is: 0.8421052631578947 Model Coeff: [-2.09591812e-01 -1.36143322e+00 -1.93491426e-01 -2.79837963e+00   1.95546835e-01  1.98797637e-02  1.35146931e-01  1.98252590e-01   8.30398315e-02  4.02839523e-02  6.79710848e-04 -5.90609905e-02  -5.65538428e-01  3.53512360e-01 -1.70845489e-01  1.67219624e-03   3.13233944e-02  4.93710303e-02  1.26797117e-02 -4.97858708e-05   2.74919020e-03 -1.62003805e+00  1.29460052e+00  1.59354847e+00  -1.84185366e-02  3.88048222e-02  3.95983998e-01  5.25059609e-01   1.65867274e-01  8.30782040e-02  2.55845262e-02] You can change the penalty to ‘l1’ for lasso regularisation. Now as an exercise let’s do the following. For a list of l1 and l2 penalty scores, lets calculate the average accuracy over 500 runs of L1 and L2 regularised Logistic regression. Let’s begin by defining a function that takes in data and penalty types and values. For a fixed number of runs (‘trials’), the data is randomly split 70/30 as train/test. The average test accuracy is calculated. Code example 5 def runLRmodel(trials, data, label, penalty_type, penalty_score):    model_acc     = 0    model_weights = np.zeros([1,31])    for i in range(0,trials):       Dtrain, Dtest = train_test_split(train_data, test_size=0.3)       lr = LogisticRegression(C=1/penalty_score, penalty=penalty_type, solver='liblinear')       lr.fit(Dtrain.iloc[:,2:], Dtrain[label])       y_predict = lr.predict(Dtest.iloc[:,2:])       model_acc += accuracy_score(y_predict, Dtest[label])       model_weights += np.append(lr.intercept_, lr.coef_)    model_acc /= trials    model_weights /= trials    return np.round(model_acc, decimals=2), np.round(model_weights,decimals=2) Lets now try to find the best lambda value from 500 random splits of our data. Code example 6 lambda_vals = [.0001,.0003,.001,.003,.01,.03,.1,.3,1,3,5,10] l2_acc = np.zeros(len(lambda_vals)) index = 0 #L2 regularization for l in lambda_vals:    l2_acc[index], w = runLRmodel(500,train_data, 'Diagnosis', 'l2', np.float(l))    index += 1 print("Acc: {}".format(l2_acc)) # penalty at which validation accuracy is maximum max_index_l2  = np.argmax(l2_acc) best_lambda = lambda_vals[max_index_l2] print("Best Lambda: {}".format(best_lambda)) Outputs : Acc: [0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.95 0.94 0.94 0.93 0.93] Best Lambda: 0.0001 Code example 7 Now find the best alpha value from 500 random splits of our data. alpha_vals = [.0001,.0003,.001,.003,.01,.03,.1,.3,1,3,5,10] l1_acc = np.zeros(len(alpha_vals)) index = 0 #L2 regularization for l in alpha_vals:    l1_acc[index], w = runLRmodel(500,train_data, 'Diagnosis', 'l1', np.float(l))    index += 1 print("Acc: {}".format(l1_acc)) # penalty at which validation accuracy is maximum max_index_l1  = np.argmax(l1_acc) best_alpha = alpha_vals[max_index_l1] print("Best Alpha: {}".format(best_alpha)) Outputs : Acc: [0.95 0.95 0.96 0.96 0.96 0.95 0.96 0.96 0.94 0.92 0.92 0.91] Best Alpha: 0.001 Code example 8 We now plot the average model accuracy with respect to the parameters alpha and lambda. Also, we mark the best values of alpha and lambda. #plot the accuracy curve plt.plot(range(0,len(lambda_vals)), l2_acc, color='b', label='L2') plt.plot(range(0,len(lambda_vals)), l1_acc, color='r', label='L1') #replace the x-axis labels with penalty values plt.xticks(range(0,len(lambda_vals)), lambda_vals, rotation='vertical') #Highlight the best values of alpha and lambda plt.plot((max_index_l2, max_index_l2), (0, l2_acc[max_index_l2]), ls='dotted', color='b') plt.plot((max_index_l1, max_index_l1), (0, l1_acc[max_index_l1]), ls='dotted', color='r') #Set the y-axis from 0 to 1.0 axes = plt.gca() axes.set_ylim([0, 1.0]) plt.legend(loc="lower left") plt.show() Model complexity Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model. We have already encountered over-fitting and under-fitting in previous lessons. The figure below illustrates some of these concepts. Over-fitting will happen when your model starts to capture some irrelevant noise points in the data while building the model, rather than the whole pattern (right image on the figure). Under-fitting is the result of an extremely simple model (left image on the figure). Figure. Over-fitting and Under-fitting. Under-fitting occurs if the complexity of the model is lower than necessary. Scenario-1: We may be using a linear model, when the data requires a nonlinear model. Scenario-2: We may be using the right hypothesis (linear or nonlinear) but the number of variables might be falling short of what is required. For example, to predict the income of a person, age alone may not be sufficient. We can detect under-fitting by checking if the model fitting error on the training data is high. Example 1: To predict a person’s income, knowing age alone is not sufficient. Assuming our dataset has information about age, sex, education; we could add them as explaining variables. Our model becomes more interesting and more complex. The new model explains the data better but is still not good enough. We need to add even more variables (i.e. location, profession of parents, social background, number of children, weight, preferred colour, best meal, last holiday destination). Our model will be even better but will probably be over-fitting now. It will probably produce poor predictions on unseen data. It has learnt too many specifics of the training data and will probably have learnt the unhelpful background noise. Example 2 (Overfitting): Let’s say you attend a symphony and want to record the clearest sound possible. You buy a super-sensitive microphone and audio system to pick up all the sounds in the auditorium. Now, you have started to over-fit. You are detecting unhelpful, undesirable noise such as: you hear your neighbours shuffling in their seats the musicians turning their pages even the swishing of the conductor’s coat jacket. So fitting a perfect model is only listening to the Symphony (signal) and not to the background noise. Bias Variance Decomposition Let us assume our data (x,y){"version":"1.1","math":"\((x,y)\) "} has the true relation y=f(x)+ϵ{"version":"1.1","math":"\(y=f(x) + \epsilon\)"}, where ϵ{"version":"1.1","math":"\(\epsilon\)"} is a measurement of noise in y{"version":"1.1","math":"\(y\)"} with mean zero and variance σϵ2{"version":"1.1","math":"\(\sigma_{\epsilon}^2\)"}. Also assume that we are fitting an hypothesis function (or model) hD(x){"version":"1.1","math":"\(h_D(x)\)"} using dataset D{"version":"1.1","math":"\(D\)"}.  The expected loss (or risk) has three components.  Risk={ED[hD(x)−f(x)]}2+ED[{hD(x)−ED[hD(x)]}2]+σϵ2{"version":"1.1","math":"\\ Risk = \{E_{D}[h_{D}(x) - f(x)]\}^2 + E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2] + \sigma_{\epsilon}^2 \\"} where: Risk={ED[hD(x)−f(x)]}2+ED[{hD(x)−ED[hD(x)]}2]+σϵ2{"version":"1.1","math":"\(\\ Risk = \{E_{D}[h_{D}(x) - f(x)]\}^2 + E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2] + \sigma_{\epsilon}^2 \\\)"} is the (bias)2{"version":"1.1","math":"\(^2\)"} ED[{hD(x)−ED[hD(x)]}2]{"version":"1.1","math":"\(E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2]\)"} is the variance  σϵ2{"version":"1.1","math":"\(\sigma_{\epsilon}^2\)"} is the noise (Irreducible error). Let’s see more details about these separate parts:  (bias)2{"version":"1.1","math":"\(^2\)"}:  {ED[hD(x)−f(x)]}2{"version":"1.1","math":"\(\{E_{D}[h_{D}(x) - f(x)]\}^2\)"} This term shows how accurate your hypothesis function is (in other words how accurate your designed model is: hd(x){"version":"1.1","math":"\(h_d(x)\)"}. The E{"version":"1.1","math":"\(E\)"} (expectation) means you need to average out this error to find out the expectation of error regarding this hypothesis (hD(x){"version":"1.1","math":"\(h_{D}(x)\)"}) and the true function output (f(x){"version":"1.1","math":"\(f(x)\)"}) As long as you are building an accurate model with a low error rate, (bias)2{"version":"1.1","math":"\(^2\)"} is a small value and possibly close to 0{"version":"1.1","math":"\(0\)"}. variance: ED[{hD(x)−ED[hD(x)]}2]{"version":"1.1","math":"\(E_{D}[\{h_{D}(x) - E_D[h_D({x})]\}^2]\)"} You will notice that this term does not have f(x){"version":"1.1","math":"\(f(x)\)"} inside it. That means it solely relies on your model which is hD(x){"version":"1.1","math":"\(h_D(x)\)"} To put it simply, this model measures the tolerance of your calculated model while changing just the data set D{"version":"1.1","math":"\(D\)"}.  If it varies too much that’s a problem! The E{"version":"1.1","math":"\(E\)"} or the expectation of this term measures the complexity of your model. The higher the variance the more complex the model. Hence, you can see that increasing the variance of a model means lowering bias as the model becomes more complex. On the other hand you can see that the low complexity for a model will result in high bias and low variance. So higher bias results in lower variance and high variance results in lower bias. This illustrates another trade-off problem in machine learning. Basically, we can only minimise bias and variance since you should not be worried about noise σϵ2{"version":"1.1","math":"\(\sigma_{\epsilon}^2\)"} for now. Noise is related to your observations from the function. Variance bias trade off To better illustrate the variance-bias trade-off examine the following figure. As you can see, the best model is a model with low variance and low bias. It means the model is not too complex but is properly accurate. The worst model would have high bias, which means it’s not accurate based on the training data, and high variance which means it’s far too complex. Figure. Bias Variance Trade-off As another example lets consider these two situations: Models with too few parameters are inaccurate because of a large bias (not enough flexibility): under-fitting. Figure. Under-fitting with a large bias. Models with too many parameters are inaccurate because of a large variance (too much sensitivity to the sample): over-fitting. Figure. Over-fitting with large variance Summary Based on the above information on the bias-variance trade-off, we know that: Low bias implies high variance, and high bias implies low variance We need to find the sweet spot where Risk = bias2{"version":"1.1","math":"\(^2\)"} + variance + noise is the minimum. The minimum error is at the right model complexity. Another interesting question arises here is when using linear models, can we still over-fit? It depends! Depends on our model complexity. In linear models, the model complexity grows with the number of features. Using all data dimensions as features may fit the model on background noise as well as true patterns (signal). In the next section we are going to introduce Regularisation as a technique used to control the model complexity. Activity Check out this video from Stanford University on diagnosing bias and variance: Linear Regression in Python In this practical, you will apply regularised linear and logistic regression models to datasets. The effects of L1{"version":"1.1","math":"\(L_1\)"} and L2{"version":"1.1","math":"\(L_2\)"} on polynomial regression We’ll start by doing polynomial regression. Let’s start by setting up the environment. Code example 1 import numpy as np import pandas as pd import matplotlib.pyplot as plt Download the poly_data.csv data file used in the code. Add it to your data store and rename it if needed. Let’s read in the data from our data file and preview it. data = pd.read_csv('data/poly_data.csv') rows, cols = data.shape print("Data has {} rows with  {} columns".format(rows, cols)) data.head() Output: Data has 60 rows with  2 columns Which displays data such as: y x 0 1.065763 1.047198 1 1.006086 1.117011 2 0.695374 1.186824 3 0.949799 1.256637 4 1.063496 1.326450 Code example 2 Let’s plot it to see how it looks like. # Separate the data into features and response. predictors = ['x'] response = ['y'] # Visualize the data plt.plot(data[predictors], data[response], '.') plt.xlabel('X') plt.ylabel('Y') plt.title('Scatter plot of our data') plt.show() Outputs : Text(0.5,1,'Scatter plot of our data') Figure. A scatter plot of our data Now that we have visualised the data, the next step is to fit a model. At this point, you should be able to look at this data and answer the following questions: is there a correlation between x{"version":"1.1","math":"\(x\)"} and y{"version":"1.1","math":"\(y\)"}? is the correlation linear or non-linear? In this case, we have a non-linear correlation between  x{"version":"1.1","math":"\(x\)"} and y{"version":"1.1","math":"\(y\)"}. The data looks like a sine curve. To keep things very simple, we start with a normal linear regression model. Code example 3 # Lets fit a linear regression model to this data from sklearn.linear_model import LinearRegression lr1 = LinearRegression() lr1.fit(data[predictors], data[response]) y_pred = lr1.predict(data[predictors]) #Evaluate our model with mean square error mse1 = np.mean((y_pred - data[response])**2) print("Model MSE: {}".format(mse1[0])) plt.plot(data['x'], data['y'], '.', data['x'], y_pred, '-') plt.show() Outputs : Model MSE: 0.0546719266491 [<matplotlib.lines.Line2D at 0x7f1d9aa72390>, <matplotlib.lines.Line2D at 0x7f1d9aa72890>] Figure. Our scatter plot with linear regression applied The blue dots are our data, the yellow line is the fitted model with simple linear regression. Is this model over-fitting or under-fitting? Linear regression with polynomial features (polynomial regression) For linear regression, our hypothesis function was in the form h(x)=wx+b{"version":"1.1","math":"\(h(x)=wx+b\)"}.  We have to use polynomial regression to get a better fit. For this, we have to generate extra features as powers of feature x{"version":"1.1","math":"\(x\)"}. So our final model will be of the form h(x)=wnxn+wn−1xn−1+⋯+w2x2+wx+b{"version":"1.1","math":"\(h(x)=w_{n}x^{n}+w_{n-1}x^{n-1}+\cdots+w_{2}x^{2}+wx+b\)"} We can use the same Linear Regression from Scikit learn for this, but we now have to generate the features as powers of x{"version":"1.1","math":"\(x\)"}.  Code example 4: Lets generate a model with n=5{"version":"1.1","math":"\(n=5\)"} (features until x5{"version":"1.1","math":"\(x^5\)"})  #We are going to add columns to our exiting data frame #To generate a name starting with a character and ending with a number, lets try this: print("x_%d"%5) Outputs: x_5 Here, “%d” (as found in coding languages C, C++, Matlab) stands for digit, and is replaced by the value following % after the quotes. We can now generate column names and columns in our dataframe as: for i in range(2,6):    colname = "x_%d"%i    data[colname] = data.x**i data.head() Which displays data such as: y x x_2 x_3 x_4 x_5 0 1.065763 1.047198 1.096623 1.148381 1.202581 1.259340 1 1.006086 1.117011 1.247713 1.393709 1.556788 1.738948 2 0.695374 1.186824 1.408551 1.671702 1.984016 2.354677 3 0.949799 1.256637 1.579137 1.984402 2.493673 3.133642 4 1.063496 1.326450 1.759470 2.333850 3.095735 4.106339 This matrix is the extended version of the original data, suitable for performing polynomial regression. Remember all we did was take numbers in x{"version":"1.1","math":"\(x\)"}to power 2,3,4{"version":"1.1","math":"\(2,3,4\)"} and 5{"version":"1.1","math":"\(5\)"}.  to generate the new features. Code example 5 Let’s fit the model and check the fit. predictors = data.columns.values[1:] lr2 = LinearRegression() lr2.fit(data[predictors], data[response]) y_pred2 = lr2.predict(data[predictors]) #Evaluate our model with mean square error mse2 = np.mean((y_pred2 - data[response])**2) print("Model MSE: {}".format(mse2[0])) plt.plot(data['x'], data['y'], '.', data['x'], y_pred2, '-') plt.show() Outputs : Model MSE: 0.0169762265929 Figure. Linear regression with polynomial features You can also print the coefficients in  h(x)=w5x5+w4x4+⋯+w2x2+wx+b{"version":"1.1","math":"\(h(x)=w_{5}x^{5}+w_{4}x^{4}+\cdots+w_{2}x^{2}+wx+b\)"}  by calling the attribute coef: print(lr2.coef_) Outputs : [[-5.11776235  4.72461232 -1.92856217  0.33473526 -0.02065326]] Code example 6: Lets generate a model with features until x15{"version":"1.1","math":"\(x^{15}\)"}  for i in range(2,16):    colname = "x_%d"%i    data[colname] = data.x**i data.head() Fit the model and check the fit: predictors = data.columns.values[1:] lr3 = LinearRegression() lr3.fit(data[predictors], data[response]) y_pred3 = lr3.predict(data[predictors]) #Evaluate our model with mean square error mse3 = np.mean((y_pred3 - data[response])**2) print("Model MSE: {}".format(mse3)) plt.plot(data['x'], data['y'], '.', data['x'], y_pred3, '-') plt.show() Outputs : Model MSE: y    0.014352 dtype: float64 [<matplotlib.lines.Line2D at 0x7f1e1503d0d0>, <matplotlib.lines.Line2D at 0x7f1e1503d510>] Figure. Model with more features Lets see the coefficients again: print(lr3.coef_) Outputs : [[ 9.73644025e+02 -1.10259891e+03 -4.82636085e+01  1.01428676e+03   -4.54613050e+02 -7.08990734e+02  1.12914080e+03 -7.95836619e+02    3.49506331e+02 -1.03969293e+02  2.14209993e+01 -3.02509204e+00    2.80268374e-01 -1.53749631e-02  3.79005355e-04]] Is this model under-fitting or over-fitting? Lets check the MSE of each of our models. Lets also check the value of our the coefficients: The MSE of each of the 3 models is: Code example 7 print("MSE Simple LR: {}".format(mse1[0])) print("MSE Polynomial LR with power = 5:  {}".format(mse2[0])) print("MSE Polynomial LR with power = 15: {}".format(mse3[0])) Outputs : MSE Simple LR: 0.05467192664907909 MSE Polynomial LR with power = 5:  0.01697622659290215 MSE Polynomial LR with power = 15: 0.014351902278257937 print("Coefficients Simple LR: {}".format(lr1.coef_)) print() print("Coefficients Polynomial LR with power = 5:  {}".format(lr2.coef_)) print() print("Coefficients Polynomial LR with power = 15: {}".format(lr3.coef_)) Outputs : Coefficients Simple LR: [[-0.61957457]] Coefficients Polynomial LR with power = 5:  [[-5.11776235  4.72461232 -1.92856217  0.33473526 -0.02065326]] Coefficients Polynomial LR with power = 15: [[ 9.73644025e+02 -1.10259891e+03 -4.82636085e+01  1.01428676e+03   -4.54613050e+02 -7.08990734e+02  1.12914080e+03 -7.95836619e+02    3.49506331e+02 -1.03969293e+02  2.14209993e+01 -3.02509204e+00    2.80268374e-01 -1.53749631e-02  3.79005355e-04]] Observations: MSE decreases with increasing model complexity Size of coefficients, in general, increasing model complexity What does a large coefficient signify? It means that we’re putting a lot of emphasis on that feature, i.e. the particular feature is a good predictor for the outcome. L2{"version":"1.1","math":"\(L_2\)"} Regularised linear regression As you know, a regulariser prevents over-fitting by restricting the feature weights from taking very large values. L2{"version":"1.1","math":"\(L_2\)"} regularised regression modifies the objective function as: Objective_fn = Regression_Loss_Function + lambda * (sum of square of coefficients) So, when lambda = 0, the objective becomes the same as the simple linear regression. When lambda = infinity, coefficients will be zero. Why? Because of infinite weightage on square of coefficients. When 0 < lambda < infinity, the magnitude of lambda will decide what weights to be given to each feature.  L2{"version":"1.1","math":"\(L_2\)"} regularisation is also called Ridge regression. The Scikit-learn documentation provides more information. Note: In general literature, the L2{"version":"1.1","math":"\(L_2\)"} penalty is called lambda, but in Ridge regression, the parameter is called alpha. Code example 8 from sklearn.linear_model import Ridge #call the ridge regression model with penalty (lambda) = 0.003 ridgelr = Ridge(alpha=0.003) # alpha in the function represents lambda #Fit our data ridgelr.fit(data[predictors], data[response]) # Do a prediction y_pred4 = ridgelr.predict(data[predictors]) #Evaluate our model with mean square error mse4 = np.mean((y_pred4 - data[response])**2) plt.plot(data['x'], data['y'], '.', data['x'], y_pred4, '-') plt.show() print() print("Model MSE: {}".format(mse4[0])) print() print("Model Coeff: {}".format(ridgelr.coef_)) Outputs : Model MSE: 0.015319161587829666 Model Coeff: [[-9.38462028e-01 -7.82157736e-01  1.16538733e-01  8.73012725e-01    5.44327890e-01 -7.04789892e-01 -8.25282409e-01  1.34943344e+00   -7.59190225e-01  2.18233625e-01 -3.04411100e-02  3.46090967e-04    5.08771211e-04 -6.66996145e-05  2.80697549e-06]] Figure. With Ridge regulariser: lambda = 0.003 Now let us check the shape of y_pred4 y_pred4.shape Outputs : (60, 1) Your turn Try changing the penalty (lambda value in our discussions, but “alpha=” in scikit-learn) as the following cases: .001,.003,.01,.03,1,3,5,10,30 As the penalty increases, the weights of coefficients should decrease. The model starts under-fitting when training error (MSE) starts increasing sufficiently, when penalty > 1 in this case. L1{"version":"1.1","math":"\(L_1\)"} Regularised Linear Regression L1{"version":"1.1","math":"\(L_1\)"} regularisation or LASSO is another regulariser which is of the form: Objective_fn = Regression_Loss_Function + alpha * (sum of absolute value of coefficients) L1{"version":"1.1","math":"\(L_1\)"} regularisation works similar toL2{"version":"1.1","math":"\(L_2\)"} but here it tries to make the weights of unimportant features to be zero. Hence this is also a form of feature selection. As with the previous example, we will import Lasso from scikit-learn. The Scikitlearn documentation provides more details. We will run a L1{"version":"1.1","math":"\(L_1\)"} regularisation initially with penalty = 0.01. To be safer, we also specify 105{"version":"1.1","math":"\(10^5\)"} iterations for the Lasso algorithm to ensure it converges. Code example 9 from sklearn.linear_model import Lasso #call the lasso regression model with penalty (alpha) = 0.01 # we also specify the max number of iterations as 10^5 lassoreg = Lasso(alpha=0.01, max_iter=100000) # alpha in the function represents lambda  #Fit our data lassoreg.fit(data[predictors], data[response]) # Do a prediction y_pred5 = lassoreg.predict(data[predictors]) #Evaluate our model with mean square error mse5 = np.mean((y_pred5 - data['y'])**2) plt.plot(data['x'], data['y'], '.', data['x'], y_pred5, '-') plt.show() print() print ("Model MSE: {}".format(mse5)) print() print("Model Coeff: {}".format(lassoreg.coef_)) Outputs : Model MSE: 0.016489981168475613 Model Coeff: [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -4.17937751e-03  -3.75600052e-03  2.59803210e-04  8.48182829e-05  8.73340926e-06   6.72863048e-07  2.49206509e-08 -3.96051840e-09 -1.49720339e-09  -4.29890443e-10 -1.32609247e-10 -4.17620418e-11] Figure. With Lasso: lambda = 0.01 You will immediately notice that the model is under-fitting. A Lasso penalty of .01 is too high, there are only 2 non-zero coefficients. This is a simplified version of L1{"version":"1.1","math":"\(L_1\) "} and L2{"version":"1.1","math":"\(L_2\)"} regularisation in Linear regression. For further reading and code, you can refer to this tutorial. Let’s look at a simple example of linear regression. Five randomly selected students took a math aptitude test before they began their statistics course. The Statistics Department wants to know: What linear regression equation best predicts statistics performance, based on math aptitude scores? The following figure illustrates the data. Xi{"version":"1.1","math":"\(X_i\)"} is the math aptitude scores and yi{"version":"1.1","math":"\(y_i\)"} is the statistics performance. Figure. Illustration of the data. For solving this problem, first we need to create a dummy feature which contains 1{"version":"1.1","math":"\(1\)"} and append it the X{"version":"1.1","math":"\(X\)"}. So we have: X=[195185180170160]{"version":"1.1","math":"X = \begin{bmatrix} 1 & 95 \\ 1 & 85 \\ 1 & 80 \\ 1 & 70 \\ 1 & 60 \\ \end{bmatrix} %"} Y=[8595706570]{"version":"1.1","math":"Y = \begin{bmatrix} 85 \\ 95 \\ 70 \\ 65 \\ 70 \\ \end{bmatrix}"} Based on the minimisation of error function, we know thatw{"version":"1.1","math":"\(\textbf{w}\)"} should be w=(XTX)−1XTy{"version":"1.1","math":"\(\textbf{w} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}\)"} in order to have the minimum mean square error. So we can find the values of w{"version":"1.1","math":"\(\textbf{w}\)"} as: w=(XTX)−1XTy=Y=[w0w1]=[26.7810.644]{"version":"1.1","math":"\(\textbf{w} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y} = Y = \begin{bmatrix} w_0 \\ w_1 \\ \end{bmatrix} = \begin{bmatrix} 26.781 \\ 0.644 \\ \end{bmatrix}\)"} Now that we have the w{"version":"1.1","math":"\(\textbf{w}\)"},  how can we actually use it? Let’s make a prediction based on the trained model. Prediction:If a student made an 80{"version":"1.1","math":"\(80\)"} on the aptitude test, what grade would we expect her to make in statistics? Answer:. Predicted Statistics Grade is computed as Statistics grade: [1   80][w0w1]=1×26.781+80×0.644=78.288{"version":"1.1","math":"\([1\ \ \ 80]\begin{bmatrix}w_0\\w_1\end{bmatrix} = 1 \times 26.781 + 80 \times 0.644 = 78.288\)"}  Therefore based on the trained model we expect the statistics grade of this student to be 78.288{"version":"1.1","math":"\(78.288\)"}. View transcript SPEAKER: In this tutorial, we're going to show you a real example of linear regression. So consider this problem. Our training data is five randomly selected students took a math aptitude test before they began their statistic course. The statistics department wants to know a linear regression equation which best predicts the statistic performance based on the math aptitude score. So what we need here is we want to map this x-i, which is the math aptitude score, to the y-i, which is the score of the statistics performance. In order to find this linear model, which maps x-i to y-i, first we need to construct the matrix x and y. As the first step, if you remember, we need to put in the value of the x into the matrix x, also add a column of dummy feature 1, also we do the same for the y-i and put this in the matrix or vector of y. Now as you remember for finding the weights in the linear regression, it was a pretty simple formula-- x transpose x, inverse, x transpose y. So all we need to do is to find this value, which is a two by one vector. As you see, if you calculate value it would be 26.781 and 0.644. This is basically the weights of the linear model we found. But what if a new student cames in and she has a score of 80 on the math aptitude test? So what should we expect for her score in the statistics? As we already have the value of the weights, all we need to do is to take the score and the math aptitude, which right now here is we assume it is 80. Also, we add this one dummy feature to that and then we multiply that to the vector of the weights. As you can see, it is 1 multiplied by 26.781 plus 80 multiplied by 0.644. And our predicted result is 78.288. Template for all occasions Logistic regression example An example of Logistic Regression Suppose that we are working with doctors, who are investigating patients who have suffered a heart attack. The dependent variable is whether the patient has had a second heart attack within 1{"version":"1.1","math":"\(1\)"} year (yes =1{"version":"1.1","math":"\(=1\)"}). We have two independent variables (IV), one is whether the patient completed a treatment consistent of anger control practices (yes =1{"version":"1.1","math":"\(=1\)"}). The other IV is a score on a trait anxiety scale (a higher score means more anxious). Person  2nd HeartAttack Treatmentof Anger TraitAnxiety 1 1 1 70 2 1 1 80 3 1 1 50 4 1 0 60 5 1 0 40 6 1 0 65 7 1 0 75 8 1 0 80 9 1 0 70 10 1 0 60 11 0 1 65 12 0 1 50 13 0 1 45 14 0 1 35 15 0 1 40 16 0 1 50 17 0 0 55 18 0 0 45 19 0 0 50 20 0 0 60 After running logistic regression, we get: w0=−6.36{"version":"1.1","math":"\(w_0=-6.36\)"} (bias term) w1=−1.02{"version":"1.1","math":"\(w_1=-1.02\)"} (effect of anger treatment) w2=0.12{"version":"1.1","math":"\(w_2= 0.12\)"} (effect of anxiety trait) Now let us see what is the prediction rule for class 1{"version":"1.1","math":"\(1\)"}: −6.36−1.02×(treatment of anger)+0.12×(trait of anxiety)>0{"version":"1.1","math":"-6.36-1.02×(treatment\ of\ anger) + 0.12\times (trait\ of \ anxiety )>0"} What do we understand from these weights: When both independent variables are zero, we classify no 2nd Heart attack (Do you know why?)Because −6.36−1.02×(treatment of anger=0)+0.12×(trait of anxiety=0)=−6.36<0{"version":"1.1","math":"\(-6.36-1.02×(treatment\ of\ anger = 0) +0.12\times (trait\ of \ anxiety = 0 ) \\ = -6.36 < 0 %\)"}  The Anger treatment seems to be lowering (negative effect) the risk score of a 2nd Heart attack. The Anxiety trait seems to be increasing (positive effect) the risk score of a 2nd Heart attack. Logistic regression is the appropriate regression analysis to conduct when the output values of the feature vectors are binary. Like all regression analyses, the logistic regression is a predictive analysis. Before talking about logistic regression, lets first review linear classification. Reviewing linear classification Consider a set of training data of the form: {xi,yi}i=1,...,n{"version":"1.1","math":"\(\{\textbf{x}_i,y_i\} \quad i=1,...,n\)"}. For each data point xi{"version":"1.1","math":"\(\textbf{x}_i\)"}, there is a output yi{"version":"1.1","math":"\(y_i\)"}, which can take discrete values eg: 0/1,−1/+1,or{1,2,…,K}{"version":"1.1","math":"\(0/1, -1/+1, or \{1,2,…,K\}\)"}. When there are only two possible values for output, we call the problem a binary classification problem. If there are more values it’s a multi-class classification problem. Example 1: binary Given an image, the task maybe to classify if it is an image of a fruit or not fruit. Example 2: multi-class Given an image of a fruit, the task may be to classify whether it is an orange, an apple or a banana. But what do we mean by linear classification? By linear classification, we mean that the separation boundary between any two classes is linear. This is just our hypothesis. It may not be true! First, we start with a linear assumption, later we will derive more complicated scenarios. Figure. The green line indicates the linear boundary for classification of two classes. If you think in terms of regression in which y{"version":"1.1","math":"\(y\)"}'s are continuous different numbers, classification is intrinsically non-linear. Why? Because two instances xi{"version":"1.1","math":"\(\textbf{x}_i\)"} and xj{"version":"1.1","math":"\(\textbf{x}_j\)"} and their outputs may be different but may belong to the same class. Take the red bubbles in the above figure as an example. You can see they are quite different points but they have the same class label because they are under the line. They are in the same class but they are not linear with respect to each other. So how can we handle this non-linear ambiguity in forms of regression? Lets assume it is just a regression case. Then we handle the non-linearityas h(x)=xTw{"version":"1.1","math":"\(h(\textbf{x}) = \textbf{x}^T\textbf{w}\)"}. So basically we project x{"version":"1.1","math":"\(\textbf{x}\)"} to the new line h(x){"version":"1.1","math":"\(h(\textbf{x})\)"}. Logistic regression But how can we make a classification decision? It seems that we need another function such as a decision function σ(h(x)){"version":"1.1","math":"\(\sigma(h(\textbf{x}))\)"} which uses a fixed non-linear link function and projects the value of h(x){"version":"1.1","math":"\(h(\textbf{x})\)"} into [0,1]{"version":"1.1","math":"\([0,1]\)"} interval. So if the final score is close to 1{"version":"1.1","math":"\(1\)"}, we would label that instance label 1{"version":"1.1","math":"\(1\)"}. If it was closer to 0{"version":"1.1","math":"\(0\)"}, we call it label 0{"version":"1.1","math":"\(0\)"} and if it is close to the middle of 0{"version":"1.1","math":"\(0\)"} and 1{"version":"1.1","math":"\(1\)"} it is much closer to the decision boundary and it is not so clear which side it should be classified to. This was the really simple illustration of Logistic Regression approach. To conclude:  x→ h(x)→σ(h(x)){"version":"1.1","math":"\(\textbf{x} \rightarrow \ h(\textbf{x})\rightarrow \sigma(h(\textbf{x}))\)"} So two approaches are generally available: Ignore non-linearity:Using least squares for classification: treat binary outputs like the outputs in the regression problem. This may not be the best method but it is easy! Using link function:Another approach is to use the conditional probability of the class as the output in the regression problem, i.e. fitting regression on P(y=1|x){"version":"1.1","math":"\(P(y=1\vert\textbf{x})\)"} instead of y{"version":"1.1","math":"\(y\)"}.  In other words using a link function to transform the output to the classification scenario. In here, in a simple case, if P(y=1|x)>0.5{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) > 0.5\)"},  we may want to choose class label 1{"version":"1.1","math":"\(1\)"} for the data point x{"version":"1.1","math":"\(\textbf{x}\)"} unless, it seems more logical to select label 0{"version":"1.1","math":"\(0\)"} for this data point. Figure. Logistic link function  Least squares regression can perform very badly when some points in the training data have excessively large or small values for the dependent variable compared to the rest of the training data (see figure below). The reason for this is that since the least squares method is concerned with minimising the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the data will have a disproportionately large effect on the resulting constants that are being solved for. Figure. The flaws in Least Square. When using Logistic link function to relate xTw{"version":"1.1","math":"\(\textbf{x}^T \textbf{w}\)"} to y{"version":"1.1","math":"\(y\)"}, the linear model is called Logistic Regression. Logistic regression is a popular model for classification. (Misnomer? Yes! but it is actually a classification method) Other link functions used are Probit link function, tanh function (see figure below). Figure. tanh(x){"version":"1.1","math":"\(tanh(\textbf{x})\)"} function       SIT 307 and SIT720 student must complete this module.  Linear regression for feature selection Principal component analysis (PCA), correlation-based feature selection, and recursive feature elimination are typical feature selection methods. The specific challenge at hand and the features in the dataset determine the feature selection method to use. To find the most significant features in a dataset, linear regression can be used as a feature selection strategy. The basic idea behind using linear regression for feature selection is to evaluate the strength of the relationship between each feature and the target variable. The features with the highest absolute coefficient values can be found using linear regression. Please explore further for further knowledge regarding this idea of using linear regression in the feature selection process.   References: Chen, Xiaojun, et al. "Semi-supervised Feature Selection via Rescaled Linear Regression." IJCAI. Vol. 2017. 2017.  Linear regression formulation In linear regression we want to find a line similar to h{"version":"1.1","math":"\(h\)"}.  The linear equation should allow us to summarise and study relationships between two continuous (quantitative) variables. First we are defining a line (see the figure below): y=h(x)=wx+b{"version":"1.1","math":"\(y = h(\textbf{x}) = w\textbf{x} + b\)"} Figure. Illustration of w{"version":"1.1","math":"\(w\)"} and b{"version":"1.1","math":"\(b\)"} in a linear formulation. Linear hypothesis So how can we find the line? The line has two parameters. w{"version":"1.1","math":"\(w\)"},  the slope of the line, and b{"version":"1.1","math":"\(b\)"}, the y{"version":"1.1","math":"\(y\)"} intercept of the line. If we have these two parameters, we can use them to find our straight line. If we have a point, xi{"version":"1.1","math":"\(\textbf{x}_i\)"} we can estimate the value of y^(xi){"version":"1.1","math":"\(\hat{y}(\textbf{x}_i)\)"}. We will call the estimate y(xi){"version":"1.1","math":"\(y(\textbf{x}_i)\)"}.  So the line predicts y^(xi){"version":"1.1","math":"\(\hat{y}(\textbf{x}_i)\)"} for xi{"version":"1.1","math":"\(\textbf{x}_i\)"}. Now, what if x{"version":"1.1","math":"\(x\)"} is not a single dimension value? Because we may have more than one single feature in the problem, the problem can be in d{"version":"1.1","math":"\(d\)"} dimensions. In this case we write the linear regression as: y^(xi)=w0xi0+w1xi1+w2xi2+...+wdxid{"version":"1.1","math":"\(\hat{y}(\textbf{x}_i) = w_0x_{i0} + w_1x_{i1} + w_2x_{i2} + ... + w_dx_{id}\)"}  =b+w1xi1+w2xi2+...+wdxid{"version":"1.1","math":"\(\quad \quad \ = b + w_1x_{i1} + w_2x_{i2} + ... + w_dx_{id}\)"} Where xi0{"version":"1.1","math":"\(x_{i0}\)"} is a dummy feature with the value of xi0=1{"version":"1.1","math":"\(x_{i0} = 1\)"} and also w0=b{"version":"1.1","math":"\(w_0 = b\)"}. As you can see the dimensioned formula is almost the same as the single dimension formula. The only difference is regarding the multiplication of w{"version":"1.1","math":"\(w\)"} and x{"version":"1.1","math":"\(\textbf{x}\)"} which should be handled in all d{"version":"1.1","math":"\(d\)"} dimensions in the introduced formulation. Using vector notation, we can write the above as y^i=xiTw{"version":"1.1","math":"\(\hat{y}_i = \textbf{x}_i^T w\)"} using d+1{"version":"1.1","math":"\(d+1\)"} dimensional vectors. For i=1,...,n{"version":"1.1","math":"\(i=1,...,n\)"} we have: y^1=x1Tw,for i=1{"version":"1.1","math":"\(\hat{y}_1 = \textbf{x}_{1}^{T} w, \quad for \ i= 1\)"} y^n=xnTw,for i=n{"version":"1.1","math":"\(\hat{y}_n = \textbf{x}_{n}^{T} w, \quad for \ i= n\)"} Therefore, collectively we can write:  y^=Xw{"version":"1.1","math":"\(\hat{\textbf{y}} = \textbf{X}w\)"} where y^=[y^1,...,y^n]T{"version":"1.1","math":"\(\hat{y} = [\hat{y}_1,...,\hat{y}_n]^T\)"} and w=[w0…wd]T{"version":"1.1","math":"\(w = [w_0 \ldots w_d]^T\)"}. Now lets move to the next step. How can we fit this line to data points? But before that, lets review what is the error of value prediction in regression. The difference between what we predicted and the true valueor output of that point, is considered to be the error. We show the error for data point i{"version":"1.1","math":"\(i\)"} and ei{"version":"1.1","math":"\(e_i\)"}: ei=yi−y^i{"version":"1.1","math":"\(e_i = y_i - \hat{y}_i\)"}  Obviously the linear model seeks to minimise the empirical risk R(w){"version":"1.1","math":"\(R(\textbf{w})\)"} via the square loss (yi−y^i)2{"version":"1.1","math":"\((y_i - \hat{y}_i)^2\)"} as: minw1n∑i=1n(yi−y^i)2{"version":"1.1","math":"\(\min_{w} \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)"} We can also rewrite the formula by replacing the y^i{"version":"1.1","math":"\(\hat{y}_i\)"} with xiTw{"version":"1.1","math":"\(x_i^T\textbf{w}\)"}: minw1n∑i=1n(yi−xiTw)2{"version":"1.1","math":"\(\min_{w} \frac{1}{n} \sum_{i=1}^{n} (y_i - x_i^T\textbf{w})^2\)"} The above formulation is just the mean of the square error function which has already been introduced as a proper measurement for regression problems. But how can we solve this minimisation problem? As with other optimisation problems, with a closed form function, we can take the derivative of the error function with respect to w{"version":"1.1","math":"\(w\)"} and equate it to 0{"version":"1.1","math":"\(0\)"}. Then we are able to find the w{"version":"1.1","math":"\(w\)"} which can minimise this error. You may ask yourself, why the derivative is with respect to w{"version":"1.1","math":"\(\textbf{w}\)"} and not x{"version":"1.1","math":"\(\textbf{x}\)"}? The answer is really easy, do not forget that we have the feature vectors of data points which represent  x{"version":"1.1","math":"\(\textbf{x}\)"} and we are looking for proper w{"version":"1.1","math":"\(\textbf{w}\)"} to fit the line on feature vectors. So by taking the derivative of the error function and equating it to 0{"version":"1.1","math":"\(0\)"}, we will find that: w=(XTX)−1XTy{"version":"1.1","math":"\textbf{w} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}"} Just for your knowledge, the matrix  (XTX)−1XTy{"version":"1.1","math":"\((\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}\)"}  is known as the Moore-Penrose pseudo-inverse of the matrix X{"version":"1.1","math":"\(\textbf{X}\)"} and often denoted as X†{"version":"1.1","math":"\( X^{\dagger}\)"}. This can be thought as generalisation of the notion of matrix inverse to non-square matrices. There is an example of linear regression next that may help to further explain this concept. Training a logistic regression model Training a logistic regression model means using training data to estimate the regression coefficient vector w{"version":"1.1","math":"\(\textbf{w}\)"}. In training a logistic regression model, we can use maximum likelihood estimation (MLE) to estimate w{"version":"1.1","math":"\(\textbf{w}\)"}. The likelihood function of w{"version":"1.1","math":"\(\textbf{w}\)"} using data (xi,yi){"version":"1.1","math":"\((x_i,y_i)\)"} is given as: l(w)=11+exp(−yixiTw){"version":"1.1","math":"l(\textbf{w})=\frac{1}{1+exp(-y_i\textbf{x}_i^T\textbf{w})}"} In the above, we assuming a Bernoulli distribution on yi{"version":"1.1","math":"\(y_i\)"} because of the binary forms of the outputs. But how did we come up with this likelihood function? Lets see the mathematical derivations: P(y=1|x)=σ(xTw)=11+exp(−xTw){"version":"1.1","math":"\\ P(y=1\vert\textbf{x}) = \sigma(\textbf{x}^T\textbf{w}) = \frac{1}{1+exp(-\textbf{x}^T\textbf{w})} \\"} Also when y=−1{"version":"1.1","math":"\(y=-1\)"} we have: P(y=−1|x)=1−σ(xTw)=11+exp(xTw){"version":"1.1","math":"\\ P(y=-1\vert\textbf{x}) = 1 - \sigma(\textbf{x}^T\textbf{w}) = \frac{1}{1+exp(\textbf{x}^T\textbf{w})} \\"} So as you can see P(y|x){"version":"1.1","math":"\(P(y\vert\textbf{x})\)"} is: 11+exp(−yxTw){"version":"1.1","math":"\\ \frac{1}{1+exp(-y\textbf{x}^T\textbf{w})} \\"} When y=1{"version":"1.1","math":"\(y=1\)"} the above equation becomes: P(y=1|x)=11+exp(−xTw){"version":"1.1","math":"\\ P(y=1\vert\textbf{x}) = \frac{1}{1+exp(-\textbf{x}^T\textbf{w})} \\"} Wheny=−1{"version":"1.1","math":"\(y=-1\)"} the above equation becomes: P(y=−1|x)=11+exp(xTw){"version":"1.1","math":"\\ P(y=-1\vert\textbf{x}) = \frac{1}{1+exp(\textbf{x}^T\textbf{w})} \\"} Therefore the likelihood function of w{"version":"1.1","math":"\(\textbf{w}\)"} using data (xi,yi){"version":"1.1","math":"\((\textbf{x}_i,y_i)\)"} is given as: l(w)=11+exp(−yxTw){"version":"1.1","math":"\\ l(w) = \frac{1}{1+exp(-y\textbf{x}^T\textbf{w})} \\"} Assuming training data with n{"version":"1.1","math":"\(n\)"} independent instances {(x1,y1),...,(xn,yn)}{"version":"1.1","math":"\(\{(\textbf{x}_1,\textbf{y}_1),...,(\textbf{x}_n,\textbf{y}_n)\}\)"}, we have joint likelihood as: l(w)=11+exp(−y1x1Tw)×...×11+exp(−ynxnTw){"version":"1.1","math":"\\ l(\textbf{w}) = \frac{1}{1+exp(-y_1\textbf{x}_1^T\textbf{w})} \times ... \times \frac{1}{1+exp(-y_n\textbf{x}_n^T\textbf{w})} \\"} =Πi=1n11+exp(−yixiTw){"version":"1.1","math":"\\ = \Pi_{i=1}^{n} \frac{1}{1+exp(-y_i\textbf{x}_i^T\textbf{w})} \\"} So the Joint likelihood function while having n{"version":"1.1","math":"\(n\)"} independent samples using training data is the multiplication of the likelihood of each point.  l(w)=Πi=1n11+exp(−yixiTw){"version":"1.1","math":"\\ l(\textbf{w}) = \Pi_{i=1}^{n} \frac{1}{1+exp(-y_i\textbf{x}_i^T\textbf{w})} \\"} Maximum likelihood estimation method maximises l(w){"version":"1.1","math":"\(l(\textbf{w})\)"} with respect to w{"version":"1.1","math":"\(\textbf{w}\)"}. Logistic Loss Function Maximising likelihood is equivalent to maximising the log of the likelihood function because both provide the same solution for w{"version":"1.1","math":"\(\textbf{w}\)"}. Remember by taking the log of the function you are still able to find the maximum or minimum of the function since the logarithmic functions are monotone increasing functions. Thus, you can write the Log of the likelihood function by taking the log of l(w){"version":"1.1","math":"\(l(\textbf{w})\)"} as: L(w)=log l(w)=−∑i=1nlog(1+exp(−yixiTw)){"version":"1.1","math":"\\ L(\textbf{w}) = log\ l(\textbf{w}) = - \sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w})) \\"} Maximising log l(w){"version":"1.1","math":"\(log\ l(\textbf{w})\)"} is equivalent to minimising −log l(w){"version":"1.1","math":"\(-log\ l(\textbf{w})\)"}, which brings us to the following minimisation problem: minw∑i=1nlog(1+exp(−yixiTw)){"version":"1.1","math":"\\ \min_{w} \sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w})) \\"} But how so we minimise the above function with respect to w{"version":"1.1","math":"\(\textbf{w}\)"}? Remember that we would like to minimise the following function:  minw∑i=1nlog(1+exp(−yixiTw)){"version":"1.1","math":"\\ \min_{w} \sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w})) \\"} The above function is also called the Logistic Loss function. The usual approach is to take the derivative and equate it to zero to solve forw{"version":"1.1","math":"\(\textbf{w}\)"}. However, in this case, the solution does not have a closed form. Therefore, we need to solve the problem iteratively. It is important to remember that sometimes, we can derive a closed form formula for the minimiser (e.g. linear regression) meaning we can compute the minimiser in one step. If that’s not possible, we take multiple steps iteratively to reach to the minimum (e.g. logistic regression and Kmeans). So in this case, we need to perform Coordinate-wise Gradient Descent Optimisation. The question still remains, how do we find the minimum? Computing the minimum Before answering this question, lets recap the difference of two types of functions Convex and Non-convex. Consider the following figure as illustration of two types of functions. Figure. Convex and non-convex functions. The basic difference between the two categories is that: Convex optimisations can deal with only one optimal solution, which is globally optimal. The other possibility is that you prove that there is no feasible solution to the problem (right image on the figure above) In non-convex optimisations, you may have multiple locally optimal points. It can take a lot of time to identify whether the problem has no solution or if the solution is global (left image on figure). Hence, the time efficiency of the convex optimisation problem is much better. So: Sometimes, we can derive a closed form formula for the minimiser (e.g. linear regression) meaning we can compute the minimiser in one step. If there is no closed form formula, we must take multiple steps iteratively to reach to the minimum. (eg. logistic regression and Kmeans) Strategies for finding your way forward. © Getty Images (2018) Imagine you’re blindfolded, but you can see out of the bottom of the blindfold to the ground at your feet. I drop you off somewhere and tell you that you’re in a convex shaped valley and escape is at the bottom/minimum. How do you get out? The simplest way is to look for steepest slope down! Basically you start walking and you look for slopes going down, preferably the steepest slopes. In maths, we call the slopes  derivatives! The slope of a secant line (line connecting two points on a graph) approaches the derivative when the interval between the points shrinks down to zero. That is the basic idea for optimising these scenarios. Iterative optimising Lets get back to the iterative optimising. As we said before, optimisation theory has many methods for iterative optimisation. Two popular methods to compute gradient (derivatives) of the objective function are: Gradient Descent (uses first derivative): To minimise L(w){"version":"1.1","math":"\(L(w)\)"}, we use the iterative update:wt+1←wt−ηt∂∂wL(w){"version":"1.1","math":"\(w_{t+1} \leftarrow w_t - \eta_t \frac{\partial}{\partial w} L(w)\)"} Where H{"version":"1.1","math":"\(H\)"}  is the Hessian matrix with Hij{"version":"1.1","math":"\(H_{ij}\)"} being ∂2∂wi∂wjL(w){"version":"1.1","math":"\(\frac{\partial^2}{\partial w_i \partial w_j} L(w)\)"}. Newton’s method is an iterative method for finding the roots of a differentiable function. Remember, Gradient Descent maximises a function using knowledge of its derivative. While Newton’s method, a root finding algorithm, maximises a function using knowledge of its second derivative. This can be faster when the second derivative is known and easy to compute. However, the analytic expression for the second derivative is often complicated or intractable, requiring a lot of computation. Figure. Find the minimum iteratively. Coordinate-wise Gradient Descent Optimisation Now, lets get back to Coordinate-wise Gradient Descent Optimisation. In order to complete the original task. First randomly initialise w{"version":"1.1","math":"\(\textbf{w}\)"}. Fix all the variables except for one. That is, for each j{"version":"1.1","math":"\(j\)"}, optimise wj{"version":"1.1","math":"\(w_j\)"}  fixing w1,...,wj−1,wj+1,...,wd{"version":"1.1","math":"\(w_1,...,w_{j-1},w_{j+1},..., w_{d}\)"}.  Then, minimise the objective function with respect to wj{"version":"1.1","math":"\(w_j\)"} using Gradient descent as: wj←wj+η∂∂wj  (∑i=1nlog(1+exp(−yixiTw))){"version":"1.1","math":"\\ w_{j} \leftarrow w_{j} + \eta \frac{\partial}{\partial w_j}\ \ \Big(\sum_{i=1}^{n} log(1+exp(-y_i\textbf{x}_i^T\textbf{w}))\Big) \\"} Similarly optimise for otherwj{"version":"1.1","math":"\(w_j\)"}'s (j{"version":"1.1","math":"\(j\)"} is from 1{"version":"1.1","math":"\(1\)"} to d{"version":"1.1","math":"\(d\)"}). Continue until the objective function stops changing. The solution is unique (due to the convexity of the objective function), irrespective of the initialisation of w{"version":"1.1","math":"\(\textbf{w}\)"}. Note that there is always a chance of getting stuck in local minimums rather than the global minimum. When you’re dealing with convex functions, that’s not a problem but with non-convex functions, it could be a serious problem. Logistic regression formulation Logistic regression is named after the function used at the core of the method, the logistic function. The logistic function is also called the sigmoid function. It’s an S-shaped curve (see the above figure) and it can take any real-valued number and map it into a value between 0{"version":"1.1","math":"\(0\)"} and 1{"version":"1.1","math":"\(1\)"} but never exactly at those limits. The value approaches but never reaches 0{"version":"1.1","math":"\(0\)"} or 1{"version":"1.1","math":"\(1\)"}. Let x{"version":"1.1","math":"\(\textbf{x}\)"} be a data instance, and y{"version":"1.1","math":"\(y\)"} be its class label in {−1,1}{"version":"1.1","math":"\(\{-1,1\}\)"}.  Logistic regression does not directly model y{"version":"1.1","math":"\(y\) "} in terms x{"version":"1.1","math":"\(\textbf{x}\)"}. Instead, it models something called logit value or log of odds against  via linear regression. So generally we are modelling log of odds based on x{"version":"1.1","math":"\(\textbf{x}\)"}. But what are the odds? The odds of class −1{"version":"1.1","math":"\(-1\)"} is defined as: Odds=P(y=1|x)1−P(y=1|x){"version":"1.1","math":"\\ Odds = \frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})} \\"} Example: The odds that a randomly chosen day of the topic is a topicend are: 271−(27)=25{"version":"1.1","math":"\(\\ \frac{\frac{2}{7}}{1-(\frac{2}{7})} = \frac{2}{5} \\\)"}  Logit This Log of odds is called logit. Logistic regression uses the following linear model: logP(y=1|x)1−P(y=1|x)=w0+w1x{"version":"1.1","math":"\(\\ log\frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})} = \textbf{w}_0 + \textbf{w}_1 \textbf{x} \\\)"} So you can easily see the similarity to linear regression! Here we are faced with logP(y=1|x)1−P(y=1|x){"version":"1.1","math":"\(log\frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})}\)"} in linear regression. So we are modelling the logit (log of odds) of a data point such as y{"version":"1.1","math":"\(y\) "} rather than x{"version":"1.1","math":"\(\textbf{x}\)"} to be labelled as class 1{"version":"1.1","math":"\(1\)"}. The above expression means that as x{"version":"1.1","math":"\(\textbf{x}\)"} increases by 1{"version":"1.1","math":"\(1\) "} the log of the odds increases by w1{"version":"1.1","math":"\(\textbf{w}_1\)"}. in d−{"version":"1.1","math":"\(d-\)"}dim case, we also have:  logP(y=1|x)1−P(y=1|x)=w0+w1x1+...+wdxd{"version":"1.1","math":"\(\\ log\frac{P(y=1\vert\textbf{x})}{1-P(y=1\vert\textbf{x})} = w_0 + w_1x_1+...+w_dx_d \\\)"} Where x=[1,x1,x2,...,xd]{"version":"1.1","math":"\(\textbf{x} = [1,x_1,x_2,...,x_d]\)"} and w=[w0,...,wd]{"version":"1.1","math":"\(\textbf{w} = [w_0,...,w_d]\)"}. We now we have all the requirements for the logistic regression except we need to know how to calculate P(y=1|x){"version":"1.1","math":"\(P(y=1\vert\textbf{x})\)"}? As you know in linear regression we knew the value of y{"version":"1.1","math":"\(y\)"}, but here we do not know the probability of P(y=1|x){"version":"1.1","math":"\(P(y=1\vert\textbf{x})\)"}. We can estimate this probability using the following equation: P(y=1|x)=11+exp(−xTw){"version":"1.1","math":"\\ P(y=1\vert\textbf{x}) = \frac{1}{1+exp(-\textbf{x}^T\textbf{w})} \\"} The model leads to the following classification rules: when xTw>0{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} > 0\)"}, P(y=1|x)>0.5{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) > 0.5\)"}, we decide in favour of class 1{"version":"1.1","math":"\(1\)"}. when xTw<0{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} < 0 %\)"},  P(y=1|x)<0.5{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) < 0.5 %\)"}, we decide in favour of class −1{"version":"1.1","math":"\(-1\)"}. when xTw=0{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} = 0\)"},  P(y=1|x)=0.5{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) = 0.5\)"},  both classes are equally possible. Testing the model So assume you have trained a logistic regression model and you have come up with proper values of w{"version":"1.1","math":"\(\textbf{w}\)"}. Now by having a test point such as x{"version":"1.1","math":"\(\textbf{x}\)"} you calculate the value of xTw{"version":"1.1","math":"\(\textbf{x}^T\textbf{w}\)"}.  If this value is xTw>0{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} > 0\)"} then it means P(y=1|x)>0.5{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) > 0.5\)"}, the point is allocated to class 1{"version":"1.1","math":"\(1\)"}. On the other hand, if the value of xTw<0{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} < 0 %\)"} then it means P(y=1|x)<0.5{"version":"1.1","math":"\(P(y=1\vert\textbf{x}) < 0.5 %\)"}, the point is allocated to class 0{"version":"1.1","math":"\(0\)"}. In case of xTw=0{"version":"1.1","math":"\(\textbf{x}^T\textbf{w} = 0\)"}, your model is confused and it returns the same value for both of them. Generalisation and complexity Linear regression has a closed form solution. Python implementation uses Singular Value Decomposition (SVD) to compute the Moore-Penrose inverse of matrix X{"version":"1.1","math":"\(\textbf{X}\)"}. If matrix X{"version":"1.1","math":"\(\textbf{X}\)"} is of size n×d{"version":"1.1","math":"\(n\times d\)"}, we incur the computations of the order O(nd2){"version":"1.1","math":"\(O(nd^2)\)"} assuming n≥d{"version":"1.1","math":"\(n \geq d\)"}. As you can see here n≥d{"version":"1.1","math":"\(n \geq d\)"}, the dimension that is larger will be in a linear form in the final complexity (n{"version":"1.1","math":"\(n\)"}) and the smaller value will result in a squared form in complexity d2{"version":"1.1","math":"\(d^2\)"}. So all in all, O(nd2){"version":"1.1","math":"\(O(nd^2)\)"}.  If d≥n{"version":"1.1","math":"\(d \geq n\)"}, the complexity would be O(dn2){"version":"1.1","math":"\(O(dn^2)\)"}. We considered the linear regression problem as y=Xw{"version":"1.1","math":"\(\textbf{y} = \textbf{X}\textbf{w}\)"} where we used the hypothesis function h(xi)=xiTw{"version":"1.1","math":"\(h(\textbf{x}_i) = \textbf{x}_i^T \textbf{w}\)"}.  We can use any derived features of the original features such as ϕ(xi){"version":"1.1","math":"\(\phi(\textbf{x}_i)\)"} instead of xi{"version":"1.1","math":"\(\textbf{x}_i\)"}.   In other words, we can create our own features. They could be even non-linear! For example, we can have ϕ(xij)=[xij,xij2]{"version":"1.1","math":"\(\phi(\textbf{x}_{ij}) = [x_{ij},x_{ij}^2]\)"}. As you can see we have added xij2{"version":"1.1","math":"\(x_{ij}^2\)"} to the feature vector as a new feature. As long as we are using a linear formulation, the problem remains a linear regression. For example, we can have h(xi)=ϕT(xi)w{"version":"1.1","math":"\(h(\textbf{x}_i) = \phi^T(\textbf{x}_i)\textbf{w}\)"} and solve it using linear regression. This is called the generalised form of linear regression in which we are adopting our own list of features and try to fit a line based on the new features. Given expressive features, linear regression can be powerful! Generalisation (Prediction on unseen data) After training a linear regression model, we can start to predict the output y^{"version":"1.1","math":"\(\hat{y}\)"} for a new instance x{"version":"1.1","math":"\(\textbf{x}\)"}. The predicted output is computed as y^=xTw{"version":"1.1","math":"\(\hat{y} = \textbf{x}^T\textbf{w}\)"}. Given an unseen set of instances as a test set, we can measure the error in prediction as: Etest=1ntest∑i=1ntest(yi−xiTw)2{"version":"1.1","math":"\(\\ E_{test} = \frac{1}{n_{test}} \sum_{i=1}^{n_{test}} (y_i - \textbf{x}_i^T \textbf{w})^2 \\\)"} The above error is called the mean square error (MSE). This is an example of a performance measure. We can also compute many other measures such as Mean Absolute Error (MAE) or explained variance (R2){"version":"1.1","math":"\((R^2)\)"}.  Model complexity of Linear Regression Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. The reason is the risk of over-fitting on this limited set of training data. Using a limited number of features may also be problematic as it could cause under-fitting. Activity Regularised linear models You have learned that even in linear models, using all data dimensions as features may fit the model to true patterns (signal) but also to background noise. A regulariser is an additional term in the loss function to avoid overfitting. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values. What will happen if one or more weights are excessively large? It implies your model is highly dependent on that one feature. What if this feature is noise or highly affected by noisy observations? We do not want to rely too much on any one thing when we are designing a model in machine learning! So, this procedure is a way to guide the training process to prefer certain types of weights over others. Thus our loss function now has another term which is λ Regulariser(w){"version":"1.1","math":"\(\lambda \ Regulariser(\textbf{w})\)"}. You can consider this term as complexity of the model. Figure. Loss function including a regulariser. How do regularisers works in linear models? Remember the linear model is y=w0+∑j=1d wjxj{"version":"1.1","math":"\(y = w_0 + \sum_{j=1}^{d} \ w_jx_j\)"}  Should we allow all possible weights? Or impose preferences? What makes a simpler linear model? Our preference We do not want huge weights (i.e. do not want to over-rely on any one feature). If weights are huge, a small change in a feature would result in a large change in the prediction! In fact, since we may even have irrelevant features, we want some of the weights to be zero so we can discard some features. In the following formulation there is a regulariser:  minw 1n∑i L(yi,xiTw)+λ Regulariser(w){"version":"1.1","math":"\(\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda\ Regulariser (\textbf{w}) \\\)"} How do we create a regulariser that penalises large weights or encourages small/zero weights ? What should our regulariser function be? There are two popular regulariser functions: Option 1:  Regulariser(w)=∑j|wj|=||w||1,(l1−norm){"version":"1.1","math":"Regulariser(\textbf{w}) = \sum_{j} |w_j| = ||\textbf{w}||_1, (l_1-norm)"} This encourages 0{"version":"1.1","math":"\(0\)"} weights (sparsity). This function implies the closed form function of a square. Option 2: Regulariser(w)=∑j|wj|2=||w||2,(l2−norm){"version":"1.1","math":"Regulariser(\textbf{w}) = \sum_{j} |w_j|^2 = ||\textbf{w}||_2, (l_2-norm)"}  This penalises large weights. This function implies the closed form function of a circle. Regularisation impact Consider the figure below. The left image is an illustration of regularisation impact. Figure. Illustration of regularisation impact The l1{"version":"1.1","math":"\(l_1\)"}-norm forms a square shape, assuming the loss function is in the form of ellipses in the plot. Since we are minimising the loss function which actually has l1{"version":"1.1","math":"\(l_1\)"}  -norm regularisation inside it, we need to find a sweet spot which is the intersection of these two regions. If you keep drawing the ellipses, you can find the intersection. The image on the right indicates the same concept with a different regulariserl2{"version":"1.1","math":"\(l_2\)"}-norm which is circular. As you can see in this figure, there is an increased chance of intersection. Also there is less chance of having 0{"version":"1.1","math":"\(0\)"} weights forβ1{"version":"1.1","math":"\(\beta_1\)"} or w1{"version":"1.1","math":"\(w_1\)"} andβ2{"version":"1.1","math":"\(\beta_2\)"} orw2{"version":"1.1","math":"\(w_2\)"}because it looks we can have more options for selection of w1{"version":"1.1","math":"\(w_1\)"} and w2{"version":"1.1","math":"\(w_2\)"}.  Now, consider the visualisation of the lp−{"version":"1.1","math":"\(l_p-\)"} norm regulariser below. As you can see, by using a l1{"version":"1.1","math":"\(l_1\)"} norm regulariser you are using a square shaped regulariser (the pink square) and by using thel2−{"version":"1.1","math":"\(l_2-\)"}norm you using a circle shaped regulariser (light blue). The ∞−{"version":"1.1","math":"\(\infty-\)"}norm regulariser is a square too (in red). Remember that all of these lp{"version":"1.1","math":"\(l_p\)"} norm regularisers penalise larger weights. p≤1{"version":"1.1","math":"\(p \leq 1\)"} tends to create sparse weights (i.e. lots of 0{"version":"1.1","math":"\(0\)"} weights). Higher values of p{"version":"1.1","math":"\(p\)"} tends to like similar and small weights. Next, we are going to review two regularisation methods, LASSO and Ridge. Figure. Visualisation of lp−{"version":"1.1","math":"\(l_p-\)"} norm regulariser. L1{"version":"1.1","math":"\(L_1\)"} Regularisation (LASSO) Lasso (Least Absolute Shrinkage and Selection Operator) (also seen in uppercase: LASSO) is a regression analysis method that performs both variable selection and regularisation in order to enhance the prediction accuracy and interpretability of the statistical model it produces. It is therefore quite common to use the following formulation: minw 1n∑i L(yi,xiTw)+λ1||w||1{"version":"1.1","math":"\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_1 ||\textbf{w}||_1 \\"} Now you can see our normal risk function +{"version":"1.1","math":"\(+\)"} l1{"version":"1.1","math":"\(l_1\)"}-norm regulariser. Another method of regularisation is called Ridge. L2{"version":"1.1","math":"\(L_2\)"} Regularisation (Ridge) The common formulation of Ridge is: minw 1n∑i L(yi,xiTw)+λ2||w||22{"version":"1.1","math":"\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_2 ||\textbf{w}||^2_2 \\"} This is known as Elastic Net. LASSO and Ridge regularisation are then special cases if Elastic Net for λ2=0{"version":"1.1","math":"\(\lambda_2 = 0\) "} and λ1=0{"version":"1.1","math":"\(\lambda_1 = 0\)"}. Elastic Net overcomes a limitation of LASSO. What is this limitation? When presented with few samples in high dimension spaces (d>n{"version":"1.1","math":"\(d > n\)"} cases, such as bioinformatics datasets), LASSO selects at most n{"version":"1.1","math":"\(n\)"} variables before it saturates. But Elastic Net can overcome this problem since it can select a greater number of variables despite the number of data points. For Linear regression we use square loss functions, i.e.:  L(yi,xiTw)=(yi−xiTw)2{"version":"1.1","math":"\\ L(y_i,\textbf{x}_{i}^{T}\textbf{w}) = (y_i - \textbf{x}_{i}^{T}\textbf{w})^2 \\"} For Logistic regression we use logistic loss function, i.e.: L(yi,xiTw)=log(1+exp(−yixiTw)){"version":"1.1","math":"\\ L(y_i,\textbf{x}_{i}^{T}\textbf{w}) = log(1+exp(-y_i\textbf{x}_{i}^{T} \textbf{w})) \\"} So all in all, we solve the following optimisation: minw 1n∑i L(yi,xiTw)+λ1||w||1+λ2||w||22{"version":"1.1","math":"\\ \min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_1 ||\textbf{w}||_1+\lambda_2 ||\textbf{w}||^2_2 \\"} For Ridge regularisation (when λ1=0{"version":"1.1","math":"\(\lambda_1 = 0\)"}, the solution is closed form and is given as:  w=(XTX+λ2I)−1XTy{"version":"1.1","math":"\\ \textbf{w} = (\textbf{X}^TX + \lambda_2 \textbf{I})^{-1} \textbf{X}^T \textbf{y} \\"} For LASSO and Elastic Net, we have to perform iterative optimisation. Since  L1{"version":"1.1","math":"\(L_1\)"} norm is non-differentiable and non-smooth, a proximal gradient should be used to optimise this loss function. Also while working with logistic regression, we solve the following optimisation: minw∑ilog(1+exp(−yixiTw))+λ1||w||1+λ2||w||22{"version":"1.1","math":"\\ \min_{\textbf{w}} \sum_{i} log(1+exp(-y_i\textbf{x}_{i}^{T} \textbf{w}))+ \lambda_1 ||\textbf{w}||_1+\lambda_2 ||\textbf{w}||^2_2 \\"} As you can see the only difference is in the fist term log⁡(1+exp(−yixiTw))\){"version":"1.1","math":"\log(1+exp(-y_i\textbf{x}_{i}^{T} \textbf{w}))\)"} which is the log of the likelihood function. For regularised logistic regression, we always have to perform iterative optimisation. What are the effects of Regularisation on bias and variance? We know that regularisation increases bias in our model. We are only partially listening to our training data! Why regularisation might make sense? Because it greatly reduces the variance. To conclude, it is useful when the net effect (i.e. bias2{"version":"1.1","math":"\(^2\)"} +variance) reduces. The following figure illustrates the effects of bias and variance on model complexity. As you can see, there is a trade-off or optimum model complexity which we always look for. Figure. Model complexity vs. bias and variance. Another important element is λ{"version":"1.1","math":"\(\lambda\)"}'s.  Obviously by raising the value of λ{"version":"1.1","math":"\(\lambda\)"}, i.e.,λ2{"version":"1.1","math":"\(\lambda_2\)"} in Ridge regularisation, minw 1n∑i L(yi,xiTw)+λ2||w||22{"version":"1.1","math":"\(\min_{w}\ \frac{1}{n} \sum_{i}^{} \ L(y_i,\textbf{x}_i^T\textbf{w}) + \lambda_2 \vert \vert \textbf{w}\vert \vert ^2_2\)"}, we are forcing more regularisation on the loss function. So we should expect smaller values of weights (w{"version":"1.1","math":"\(\textbf{w}\)"}). As if we reduce the value of λ2{"version":"1.1","math":"\(\lambda_2\)"}, we can see larger values for the obtained weights. The following figure illustrates this point by showing the results of λ2{"version":"1.1","math":"\(\lambda_2\)"} increment on the calculated weights. As you can see with larger values of λ2{"version":"1.1","math":"\(\lambda_2\)"}, we obtain smaller values of weights. On the other hand, by enforcing less regularisation on the loss function (smaller λ2{"version":"1.1","math":"\(\lambda_2\)"}),  you will get bigger values for weights. Figure. Effects of Ridge regularisation on weights. Similarly, in case of Lasso and Elastic Net (see the figure below), by imposing more regularisation, you will get smaller values for weights and vice versa. Note that the figure shows −log λ1{"version":"1.1","math":"\(-log \ \lambda_1\)"} rather than λ1{"version":"1.1","math":"\(\lambda_1\)"}. Figure. Effects of Lasso and Elastic-Net regularisation on weights. There is one more interesting fact about LASSO. Because L1{"version":"1.1","math":"\(L_1\)"} regularisation shrinks the weights of noisy dimensions to zero, these dimensions do not participate in the prediction model. Only those dimensions that have non-zero weights participate in the prediction. Therefore, LASSO is also used to select predictive features among all dimensions. This is the feature selection property of LASSO. Activity Check out the this additional video on regularisation: 