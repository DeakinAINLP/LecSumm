 Linear regression fits a straight line to data, with covariance showing how much one variable tells about the other. Three scenarios:  a)  both variables go up together. b)  one goes up, the other down. c)  no connection between them.  Pearson's Correlation Coefficient measures how closely two variables move together on a scale from -1 to 1. It's all about straight line relationships, not curves or other shapes. Figures show different relationships: straight line vs curve, strong vs weak, and none at all.  Linear regression can predict outcomes. It can used to predict a student's statistics grade based on their math aptitude score. The prediction equation is found by minimizing the mean square error. Once the equation is determined, it can be used to predict future outcomes. For instance, you can predict a student's statistics grade based on their score on the aptitude test.  Linear regression finds the best fit line for data points using two parameters: the slope and the intercept. This line can predict output for given inputs. The goal is to minimize the error between predicted and actual values. The best parameters are found by taking the derivative of the error function, leading to the Moore-Penrose pseudo-inverse of the matrix. This technique works for single and multiple features.  Logistic regression deals with binary results, predicting either an 'A' or 'B' outcome. The method either simplifies the issue, treating it like a regular regression, or applies a special function to adjust the result, making it suitable for binary classification.  Linear regression can be modified to handle non-linear features by adding them to the feature vector, while still using a linear formulation. This generalized form keeps the problem as a linear regression.  Logistic regression uses the logistic (or sigmoid) function to map any real number into a value between 0 and 1, modelling the log of odds of a class label. After training, the model assigns a class based on whether the calculated value is greater or less than 0.5. If it's exactly 0.5, the model can't decide.  Training a logistic regression model involves using data to estimate the regression coefficient vector through maximum likelihood estimation.  Optimising the model involves minimising the Logistic Loss function which doesn't have a closed form solution, so it requires iterative solutions such as Coordinate-wise Gradient Descent Optimisation.  Overfitting means a model learns too much detail, underfitting means it misses important info. Aim for a balance between detail and simplicity to minimize total error (bias + variance + noise).  Regularisation helps prevent overfitting in linear models by controlling the size of weights. LASSO and Ridge are types of regularisations; LASSO also selects predictive features.  Linear regression can be used for feature selection by evaluating the relationship between each feature and the target variable. The features with the highest absolute coefficient values are typically the most significant.  Reflect on the knowledge that you have gained by reading contents of this topic with respect to machine learning (the text above).  This topic, I learned how linear regression can predict outcomes and select key features in a dataset. I also grasped the role of Pearson's Correlation Coefficient, overfitting, underfitting, and regularization techniques such as LASSO and Ridge. Lastly, I delved into logistic regression, its application in binary classifications, and techniques to optimize the model.  