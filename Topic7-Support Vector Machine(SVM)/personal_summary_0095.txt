  Summary/Reflection of the main points during topic 6:  1.  Assessing a trained ML model: Evaluating a machine learning model is the process of  determining its performance on unseen data. Key evaluation metrics include accuracy, precision, recall, F1-score, and mean squared error. Examples of assessing a trained ML model include using a confusion matrix to visualize classification results or computing the R- squared value for a regression model.  2.  Relevance and covariance amongst features/variables: Relevance refers to the importance of a feature in predicting the target variable, whereas covariance measures the degree to which two features change together. High covariance between two features may indicate multicollinearity, which can negatively impact model performance. Example: In a house price prediction model, the square footage of the house (relevant feature) may be highly correlated with the number of rooms (covariant feature).        3.  Linear regression and examples of it: Linear regression is a statistical method that models the  relationship between a dependent variable and one or more independent variables. Examples include predicting house prices based on square footage, predicting a student's final grade based on hours of study, or forecasting sales based on advertising expenditure.  4.  Linear regression formulation/mathematics behind the hood: Linear regression can be  expressed as Y = β0 + β1X1 + β2X2 + ... + βnXn + ε, where Y is the dependent variable, X1 to Xn are the independent variables, β0 to βn are the coefficients, and ε is the error term. The goal is to minimize the sum of the squared differences between the actual and predicted values (least squares method). This can be achieved using techniques like gradient descent or normal equations.  5.  Linear classification: Linear classification is a method for separating data points into different  classes using a linear decision boundary. Examples of linear classifiers include logistic regression, support vector machines (with a linear kernel), and perceptron. Linear classifiers work well when the data points can be separated by a straight line or hyperplane in the feature space.       6.  Generalization and complexity: Generalization refers to a model's ability to make accurate  predictions on new, unseen data. A model with high complexity can fit the training data well but may perform poorly on unseen data due to overfitting. On the other hand, a model with low complexity may underfit the data, leading to suboptimal performance on both training and test data.  7.  Logistic regression formulation: Logistic regression is a type of linear classifier used for binary classification problems. It models the probability of a sample belonging to a certain class using the logistic function, which can be expressed as P(Y=1|X) = 1 / (1 + exp(- (β0 + β1X1 + ... + βnXn))).  8.  Training a logistic regression model: The logistic regression model is trained by minimizing  the logistic loss function, also known as cross-entropy loss. Iterative optimization algorithms such as gradient descent or stochastic gradient descent are used to find the optimal set of coefficients that minimize the loss function.  9.  Examples of logistic regression: Logistic regression can be used to predict whether a  customer will churn, if an email is spam or not, or if a tumour is malignant or benign based on a set of medical features.  10. Model complexity and its effect: Increasing model complexity can improve the fit on training  data but may lead to overfitting and poor generalization to unseen data. Conversely, reducing model complexity can decrease overfitting but may result in underfitting, where the model does not capture the underlying patterns in the data.       11. Regularized linear models: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The two most common regularization techniques are L1 (Lasso) and L2 (Ridge) regularization, which penalize the absolute and squared values of the coefficients, respectively.  12. Linear regression for feature selection: Lasso (L1) regularization in linear regression can be used for feature selection because it tends to drive some coefficients to zero, effectively excluding irrelevant features from the model.  13. Regularized linear regression: Regularized linear regression combines linear regression with a regularization term, such as L1 (Lasso) or L2 (Ridge) regularization, to prevent overfitting and improve generalization.  14. Polynomial regression: Polynomial regression is a type of linear regression that models the relationship between a dependent variable and one or more independent variables using polynomial functions. It allows for capturing non-linear relationships in the data while still using a linear model.    