 ● Linear Models are perhaps the most popular methods that help understand the basics of Machine Learning (ML). This is because they begin with the concepts of relevance and covariance of features which are essential in identifying the features that affect (or not) the target variable the most. After all, we are after predicting the target variable.  ● The relationship between two variables can be quantified using covariance. Pearson’s correlation coefficient is a common measure used to understand the linearity between two variables. They may or may not contain the target variables, but this is still relevant because we do not wish to have highly correlated features in the same data, as any one of them is good enough to influence the target variable and we can let go of the other one. This helps to reduce the number of features present in the dataset.  ● Linear Regression is one of the basic ML models that use linear relationships between the target variable and independent feature variables to predict the outcome of unseen data. The goal here is to find a straight line that fits best with the training dataset such that the slope of the line represents the relationship between the target and feature variables.  ● Linear Models can also be used for classification, especially in binary classification cases and where the data points are linearly seperable. This is known as Logistic Regression, but it also works for Multinomial classification. Logistic Regression is nothing but a logistic function applied over a linear regression, for example a sigmoid function.  ● Model complexity is a measure of how many independent variables it needs to account for in order to predict the target variable. Understand that, it does not mean all teh features of the dataset, only the ones that affect the outcome of the target variable. Another factor of model complexity is how well it learns the training data, if it has a high bias then it would underfit the data on the other hand if it has high variance, then it would oberfit the data  ● The noise in the data is an organic part of data collection and it cannot be avoided. It is near impossible to obtain pure data which has the exact data points that our model needs. But it is possible to handle the noise, once we understand how we can manage it better. One common way to handle noise in Linear Models is to introduce a regularisation term in the loss function we helps the model avoid overfit the data. There are two ways to regularise the model - L1 and L2 regualrisers  ● L1 regulariser encourages the allocation of sparse weights in the model making it memory efficient to process. L2 on the other hand, discourages allocation of large weights in the model making it lighter and less biased.  