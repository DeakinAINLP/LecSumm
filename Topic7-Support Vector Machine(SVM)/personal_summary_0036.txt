 Assessing a train model After a model is trained; in order to measure its accuracy, you need to assess the model. But the important question is:  How can we get an unbiased estimate of the accuracy of a trained model?  When training a model, you should always separate a test set. Pretend, that you do not have access to the test set yet. Why?  Because if the test set labels influence the trained model in any way, the accuracy estimate will be biased. In other words, you are not allowed to use the same data points you used for training your model.  Relevance and Covariance among features or variables Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data, in other words an equation that graphs as a straight line.  We can measure the linear relationship between variable x and output y using covariance Covariance measures the amount of information a specific x can provide for y  Pearson’s Correlation Coefficient Pearnson’s correlation Coefficient is a measure of the linear correlation between two variables, x and y.  It has a value between +1 and -1 where 1 indicates total positive linear correlation, 0 is no linear correlation, and -1 shows total negative linear correlation.  Linear classification  Logistic regression is the appropriate regression analysis to conduct when the output values of the feature vectors are binary.  Like all regression analyses, the logistic regression is a predictive analysis. Before talking about logistic regression, lets first review linear classification.  By linear classification, we mean that the separation boundary between any two classes is linear. This is just our hypothesis. It may not be true!  Generalisation and complexity Linear regression has a closed form solution. Python implementation uses Singular Value Decomposition (SVD) to compute the Moore-Penrose inverse of matrix X  Model complexity of Linear Regression Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. The reason is the risk of over- fitting on this limited set of training data. Using a limited number of features may also be problematic as it could cause under-fitting.  Logistic regression formulation Logistic regression is named after the function used at the core of the method, the logistic function.  The logistic function is also called the sigmoid function. It’s an S-shaped curve and it can take any real-valued number and map it into a value between 0 and 1 but never exactly at those limits. The value approaches but never reaches 0 or 1.  Logistic regression does not directly model y in terms x. Instead, it models something called logit value or log of odds against via linear regression. So generally we are modelling log of odds based on x.  Model complexity Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model.  Over-fitting will happen when your model starts to capture some irrelevant noise points in the data while building the model, rather than the whole pattern (right image on the figure). Under-fitting is the result of an extremely simple model (left image on the figure).  Under-fitting occurs if the complexity of the model is lower than necessary.  Scenario-1: We may be using a linear model, when the data requires a nonlinear model. Scenario-2: We may be using the right hypothesis (linear or nonlinear) but the number of  variables might be falling short of what is required. For example, to predict the income of a person, age alone may not be sufficient.  Low bias implies high variance, and high bias implies low variance   We need to find the sweet spot where Risk = bias + variance + noise is the minimum. The minimum error is at the right model complexity.   Regularised linear models A regulariser is an additional term in the loss function to avoid overfitting. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values. What will happen if one or more weights are excessively large? It implies your model is highly dependent on that one feature.  What are the effects of Regularisation on bias and variance? We know that regularisation increases bias in our model. We are only partially listening to our training data!      