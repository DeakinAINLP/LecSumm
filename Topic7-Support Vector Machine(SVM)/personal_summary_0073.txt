 This topic, we dived deeper into supervised learning and the first topic we covered was linear  regression which models the relationship between two variables via a linear equation (a line) where we  try to map the feature values variables to the target/output variables. Here, we are essentially trying to  find the best line that can fit the data to perform accurate predictions by minimizing the empirical risk via  the squared loss function. Ultimately, we are trying to find a value of w (part of the formulation as a  constant) that minimizes the error. We also have a regression technique called logistic regression which is  for classification and can be good for binary classification. Instead of using the squared error function like  in linear regression, we use the maximum likelihood estimation for the estimation of the constant w. The  lecture then went into detail on how to compute the minimum where we use derivatives (essentially  looking for steep slopes) and techniques such as the gradient descent and newton’s method can be used  to achieve this. Gradient descent uses the first derivative and is iterative in its process (steps to find the  direction) to find the minimum. Essentially, to my understanding, we make use of step sizes (can impact  solution depending on size – large size might be faster but may not find global minimum and vice versa  for small sizes) to jump from one solution to another as we iteratively try and find the global minimum  solution. Newton’s method uses the second derivative to find the minimum and uses quadratic  approximation.  The next topic was on model complexity which was covered briefly in the previous topic.  Essentially, if we have an extremely simple model, we can have underfitting where our model does not fit  the pattern of the data well. If we have an over complex model, the model fits the data too well where it  has learnt too much of the specifics of the data, including background noise, which can impact the  performance when testing the model with unseen data. Complexity of the model can occur when we  increase the number of features. We have something known as bias variance decomposition where we  can add in noise and see how the model performs and shows us the bias and variance. This can help with  checking if the model is underfitting and overfitting (by adding the noise) where if the bias is close to 0,  we can then say that our model is accurate. In terms of variance, it shows us how complex the model is  where if the variance increases, the model is more complex (and the bias also becomes lower) and vice  versa for lower variance. We essentially want to find a balance or a model that is low variance (less  complex) and low bias (more accurate). Regularised linear models can help us with avoiding overfitting by  preventing coefficients (w) from becoming too large which can cause the model to be highly dependent  on particular features (also increases complexity of the model). Essentially, if we try to change values of  that dependent feature, it can impact the performance of the model and lead to inaccurate results. We  have two types of regularisers, one of which is the L1-norm (good for sparsity by removing features from  0 weights) and L2-norm (we have the possibility for getting all weights in the model) which forms  different shapes (square and circle respectively) where we try to find the intersection between the shape     and the loss function. In terms of L1 regularisation, we have LASSO which performs variable selection and  regularization and also enhances accuracy of predictions and interpretability of models. For L2  regularisation, we have Ridge which only takes into account the L2 norm and Elastic net which takes into  account both L1 and L2 norm.  Reflection  This topic went into a lot more detail regarding aspects of supervised learning. The main take  away for me was the content involving model complexity. I have a better understanding of the  importance of taking model complexity into consideration when doing supervised machine learning. I  also now have more techniques that I can use when I am doing machine learning which include L1 and L2-  norm.  Not much to say about the quiz. For question 4, I thought that logistic regression’s primary  purpose is to be used for binary classification. In terms of question 7, my line of thinking was turning categorical values into numerical using techniques such as one hot encoder.   