Topic 5 Topics to cover ❑ Supervised learning  ❑ Forms of supervised learning algorithm ❑ Example of a supervised learning  ❑ Model complexity  ❑ Concept of model complexity ❑ Model complexity and Occam’s razor ❑ Structural risk minimization  ❑ Data partitioning schemes ❑ Finding the best hyperparameter ❑ Effects of imbalanced class distribution  Supervised learning: We explored different forms of supervised learning, including regression, classification and ranking problems. Supervised learning involves learning mapping functions from labelled training data, which helps us to be able to learn direct relationships between feature vectors and outputs.  Model Complexity: Finding the right balance in model complexity to prevent overfitting (too complex) and underfitting (too simple) is important. Achieving this balance is essential for ensuring good generalization on unseen data.  Occam's Razor and Structural Risk Minimization: Occam's Razor is a problem-solving principle that encourages choosing the simplest solution among multiple candidates with similar fits. Structural risk minimization based on Occam's Razor, used to prevent overfitting by penalizing model complexity and preferring simpler functions over more complex ones.  Data Partitioning Schemes: We examined various data partitioning schemes for splitting data into training and testing sets, such as random sub-sampling, stratified sampling, and cross-validation. These methods help ensure a reliable estimate of model performance and accuracy.  Hyperparameter selection: The hyperparameters and their impact on model performance should be set before the learning process begins. In order to find the best hyperparameter, the training data needs to be divided into separate training sets and verification sets.  Imbalanced Class Distribution: Imbalanced class distribution can skew model performance towards the majority class. It’s difficult to develop a model with imbalanced data, such as unequal costs of misclassification errors and other compounding factors like data size, label noise, and data distribution.    