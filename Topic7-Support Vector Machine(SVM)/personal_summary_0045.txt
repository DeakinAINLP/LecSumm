 OVERVIEW The topic of this topic is a continuation of last topics supervised learning. The focus however is on linear models, their formulation, implementation, assessment and selection. Model complexity, bias and variance, as well as regularisation in model training. ßTwo main example models are covered: Linear and Logistic Regression.  READING I read Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow by Aurelien Geron to revise logistic regression, I read over the course notes to familiarise myself with what is considered to be relevant and indicative of model complexity in this course.  SUMMARY OF THE MAIN POINTS FOR TOPIC 6 LEARNING Linear regression is method of predicting an output value from a vector of input values with the presumption   that   the   relationship   between   the   output   and   inputs   is   that   of   a   straight   line.   As the number of features in a model increases, it’s complexity increases, and conversely, as the number of features in a model decreases, it’s complexity decreases.  If the ratio of features to training data is high, then there is insufficient training data for the number of features and the model is likely to be over-fit, meaning that the model is very well fit to the training data but the model parameters are mostly representative of the training data only and the model does not posses sufficient generalisation to make accurate predictions for unseen data. On the other hand, if you model a parabola with a 100th order polynomial you will no doubt pick up noise in the training data which does not generalise to unseen data.  On the other hand, if the ratio of features to training data is low then the model is likely to be under- fit, meaning there are insufficient model parameters to represent the whole input/output relationship and so the model cannot make accurate predictions from either the training or unseen data.  Over fitting and under fitting can also occur if the model itself is not representative of the true input/output relationship. i.e. using a straight line to represent a sin function is going to lead to under fit since the straight line function cannot represent any part of the sin function other than at periodic intervals where the line intersects the sin function.  A model that is under fit is said to have high bias. A model that is over-fit is said to have high variance. There is a tradeoff between bias and variance but the goal is of course to minimise both.  One way to reduce variance (over fitting) is to use regularisation. Regularisation is simply the addition of constraints on an objective function. The main types of regularisation used are L1, L2 and elasticnet (both L1 and L2). L1 regularisation introduces a sparsity constraint which can reduce the number of features, it penalises having a large number of parameters and will thus “select” those parameters that are most representative and eliminate those parameters that add little to predicting the correct result e.g. noise. L2 regularisation reduces the influence of features in the model by forcing   them   to   be   small,   it   penalises   having   parameters   with   large   values   which   would   then dominate the final result and instead spreads out the influence among the parameters. Elastic net is just a combination of L1 and L2 regularisation. Regularisation does increase bias though so as previously stated, it is important to ensure a balance between bias and variance and this is especially true when performing regularisation because the regularisation parameter is a hyper parameter to tune when training a model and can greatly influence the accuracy and generalisability of a model.   