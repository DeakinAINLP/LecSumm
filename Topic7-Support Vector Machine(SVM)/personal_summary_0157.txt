Covariance and Relevance of Features or Variables  The average of the product of each variable's departures from its mean is used to determine covariance. The  two  variables  tend  to  move  in  the  same  direction  when  there  is  a  positive covariance, but the converse is true when there is a negative covariance. The amount of the covariance, however, does not reveal anything about how strongly the variables are related. The covariance is divided by the product of the two variables' standard deviations to compute correlation.  The  correlation  coefficient  runs  from  -1  to  1,  with  1  denoting  a  perfect  linear relationship  and  -1,  a  perfect  linear  relationship  in  the  other  direction,  and  0  denoting  no association at all.  Linear Regression  By fitting a linear equation to the observed data, linear regression is a statistical technique used to  model  the  connection  between  two  variables.  The  dependent  variable  is  one,  while  the independent  variable  is  another.  Finding  the  line  of  best  fit  that  minimises  the  sum  of  the squared deviations between the observed data points  and the line's anticipated values is the objective of linear regression.  Linear Classification  A model known as a linear classifier decides which discrete class to assign a collection of data points  based  on  a  linear  combination  of  its  explanatory  factors.  For  instance,  a  model  may utilise a dog's weight, height, colour, and other factors to combine to determine the species of the  dog.  The  strength  of  these  models  lies  in  their  capacity  to  identify  this  mathematical combination of attributes that separates data points when they belong to different classes and puts  them  together  when  they  belong  to  the  same  class,  giving  us  distinct  classification boundaries.  Logistic Regression  A statistical model called logistic regression is applied to classification and prediction issues. By making the log-odds for the event a linear mixture of one or more independent factors, it predicts the likelihood that an event will occur. The objective of logistic regression, which uses a categorical dependent variable, is to estimate the likelihood that an observation will fall into a certain category. For instance, it may be applied to the identification of fraud, where specific actions or traits may be more frequently associated with illegal activity. In logistic regression, any  real  input  is  translated  to  a  value  between  zero  and  one  using  the  logistic  function,   sometimes  referred  to  as  the  sigmoid  function.  The  likelihood  that  the  event  will  occur  is inferred from the logistic function's output.  Model Complexity  In order for a model to produce reliable predictions, a certain number of predictors, independent variables,  or  characteristics  must  be  included. A  model  with  several  variables  or  non-linear interactions is more complex than one with only one independent variable, for instance, in a linear regression. A complicated model may be able to capture more variability in the data, but    it will also be more challenging to train and may be more prone to overfitting. A model with a low level of complexity, on the other hand, could be simpler to train but might not be able to extract all the pertinent information from the data. Successful machine learning depends on striking the correct balance between model complexity and predictive capability. In the image below, a sophisticated model is shown on the extreme right, while a basic model is shown on the extreme left. Take note of the relationship between a few parameters and model complexity.  The following are important variables that control model complexity and influence the model's accuracy when used with omitted data:    The  quantity  of  variables: The  models  are  typically  more  prone  to  overfitting  when there are a lot of tuneable parameters, commonly referred to as degrees of freedom.   The  range  of  values  that  the  parameters  can  accept:  Models  are  more  prone  to  overfitting when the parameter range is greater.    The  quantity  of  training  cases:  Even  though  the  model  is  simpler,  it  gets  easier  for models to overfit a dataset with fewer or fewer datasets. Millions of training samples in a dataset need an exceedingly sophisticated model to overfit it.  