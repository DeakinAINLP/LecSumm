Topic 6 Learning Summary Linear Regression  Linear regression is a method to model the relationship between a dependent variable and one or more independent variables.  Simple linear regression focuses on one independent variable, while multiple linear regression considers multiple independent variables.  The least squares method is used to estimate the parameters of the linear regression model by minimizing the sum of squared residuals.  Polynomial Regression  Polynomial regression is an extension of linear regression, where the relationship between the dependent and independent variables is modeled as an nth-degree polynomial.  It helps in modelling complex, non-linear relationships between variables.  Overfitting can be an issue with high-degree polynomials, and regularization techniques or cross-validation can be used to mitigate this problem.  Logistic Regression  Logistic regression is a classification algorithm used to model the probability of a binary outcome (e.g., success/failure or 0/1) based on one or more independent variables.  It uses the logistic function to transform the linear regression output into a probability value ranging between 0 and 1.  The Maximum Likelihood Estimation (MLE) method is used to estimate the parameters of the logistic regression model.  Multinomial Logistic Regression  Multinomial logistic regression is an extension of logistic regression to handle multi-class classification problems where there are more than two outcome categories.  It models the relationship between the independent variables and the log-odds of each outcome category compared to a reference category.  The model parameters are estimated using MLE or other optimization techniques.  Model Evaluation and Selection  Evaluating the performance of regression models using metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared.  For logistic regression, evaluation metrics include accuracy, precision, recall, F1-score, and area under the ROC curve (AUC-ROC).  Model selection techniques like cross-validation, regularization, and information criteria (AIC, BIC) are used to prevent overfitting and choose the best model.  Regularization  Regularization techniques, such as L1 (Lasso) and L2 (Ridge) regularization, are used to prevent overfitting by adding a penalty term to the loss function based on the magnitude of the model parameters.  Lasso regularization helps in feature selection by shrinking some coefficients to zero, while Ridge regularization prevents multicollinearity by reducing the magnitude of the coefficients.  Elastic Net regularization combines L1 and L2 regularization, balancing between feature selection and multicollinearity control.  