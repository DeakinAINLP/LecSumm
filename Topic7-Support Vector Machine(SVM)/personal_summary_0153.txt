A linear regression models the relationship between variable x and output y by fitting it to a linear equation based on an observed data. Y supposedly only has one dimension since it is the output variable based on the dataset with n dimensions, x. the formation x can provide for y can be measured in covariance which indicates a value more than 0, indicates positive correlation, less than 0 indicates negative correlation and equal to 0 indicates x and y are independent.  Pearsonâ€™s correlation coefficient is another measure of linear correlation that shows positive, negative and no linear correlations based on respective values of +1, -1 and 0.  A linear equation concludes the relationship two continuous variables. Linear regression is used when the output values of the feature vectors are binary. A binary classification problem is when there are only two possible values for output, whereas a multi-class classification problem is when there are more values for an output. There are two approaches to Logistic Regression that is ignoring non-linearity and using link function that is fitting regression on a logistic link function, P(y=1|x) instead of y.  Least square regression performs badly when training data consists of excessively the squared error, where any large or small values as the minimizing sum f dependent value varying a lot from the rest of the data can have a large effect on the resulting constants.  Following training the data, performance matrices to measure the model include mean squared error (MSE), mean absolute error (MAE) or explained variance (R^2). The complexity of a model can increase with the number of features it has. Limited set of training data can result in over-fitting, whereas, limited number of features can result in under-fitting.  A logistic function is also referred to as a sigmoid function and models a logit values, that generally models log of odds based on x. when training a logistic regression model, the maximum likelihood estimate can be used to estimate the regression coefficient vector, w. Maximizing the log likelihood is equivalent to maximizing the likelihood. When computing the minimum there are two types of functions namely, convex (only deals with one optimal solution) and non-convex (deals with multiple locally optimal points).  In bias-variance trade-off, low bias can imply increased variance, whereas high bias implies decreased variance, the minimum error is at the right model complexity and identifying an a point where the risk equals to bias^2 + variance + noise is the minimum.  A regulariser helps in maintaining the parameters and to avoid overfitting. This does not allow regression coefficients to take on increasingly large values. Regularisation increases the bias in a model while reducing the variance. Linear however, regression can be used for feature selection in order to evaluate the relationship between the features and target variable.   