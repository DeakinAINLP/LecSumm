Topic6 summary  This topic has covered the linear model, the evaluation of linear and logistic model, how the performance of machine learning algorithms is measured and compared. And last, the python programming for model assessment and selection.  Linear regression attempts to model the relationship between two variables by fitting a linear equation to the observed data. We are looking for a relationship between a feature and the output, so we can use covariance to measure the linear relationship between the variable x and output y. another measurement is Pearson’s correlation coefficient, it is a measure of a linear correlation between two variables, x and y.  Linear classification is when there are only two possible values for output if there are more it’s a multi-class classification problem. To make a classification decision, we need another function that uses a fixed non-linear link function. The logistic regression is being introduced here, there are two approaches available, one is ignoring non-linearity by using least squares for classification, treat binary outputs like the outputs in the regression problem. Another is using link function, use the conditional probability of the class as the output in the regression problem.  linear regression has a closed form solution. Python implementation uses Singular Value Decomposition (SVD) to compute the Moore-Penrose inverse of matrix X. generalisation is the prediction on unseen data. The error in prediction is called the mean square error. Model complexity of linear models increases with the number of features. We should be aware of model complexity especially if we have a limited set of training data. The reason is the risk of over-fitting on this limited set of training data. Using a limited number of features may also be problematic as it could cause under-fitting.  Logistic regression is named after the function used at the core of the method, the logistic function. The logistic function is also called the sigmoid function, it’s an S-shaped curve and it can take ny real- valued number and map it into a value between 0 and 1 but never exactly at those lines. The value approaches but never reaches 0 and 1.  Model complexity can be separated into over-fitting and under-fitting. Over-fitting happens when we find an overly complex model based on the data. Under-fitting is the result of an extremely simple model. Over-fitting will happen when the model starts to capture some irrelevant noise points in the data while building the model, rather than the whole pattern. Under-fitting is the result of an extremely simple model.  A regulariser is an additional term in the loss function to avoid overfitting. It is called a regulariser since it tries to keep the parameters more normal or regular. In other words, it does not allow regression coefficients (or weights) to take excessively large values.  