Summary In this module we covered further topics in unsupervised machine learning. Specifically the focus was the curse of dimensionality and topics related to reducing it.  Curse of Dimensionality Data in high-dimension spaces can cause many issues. This is known as the curse of dimensionality. Text, image, genomic data can have 10s or even 100s of thousands of dimensions. As the number of dimensions increase, the  number of regions in the dataset increases exponentially and causes data to become sparse. The difference in distances between points becomes negligible, and the data becomes skewed to the corners of the hypercube defining the feature space. In other words, data points become less distinctive in higher dimensions, making it much harder to perform meaningful analysis.  This behaviour is defined by the concentration effect, which states that the proportional distance between the nearest and farthest points tends towards 0 in higher dimensions. The relative contrast diminishes and this leads to clustering algorithms to become effectively useless.  A key way of removing dimensions is finding unnecessary and correlated features. Unnecessary features can be removed, while correlated features can be grouped together. Dimensionality reduction aims to reduce data sets to fewer dimensions while maintaining similar information.  Eigen Values / Eigenvectors: Used in analysis of linear transformations. Goal of linear transformations in machine learning is to change the direction or shape of data while maintaining its key properties for better analysis, such as de-correlating data dimensions.  For a square Matrix M, if a scalar λ and vector u satisfy: Mu = Au, then λ is an eigenvalue and u is the corresponding eigenvector of M. For a d-sized square matrix there are d eigenvalue/eigenvector pairs. The rank of the matrix (number linearly independent vectors) determines the number of non- zero eigenvalues.  The eigenvectors can be put into a row-vector U = [u1, u2, … , ud]. This can be converted into a diagonal matrix. U is always orthogonal.  Eigenvalues for a matrix A are found with the determinant of A: det(A – λI)u = 0, where I is the identity matrix. This simplifies down to a polynomial equation in terms of λ. The results can be used to find the corresponding eigenvectors.  Decomposition Singular Value Decomposition (SVD) decomposes a matrix into three other matrices. X = USV(T). Where A is the original nxd matrix. U is an nxd orthogonal matrix S is a dxd diagonal matrix V is a dxd orthogonal matrix  To find SVD we need the eigenvalues/eigenvectors for XX(T) and X(T)X. U columns are constructed with the eigenvectors of XX(T). V columns are constructed with the eigenvectors of X(T)X). The square roots of either set of eigenvectors make up S. The diagonals in S are the square roots of the eigenvalues from U or V, and are called the singular values.  The minimum number of singular values is min (n, d), and is equal to the rank of the matrix, the number of linearly independent (uncorrelated) features.  Dimensionality Reduction A lot of datasets have monumental amounts of dimensions which leads to overfitting and data sparsity, making it harder for the model to find patterns. Dimensionality reduction speeds up analysis and improves performance by reducing the number of features.  The curse of dimensionality means that as the dimensions increases the number of data regions does so exponentially. Training data tends to reside on the surfaces of hyperspheres or corners of hypercube. As dimensions increase the distances from the original to all datapoints becomes increasingly similar. This means that the relative contrast between near and far neighbours diminishes and has less distinctive distances. Thus, this causes the relative distances to become negligible in sufficiently high dimensions. This is known as the concentration effect.  Solutions are to reduce dimensionality. Irrelevant (or dubiously relevant) features can be safely removed to reduce dimensions. Correlated variables are redundant and can be combined to reduce dimensions. Dimensionality reduction helps combat the curse of dimensionality.  One method is to use projection vectors. A projection vector is multiplied by the original vector to produce a lower-dimensional output: X => X’ that is mapped in the direction of maximum variance in data.  Principle Component Analysis: PCA is a deterministic algorithm used for dimensionality reduction. It converts potentially correlated data into a lower-dimensional dataset mapped to uncorrelated axes, known as principal components. The axes are linear combinations of the original dimensions, sorted by descending order of captured variance. That is the highest-variant axes are prioritized.  PCA assumes the first axis vector (u1) is a unit length vector (magnitude of 1), allowing the other axis vectors to be projected onto it. Essentially, we need to find the eigenvalues of the dataset matrix in order of size, and their corresponding eigenvectors. The eigenvector matrix is sorted by eigenvalue size to generate the orthogonal (and thus uncorrelated) axes in descending order of variance.  The principal axes can be used to reduce dimensions. As the decorrelated data has been organized in terms of maximum variance, dimensions can be effectively and safely reduced by dropping the minimal-variance features. Thus the number of dimensions can be reduced while maximising the total variance.  PCA uses Eigen-Value Decomposition (EVD). This finds C = UDU(T), where C is the covariance matrix of the original dataset. SVD can also be used for PCA, as the U in EVD is the same as the U in SVD. PCA can be performed via EVD or SVD. For N data points and D dimensions, EVD has a runtime of O(D^3) while SVD has a runtime of O(N^3), so the optimal choice depends on if there’s more dimensions than datapoints.          