 This topic covered dimensionality in data; what it is, and how to reduce it. Dimensionality of data can rely on several things and comes in part from feature representation. Some apparently-simple data may have thousands of dimensions. High-dimensional data is known to be sparse for various reasons; the main reason for this is known as the concentration effect where higher dimensional data loses contrast in relative near and far distances from other data points, which can make clustering practically ineffective. A method for reducing dimensionality is finding features that might be the same and converging them to a single feature. Linear transformation was covered as it is involved in several dimensionality reduction methods. Principal Component Analysis (PCA) aims to find the directions of maximum variance in high- dimensional data and projects it onto a new subspace with equal or fewer dimensions. This process uses linear transformation (a very interesting topic). There are several methods for implementing PCA. Another interesting form of dimensionality reduction is called Independent Component Analysis (ICA). Some other more advanced methods were mentioned, and remaining modules contained python implementations.  Quiz Result     