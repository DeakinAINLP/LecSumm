  Data dimensionality  §  §  §  Text data: Imagine a  news website.  If you start crawling news on your Web  site in a short period of time, such as one topic, you can crawl 10,000   It is common to have dimensions that exceed . This number is the size of the dictionary that should be built on the basis of words extracted from news documents. Each document must be represented based on a word  in the dictionary  (  see Feature vectors covered in topics 1 and 2: remember the data representation )。  Image data: Suppose you want to use pixels as features.  A 64×64 image has 4,096 dimensions. Genomic data: Take  Parkinson's disease's case-control data as an example.  There are 408,803  single nucleotide polymorphisms (SNPs) and  380,157  SNPs for Alzheimer's disease.   Dimensional Curse    Dimensional curses occur when machine learning algorithms are applied  to high-dimensional data.    Machine learning faces unique challenges in analyzing and organizing data in high-dimensional spaces. As dimensions increase,  the amount of space increases rapidly, resulting in sparse data available. This is very problematic because we don't have enough data locally.  At its core, the Dimension Curse, indicates that as the number of dimensions increases, the number of regions increases exponentially. As the number of regions increases and the space increases, the space for each data point increases. This makes the data sparse and somehow  useless.     That is why the usefulness of means of distinguishing near and distant neighbors decreases. As the dimension increases, the relative contrast between neighbors and distant neighbors decreases. This is known as the concentration effect of distance  scales .  This issue can mean the following:  §  §  Clustering or the KNN  algorithm may not make sense at a high level. However, higher- dimensional patterns may still exist. We just need a better distance metric. So research is needed! Until we develop better distance metrics, we should aim to reduce the dimension as much as possible.  o  Break the curse o  o  In some cases, there are too many variables. But do all variables matter?  If not some of them are irrelevant to our purpose and can be removed. If all variables are numeric, what if they are correlated?  If they are exactly the same, this means redundancy! Can you group them? o  Dimensional curses require dimensional reduction. Dimensionality reduction refers to the process of transforming a set of data with a large number of dimensions into less dimensional data, while ensuring that similar information is conveyed concisely.   Eigenvalues and eigenvectors       Singular value decomposition   nomination  The purpose of PCA is to take n data points in the d dimension that may be correlated and summarize them in a new set of uncorrelated axes.    Uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original d-dimensions. The first k components capture as much variation (or variance) between data points as possible. But first, let's see how to define the variance for each variable. Variance for each variable Data is represented as a point cloud in multidimensional space with one axis for each variable. The centroid of a point is defined by the average of each variable. The variance of each variable j is the mean squared deviation of n values around the mean of that variable.  Co-dispersion between variables  Simply put, covariance is a measure of how changes in  one  variable relate to changes in the second. The degree to which variables are linearly correlated is expressed by the  covariance of the variables.   PCA formulation and principal  component derivation         PCA Implementation  There are several alternative ways to implement PCA.  PCA n<d of data  The number of data points (n) is less than the number of dimensions d i.e. n<d  Consider the following scenario:  Say we have 100 images with 64×64 dimensions, =100 and =64×64=4096. In this case, the number of non-zero eigenvalues in the data covariance matrix is:  (cid:0).  § § §  When you use eigenvalue decomposition (EVD)  with a covariance matrix of sizes, you must  perform calculations on the degree of ×.  〇((cid:0)3).  This may be too expensive! In such cases, SVD computes less than or equal to  〇(3).  §  Using SVD for PCA  You can use SVD to run  PCA. As we saw in the previous section, the n×d matrix and the singular value decomposition (SVD) are given by:  Y=USV^T    Where:  § § §  U is n  ×d orthogonal matrix (same as U in the previous section) S is  a diagonal matrix  S(i.i)=δi with d×d elements V. is  d×d orthogonal column  Now if yo is the  mean-centric version of  x the  covariance of Y is:  (n−1)C=YY^T=US(Ⅴ^TⅤ)SU^T=USISU^T=US^2U^T  Remember V.^TV.=I, therefore:  What we've done is create a connection between SVD and EVD. Also U SVD   EVD. Therefore, the singular vector of SVD is the same as the eigenvector of EVD, and D=s^2/n-1 we have the relationship λd=δd^2/n-1.  If you don't want to use EVD, you can use SVD to get the matrix.  Or take a singular value and calculate the eigenvalue ( = 2 −1). This will give you what you need to run PCA.  Remember that performing a PCA is only a multiplication.   Examples of using PCA  in facial image  analysis  Many studies have used PCA  to reduce the dimensionality of facial recognition problems. In this example, each image in the data set is represented by a vector of size 1024  (for example, each image is represented by 1024  pixels  ).  The process described in the video involves the following steps:  1.  Generating a covariance matrix for data Find the principal eigenvector that represents your data 2. 3.  When calculating the energy of the face image  saving k principal eigenvectors are used 4.  Projection data after save only k variation axes (use only k  principal eigenvectors)  You will also see how the final image quality is affected by different factors. Why choose K values and smaller values? It may be beneficial for the K classifier.        Other Dimensionality Reduction Techniques  Independent Component Analysis (ICA).  ICA is a method of separating multivariate signals into independent non- Gaussian components.  In signal processing, machine learning, and neuroscience, ICA is commonly used to isolate mixed signals, such as complex audio and image signals.  The goal of ICA is to find a set of basis functions that capture the underlying source of a signal, and use these functions to isolate the signal into its constituent parts.  Nonlinear Dimensionality Reduction Method  Nonlinear methods use more sophisticated mathematical techniques to identify and capture the underlying structure of data, as opposed to linear methods that use linear algebra to identify patterns in data.  t-SNE (t-Distributed Stochastic Neighbor Embedding  ) techniques are used to visualize high-dimensional data in two or three dimensions. It works by finding patterns and relationships in the data and representing those patterns in a low-dimensional space to reduce the complexity of the data.  