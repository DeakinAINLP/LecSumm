In last topic‚Äôs module, we were learning about clustering using KMeans. We were only looking  at dataset that is only 2 Dimensions, but what if the feature or dimension of the dataset increase? When a feature/dimension of a dataset increase, the region of the dataset exponentially increases. The larger the dimension the more the data will be come sparse which makes it hard to cluster and predict pattern of the dataset.  In this topic around, we will focus on:  -  Dimensionality Reduction - Eigen Value and Vectors - PCA also known as Principal Component Analysis which is used for dimensionality reduction and visualizing those dimension data. Then we will be implementing these in Python  - Dimensionality Reduction  Dimensional Space in a data exist everywhere on the internet, when searching for something, it takes in a lot of features and return a lot of websites. If the features are close or larger than the number of websites, then we consider that to be a high dimensional dataset. If you search up or research about Dimensional Dataset, the thing you would see the most would be about the Curse of Dimension. As I said earlier, the region of the data exponentially increases as the dimension increase. This makes the data point from each instance looks close to each other, but, it‚Äôs in a different region making it sparse and hard to cluster.  However, when looking at a dataset, not all features of a sample is important, so we can take  those variables out. This will allow the dimension to decrease and keeping only the important ones. But the problem is, taking out too many features might make it hard to predict patterns or describe the information as well.  In the figure below you can see how as dimension increase, the Euclidean distance between  each data point has increase, but the standard deviation decrease. Standard Deviation determine the dispersion of the dataset. The problem is that usually SD is dependent on Mean and Median as it moves how those two moves. This is due to the fact the dimension increase, causing concentration effect. This explains that the dataset loses its ability to determine close and far targets, making it hard to cluster and making use of KMeans as dimension increase.  So how do we fix this? The first key was to remove unimportant feature which will allow you to decrease the dimension. The next step is to find redundancy in the data, this will reduce the amount of samples, while keeping the same information. If there‚Äôs also a redundancy of the feature, we can also just use one feature, allowing us to decrease the dimension as well.  Eigen  Another method of decreasing Dimension of a dataset is using Eigen. There are two instance we  need to know, Eigen vector and Eigen Value. Eigen value will be a constant and eigen vector will be the corresponding vector of the original one. So what this does is basically decreasing the original vector dimension but still keeping the same direction and factors. The eigen value will scale the vector size.  (A is original Vector, u is Eigen Vector, Lambda is eigen value)  Finding Eigen values and vectors are a bit tricky, although there are some steps procedures  allowing us to calculate it, but as dimension increase, the harder it is, since there are more eigen value roots.  Besides using Eigen value to scale down matrices, there are a two more other method we will  learn in this module to decrease dimensional matrix.  Singular Value Decomposition  SVD can be used for Dimensional Reduction as it divides the original matrix into three matrices to untangle data and makes it easier to predict pattern. The advantage of using SVD is that we can also use it on a non-squared matrix. The formula for this is ùëã = ùëàùëÜùëâùëá where U and V is the orthogonal  eigenvector and S is the eigen value that can only be positive/zero which is the roots from U and V. Note: U and V has the same eigen values.  Principal Component Analysis PCA is an unsupervised learning in machine learning that can also be used for dimensional  reduction. The main used of PCA is to be used for visualizing data to find the variance and covariance in the dataset. The point of using PCA is to remove useless features and captures only the importance. This will decrease the dimension and quality of a data, but it will help you identify the correlation. One real- life use of PCA is FaceID. If FaceID were to record the exact same face every single time, it would be hard for the machine to recognize your face every time. But if were drop the details and record only the importance, we would be able to increase the success rate of scanning faces. We can find PCA by using Eigen Value Decomposition, where we find the Covariance of the matrix.  