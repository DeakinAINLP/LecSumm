In topic four, I learnt about clustering and dimensionality reduction. Previously we learnt about clustering that means grouping data that are alike. Clustering is done to label it and make it describable. It is about supervised learning. In unsupervised learning we use Kmean method in clustering whereas in supervised learning we use K-nearest neighbor. Kmean partition data into k clusters. It randomly selects a datapoint and then measure the distance accordingly. The second algorithm of clustering is hierarchical clustering which creates a hierarchy of smaller clusters. The third algorithm DBSCAN groups data points into clusters based on their proximity to each other.  An eigenvector is a vector that when multiplied by matrix results only in a scalar multiple of the original vector. This scalar multiple is known as the eigen value of the matrix. Essentially, an eigenvector is a special vector of the matrix that remains in the same direction, while only being scaled by a factor. In machine learning both eigenvector and eigenvalues are used in techniques such as principle component analysis and singular value decomposition to reduce the dimensionality of dataset. Eigenvectors are vectors that do not change direction when a linear transformation is applied to them, except for scaling. In other words, when a matrix is multiplied by an eigenvector the result is a scalar multiple of the original eigenvector. Eigenvectors are commonly used in principle component analysis to reduce the dimensionality of data by projecting it onto a lower- dimensional subspace. However, singular value decomposition, is a factorization of a matrix into three matrix: U, E and V. Where U and V are orthogonal matrices, while E is a diagonal matrix containing the singular values of the original matrix. SVD can be used for dimensionality reduction, but it can also be used for other tasks, such as matrix inversion, linear regression and image compression.  In machine learning we have different types of data, and there are different types of dimensionalities in different data. Text data, image data and generic data are typical dimensions of data. Similarly, when we increase the dimensionality of data the complexity of data increase.  Dimensionality in machine learning refers to several features of attributes that are present in a dataset. The types of dimensionalities in data are as follow:    Low-dimensional data: data with a relatively smaller number of features or attributes is considered low-dimensional. This type of data is easy to work with and visualizing it is usually straightforward.    High- dimensional data: Data with a large number of features or attributes is    considered high-dimensional. It can be challenging to work with since it is difficult to visualize and analyze. Intrinsic dimensionality: The intrinsic dimentionality of dataset is the number of dimension required to represent the essential information in the data. It often lowers than the number of features.    Curse of dimensionality: It refers to the difficulties that arise when working with  high-dimensionality data. As the number of dimensions that arise when working with high-dimensional data.  The curse of dimensionality can be removed by following various steps:    Feature selection: it’s the process of selecting a subset of the most informative  features from the original dataset. It help reduce the dimensionality of data while retaining the most important information.    Feature extraction: it’s the process of transforming the original features into lower-  dimensional space that still remains the most of the important.    Regularization: it’s the process that adds penalty tern to the objective function being  optimized.    Clustering: its techniques that groups similar data points together.  Independent component analysis is a technique which is used to feature extraction and dimensionality reduction. It is a statistical technique that can be used to separate a multivariate signal into independent, non-Gaussion components. It can also be used to extract underlying signals from a mixture of signals. For instance, an audio signal, from different sources, such as speed, music and background noise can be mixed together. So, in that case ICA, can be used to separate these source into independent components, which can be further analyzed. The basic idea behind ICA is to find a linear transformation of the data that maximums the statistical independence of the resulting components.  T-SNE in machine learning is used for visualizing high-dimensional data in a low- dimensional space. It is particularly useful for exploring and understanding complex dataset which cannot be easily visualized in their original form. The t-SNE algorithm works by first computing a similarity matrix between all pairs of data points in the high-dimensional space. It then construct a probability distribution over pairs of points in the low-dimensional space. And aims to minimize the difference between the high-dimensional and low-dimensional probability using an optimization algorithm such as gradient descent. t-SNE has been used in various machine learning applications including image recognition, natural language processing and bioinformatics.  