Dimensionality in Data Text information: Picture a news website. It is common to have more than 10,000 dimensions if you start  crawling  the  news  on  the  website  for  a  litle  period,  like  a  topic,  depending  on  how  many documents you crawl. This  ﬁgure is the size of the dictionary you must create using  the terms you pulled out of the news documents. Remember the feature vector discussed in topics 1 and 2: Data representation? We must represent each document using words from a dictionary.  Image data: Just a 64x64 picture would have 4,096 dimensions if pixels were used as features.  Genomic  data:  As  an  illustration,  consider  case-control  data  for  Parkinson's  illness.  It  possesses 408,803 single-nucleotide polymorphisms (SNPs), compared to 380,157 SNPs in Alzheimer's disease.  Curse of Dimensionality When machine learning algorithms are used on highly dimensional data, The Curse of Dimensionality occurs. When assessing and arranging data in high-dimensional areas, machine learning presents a set of particularly challenging issues. The volume of the space grows so quickly as the dimensionality rises that the accessible data become sparse. Given the dearth of local data, this is quite problematic.  Solving the Curse Too many variables might be present in some issues. But do all variables mater? If not, we can delete some of them as they are not necessary for our goal.  What if the variables are associated if they are all numerical? Redundancy arises if they are identical in every way. Could we combine them?  The solution to The Curse of Dimensionality is Dimensionality Reduction. Dimensionality reduction is the process of transforming a piece of data with many dimensions into data with fewer dimensions while ensuring that it still eﬀectively communicates the same information.  Eigenvalues and Eigenvectors Eigenvalues are a unique  collection of scalar values connected to a set of linear equations that  are most likely seen in matrix equations. The characteristic roots are another name for the eigenvectors. It  is  a  non-zero  vector  that,  a(cid:332)er  applying  linear  transformations,  can  only  be  altered  by  its  scalar component.  Singular Value Decomposition The length and breadth of the modiﬁed square are the singular values that the term "singular value decomposition" refers to, and they contain a wealth of information. For instance, our transformation ﬂatens  our  square  if  one  of  the  singular  values  is  0.  Additionally,  the  greater  of  the  two  unique numbers provides information on the maximal "action" of the change.  PCA, or principal component analysis. To project the data into a reduced dimensional environment, linear dimensionality reduction is used. Before using the SVD, the input data is scaled but not centred for each feature.  Independent component analysis (ICA) A multivariate signal can be divided into separate, non-Gaussian components using the ICA technique. ICA  is  frequently  used  in  signal  processing,  machine  learning,  and  neuroscience  to  separate  mixed signals, such as in a complicated sound or picture signal. To decompose a signal into its component pieces, ICA aims to identify a set of basic functions that capture the signal's underlying sources.  Nonlinear dimensionality reduction technique In  contrast  to  linear  approaches,  which  employ  linear  algebra  to  ﬁnd  paterns  in  data,  nonlinear techniques  use  more  complex  mathematical  methods  to  recognise  and  capture  the  underlying structure of the data.  For  the  purpose  of  visualising  high-dimensional  data  in  two  or  three  dimensions,  the  t-SNE  (t- Distributed  Stochastic  Neighbour  Embedding)  approach  is  utilised.  In  order  to  simplify  the  data,  it works  by  identifying  paterns  and  correlations  in  the  data  and  then  portraying  those  paterns  in  a lower-dimensional space.  uMap: uMap is a new technique for dimensionality reduction which works almost the same as tSNE. However, it has some added advantages over tSNE. To understand and learn more about uMap, please refer to the following resources.     