 After studying clustering, we had to go though some key areas such a: solving the curse of dimensionality, eigenvalues and eigenvectors, principal Component Analysis (PCA) and Python programming: implementing PCA during topic 4.  In regard to dimensionality, we are dealing with text, image and genomic data. When applying machine learning algorithms to highly dimensional data, when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This is what we called Curse of Dimensionality. We can solve this by converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely. Also, I have learnt how to find eigenvectors and eigenvalues of a matrix to be used in the analysis of linear transformations.  SVD is another method of decomposing a matrix into three other matrices. Definition of variance, covariance, decorrelation was studied along with their equations as well. Also, we can perform PCA with eigenvalue decomposition by computing data covariance matrix, performing Eigen value decomposition and reducing dimension data.  There are several ways to implement PCA an using SVD to perform PCA is a one we had to study in topic 4. Also, there was an interesting topic of how PCA is used to analyze facial images. To separate a multivariate signal into independent, non-Gaussian component we can use independent component analysis (ICA). Nonlinear techniques, as opposed to linear techniques, which use linear algebra to identify patterns in data, use more sophisticated mathematical techniques to identify and capture the underlying structure of the data.          