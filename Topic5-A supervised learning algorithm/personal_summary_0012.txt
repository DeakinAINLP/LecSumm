 Dimensionality in Data There are many issues that arise when analysing and organising data in high-dimensional spaces. Let us look at some typical dimensions of the data we’re dealing with:   Text data: Imagine a News website. If you start crawling the news on the website for a short period of time such as a topic, depending on the number of documents you crawl, it is typical to have more than 10,000 dimensions. This number is the size of the dictionary you have to build based on the words you extracted from the News documents. We need to represent each document based on the words in a dictionary (remember. the feature vector covered in topics 1 and 2: Data representation). Image data:  Imagine we would like to use pixels as features, just an 64×64 image would have 4,096 dimensions! Genomic data: Take Parkinsons disease case-control data as an example. It has 408,803 Single-nucleotide polymorphisms (SNPs) and Alzheimer’s disease has 380,157 SNPs. Curse of Dimensionality The Curse of Dimensionality is a challenge that arises when applying machine learning algorithms to high-dimensional data. As the number of dimensions increases, the available data becomes sparse, resulting in less useful information for analysis. This leads to problems such as difficulty in finding local neighbours and the exponential growth of regions in the data space. Describing objects or distinguishing between them becomes increasingly complex with higher dimensions, making it challenging to handle the computational load. Furthermore, the curse of dimensionality leads to less distinctive distances in high-dimensional spaces. The relative distance between points far from a given point and those close to it becomes negligible, reducing the discriminative power of distance measures. This effect is known as the concentration effect. As a result, clustering or k-nearest neighbors (KNN) algorithms may lose their meaning in high dimensions, necessitating the development of better distance metrics. To mitigate the curse of dimensionality, it is advisable to reduce dimensionality whenever possible until better distance metrics are developed. This can help improve the usefulness and efficiency of machine learning algorithms applied to high-dimensional data. Solving the Curse Solving the curse of dimensionality requires dimensionality reduction techniques. Dimensionality reduction involves transforming high-dimensional data into a lower-dimensional representation that preserves relevant information. One approach to dimensionality reduction is feature selection, where irrelevant or redundant variables are identified and removed. If all variables are numeric and highly correlated, they can be grouped together to eliminate redundancy. An example of dimensionality reduction is demonstrated with 2D data. By observing that two features are identical, we can use a projection vector to transform the data onto a one-dimensional axis, reducing the dimensionality. The projection vector is chosen based on the direction of maximum variance in the data, which captures the most informative aspects. In another example, data points are shown to lie on a noisy line. The vector pointing towards the direction of highest variance captures more information when projecting the data, allowing for better analysis. Similarly, when data points lie on noisy curves or shapes, more advanced techniques like kernel principal component analysis can be used to handle nonlinear problems. While the examples focus on linear dimensionality reduction, it's important to note that there are machine learning methods available to address nonlinear problems as well. Eigenvalues and Eigenvectors Eigenvalues and eigenvectors are important concepts in the analysis of linear transformations. For a square matrix, an eigenvalue is a scalar that, when multiplied by the corresponding eigenvector, gives the same result as multiplying the vector by the matrix. In a matrix of size n, there can be at most n eigenvalue-eigenvector pairs. The number of nonzero eigenvalues is equal to the rank of the matrix. The eigenvalues and eigenvectors satisfy a specific equation that relates them to the matrix. This equation can be collectively written using matrices. The matrix of eigenvectors is always orthogonal, meaning it rotates the coordinates to decorrelate the data dimensions. When applied as a linear transformation, an orthogonal matrix either rotates or reflects the vector without changing its length. To find the eigenvalues of a matrix, the characteristic polynomial is solved. The eigenvalues are the solutions to this polynomial equation. Once the eigenvalues are obtained, the corresponding eigenvectors can be found by solving a system of equations. Singular value decomposition Singular value decomposition (SVD) is a matrix factorization method that decomposes a matrix into three constituent matrices. The original matrix is represented as the product of a left singular matrix, a diagonal matrix of singular values, and a right singular matrix. The SVD provides a way to express the original data in a coordinate system where the covariance matrix is diagonal. The diagonal elements of the singular value matrix represent the singular values of the matrix. The number of nonzero singular values is equal to the rank of the matrix and is less than or equal to the minimum of the number of rows and columns. Calculating the SVD involves finding the eigenvalues and eigenvectors of the original matrix and its transpose. The eigenvectors of the original matrix form the columns of the left singular matrix, while the eigenvectors of the transpose matrix form the columns of the right singular matrix. The singular values are the square roots of the eigenvalues and are arranged in descending order in the diagonal matrix. SVD is a powerful tool in linear algebra and has various applications in fields such as data analysis, image processing, and machine learning. It provides a compact representation of data and allows for efficient manipulation and analysis. Preliminaries Principal Component Analysis (PCA) is a technique used to summarize a set of correlated data points in high dimensions by finding a new set of uncorrelated axes called principal components. The goal is to capture as much of the variation in the data as possible with the first few principal components. Variance is a measure of the spread or dispersion of data points along a particular variable. The variance of each variable is calculated as the average squared deviation of its values from the mean. Covariance measures the relationship between two variables and indicates how changes in one variable are associated with changes in another. The covariance matrix contains variances on the diagonal and covariances between pairs of variables in the off-diagonal entries. PCA aims to decorrelate the original dimensions by rotating them to a new set of axes. The principal axes are ordered in terms of capturing the highest variance first, and they are uncorrelated with each other (orthogonal). This transformation allows for a concise representation of the data with fewer dimensions while retaining most of the information. By applying PCA, we can reduce the dimensionality of the data, remove correlation among variables, and extract the most important features that contribute to the overall variation in the dataset. This can be useful for data visualization, feature selection, and dimensionality reduction in various data analysis and machine learning applications. Formulation of PCA and deriving principal components Principal Component Analysis (PCA) involves finding the directions of maximum variance in a dataset. The first step is to project the data onto a unit-length vector, which represents the direction of maximum variance. The variance of the projected data is then calculated. To maximize the variance, a Lagrange multiplier is introduced, and the problem is transformed into an unconstrained maximization problem. Taking the derivative and setting it to zero leads to an eigenvalue problem, where the largest eigenvalue and its corresponding eigenvector represent the first principal component. To find the subsequent principal components, which are orthogonal to the previous ones, the process is repeated iteratively. The principal axes can be collectively expressed using the eigenvector matrix in the order of decreasing eigenvalues. By projecting the data onto the top principal components, dimensionality reduction is achieved while maintaining uncorrelatedness among the new dimensions. PCA can also be performed using eigenvalue decomposition. The data covariance matrix is computed, and eigenvalue decomposition is applied to obtain the reduced dimension data. The reduced dimension data is obtained by multiplying the original data with the eigenvectors in the decreasing order of eigenvalues. An alternative formulation of PCA is based on minimizing the projection error. The goal is to find a set of mutually orthogonal axes that minimize the mean square error due to projection onto the new dimensions. It is found that the top eigenvectors of the covariance matrix provide the optimal solutions for this error minimization. Overall, PCA provides a way to summarize and reduce the dimensionality of data while capturing the most significant information and maintaining uncorrelatedness among the new dimensions. Implementation of PCA PCA can be implemented in different ways depending on the characteristics of the data. One scenario is when the number of data points is less than the number of dimensions. In such cases, performing Eigen Value Decomposition (EVD) directly on the covariance matrix can be computationally expensive. An alternative approach is to use Singular Value Decomposition (SVD) to reduce the computations significantly. By applying SVD to the mean-centered data matrix, a connection is established between SVD and EVD. The singular vectors obtained from SVD are equivalent to the eigenvectors from EVD, and the singular values correspond to the square roots of the eigenvalues. Therefore, by using SVD, we can obtain the singular vectors or singular values and compute the eigenvalues to perform PCA. To perform PCA using SVD, the data matrix is multiplied by the matrix of singular vectors or singular values. This transformation projects the data onto the principal components, where each principal component represents a direction of maximum variance. Overall, PCA can be implemented using SVD, which provides an efficient way to perform the calculations, especially when the number of data points is smaller than the number of dimensions. PCA allows for dimensionality reduction and decorrelation of the data, capturing the most important information and simplifying its representation. Independent component analysis (ICA) ICA is a method of separating a multivariate signal into independent, non-Gaussian components. In signal processing, machine learning, and neuroscience, ICA is commonly used to separate signals that are mixed together, such as in a complex sound or image signal. The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then use these functions to separate the signal into its constituent parts. Nonlinear dimensionality reduction technique Nonlinear techniques, as opposed to linear techniques, which use linear algebra to identify patterns in data, use more sophisticated mathematical techniques to identify and capture the underlying structure of the data. The technique of t-SNE (t-Distributed Stochastic Neighbor Embedding) is used to visualise high-dimensional data in two or three dimensions. It works by finding patterns and relationships in the data and then representing those patterns in a lower-dimensional space to reduce the complexity of the data. uMap uMap is a new technique for dimensionality reduction which works almost the same as tSNE. However, it has some added advantages over tSNE. To understand and learn more about uMap, please refer to the following resources. 