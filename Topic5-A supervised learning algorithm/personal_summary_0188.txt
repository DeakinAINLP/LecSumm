When applying machine learning algorithms to highly-dimensional data, the curse of dimensionality arises.  the curse of dimensionality, dictates that as the number of dimensions increases, the number of regions grows exponentially. In high dimensional spaces, most of the training data resides in the corners of the hypercube defining the feature space. The concentration effect of the distance measure is defined when relative contrast between near and far neighbours diminishes as the dimensionality increases. Dimensionality reduction refers to the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely.  Eigenvalues and eigenvectors are prominently used in the analysis of linear transformations. For a given square matrix A if a number λ and a vector u satisfy the condition Au=λu  then λ is called an eigenvalue and u is the  corresponding eigenvector of  A . When applied as a linear transformation to a vector, every orthogonal matrix will either rotate or reflect the vector without changing its length.  Singular value decomposition (SVD) is a method of decomposing a matrix into three other matrices: X = USV-T. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. The goal of PCA is to take n data points in d dimensions, which may be correlated, and summarises them by a new set of uncorrelated axes. The uncorrelated axes are called principal components or principal axes.  Covariance is a measure of how changes in one variable are associated with changes in a second variable. The covariance matrix is a matrix that contains variances of all variables on the diagonal and co-variances among all pairs of variables in the off-diagonal entries.  ICA is a method of separating a multivariate signal into independent, non-Gaussian components. The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then use these functions to separate the signal into its constituent parts. The main objective of PCA is orthogonal transformation of given data into its principal components  