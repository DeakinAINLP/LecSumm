 ● The curse of dimensionality is infamous in the Machine Learning (ML) community but is also one of the most common obstacles in the course of developing an ML model. This is because when the data is recorded, it is usually done with an attempt to gather as much information as possible. While this ultimately proves useful, it does produce a challenge where we need to find ways to concentrate the amount of information in a simpler manner which can then be used to draw meaning out of  ● Fortunately, over the years, the ML community has developed ways to reduce the number of dimensions required and still retain most of the information. The first technique usually taught is Principal Component Analysis (PCA)  ● But to understand PCA first we need to get familiar with Eigenvectors and Eigenvalues. They are an important case in linear transformations and are used widely among the many fields of technology. A linear transformation when applied to unit vectors, transforms the unit vectors and changes their magnitude and direction. The amount by which these values change is given by the Eigenvalues and the vectors they result in become the Eigenvectors for that linear transformation  ● Since the data can be represented in a matrix form it becomes necessary to process such high-dimensional data. And to do this we can use Singular Value Decomposition (SVD) which breaks down the target matrix into 3 matrices which makes it computationally efficient to process. This is because the middle of the three matrices is a diagonal matrix which is extremely easy for a computer to process  ● PCA can make use of either of the above mathematical concepts to perform the  analysis. The variance of each data point from the center of its cluster is calculated and a covariance matrix is generated and then decomposed using SVD or eigen decomposition. The resulting axes are the principal components of the data which capture the highest variance in descending order. These axes are uncorrelated to each other i.e they are orthogonal to each other.  ● But dimensionality reduction has its cons as well. We have to understand that in the  process of applying PCA on a dataset we tend to lose some information. Depending on the dataset we can lose anywhere between 15% to 40% of the information.  ● Surprisingly this isn’t as bad as it seems because you do not want your model to overfit  the dataset which may result in biases. Image processing gives a good visual understanfing of this concept. During face recognition, any dataset containing images can have thousands of pixels representing a single image and multiple features can be drawn from this.  ● If the user of this face recognition model wears glasses or grows a beard, the model  should not fail to recognise the face. Losing some of the information during PCA actually makes sure that minor changes on the face do not affect the final result of the model prediction.  ● There are other dimensionality reduction techniques such as Idependent Component  Analysis (ICA), t-distributed Stochastic Neighbor Embedding (t-SNE) and uMap (which is an improved version of t-SNE). All of these methods should be considered when working with high dimensional data  