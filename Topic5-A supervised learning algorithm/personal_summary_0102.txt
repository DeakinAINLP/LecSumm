 Main learning points:  Principle Component Analysis (PCA) is for:  1.  Dimension reduction 2.  Data visualisation 3.  Feature extraction  PCA can represent multiple features with some loss in the variance:    Projects datapoints onto principal components’ axis, which are defined to capture the  maximal variance in the data and have minimal correlation with other principal components.   Principal components can be regarded as the ‘new features’. Since there are less PCs than  original features, the model can work with a lower dimensional space.  Check the variance ratio explained by each PC to see how much variance is captured and lost, given n_components.  T-SNE: Reduces high dimensional dataset to low dimensional graph:  1.  Determine similarity of all points on the scatterplot 2.  Technique is not linear.  Procedure of t-SNE (example for 2D to 1D):  Getting the ‘similarities’:  1.  Construct a normal probability distribution, where a given point is the mean. 2.  All other points of the 2D plot will each have its corresponding x-value on this distribution.    This ‘x-distance’ is the Euclidean distance from the point of interest on the 2D plot.   If a point is similar (close) it will map to a higher probability.  3.  Scale all the probabilities so that its sum equals one.    These are the similarity scores for the given point of interest.  4.  Get the similarity scores for every point (treat every point as the point of interest). The  similarities scores can be summarised in a table form like below:  ‘Recreating’ the similarities at lower dimension:  1.  Randomly project all points of the 2D plot onto a 1D line. 2.  Calculate similarities of the all the points on this 1D line.   This time, use the t-distribution’s respective probabilities as the similarity scores (after  scaling).  o  The t-distribution has a heavier tail, which helps disperse the clustering.  3.   Results can be summarised as a table form:  4.  Incrementally through multiple steps:   Points on the 1D line move towards its similar points and away from dissimilar points.   This movement causes the ‘second table approaches the first table’.  The parameter, ‘Perplexity’, relates to a neighbouring system which reflects the density surrounding a given point. Tends to be larger given a larger sample size.  