Summarise the main points that were covered. Main points covered in topic 4 of the unit content:    Continued learning about unsupervised learning, more specifically dimensionality reduction.   Eigenvalues and Eigenvectors.   PCA (Principal component analysis).  Implementation of PCA via python programming in the topicly tasks.  Reflection My reflection on the knowledge gained this topic from reading the unit contents for this topic with respect to machine learning.  This is a reflection on the knowledge that I gained during topic 4 in regard to machine learning. Above I also listed the main points that were covered in the unit content, that also doubles as knowledge related to machine learning that I gained this topic.  This topic I continued learning the topic of unsupervised learning which focussed on learning concepts about dimensionality reduction.  I was introduced to the interesting concept called ‘the curse of dimensionality’. When data is multi- dimensional, combinations within the dataset are less frequently observed due to the large number of possibilities. It's therefore difficult to gain insights from the frequency of combinations that appear, as many combinations may never arise due to the extremely large number of possible combinations.  When data becomes more multi-dimensional, it also can by nature become very sparse. Very sparse data also becomes less useful. A problem with this sparse data is that a data point may have no nearby neighbours, meaning the idea of distance is less useful when analysing the data. However, the increase in data sparsity also correlates with the likely hood of observing more hubs within the data that could provide insights.  I also gained knowledge about PCA (Principal component analysis). PCA enables the transformation of multi-dimensional data into a dataset with less dimensions. For instance, a data set with multiple dimensions can be transformed into a data set to contain fewer dimensions. This way the data is more useful to gain insights from. In summary, this is achieved though calculating the principal component axis line within the graph for each feature value in the data set, and then selecting the principal components that contain the highest amount of variation. By selecting the first 2 (or however many suit the requirements) with the highest variation, the aim is to obtain a dataset with the highest possible amount of information from the original dataset, while also being useful to work with.  I also learnt about eigenvectors and eigenvalues. An Eigenvector is a representation of the principal component axis line, whereas the eigenvalue is its corresponding value. An Eigen value can be found for each principal component and is found by using the average of the sum of the squared distances. These distances that make up the sum of the squared distances are found via Pythagoras theorem and are the distance from the projected point to the origin. The aim of having eigenvectors and  eigenvalues is to use them for dimensionality reduction. The new dimensionally reduced dataset contains important information from the dataset.  