Dimensionality  A dimension is a data feature  Many data sources have a high number of dimensions, which makes analysing the data diﬃcult or  infeasible. For example:  A word frequency dictionary generated from a few news articles  The pixels in an image of even low quality vga image of 640x480 pixels - a total of 307200 pixels  Curse of Dimensionality  Data sets with many dimensions suﬀer from the curse of dimensionality. As the number of dimensions increases, the possible values of a given data point increases exponentially.  For example, consider a data set where each dimension has x possible values.  In one dimension, a data point has x possibilities. In two, it has x2 the data point has xn options.  Because the available data space increases exponentially, as the number of dimensions increases our data becomes more sparse.  Sparse data is of reduced value to determine reliable as it becomes harder to analyse. This is because the distances between data points become less distinct.  As such, the need for data may also increase exponentially with the number of dimensions.  Solving the curse  The general approach to solving the curse of dimensionality is to reduce the number of dimensions.  Irrelevant dimensions can be removed from the data.  Redundant dimensions can be removed. For example, if a dimension is repeating data already  captured in a another dimension it is not needed.  If some dimensions are correlated, then they can combined into a single dimension while still  conveying information about diﬀerent data points.  This technique can also be applied to data that has a noisy linear relationship by ﬁnding the direction  of maximum variance to maximise information available from the data whilst reducing its  dimensions.  Eigenvectors and Eigenvalues  Eigenvector is the vector representing the axis of rotation for a given transform.  Eigenvalue represents how much the initial vector is scaled during the transform   Principle Component Analysis (PCA)  Principle Component Analysis is the process of identifying the primary factors that cause variation in  data points in order to maximise information from the number of dimensions that can be used.  This principal components are uncorrelated, and maximise the amount of variance that is captured,  whilst reducing the number of dimensions.  The output is a set of eigenvalues and eigenvectors that represent the scale and direction of the  primary components respectively. The output is ordered from the principle component representing  largest variation to smallest variation.  Data dimensionality can be reduced by selecting the ﬁrst  k  components to represent the data.  This may lose some detail from the data, however enables the data to be analysed eﬀectively.  Sometimes, the loss of detail is useful, as it can avoid causing a model to overﬁt available data.  PCA Implementation  The process of implementing PCA is as follows:  1. Normalise the data about the mean. For this step the mean value of each dimension is  calculated, and then subtracted from the input values. This results in normalised data that  represents the diﬀerence between a data point and the mean of all data points.  primary components  Independent Component Analysis (ICA)  Independent Componenent Analysis is a process that seeks to identify the statistically independent  components of a data set.  Here an independent component is one that does not share mutual information with another  component.  The independent components are required to be non-gaussian in distribution.  Whilst the process seeks to minimse mutual information between dimensions, it also seeks to  maximise the mutual information between the original data set and the output data set.  ICA diﬀers from PCA in that it does not seek to maximise data variance along components, and  identiﬁed components may not be orthogonal.  For example, when considering a dataset of face images PCA will ﬁnd the features of maximum  variance in descending order such as image brightness, average face, ...  ICA will ﬁnd features that are independent from others such as nose shape, eye shape, etc...  As such, ICA is well suited to determining the structure of the data that underlies a particular data  set.  ICA is often used to perform blind source separation. For example, identifying independent sound  sources from a number of microphones which record sound from each of the sources.  