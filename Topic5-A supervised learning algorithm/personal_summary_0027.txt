For this topic, we would have involved in dimensional reduction using Principal Component Analysis (PCA). In topic 3, we have learnt different methods to calculate distances between data points. What happens if the features of dataset are large like thousands of them? They would increase the distances between points and the dataset become sparse. This would lead to the curse of dimensionality:  In order to solve the curse of dimensionality, we would reduce the dimensionality by projecting most of the data vectors into a few principal components. It would make easier to visualize and analyze the dataset.  Let‚Äôs look at some mathematical concepts in helping solving dimensionality reduction: eigenvalues and eigenvectors. Suppose we have a square matrix A, what if we could find a vector u and an instance ùúÜ which satisfies the condition:  u is called eigenvector and ùúÜ is called eigenvalue. Method to find eigenvalues and eigenvectors:  Singular value decomposition (SVD): a method to decompose a matrix into 3 other matrices:  Principal component analysis (PCA): statistical method to analyze high-dimension dataset by projecting those features into a few main components. The steps to implement PCA include: -  Standardize the dataset -  Find covariance matrix -  Compute eigenvectors and eigenvalues -  Select the N components -  Transform the original dataset using n-components  The rest of materials involve the instruction of how to use python to perform dimensional reduction.            