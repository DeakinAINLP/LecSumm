Topic4-Dimensionality reduction  -Curse of Dimensionality  : when the dimensionality increases, the volume of the space increases so fast(exponentially) that the available data become sparse  : less distinctive in high dimension  -  Concentration effect:  relative contrast between near and far neighbors diminishes as the  -  dimensionality increases. Therefore, Clustering or KNN algorithms may be meaningless in high dimensions, but there may still be patterns, so we need better distance metrics by research  -  Aim to reduce the dimensionality where possible  -Dimensionality reduction  : the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely.  1.  Eigenvector: the directions along which data points are transformed 2.  Eigenvalues: the amount of variability captured by each eigenvector 3.  Finding eigenvector and eigenvalue  -  -  det(A - Î»I) = 0 Singular value decomposition(SVD): a method of decomposing a matrix into three other matrices  ğ‘‹ = ğ‘ˆğ‘†ğ‘‰ğ‘‡  X is n x d matrix U is n x d orthogonal matrix S is d x d diagonal matrix with elements of singular values V is d x d orthogonal matrix Principal Component Analysis(PCA) 1.  Goal: to take n data points in d dimensions, which may be correlated, and summaries  them by a new set of uncorrelated axes. 2.  Principal components: the uncorrelated axes 3.  Variance across each variable 4.  Covariance among variables  : covariance is a measure of how changes in one variable are associated with changes in a second variable. Degree to which the variables are linearly correlated is represented by their co-variances  5.  Covariance matrix: matrix contains variances of all variables on the diagonal and co-  variances among all pairs of variables in the off-diagonal entries  6.  Objective of PCA is to rigidly rotate the axes of t- dimensional axes to a new set of  axes(principal axes) that have following properties: a.  Ordered such that principal axis from highest to lowest b.  Covariance among each pair of the principal axes is zero(the principal axes are  uncorrelated), that is called decorrelation property  -  Formulation of PCA and deriving principal components 1.  PCA via eigen value decomposition  a.  Compute data covariance matrix C b.  Perform Eigen Value Decomposition(EVD) as  ğ¶ = ğ‘ˆğ·ğ‘ˆğ‘‡  c.  Reduced dimension data is given by  ğ‘Œğ‘›Ã—ğ‘˜ = ğ‘‹ğ‘›Ã—ğ‘‘ğ‘ˆğ‘‘Ã—ğ‘˜ Top k eigen vectors in the decreasing order of eigenvalues  2.  PCA minimum error formulation  -  : based on projection error minimization, using the error while using PCA`s best k  Implementation of PCA : where n < d 1.  Using SVD for PCA 2.  Example of using PCA in facial image analysis a.  Generate the covariance matrix for data b.  Find principle eigen vectors that represent the data c.  Calculate face image preservation of energy when K principle eigenvectors are used d.  Projecting data back after preserving only K axis of variation(using only K principle  eigenvectors)  1.  -  Other dimensionality reduction techniques Independent component analysis (ICA) : a method of separating a multivariate signal into independent, non-Gaussian components a.  Used to separate signals that are mixed together such as in a complex sound or  image signal  b.  Goal: to find a set of basis functions that capture the signal`s underlying sources,  then use these functions to separate the signal into its constituent parts  2.  Nonlinear dimensionality reduction technique  a.  t-SNE(t-Distributed Stochastic Neighbor Embedding)  : used to visualize high dimensional data in two or three dimensions. : works by finding patterns and relationships in the data and then representing those patterns in a lower-dimensional space to reduce the complexity of the data  b.  uMap  : similar to t-SNE but has some added advantages  