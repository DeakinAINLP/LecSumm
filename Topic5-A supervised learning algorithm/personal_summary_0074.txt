Summary and Reflection of Topic 4:  Dimensionality reduction    There are several issues that arise when analyzing and organizing data in high-dimensional spaces using machine learning concepts. This includes the volume of the space increasing so fast that the available data becomes sparse. That is the number of regions grows exponentially.    In summary, issues with high dimensionality includes the following relevant concepts:  -  Sparsity – less distinctive distances. -  Distance concentration – notion of distance becomes less and less useful in high dimensional spaces. That is the values of the distances becomes more and more uniform (hence less useful). -  Presence of hubs - of a few instances which appear surprisingly frequently as nearest neighbors of other frequency which has been shown to be related to the dimensionality of the data space. The higher the dimensionality – the higher the presence of hubs.  The above is an issue as Clustering or KNN algorithms may be meaningless in high dimensions.  Until  we  develop  better  distance  metrics,  we  should  aim  to  reduce  the  dimensionality  where possible.  Dimensionality Reduction – refers to the process of converting a set of data having vast dimensions into data with fewer dimensions without losing the value of the information. For example, not all of the values may be relevant and thus can be removed and/or some values may be correlated meaning that there is redundancy.  Reduce to a low dimensionality – we get rid of the presence of hubs and lose information / the resulting data  set  will  not  be  useful  for  predictions.  If  we  preserve  enough  information,  for  example  99%  of  the variance,  then  hubs  will  still  be  present  in  the  data.  Presence  of  hubs  is  related  to  the  intrinsic dimensionality of the data – meaningful dimensionality of the data.  Eigenvalues and Eigenvectors  Are prominently used in the analysis of linear transformations.  Eigenvectors are the vectors (non-zero) that do not change the direction when any linear transformation is applied. It changes by only a scalar factor. In a brief, we can say, if A is a linear transformation from a vector space V and x is a vector in V, which is not a zero vector, then v is an eigenvector of A if A(X) is a scalar multiple of x.  Singular value decomposition  This is a method of decomposing a matrix into three other matrices.  Principal component analysis, or PCA, is form of a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.  Smaller data sets are easier to explore and visualize and make analyzing data points much easier and faster for machine learning algorithms without extraneous variables to process.  So,  to  sum  up,  the  idea  of  PCA  is  simple  — reduce  the  number  of  variables  of  a  data  set,  while preserving as much information as possible.      Steps in PCA  1.  Standardization  The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis. This is critical as PCA is sensitive to variances in initial variables. Once the standardization is done, all the variables will be transformed to the same scale.  2.  Covariance matrix computations  The aim of this step is to understand how the variables of the input data set are varying from the mean with respect  to  each  other,  or  in  other  words,  to  see  if  there  is  any  relationship  between  them.  Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, to identify these correlations, we compute the covariance matrix.  - -  If positive, then: the two variables increase or decrease together (correlated) If negative, then: one increases when the other decreases (inversely correlated)  3.  Calculate  eigenvalues  and  eigen  vectors  of  the  covariance  matrix  to  identify  the  principal  components.  Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components.  Nonlinear dimensionality reduction technique  Nonlinear techniques, as opposed to linear techniques, which use linear algebra to identify patterns in data, use more sophisticated mathematical techniques to identify and capture the underlying structure of the data.  The technique of t-SNE (t-Distributed Stochastic Neighbor Embedding) is used to visualise high- dimensional data in two or three dimensions. It works by finding patterns and relationships in the data and then representing those patterns in a lower-dimensional space to reduce the complexity of the data.  PCA is able to be performed in different ways:    Using linear algebra techniques learnt in class on a 2D dataset.   Inbuilt functions in python to perform PCA.  The  above  summary  has  been  created  using  the  full  course  material  for  Topic  4  (including  the relevant YouTube clips)      