This topic deals with high dimensionality data. In practice there are plenty of cases when data that has high dimensions is given for analysis. High Dimensionality means that the number of dimensions is high, in other words it can be said that the number of features of data are greater than the number of observations. This can pose a problem while performing data analysis. Example of high dimensional data can be data collected from a hospital, which has record of patients and their symptoms and ecological data can be another example.  Curse of dimensionality is the collection of all the difficulties that are faced when dealing with high dimensional data, that were not present for low dimensional data. One of the problems is there are not enough observations when compared to the number of features. Another outcome of data high dimension data is that the region grows exponentially with increase in dimensionality. As a result, the distance between two data points will increase and locating nearby data points or nearby neighbours is difficult. Another problem that is noticed is that most of the data in a high dimensional space can be located at the corners.  With high dimensional data the distance between near and far data points vanishes. This reduction in distance is proportional to the increase in dimensionality, this effect complicates k-nn and other distance metric calculations. This effect is called the concentration effect.  To solve this curse of dimensionality the data can be checked for information that is irrelevant or not required for analysis and eliminated. Besides this numeric data is checked for redundancy. In other words, dimensionality reduction is one of the solutions for this problem.  Matrix decomposition is useful for dimensionality reduction. Eigen decomposition is the process of decomposing a matrix into Eigenvector and Eigenvalue.  Eigenvalues and Eigenvectors are used for square matrices. Eigenvector and eigenvalue can be calculated while decomposing a matrix. In the eigen value equation, A*u = lambda*u A is the parent matrix that is being used as reference, u is the eigenvector and lambda is the eigenvalue.  Singular value decomposition (SVD) is another method of decomposing a matrix. In this method the parent matrix is decomposed into three vectors. X = USVT where U and V are orthogonal eigenvectors and S is a diagonal matrix with diagonal elements being singular values.  Principal component analysis (PCA) is a dimensionality reduction technique and can be descried as the linear transformation of data based on the principal components. Covariance is the term used to show change or variance in one variable based on other variable and covariance matrix is the matrix containing information about covariance among various data points. The number of principal components in PCA is equal to the number of features. There are various methods to calculate PCA one of such methods is using eigenvalue decomposition to find the principal components and thus reducing the dimensionality. After standardizing each feature, the covariance matrix is obtained. This covariance matrix is used as the parent matrix to calculate eigen decomposition. The eigenvectors thus found are sorted and the first highest eigen vector is used as the first principal component and so on. Other such methods include Singular value decomposition, Minimum error formulation.  Independent Component analysis aims to produce set of independent components by finding linear transformation of data. In other words, this technique is used to separate mixed signals and is also useful for feature extraction.  In Nonlinear dimensionality reduction, unlike linear techniques t-SNE (t- Distributed Stochastic Neighbour Embedding) which is a non-linear approach to visualize or look for patterns in data and use these patterns in low dimensional space to reduce dimensionality. Besides this another method called uMAP is discussed which is based on the assumptions about Riemannian manifold. uMAP when compared to t-SNE has better run-time performance and visualization. This method preserves more global structure. Furthermore, UMAP has no computational limits on embedding dimension, making it reliable as a general dimension reduction tool for machine learning.   