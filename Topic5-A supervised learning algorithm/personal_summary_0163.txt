Topic 4  Key Learning Points  From the Learning Resources:  Dimensionality in data  The Curse of Dimensionality arises when applying machine learning algorithms to highly dimensional data.  In machine learning we face unique problems when analysing and organising data in high- dimensional spaces. When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This is problematic since there is not enough data locally.  Finding Eigenvalues and Eigenvectors  The Singular value decomposition (SVD)    Singular value decomposition (SVD) is a method of decomposing a matrix into three other    matrices: In linear algebra, the SVD is a factorization of a real or complex matrix. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal.    Every transformation matrix does two jobs: rotating the original vector, and then  stretching it by a factor. Below shows that combination. Examples and steps below:             Preliminaries The goal of PCA is to take n data points in d dimensions, which may be correlated, and summarizes them by a new set of uncorrelated axes.    The uncorrelated axes are called principal components or principal axes.   These axes are linear combinations of the original k dimensions.   The first d components capture as much of the variation (or variance) among the data points as  possible.  Independent component analysis (ICA) ICA is a method of separating a multivariate signal into independent, non-Gaussian components.  In signal processing, machine learning, and neuroscience, ICA is commonly used to separate signals that are mixed, such as in a complex sound or image signal. The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then use these functions to separate the signal into its constituent parts.  