 A. Curse of Dimensionality: arises when applying machine learning algorithms to  highly-dimensional data. -- dimensionality increases, volume of space increases, available data become sparse, so there isn’t enough data locally.  B. Concentration effect: Relative contrast between near and far neighbors diminishes  as the dimensionality increases.  C. Dimensionality reduction: the process of converting a high-dimension dataset into  lower-dimension dataset while still keeping similar information.  D. Eigenvalues and Eigenvectors: E. Singular value decomposition: SVD is a method of decomposing a matrix into three  other matrices  F. PCA: The goal of PCA is to take n data points in d dimensions, which may be correlated, and summarizes them by a new set of uncorrelated axes. The uncorrelated axes are called principal axes. a) These axes are linear combinations of the original d dimensions. b) Principal component 1: maximize sum of squared distance (projected)  i. SS(distance for PC1) / (n-1) = Eigenvalue for PC1 ii.√SS(distance for PC1) = Singular value for PC1 iii. Eigenvalues measure variation  c) PC2 is perpendicular to PC1 i. Same rules apply to PC2  d) The first k components capture as much of the variation (or variance) among  the data points as possible.  e) PCA can lower dataset dimensions, tell us about the valuable variables, tell us  how accurate the new graph is. G. Other dimensionality reduction techniques  a)  Independent component analysis (ICA): a method of separating a multivariate signal into independent, non-Gaussian components. The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then use these functions to separate the signal into its constituent parts.  b) Nonlinear dimensionality reduction technique  i.T-SNE (t-Distributed Stochastic Neighbor Embedding) is used to visualize  high-dimensional data in two or three dimensions. It projects the data into a lower dimension while preserving the clustering in the original high dimension.  ii. uMap (uniform manifold approximation and projection): also calculates  similarity scores to preserve clusters in low dimensional graph. It is usually faster with large dataset.  