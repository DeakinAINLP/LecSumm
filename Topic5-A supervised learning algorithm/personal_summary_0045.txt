This topic learned about dimensionality reduction.   In simple terms, and what the name infers dimensionality reduction is a technique used to reduce the number of dimensions in a dataset, such that retaining information about the relation of components becomes far more eﬃcient.   Why is dimensionality reduction important? It is important because of phenomenon called “The curse of Dimensionality”. In essence, to make meaningful conclusions/ﬁnding from the dataset increases exponentially as the dimension of the dataset increases. So having less dimensionality can optimise machine learning and reduce issues such as increased computation, low model accuracy and a major one is over ﬁtting.   We learned about PCA. PCA preserves the maximum number of variance of the data while transforming the dataset from high dimensionality to low dimensionality, and is a popular technique for dimensionality reduction for this reason. The principal components, or the directions in which the data ﬂuctuates most, are represented by the Eigenvectors, and the variance explained by each main component is represented by the Eigenvalues. When multiplied by a matrix, Eigenvectors do not change direction. As a Eigenvectors is stretched or compressed after multiplied by a matrix, eigenvalues, which are scalar values, signify by which magnitude.   Commonly used in Image processing, natural language processing and signal processing, Singular Value Decomposition (SVD) is used to ﬁnd patterns in data. Since SVD is a matrix decomposition, it separates a matrix into 3 singular vector matrix: Left, diagonal, right and is another form of dimensionality reduction such as PCA.   Overall, dimensionality reduction is a key machine learning approach and is used to decrease the amount of features in a dataset while preserving the most important data, reducing the curse of dimensionality through techniques such as PCA and Singular Value Decomposition.  