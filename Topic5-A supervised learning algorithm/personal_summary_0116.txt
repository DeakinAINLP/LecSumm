In this topic, dimensionality reduction was covered which is the process of reducing the number of input features (also known as dimensions) in a dataset while retaining as much of the relevant information as possible.  Why do we reduce dimensionality?  There are many issues in analysing high-dimensional spaces such as curse of dimensionality, sparsity, visualization challenges and data storage and retrieval when dealing with large datasets such as sensor data from IoT devices and financial data including bank transaction, credit card information etc  Curse of dimensionality : The complexity and difficulty of analyzing data arises as the number of dimensions/features in the data increases. Because the amount of data required to fill the space grows drastically when the number of dimension increases. This leads to issues including risk of overfitting, computational complexity.  Dimensionality reduction : By converting datasets having vast dimensions into fewer dimensions, by making sure it still shows similar pattern. For example, one of the real-world examples where two variables have linear relationships can be oneâ€™s height and weight because when high increases, weight tends to increase linearly too. To reduce dimensionality in this case, we can select either height or weight for certain prediction.  Eigenvalues and Eigenvectors: An eigenvector of a matrix is a non-zero vector that when multiplied by the matrix results in a scalar multiple of the same vector. This scalar multiple is called the eigenvalue associated with the eigenvector. For example,  Singular value decomposition: SVD is commonly used in various machine learning applications such as collaborative filtering, image compression, and text mining. It is particularly useful for large, sparse datasets since it can be computed efficiently using iterative numerical algorithms.  How to implement PCA  In general, we can follow the steps like:  Standardize the data->Generate the covariance matrix-> Calculate elgenvectors and eignevalues -> Select the principal components->Transform the data into lower-dimensional space  Also, another example where reducing the dimensionality of a problem is critical can be in natural language processing(NLP), especially in text classification tasks. Because each word in a document is normally represented as a feature which creates a high-dimensional space  Other dimensionality reduction techniques include Independent component analysis(ICA) and t-SNE and uMap  ICA: The goal is to find components that are maximally independent and non-normal (htt1)  t-SNE: T-Distributed Stochastic Neighbor Embedding is an unsupervised machine learning to reduce the number of independent variables in a high-dimensional datasets. (htt2)   uMap: Uniform Manifold Approximation and Projection can be used to visualize datasets and similar to t-SNE.  