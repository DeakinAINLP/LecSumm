A dimension in Machine Learning is the number of input variables or feature columns in a given dataset. It doesn‚Äôt take much for text and image data to move into very high dimensions.  Curse of Dimensionality  As a dimensionality often grows exponentially, it occupies a larger volume of space.  As a datasets are typically limited in size, it may not grow with dimensionality, thereby becoming sparse. As a result of this sparseness, the reliability of any result may become negatively impacted. For example, as data moves further way from the origin, points become less distinctive.  Searching for data in higher dimensions is also problematic and can lead to a drop in computational efficiency.  Solving the Curse  Dimensionality reduction is used when the number of features describing the differences in the data attributes becomes too large to manage.  It involves the use of techniques (factorisation, duplicate removal etc.) to reduce from a higher dimensionality to a more manageable low dimensionality, with enough meaningful properties retained from the original higher dimensional space permitting intuitive results.  Singular Value Decomposition (SVD)  SVD is the factorisation or decomposition of a complex matrix into a simpler three matrix form (two orthogonal matrices and one diagonal matrix). It represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal.  Principle Components Analysis (PCA)  PCA is a method of analysis where a large dataset with a high number of dimensions (lots of features) is reduced in dimension whilst maintaining most of the variation (information) found within the original dataset.  The goal of PCA is to take ùëõ data points in ùëë dimensions, which may be correlated and summarizes them using a new set of uncorrelated axes in reduced dimensions. The uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original ùëë dimensions. Principal components are sorted in descending order based on captured variance along each axis.  Eigenvalue Decomposition (EVD)  Principle Component Analysis can be used to provide dimensionally reduced data via Eigenvalue Decomposition.  We can find the direction of the greatest variance in our data through calculation of a covariance matrix. The direction is the Eigenvector which does not rotate when we multiply it by the covariance matrix. Each eigenvector has a corresponding eigenvalue. Eigenvectors which have the largest eigenvalues will be used as our principal components; the remaining eigenvectors are less informative so are not used.    Independent Component Analysis (ICA)  The goal of ICA is to separate mixed signals into their individual components. This terchnique finds a set of basis functions that capture the signal's underlying sources, then uses these functions to separate the signal into its constituent parts.  Nonlinear Reduction of Dimensionality (t-SNE)  The technique of t-SNE (t-Distributed Stochastic Neighbour Embedding) is used to visualise high- dimensional data in two or three dimensions. It works by finding patterns and relationships in the data (constructs a probability distribution) and then represents those optimised patterns in a lower- dimensional space to reduce the complexity of the data.  Nonlinear Reduction of Dimensionality (UMAP)  The technique of UMAP (Uniform Manifold Approximation and Projection) provides a visually similar optimised representation of dimensionality reduction to t-SNE, but assumes data is uniformly distributed.     