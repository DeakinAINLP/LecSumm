This topic's task was fascinating! Not the first time I’ve come across Principal Component Analysis but like anything, the more often you revisit and use it, the deeper and better your understanding, so I was happy to rediscover it and play around with it in python.  The Curse of Dimensionality  The curse of dimensionality dictates that as the number of dimensions increases, the number of regions grows exponentially. This has the effect of making the data sparse and not very useable. It also causes other issues such as increased computational complexity, overfitting issues, and can be more difficult to interpret.  Luckily, there are a number of ways to reduce the dimensionality of data without losing the underlying information. These include but are not limited to:    Principal Component Analysis (PCA): PCA is a linear dimensionality reduction technique that      aims to identify a new set of uncorrelated variables, called principal components (PCs), that capture the most important information in the original data. t-distributed Stochastic Neighbor Embedding (t-SNE): t-SNE is a nonlinear dimensionality reduction technique that is particularly useful for visualizing high-dimensional data in two or three dimensions. Independent Component Analysis (ICA): ICA is a method of separating a multivariate signal into independent, non-Gaussian components. This is commonly used to separate signals that have been mixed together.    Non-negative Matrix Factorization (NMF): NMF is a technique that decomposes the data  matrix into two lower-dimensional matrices, one of which contains non-negative basis vectors that can be interpreted as representing different features or components of the data.  Like anything, there is no magic solution for all problems, what method you deploy largely depends on the type of data and what you’re trying to achieve.  Eigen Vectors, Values, and SVD  Eigen vectors are non-zero vectors that when multiplied a square matrix only change in magnitude and not direction. The Eigen value is the magnitude of this change.  Single Value Decomposition (SVD) is a matrix factorization technique that decomposes one matrix into three. Given some matrix A, after SVD we get 𝐴 = 𝑈𝑆𝑉𝑇 where U and V are orthogonal matrices and S is a diagonal matrix containing the singular values of A, the eigenvalues.  SVD provides a general framework for decomposing matrices, for our work in this module we used a specific application of SVD called Principal Component Analysis (PCA) which I’ll summarise next.  SIT307 Machine Learning    Principal Component Analysis  Principal Component Analysis (PCA) is a technique for identifying the most important patterns or features in a dataset, and for representing the data in a lower-dimensional space, that is, to reduce the dimensionality of the data. It works by identifying a set of orthogonal vectors (Eigen Vectors) that capture the largest amount of variation within the data. This makes the data easier to work with, interpret, and possibly visualize.  The process of PCA involves the following steps.  1.  Normalise the dataset: If variables are recorded on different scales, we need to convert them all to the same scale, standardize them to have a mean equal to zero, and variance equal to one.  2.  Calculate the covariance matrix: The matrix is calculated by multiplying the normalized  dataset N by its transpose, so: 𝐶𝑂𝑉 =  𝑁𝑇𝑁 𝑚  where 𝑚 is the length of the feature vectors. This  matrix measures the linear relationship between pairs of variables and captures the amount of variation in the data.  3.  Find principal eigen vectors: Here we can use SU Decomposition as a method of calculating  the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors are our Principal Components (PCs), and the eigenvalues represent the amount of variation captured by the corresponding eigenvector.  4.  Select the top n PCs: This step is open to interpretation and depends on the type of data and the specific goals you have. You can first calculate how many components you require based on the amount of variation you want captured.  For example, 3 PCs may capture 75% of the variation which may meet your goals, you then reduce U accordingly.  5.  Project the data: The final step is to project U onto the normalised data which will rotate our data aligning the x axis with the maximum amount of variation. To do this we simply take the dot product of our reduce U and our normalized data.  The result is a lower dimensional representation of the data which captures the most important information of the original dataset.  