This topic provided an in-depth coverage of dimensionality reduction and the inner workings of the core algorithms. We started by framing the issues with high dimensionality in data and how this is averse to machine learning.  We looked at solutions that allow us to perform dimensionality reduction.  We learned about eigenvectors and eigenvalues and the formulas for calculating them on matrices. We also explored singular value decomposition, variance and covariance amongst variables.  We explored Principal Component Analysis and how it can be used in dimensionality reduction including t-SNE as an alternative non-linear technique.  The second part of the topic focused on the use of the Python programming language and how it can be used in order to check for and visualize correlation in data.  We also learned how to normalise the data and apply PCA and t-SNE and plot the results in 3D graphs.  