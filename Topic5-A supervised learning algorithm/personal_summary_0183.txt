Curse of High Dimensionality   Concentration effect arises when working with high-dimensional data  (many features) .   Space volume grows exponentially with dimensionality, data points  becomes sparse and distances have low variance.   Distance between datapoints loses discrimination power for near and far  neighbors. Therefore distances between data points become more uniform and less informative   Hubs develop where data points that appear frequently as nearest  neighbors in high-dimensional space, often due to irrelevant features  Intrinsic dimensionality of  dataset, the minimum number of features needed to represent the data without losing meaningful information   The solution is Dimensionality reduction: the process of transforming high- dimensional data into lower-dimensional data without losing information.  Dimensionality Reduction   Variance measures how much the data points deviate from  the mean.   Eigen vectors are the directions of maximum variance in a  data set.   Dimension reduction is the process of reducing the  number of features or variables in a data set   Principal components are the linear combinations of the  original features that capture the most variance  Single value decomposition is a matrix factorization  technique that can also be used to compute the principal components  Principal Component Analysis   Eigen vector decomposition is a method to find the directions of the  data spread using eigenvectors.   PCA is a technique that uses eigen vector decomposition on the  covariance matrix of the data to reduce its dimensionality   Covariance matrix C measures how each feature is correlated with  one another.   Covariance direction captures the most variance of the data along the new feature axis and corresponds to the largest eigenvalue of the covariance matrix.   Dimension reduction can be done by projecting the data onto a  lower-dimensional subspace spanned by the eigen vectors of the covariance matrix C.  Finding Eigenvalues & Eigenvectors   To find the eigenvectors of a matrix A, we need to solve the system (A - λI)x  = 0 to find each eigenvalue λ. The number of nonzero eigenvalues of a matrix A is equal to the rank of matrix A.   Starting with solving for the characteristic polynomial of a matrix A, p(λ) =  det(A - λI), where I is the identity matrix.   The eigenvalues of a matrix A are the roots of the characteristic polynomial  p(λ)   An eigenvectors of a matrix A is a nonzero vector x that satisfies Ax = λx for  each scalar eigenvalue λ.   Eigen values are the factors by which the linear transformation stretches or  compresses the eigen vectors   Eigenvectors are vectors that do not change their direction when multiplied  by a matrix which is a linear transformation.  Eigen Value Decomposition   Eigen value decomposition is a matrix factorization technique that  decomposes a square matrix into three matrices: a diagonal matrix of eigenvalues and two matrices of eigenvectors.   The eigenvector matrix U of a matrix A is a matrix whose columns are  the eigenvectors of A.   The diagonal matrix D of a matrix A is a matrix whose diagonal  entries are the eigenvalues of A.   If A is diagonalizable, then we can write A = UDU-1, where U is the  eigenvector matrix and D is the diagonal matrix   Eigen value decomposition can also be applied finding the singular  values of a matrix.  Steps in PCA   Compute the covariance matrix C of the data with high  dimensionality.   Find the eigenvalues and eigenvectors of the covariance  matrix C.   Sort the eigenvectors by their corresponding eigenvalues  in descending order   Choose the top k eigenvectors as the principal  components.   Dimension reduction can be done by projecting the data  onto a lower-dimensional subspace spanned by the k eigen vectors  Single Value Decomposition   Single Value Decomposition (SVD) is a method to factorize a matrix  into three matrices, even non-square matrices.   Given a matrix X, SVD finds matrices U, D and V such that X = UDVT.  U and V are orthogonal matrices, meaning that UUT= I and VVT= I,  where I is the identity matrix.   D is a diagonal matrix, meaning that it has non-zero entries only on  the main diagonal.   The diagonal entries of D are called singular values and they are the  square roots of the eigenvalues of XXT or XTX.   The singular values are arranged in descending order along the  diagonal of D.   SVD can be applied to any matrix, while EVD can only be applied to  diagonalizable matrices  SVD & PCA   Using SVD of the higher dimension data matrix X.  Given a matrix X, SVD finds matrices U, D and V such  that X =U D VT   Compute U from the Covariance Matrix C of X using  eigen value decomposition as C = UDUT  Reduced dimension data is given by Y=X.U  Independent Component Analysis (ICA)   Independent component analysis (ICA) is a method  for separating a multivariate signal into independent non-gaussian components.   One way to implement ICA is to use the kubler-  liebherr convergence criterion and gradient descent optimization.   This approach minimizes the mutual information  between the estimated components and maximizes their non-gaussianity.  T-SNE   t-SNE is a technique for visualizing high-dimensional  data in low-dimensional space.   It preserves the local structure of the data by  assigning high probabilities to similar points and low probabilities to dissimilar points.   It minimizes the kubler-liebherr divergence between  the high-dimensional and low-dimensional probabilities using gradient descent.  Uniform Manifold Approximation and Projection (UMAP)   Uniform Manifold Approximation and Projection  (UMAP) is a technique for dimension reduction and visualization of high-dimensional data  .  Manifold learning is a branch of nonlinear  dimensionality reduction that aims to project high- dimensional data onto lower-dimensional latent manifolds .   UMAP preserves local neighbourhoods in a dataset  