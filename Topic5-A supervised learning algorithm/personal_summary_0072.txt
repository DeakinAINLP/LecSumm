Main points of this module:    The ‘curse of dimensionality’ refers to how the number of regions grows exponentially as  the number of dimensions increases, causing data to sparse and less useful.    Eigenvalues and eigenvectors are primarily used for analysing linear transformations, and  can be used to maintain relationships between variables, combatting the curse of dimensionality.    Singular value decomposition (SVD) is used to decompose a single matrix into three other matrices and is given by the formula X = USVT and requires finding the eigenvalues and eigenvectors.    Principal component analysis (PCA) is used to decorrelate and reduce dimensionality and can  be performed in various ways including eigen value decomposition, minimum error formulation and by using SVD.    Variance across each variable is given by the average squared deviation of values across a multidimensional space with one axis for each variable, where the centroid is given by the mean of each variable.    Covariances among variables is a measure of change in a variable associated with the  changes in another variable.    A covariance matrix can depict all variable variances along the diagonal, and covariances  elsewhere.    PCA is used in real-world solutions such as facial recognition software, by generating the covariance matrix for the data, finding principle eigen vectors, calculating face image preservation, and projecting data back.  Provide a summary of your reading list:  I simply followed the given study material (4.1 – 4.18), reading everything until I understood it confidently, as well as viewing any linked videos provided. I did spend a fair amount of time on the Python section and problem-solving activity, as I wanted to best demonstrate my understanding of this topic’s content despite there only being three questions.  