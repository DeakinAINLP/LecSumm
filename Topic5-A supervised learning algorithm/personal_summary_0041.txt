In topic 4, we are focussing on the issue of dimesionality of data, solving it suing PCA, concept of Eigen values, eigen vectors, SVD and EVD.  CURSE OF DIMENSIONALITY  Now, depending on the features that we extract from a problem, we map each feature to a dimension, but comsider the case when we are capturing n features for m people, we get m*n dimensions. Now this number can be as large as you are imagining. Hence, we have this problem of huge number of imensions which makes the data sparse, with very less variance and also difficult and meaningless to examine. Now, to come up with a technique to sovle this problem we study he concept of dimensionality reduction.  SOLUTION  PCA, EIGEN VALUES and VECTORS  To solve this issue, we have a concept of finding principal component axis, PCA. This involves re-mapping the data by obtaining two axis using the data. These axis have the highest variances from the data and are orthogonal to each other. Now the PCAi has i less than or equal to the number of dimensions in the data. The first PCA has highest variance and it reducs till PCAd. Now,these axis make the data un-correlated since  they  are  perpendicular  to  each  other.  When  we  plot  data  on  these  new  axis, PCAâ€™s,  obtained  by  our  calculations,  we  need  to  make  sure  that  the  data  is  not misplaced,  since  we  want  it  to  have  same  meaning  as  before.  Depending  upon  th percent variance contribution, from each axis we can choose to keep the first k axis and remove others, helping us in dimensionality reduction.  In order to calculate the PCA, we can calculate the eigen values and eigen vectors for a covariance matrix, which represent the possible linear transformations to keep the data intact. Using the mathematical calulations of projecting data from current position to  new  axis,  we  find  that  finding  eigen  values  and  vectors  for  the  covriance  matrix gives us the PCA vectors in eigen vectors for the natrix.  STEPS IN CALCULATING PCA  1.  Normalise the data 2.  Compute the covariance matrix that shows variance between features. 3.  Calculate eigen vectors and values of covariance matrix.  These eigen vectors are the PCA components and we can use these to reproject he data  and  then  if  we  want  to  reduce  the  dimension,  just  reproject  the  data  back  to normal axis with new number of dimensions.  A  small  concept  related  to  PCA  is  minimising  the  error  when  using  PCA  for dimensionality reduction. This involves minimising the mean sqaure error due to new projection.  