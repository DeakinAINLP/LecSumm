Topic 4 for Machine Learning dealt with Dimensionality reduc:on and how it is formulated by using Single Value Decomposi:on, Eigenvalues and Eigenvectors.  First, we learnt what is dimensionality of data and why it is a curse to have a high dimension data for analysis.  Dimensionality in Data  1.  Text Data  a.  Number of dimensions can exceed 1000 easily for a topic's worth of data  scrape  2.  Image data  a.  64x64 image has 4096 dimensions!  3.  Genomic data  a.  Huge number of dimensions for a simple genomic data  When applying Machine Learning algorithms to high-dimension data, we get Curse of dimensionality, which is –  1.  Volume of space increases with increasing dimensionality 2.  Data becomes sparse with increasing dimensionality - this is problema:c since there  isn't enough data locally  3.  As the number of dimensions increases, the number of regions grows exponen:ally 4.  In high dimensionality spaces, most of the data resides in the corners of the  hypercube deﬁning the feature space  ConcentraFon eﬀect  1.  Rela:ve contrast between near and far neighbours diminishes as the dimensionality  increases, hence rendering clustering algorithms useless in high dimensions  2.  Hence, we need to reduce dimensions in a dataset  Next, we learnt what is dimensionality reduc:on and how we can achieve it using diﬀerent methods.  Dimensionality ReducFon  1.  The process of conver:ng a set of data having vast dimensions into a data with fewer  dimensions  2.  Needs to convey similar informa:on about the data 3.  One of the processes can be to mul:ply with a matrix which is in the direc:on of  maximum variance in the data  4.  For this unit, we are conﬁning ourselves to linear dimensionality reduc:on problems  only  Eigenvalues and Eigenvector  1.  Prominently used in analysis of linear transforma:ons  2.  The number of non-zero eigenvalues for A is equal to the rank of the matrix A 3.  Finding Eigenvalue and Eigenvectors  a.  det(A – lambda mul:plied by Iden:ty matrix) = 0 and ﬁnd value(s) of lambda.  This gives the Eigenvalues of A  Singular Value DecomposiFon  1.  SVD is a method of decomposing a matrix into three other matrices, X = USV! 2.  In linear algebra, SVD is a factoriza:on of a real or complex matrix. The SVD  represents an expansion of the original fata in a coordinate system where the covariance matrix is diagonal  Principal Component Analysis  1.  Main objec:ve of PCA is to rotate the axes of a t-dimensional axes to a new set of  axes having the following proper:es -  a.  Ordered such that the principal axis captures the highest variance, second  axis captures the second highest variance and so on  b.  Covariance among each pair of principal axis is zero. This is called  decorrela:on property  Other dimensionality reducFon techniques  1.  Independent component analysis (ICA) - this is component based technique  a.  This is a method of separa:ng a mul:variate signal into independent, non-  Gaussian components.  b.  The goal of ICA is to ﬁnd a set of basis func:ons that capture the signals  underlying sources, then use these func:ons to separate the signals into its cons:tuent parts  2.  Nonlinear dimensionality reduc:on technique - these are projec:on based  techniques  a.  t-SNE or T-Distributed Stochas:c Neighbour Embedding  i.  ii.  This technique is used to visualize high-dimensional data in 2/3 dimensions. This works by ﬁnding paberns and rela:onships in the data and then represen:ng those paberns in a lower-dimensional space to reduce the complexity of the data  b.  uMAP  