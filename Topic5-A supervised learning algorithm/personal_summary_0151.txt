Topic 4 Learning Summary Dimensionality in Data Dimensions of data that are dealt with daily are text data, image data and genomic data  Curse of dimensionality When  the  dimensionality  increases,  the  amount  of  data  to  generalize  accurate  grow exponen:ally.  This  results  in  less  dis:nc:ve  distances  in  increased  dimensions.  The concentra:on eﬀect measures ra:o of the variance of the length of any point with the length of the mean point vector that converges to zero with increasing data dimensionality. Thereby decreasing the use of the measure to dis:nguish between near and distant neighbors.  Solving the curse The  curse  of  dimensionality  can  be  solved  through  dimensionality  reduc:on  where  high dimensional data is converted to low-dimensional while conserving their original proper:es. This data  points  and  projec:on  vector  to  retrieve  the  projected  data  into  a  single  dimension, thereby  reducing  the  dimensions  of  the  high  dimensional  data.  Another  example  includes capturing  variance  through  projec:on  onto  maximum  variance  direc:on  resul:ng  in  the analysis of more information. The last example is linear dimensionality reduc:on problems.  Eigenvalues and Eigenvectors Eigenvalues  and  eigenvectors  are  used  in  analyzing  linear  transforma:ons.  This  helps  in reﬂec:ng the vector without changing its length. Singular value decomposi:on is a method of decomposing the matrix into 3 other matrices such as the orthogonal, diagonal and another orthogonal matrix. SVD represents the factoriza:on of a complex or real matrix wherein the covariance matrix is diagonal. The diagonal elements are referred to as singular values and consists of ﬁnding the eigenvalues and eigenvectors.  PCA PCA takes in n data points in d dimensions that could be correlated and summarizes them by a  new  set  of  uncorrelated  axes.  PCA  aims  to  reduce  features  into  smaller  number  of components. Covariance measures how changes one variable is to changes in another. The covariance matrix consists of variances of variables present on the diagonal and covariances of variables present oﬀ diagonal entries. The main aim of PCA is to rigidly rotate the axes of the  d-dimensional  axes  to  new  principal  axes  that  are  ordered  and  such  that  covariances among each pair of the principal axes is zero.   