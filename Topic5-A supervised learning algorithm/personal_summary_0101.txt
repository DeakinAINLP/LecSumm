This  topic  in  the  Machine  Learning  unit  we  discussed  dimensionality  reduction  and  the  application of Principal Component Analysis method, we learnt:  -  The problem with high dimensional data. As dimensions increase, the potential regions for the  data entries to reside increases exponentially. This means that the available data will become  more sparse and not sufficiently representative.  -  Dimensionality reduction as a solution. In this process, the aim is to convert the data into data  -  -  with fewer dimensions while still maintaining as much information as possible.  The variance which is represent the variation of data along one axis (variable) to the mean of  the data on that axis (variable). Covariance  matrix which measures how the change in one  variable is related to another variable in multidimensional data.  The Principal Component Analysis (PCA), which is a popular method to perform dimensionality  reduction. The aim of this method is reprojecting the data into new set of axes that minimize  the  covariance  between  the  variables,  the  new  axes  are  called  the  principal  axes.  Then  depending on the accuracy needed, project back the data to the original axes after removing  the principal axes with the smallest variance direction.  -  In  implementing  PCA,  the  first  principal  axis  is  the  axis  along  the  direction  of  the  largest  variance.  The  second  one  is  along  the  direction  with  the  second  largest  variance  and  perpendicular to the First principal axis. This process is repeated until all the principal axes are  generated. Eigen values and eigen vectors are used to get the principal axes where the first  principal axis is the eigen vector with largest eigen value and so on.  -  Another  common  dimension  reduction  method  named  t-distributed  Stochastic  Neighbor  Embedding (t-SNE). This method is suitable for dimensionality reduction for nonlinear data  which uses probability distributions to decide which points are considered neighbors to each  other.  Then  the  reduction  process  happens  by  generating  random  points  in  a  lower  dimensional space while finding similar probability distributions in the original space.  