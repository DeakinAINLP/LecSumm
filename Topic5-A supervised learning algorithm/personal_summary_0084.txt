Curse of Dimensionality  The curse of dimensionality is something that arises when trying to apply machine learning algorithms to data that is highly dimensional, the dimensionality of text, image and genomic data can quickly rise into the thousands.  Analysing and organising highly-dimensional data results in high-dimensional spaces, the problem being that the space increases at an exponential rate and the available data within this space becomes sparse. This reduces the ability to measure between data points, as dimensionality increases the relative contrast between near and far neighbours diminishes. Meaning that clustering or KNN algorithms can become meaningless at high enough dimensions, while the patterns may still be there it will require better distance metrics. Until the better distance metrics are developed we must instead focus on reducing dimensionality where possible.  Solving the Curse  The curse of dimensionality requires dimensionality reduction which refers to the process of converting a set of data having vast dimensions into data with fewer dimensions without sacrificing the information being conveyed in the original data set. This can be achieved by removing variables where possible, for example variable that are the same as others or unimportant to what is being measured can be deemed redundant or irrelevant and removed.  Eigenvalues and Eigenvectors  Eigenvalues and Eigenvectors are mostly used in linear transformation analysis. For a square matrix A if a number represented by λ and a vector represented by u can satisfy Au = λu then λ is an eigenvalue while u is an eigenvector.  Singular value decomposition  Singular value decomposition (SVD) refers to a method of decomposing a matrix into three other matrices. With X representing the original matrix and U, S and VT representing three others, meaning that X = USVT. X is a n x d matrix, U is a n x d orthogonal matrix, S is a d x d diagonal matricx and V is a d x d orthogonal matrix.  Preliminaries  Principle Component Analysis (PCA) involves taking n data points in d dimensions (which may be correlated) and summarising them with a new set of uncorrelated axes known as principal components or principal axes. These are linear combinations of the original d dimensions. The first k components are meant to capture as much of the variance between the data points that they can.  The centroids of the points within multidimensional space is defined by the mean of each variable, the variance of each variable is then determined by the average squared deviations of its n values around the mean of that variable.  Covariance is a measure of the association between the changes in one variable with the changes in another variable, their co-variances are the degree to which they are affect each other and are linearly correlated.  A covariance matrix contains the variances of all variables on the diagonal and co-variances among all pairs in the off-diagonal entries.   PCA decorrelation involves the rotations of the dimensional axes to a new set of axes (the principal axes). The principal axis are ordered so that they capture the highest variance with the next axis capturing the second highest variance and so on. The covariance among each pair of principle axes is zero as they are meant to be uncorrelated, this is known as decorrelation property.  PCA via Eigen Value Decomposition  PCA can be performed using eigenvalue decomposition by computing the covariance matrix C, Performing Eigen value decomposition as C = UDUT resulting in the reduced dimension data as given by: Yn x k = Xn x d Ud x k  Implementation of PCA  There are multiple different ways of implementing PCA, when the amount of data points (n) is less than the number of dimensions (d) we can use SVD instead of EVD for PCA. We do this as it severely reduces the amount of computations required to calculate PCA. This can be done through the connection among SVD and EVD, as the U of SVD is the same as the U of EVD. SVD can be used to get the matrix S, computing the eigenvalues of this gives the tools required to calculate PCA since PCA is just multiplying the U matrix by the data matrix.  Example of using PCA in facial image analysis  This process involved the following steps:  -  Generate the covariance matrix for data -  Find the principle eigen vectors that represent the data -  Calculate face image preservation of energy when K principle eigenvectors are used -  Project data back after preserving K axis of variation (using only K principles eigenvectors)  Other dimensionality reduction techniques  Independent component analysis (ICA) is a method of separating multivariate signal into independent, non-Gaussian components. ICA is typically used to separate mixed together signals such complex sound and image signals. The goal of the technique is create a set of basis functions that capture the underlying source of the signal, these functions are then used to separate the signal into its base parts.  Reflection  Going over the content of this module introduced me to many new topics as I was not very familiar with the dimensionality of data and how we can reduce this dimensionality. These concepts used to solve the curse of dimensionality where all new to me, so I am just beginning my understanding of Principal Component Analysis while being introduced Singular value decomposition and eigenvalues and eigenvectors. I was also able to further my understanding of variance, specifically how variance and covariance exists between variable and how it can be represented with matrices. Furthermore, I now have a basic understanding of how PCA can be calculated along with examples of how it can be applicable to real world scenarios such as facial recognition software.  