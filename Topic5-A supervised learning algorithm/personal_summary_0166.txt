Dimensionality Reduc1on for Machine Learning The number of input variables or features for a dataset is referred to as its dimensionality.  Dimensionality reduc1on refers to techniques that reduce the number of input variables in a dataset.  Curse of Dimensionality: As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponen1ally.  The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data becomes sparse. This sparsity is problema1c for any method that requires sta1s1cal signiﬁcance. In order to obtain a sta1s1cally sound and reliable result, the amount of data needed to support the result oGen grows exponen1ally with the dimensionality. Also, organizing and searching data oGen relies on detec1ng areas where objects form groups with similar proper1es. In high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organiza1on strategies from being eﬃcient. Distance metrics such as Euclidean distance used on dataset of too many dimensions, all observa1ons become approximately equidistant from each other. Example: It’s harder to catch a dog if it were running around on the plane (two dimensions). It’s much harder to hunt birds, which now have an extra dimension they can move in.  Eigenvalues and Eigenvectors: Eigenvalues and Eigenvectors as these concepts are used in one of the most popular dimensionality reduc1on techniques – Principal Component Analysis (PCA). In PCA, these concepts help in reducing the dimensionality of the data (curse of dimensionality) resul1ng in a simpler model which is computa1onally eﬃcient and provides greater generaliza1on accuracy.  Singular value decomposi1on: Singular value decomposi1on takes a rectangular matrix of gene expression data (deﬁned as A, where A is a n x p matrix) in which the n rows represents the genes, and the p columns represents the experimental condi1ons. The SVD theorem states:  Anxp= Unxn Snxp VTpxp  Where UTU = Inxn  VTV = Ipxp  (i.e. U and V are orthogonal)  Where the columns of U are the leG singular vectors (gene coeﬃcient vectors); S (the same dimensions as A) has singular values and is diagonal (mode amplitudes); and VT has rows that are the right singular vectors (expression level vectors). The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal.            Principal Component Analysis (PCA): Principal Component Analysis (PCA) is one of the most commonly used unsupervised machine learning algorithms across a variety of applica1ons: exploratory data analysis, dimensionality reduc1on, informa1on compression, and data denoising.  The main aim of PCA is to ﬁnd such principal components, which can describe the data points with a set of... well, principal components.  The principal components are vectors, but they are not chosen at random. The ﬁrst principal component is computed so that it explains the greatest amount of variance in the original features. The second component is orthogonal to the ﬁrst, and it explains the greatest amount of variance leG aGer the ﬁrst principal component. The original data can be represented as feature vectors. PCA allows us to go a step further and represent the data as linear combina1ons of principal components. GeXng principal components is equivalent to a linear transforma1on of data from the feature1 x feature2 axis to a PCA1 x PCA2 axis.  Where do we use PCA?    Reducing the image size   Facial recogni1on   Medical science  