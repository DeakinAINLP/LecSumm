This topics module covered dimensionality of data with regards to machine learning, as well as methods of performing dimension reduction on datasets. While storing data with high-dimensional features is no problem for modern computers, the issue arises with analysing and processing it. It’s practically impossible for us to visualise data with more that 3 dimensions, and while its easy to overcome these human limitations through analysis with software, we run into more problems with data that has thousands to hundreds of thousands of dimensions. The sparseness of data-points becomes a large problem in high-dimensional datasets, we are unable to get meaningful results from clustering on this type of data. Thankfully, we are able to reduce the dimensions of the data while keeping the variance of the data largely intact using techniques such as PCA.  We learned how to perform PCA manually in Python as well as with inbuilt functions. This helps our understanding greatly because we are able work through the underlying theory.  Overall PCA is pretty simple, for very small datasets it’s possible to do it by hand and it is extremely easy to perform in python using NumPy on data with any number of dimensions.  