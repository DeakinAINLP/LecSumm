In topic 4, we learned about dimensional reduction, the transformation of data to a low dimensional space from a high dimensional space in order for the low dimensional representation to have meaningful properties of the original data. We covered the curse of dimensionality, this being the issue that arises as dimensionality increases. The volume of the space increases such that the available data becomes sparse, resulting in there not being enough local data. To address this issue, we went in depth about Singular Value Decomposition (SVD) and did a refresher on eigenvalues and eigenvectors. Along with this, we also thoroughly explored Principal Component Analysis (PCA) and how to implement it in order to analyze large datasets while still preserving the maximum amount of information possible. Included in this was a discussion about the covariance matrix, which is a matrix that contains variances of all variables on the diagonal and co-variances among all pairs of variables in the off-diagonal entries. One other thing we looked at this topic were other dimensionality reduction techniques used, such as Independent Component Analysis (ICA) and t-Distributed Stochastic Neighbor Embedding (t-SNE).  