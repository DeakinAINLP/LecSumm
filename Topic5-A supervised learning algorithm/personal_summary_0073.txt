Topic4 Summary  This topic has covered the data dimensionality reduction, principal component analysis and how to use inbuilt function to perform PCA and dimensionality reduction approach through python.  In machine learning, there are a lot of issues arise when analysing data in a high-dimensional spaces. It will make things complicated. The volume of the space increases with the increases of dimensionality, and the available data become sparse. That is the curse of dimensionality, as the number of regions grows, and space increases each data point has more and more room. It makes us hard to find patterns when analysing and visualizing as well as developing machine leering models.  The way to solve the curse is use dimensionality reduction, it refers to the process of converting high dimensionality dataset into data with fewer dimensions. Where eigenvectors and eigenvalues are introduced, they are used in linear algebra. An eigenvector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue is the factor by which the eigenvector is scaled.  Singular value decomposition (SVD) is a method of decomposing a matrix into three other matrices, it is a factorization of a real or complex matrix in linear algebra.  PCA is to take n data points in d dimensions, which maybe correlated and summarises them by a new set of uncorrelated axes. The uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original d dimensions. The first k components capture as much of the variation (or variance) among the data points as possible. Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.  There are other dimensionality reduction methods, such as ICA â€“ independent component analysis is a method of separating a multivariate signal into independent, non-Gaussian components. And nonlinear dimensionality reduction technique, as opposed to linear techniques, which use linear algebra to identify patterns in data, use more sophisticated mathematical techniques to identify and capture the underlying structure of the data.  