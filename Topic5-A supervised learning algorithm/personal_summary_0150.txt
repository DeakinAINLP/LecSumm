Summary:    Columns in a Machine Learning dataset are referred to as features, which are often also called "dimensions." There is a correlation between datasets that contain high dimensional data and overfitting, an increase in the complexity of computation, and a decrease in the efficiency of algorithms. The technique of reducing the number of features or variables contained within a dataset while maintaining its level of accuracy is referred to as "dimensionality reduction."    The matrix decomposition technique known as eigenvalue decomposition (or eigen  decomposition) separates a square matrix into a set of eigenvectors and eigenvalues. The eigenvalues are a representation of the degree to which each eigenvector has been stretched or compressed, and the eigenvectors are the axes along which the data is transformed when it is subjected to a linear transformation.    The Principal Component Analysis (PCA) is a well-known method for dimensionality  reduction that is used in machine learning. Its purpose is to transform high- dimensional data into a lower-dimensional space while preserving as much information as is practically possible.    The principal components, often known as PCs, are linear combinations of the original features that account for the greatest amount of variance in the data. Principal component analysis makes use of this information in order to carry out its tasks. The direction that is responsible for the greatest amount of variation in the data is referred to as the first principle component. The direction that is responsible for the greatest amount of variation in the data in the opposite direction is referred to as the second principal component, and so on.    When it comes to embedding high-dimensional data for visualisation in a low-  dimensional space of two or three dimensions, t-distributed stochastic neighbour embedding (t-SNE) is a nonlinear dimensionality reduction technique that works effectively. t-SNE is an acronym for t-distributed stochastic neighbour embedding.  