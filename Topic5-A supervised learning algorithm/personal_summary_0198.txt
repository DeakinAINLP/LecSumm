Lesson Review: Dimensionality Reduction Module Learning Objectives  1.  Use clustering for revealing patterns from unlabelled data. 2.  Learn techniques to reduce dimensionality. 3.  Apply suitable clustering/dimensionality reduction techniques to perform unsupervised  learning of data in a real-world scenario.  Summarising the content: Curse of Dimensionality – refers to the various phenomena that arise when analysing and organising data in high-dimensional spaces. In the phenomenon, As the dimensionality of the features space increases,  the  number  of  configurations  can  grow  exponentially.  This  phenomenon  typically increases the amount of computational power required for processing and analysis.  Dimensionality Reduction – refers to the process of converting a set of data having vast dimension into data with fewer dimensions whilst making sure that it conveys similar information concisely.  Eigenvalues are a special set of scalars, and Eigenvectors are a special set of vectors, associated with a linear system of equations. For a given square matrix ‘A’ if a number ‘λ’ and a vector ‘u’ satisfy the condition ‘Au = λu’ then ‘λ’ is called an eigenvalue and ‘u’ is the corresponding eigenvector of A.  Finding Eigenvalues and Eigenvectors. To determine the values of λ:  Singular Value Decomposition – is a method of decomposing a matrix into three other matrices:  Data is represented as a cloud of points in a multidimensional space with one axis for each of the variables. The centroids of the points is defined by the mean of each variable.  Variance – Variance is the average squared deviation of its value around the mean of that variable. Covariance – is a measure of how changes in one variable are associated with changes in a second. The degree to which the variables are linearly correlated is represented by their covariances:  Covariance matrix – contains variances of all variables on the diagonal and co-variances among all pairs of variables in the off-diagonal entries. Written as:  Principle Component Analysis (PCA) The goal of PCA is to take n data points in d dimensions, which may be correlated, and summarise them by a new set of uncorrelated axes.  With PCA decorrelation the objective of PCA is to rigidly rotate the axes of ‘t-‘ dimensional axes to a new set of axes (called the principal axes) with properties: ▪  Ordered  such  that  principal  axis-  captures  the  highest  variance,  axis-2  captures  the  next  highest, and axis ‘d’ has the lowest variance.  ▪  Covariance among each pair of principal axes is zero – the principals’ axes are uncorrelated.  PCA via Eigen Value Decomposition:  1.  Compute data covariance matrix ‘C’ 2.  Perform Eigenvalue decomposition (EVD) as C = UDUT 3.  Reduced dimension data is given by:  Independent  component  Analysis  (ICA)  –  is  a  method  of  separating  a  multivariate  signal  into independent, non-Gaussian components. ICA is commonly used to separate signals that are mixed together, such as in a complex sound or image signal. The goal of ICA is to find a set of base functions that capture the signal’s underlying sources and use those to functions to separate the signal into its constituent parts.  Reflecting on the content: What is the most important thing you learnt in this module? How does this relate to what you already know? The content covered in this topic’s module was new to me, I have not previously worked with or learnt  these  decomposition  techniques.  In  this  regard  I  found  it  very  enlightening  to  gain  an understanding of different approaches to deal with the curse of dimensionality.  Why do you think your course team wants you to learn the content of this module? Understanding the curse of dimensionality is a significant problem when trying to develop machine learning models that have to deal with large datasets. In this case learning techniques that allow us to reduce the amount of dimension enables us to transform the data into a more computationally friendly format.  Evidence of Learning Evidence of learning can be found in the attached Jupyter Notebook file containing the code and solutions from activity 4.18.  Evidence of Self-assessment, with a sufficient score (at least 90%)  