High-dimensional data poses challenges in analysis and organization, and effective data representation is crucial for analysing such data. Dimensions of data include text data, image data, genomic data.  Curse of Dimensionality is a problem that arises when applying machine learning algorithms to high-dimensional data, as data becomes sparse and there aren't enough data locally, making it difficult to find patterns.  Concentration Effect: In high dimensions, the relative distance between points becomes negligible, reducing the utility of measures to discriminate between near and far neighbours, making clustering or KNN algorithms meaningless.  One way to solve the curse of dimensionality is to use dimensionality reduction -reduce the number of dimensions in a dataset while still conveying similar information concisely. This is done by finding important variables and projecting them onto a lower-dimensional space.  Eigenvalues and eigenvectors are used in linear transformations. Eigenvalues are constants, and eigenvectors are corresponding vectors. Orthogonal matrices rotate or reflect vectors without changing their length. Eigenvalues can be found by solving the characteristic polynomial, and eigenvectors can be found by solving a system of linear equations.  SVD is a matrix decomposition method. A matrix can be decomposed into three other matrices: U, Σ, and V. The diagonal elements of Σ are singular values of the matrix. The number of non-zero singular values is less than or equal to the rank of matrix A. Calculating SVD involves finding eigenvalues and eigenvectors of A and A transpose.  PCA aims to summarize correlated data points in multiple dimensions into a new set of uncorrelated axes called principal components, which capture the highest variance.  PCA finds directions of maximum variance in data by projecting it onto unit-length vectors. The eigenvectors of the covariance matrix are the principal components.  PCA is a technique used to reduce dimensions of data by finding its principal components. SVD can be used to implement PCA for cases where the number of data points is less than the number of dimensions.  Other dimensionality reduction techniques include Independent component analysis (ICA), Nonlinear dimensionality reduction technique, uMap.  