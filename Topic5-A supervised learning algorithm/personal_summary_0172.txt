Summary  Concentration effect:  It is observed that the relative contrast between near and far neighbors diminishes in high dimensionality space. This is called the concentration effect of distance.  This results into problems like:  1)  Clustering using KNN seems useless on a high scale. Thus, there arises a need to  compute better distance metrics.  2)  Until a better distance metric has been found, always aim to reduce the dimension.  Solving the concentration effect problem:  Conclusion:  Successfully reduced the dimension of the data into one dimension using the projection vector.  Eigenvalues and Eigenvectors:  Eigenvectors:  ÔÇ∑  unit vectors having magnitude equal to 1. ÔÇ∑  They are referred to as right vectors.  Eigenvalues:  ÔÇ∑  They are coefficients applied to eigenvectors that give the vectors their length or  magnitude.  Why is there a need for Eigenvectors and Eigenvalues in machine learning?  ÔÇ∑  These Eigenvectors and Eigenvalues are used in the most popular dimensionality  ÔÇ∑  reduction techniques. In PCA these concepts help in reducing the dimensionality of the data resulting in simpler model which is computationally efficient and provides greater generalization accuracy.   Singular value decomposition:  It is the method of decomposing a matrix into three other matrices as: ùëã  = ùëàùëÜùëâùëá  PCA:  The aim of PCA is to take n data points in d dimensions, which may be correlated, and summarize them by a new set of uncorrelated axes.  Where:  ÔÇ∑  Uncorrelated axes are called principal components. ÔÇ∑  These axes are linear combination of original d dimensions. ÔÇ∑  The first k components capture the max variance among the data points as  possible.  Covariance:  Covariance is how change in one variable is associated with change in another variable. Degree to which the variables are linearly correlated is stated as covariance.   A covariance matrix is the one that has variances of the variables present on the diagonal and covariances of the variable in the off-diagonal entries.  Steps for PCA:  ÔÇ∑  Standardize the data. ÔÇ∑  Calculate the covariance matrix. ÔÇ∑  Calculate the eigenvalue and eigenvectors. ÔÇ∑  Choose principal components: they are the eigenvectors with the highest  eigenvalue.  ÔÇ∑  Transform the data into lower dimensional space.  Applications of PCA:  ÔÇ∑  Used to visualize multidimensional data. ÔÇ∑  Used to reduce the number of dimensions in healthcare data. ÔÇ∑  PCA can help resize an image. ÔÇ∑  PCA can help find patterns in high-dimensional datasets.  ICA:  ÔÇ∑  ÔÇ∑  Independent component analysis is machine learning approach in which multivariate data is decomposed into distinct non- gaussian signals. It decomposes the mixed signal into the signals of its separate sources.  