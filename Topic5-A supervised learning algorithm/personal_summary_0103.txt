Topic – 4  Dimensionality in data:  Dimension in data can be considered as number of features in data. Consider a tabular dataset, we can consider each column in the table a dimension and each row as a sample. In a huge dataset, there might be a lot of features.  In such case, to analyse data we want to reduce the number of dimensions in the data and not lose any import information within the data. This is where dimensional reduction is useful.  Application of Dimensionality Reduction:  Dimensional reduction mostly applied to data to help analyse data by reducing the number of dimensions in data without losing any important data as well as to understand the correlation between the features. Additionally, it helps in reducing the noisy data in the dataset.  Curse of dimensionality:  Curse of dimensionality refers to increasing number of features, that is number of number of dimensions in the data which in turn leads to increasing the sparsity of data. Where identifying the correlations between data points is much harder. In other words, more sparsity among data means a greater number of features leading to more difficulty in finding correlation in data.  To solve this problem, we make use of Dimension reduction method that help in reducing the number of dimensions in data while retaining the most important features and minimizing the sparsity. These are very powerful techniques and improve machine learning models performances.  Most commonly used techniques of dimension reduction methods are principal component analysis (PCA), Independent component analysis (ICA), Nonlinear dimensionality reduction technique and Umap.  Principal component Analysis (PCA):  The aim of PCA is to reduce the number of dimensions in dataset and by transforming to new variables such that they preserve the as much as information possible from the original dataset, such that it is easily able to remove redundant information and noise data from the original dataset which slows the progress of machine learning algorithm. The first step in PCA is identify the relationship between the features this is done by identifying the correlation among the variable, to identify this PCA computes a covariance matrix which has same number of dimensions as the original data. Later, to identify the principal component from the covariance matrix -It uses linear algebra concepts Eigenvectors and Eigenvalues to determine the first principal component of data representing the direction with maximum amount of variance, with each subsequent component less important information is produced which are orthogonal to the first principal component and uncorrelated to the first principle.  Independent component analysis (ICA):  Independent component analysis like Principal component analysis but unlike PCA which focus on reducing information and finding hidden pattern in data, ICA transform the given dataset into maximally independent set where the number of inputs is equal to the number of outputs which are mutually independent. ICA focus on two key area   a.  Independent components: Producing independent components from the provided data set where the produced datasets outputs are not dependent on other input variables.  b.  Non- Gaussian: Non – Gaussian distributions states that the produced independent variable  distribution doesn’t have a bell-shaped curve.  Non-Linear dimensionality reduction technique:  Unlike PCA and ICA, Nonlinear techniques doesn’t makes use linear techniques instead uses much more in-depth mathematical techniques to capture underlying structure of data. As such T-SNE is one of the methods.  T-Distribution Stochastic Neighbour Embedding (T-SNE):  T-SNE is a dimensionality reduction technique commonly used for visualization of higher dimensional dataset into lower dimension. T-SNE algorithm by finding similarities between datapoints with the help of pairwise in original higher dimensions, and then map the similarities onto lower dimensional data points. T-SNE has a wide range of use in applications such as image or text analysis and other such. While most prominently it used for exploratory analysis and data visualization.  Uniform Manifold Approximation and Projection (UMAP):  Umap is dimension reduction method technique makes use of visualisation technique like t-SNE. The Umap algorithm focuses by calculating similarity scores between datapoint to identify clusters in higher dimension graph and then preserve the clusters and plotting them onto lower dimension graph. This method is relatively faster compared to T-SNE.            