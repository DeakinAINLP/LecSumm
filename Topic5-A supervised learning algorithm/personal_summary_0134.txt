This topic's major content deals with data dimensions. Including dimensionality disaster, dimensionality reduction, and Python practice.  Curse of Dimensionality:  Usually  use  data  of  multiple  dimensions,  and  such  data  is  more  accurate.  However,  high- dimensional data will make it difficult for algorithms to run, and high-dimensional algorithms differ from low-dimensional ones.  Dimensionality reduction  Before  high-dimensional  algorithms  mature,  data  dimensionality  needs  to  be  reduced.  The original data can be linearly projected through the mathematical method of linear algebra. PCA is such an algorithm, which is widely used (such as face recognition). There are many other ways to reduce  dimensionality.  Examples  include  ICA  and  t-SNE.  t-SNE  is  a  non-linear  dimensionality reduction algorithm and cannot be evaluated using reconstruction error and variance.  python practice  The  amount  of  information  lost  after  PCA  dimensionality  reduction  was  looked  at  using  reconstruction error. Quickly perform PCA and t-SNE and visualize using built-in functions.  Extracurricular reading list: 1.  .corrcoef  