 This topic I have learned about Dimensionality Reduction and why it is important in Machine Learning. We use machines because we cannot visualize more than three dimensions by ourselves. After third or fourth dimension, it is nearly impossible for humans to visualize the data in higher dimensions. It is somewhat true for machines as well, however from a different perspective. Higher dimensions require higher computational power and that is also one of the reason, deep learning model take hours or even days to get trained.  Here comes the curse of dimensionality, as we increase the number of dimensions, there is more room for the data, which makes the distance between datapoints less significant. To solve this curse, we reduce the dimensions of the data yet preserve maximum amount of information.  Eigenvalue and Eigenvectors are core concepts of how PCA (Principal component analysis) works and able to reduce the dimensions of the data. PCA takes data with n number of features, where n is a high number, and reduce it to small axes where the axes are almost uncorrelated. Later, the topic, I have seen that how PCA is a great example of image recognition.  There are other dimensions reduction techniques as well, like ICA (Independent component analysis) and Nonlinear dimensionality reduction, U-Map, etc. Later, I have learned how to apply these techniques in Python programming language.     