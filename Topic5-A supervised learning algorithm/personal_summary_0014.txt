  High dimensional data is data with too many features which makes analysis complex and slow    High dimensionality makes data sparse because the number of possible combinations can grow exponentially and appear only once or twice making each observation look very different from the others.    The notion of distance between observations becomes less and less unful in high dimensional spaces, this is known as distance concentrations. When there are too many features, observations become harder to cluster as it makes all observations look more equal and measures like Euclidean distance become ineffective.    You can overcome the curse of dimensionality with dimensionality reduction. This is process of converting many dimensions of data into fewer dimensions while making sure it conveys the same meaning but just more concisely.    One way to perform dimensionality reduction is to look for linear relationships between  features and only retaining one, Eigenvalues and eigenvectors are prominently used in the analysis of linear transformations.    Eigenvectors are vectors who do not change direction when a linear transformation is performed on them. The Eigenvalue is the scalar used to transform an Eigenvector.    Eigenvectors and eigenvalues are used to reduce noise in data. They can help us improve efficiency in computationally intensive tasks. They also eliminate features that have a strong correlation between them and also help in reducing over-fitting.    Singular value decomposition (SVD) is a method of decomposing a matrix into three other matrices. It is attempting to identify a relevant sub-space of all of the dimensions created by the noise in the data, or in other words reduce all the dimensions down to a small number of meaningful ones.    Principal Component Analysis reduces a large number of features in data to a core set of the most useful features which are generally uncorrelated and have high variance. It works by looking first for the strongest underlying feature in the data set, then it looks for  the next strongest underlying feature that isn’t correlated with the previous feature found and so on until a core set of uncorrelated features is determined.    Singular Valie Decomposition (SVD) can be used to perform Principal Component Analysis (PCA)    The major drawback of PCA is that you can lose interpretability of features and without   domain expertise it’s hard to know what the top components represent. Independent component analysis (ICA) it another dimensionality reduction technique, it finds a set of basis functions that capture the signal's underlying sources and then uses these functions to separate the signal into its constituent parts. t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction technique which visualizes high dimension data into two or three dimensions. It looks for patterns and relationships in the data and then represents those patterns in lower- dimensional space to reduce the complexity of the data.    uMap is another technique for dimensionality reduction which is very similar to tSNE  