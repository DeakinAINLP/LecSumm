 In the ﬁeld of machine learning, one has to deal with large amounts of data. Large amounts of  data  implies  large  amount  of  features,  each  feature  represen(cid:415)ng  a  dimension.  A  large number of dimensions might result in ineﬃcient analysis process. Therefore, we leverage the methods of dimensionality reduc(cid:415)on, which was the core focus during the course of this topic 4. We were introduced to the following concepts:  1.  Eigen  Values  and  Eigen  Vectors:  They  are  used  in  the  analysis  of  linear  transforma(cid:415)ons. We learnt about the formula:  A*u = lambda * u  where lambda is a number (eigen value) and u is the eigen vector.  2.  Single Value Decomposi(cid:415)on (SVD): is a method of decomposing a single matrix into 3  other matrices.  X = USVT  3.  Typical dimensionality in data: With increase in dimensions, the volume of space of data also increases, so the distance between the data points also increases and the model will not be very accurate. When  viewed  on  a  sphere,  the  data  points  will  start  si(cid:427)ng  on  the  surface  and  the clustering process becomes ineﬀec(cid:415)ve because of the increase in distance.  4.  Concentra(cid:415)on Eﬀect: Rela(cid:415)ve contrast between near and far neighbors diminishes as  the dimensionality increases.  5.  Dimensionality  Reduc(cid:415)on: Refers to  the  process  of  projec(cid:415)ng/condensing the data with high number of dimensions into a dataset with smaller dimensions and therefore helps to mi(cid:415)gate the above issues of dimensionality.  Dimensionality Reduc(cid:415)on  Components Base  Projec(cid:415)on Base  PCA  ICA  t-SNE  UMAP  ISOMAP                 6.  Principal Component Analysis: This is the process of condensing all related data into 2 or 3 components which are not correlated with each other. This helps us achieve dimensionality reduc(cid:415)on and is eﬃcient while modelling and performing further analysis with the data.  7.  T-Stochas(cid:415)c  Neighbor  Embedding:  The  t-distributed  stochas(cid:415)c  neighbor  embedding  is  a nonlinear  dimensionality  reduc(cid:415)on  technique  for  embedding  high-dimensional  data  for visualiza(cid:415)on  in  a  low-dimensional  space  of  two  or  three  dimensions.  It  achieves  this  by modelling  each  high-dimensional object by a 2D  or 3D  point in such  a  manner that  similar objects are modeled by  nearby points  and  dissimilar objects are modeled by distant points with high probability.  In short, it converts similari(cid:415)es between data point to joint probabili(cid:415)es and try to minimize the Kullback-Leibler divergence between the joint possibili(cid:415)es of the low- dimensional embedding and the high-dimensional data.  Therefore, t-SNE might be more suited while reducing high dimensionality data to low dimensionality, whereas PCA might be more suitable for modelling and analysis.       