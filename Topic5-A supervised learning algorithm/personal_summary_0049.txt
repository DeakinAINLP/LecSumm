 Dimensionality in Data  When dealing with data sources with high dimensionality, there are many issues that can arise, such as data representation and interpretation. Some examples of high-dimensionality data include images, and genomic data.  The Curse of Dimensionality  The curse of dimensionality describes the unique problems that arise in machine learning when analyzing and organizing data in high dimensional spaces. As the number of dimensions increases, the number of regions grows exponentially. As the number of regions grows, the amount of space increases making the data sparse and not useful. The computational load of processing this dataset also grows.  Concentration Effect  The concentration effect describes how as dimensionality increases, the proportional difference between the farthest point distance and the closest point distance diminishes.  “So it reduces the utility of the measure to discriminate between near and far neighbors. Relative contrast between near and far neighbors diminishes as dimensionality increases. This is known as the concentration effect of the distance measure.”  Therefore clustering can become meaningless in high dimensions, although patterns still exist in high dimensions. We should aim to reduce dimensionality where possible.  Solving the Curse  If numeric variables are exactly the same, they are likely to be redundant. Some variables may be irrelevant to the model. Removing redundant or irrelevant features is important for reducing dimensionality.  Note: This unit only covers linear dimensionality reduction.  github.io/folium/modules.html  Eigenvalues and Eigenvectors   Eigenvalues and Eigenvectors are prominently used in the analysis of linear transformations.  Single Value Decomposition  Singular Value Decomposition (SVD) is a method of decomposing a matrix into three other matrices. It can be performed on a matrix of any size and dimension  Principal Component Analysis (PCA)  PCA is a deterministic algorithm (as opposed to K-means which is non-deterministic). The goal of PCA is to summarize a set of n data points and d dimensions, which may be correlated, with a new set of uncorrelated axes.  The uncorrelated axes are called principle components or principle axes. These axes are linear combinations of the original d dimensions. The first k components capture as much of the variation (or variance) among the data points as possible. ‘  Variance Across Each Variable  Variance can be defined as the average squared deviation of its n values around the mean of that variable.  Covariance Among Variables  “To put it simply, covariance is a measure of how changes in one variable are associated with changes in a second variable.  Degree to which the variables are linearly correlated is represented by their co- variances.”  Covariance Matrix     The covariance matrix contains the variances of all the variables on the diagonal and the covariances amongst pairs of variables off the diagonal.  PCA: Decorrelation  “The main objective of PCA is to rigidly rotate the axes of the t-dimensional axes to a new set of axes (called principal axes) that have the following properties:  -  Ordered such that the principle axis (1) captures the highest variance, axis 2 captures the next  highest variance, etc.. and axis-d has the lowest variance.  -  Covariance among each pair of the principal axes is zero (the principal axes are uncorrelated ie.  They are orthogonal to each other). This is called decorrelation property”  PCA Via Eigen Value Decomposition  We can use eigenvalue decomposition to perform principal component analysis.  1.  First we must comput the covariance matrix C 2.  Then perform eigenvalue decomposition as per the below.  