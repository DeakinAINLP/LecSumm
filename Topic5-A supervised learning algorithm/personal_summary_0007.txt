The curse of Dimensionality is shown in machine learning when applied to high dimensional data. As when the dimensionality increases, the volume of the space increases so quick that the available data becomes sparse. This is a problem as there isn’t enough local data. As when describing items we need to add dimensions in the form of features to differentiate items from one another. A human being classified with number of arms and legs alone would include most other mammals. So we need to keep adding features and if we want to differentiate not just the type of object but the individual objects from each other then the dimensions increase far beyond the ability to comprehend and computationally manage the data. To solve the curse we need to look at all the variables we have collected. We remove the irrelevant ones for our scenario as well as redundant variables, such as ones that are all exactly the same. Or even just grouping them all together. The curse calls for dimensionality reduction, changing the data set to have fewer dimensions whilst still holding the important information to convey the same information. Eventually we want to be able to find some sort of connections between the data, linear or non linear lines. Eigenvalues and eigenvectors are used in the analysis of linear transformations. For a square matrix A if a number x and a vector u satisfy the condition Au = xu then x is called an eigenvalue and u is the corresponding eigenvector of A. For a matrix A of size d x d eigenvectors and eigenvalue pairs. It is only possible to have k nonzero eigenvalues for A. A = UDUT is called the Eigenvalue Decomposition of matrix A. The matrix  U is called full eigenvector matrix. The matrix U is always an orthogonal matrix that rotates the coordinates in a way to decorrelate the data dimensions. When applied to a linear transformation vector every orthogonal matrix will either rotate or reflect the vector without changing its length. Singular value decomposition (SVD) is a method of decomposing a matrix into 3 other matrices. X = USVT. Where X is a n x d matrix. U is a n x d orthogonal matrix. S is a d x d diagonal matrix, V is a d x d orthogonal matrix. SVD in linear algebra is a factorization of a complex or real matrix. The SVD represents an expansion of the original data in coordinate system where the covariance matrix is diagonal. A PCA’s goal is to take n data points in  d  dimensions, which may be correlated and summaries them by a new set of uncorrelated axes. These axes are called principal components or principal axes. They are linear combinations of the original d dimensions. The first k components capture as much of the variation within the data points as possible. The data is represented as a cloud of points in a multidimensional space where each variable has its own axis. The centroid is the mean of each variable and the variance of each variable j is the average squared deviation of its  n values around the mean of that variable. Covariance is the measure of when changing one variable causes a changed in a second one and their co-variance. PCA can be used where n < d. SVD can be used to reduce the complexity of the computations in some cases instead of EVD. Independent component analysis (ICA) is a method of separating multivariate signal into independent, non-gaussian components. It separates signals that are mixed together like complex sound or image signal. ICA finds basic functions that capture the signals underlying sources and uses these function to sperate the signal. 