Learning Summary Data Dimensionality: Some of the typical dimensions of data that we deal are:    Text Data: Let’s take a news website for instance, if we initialize the crawling process of the news website for a topic let’s say it will typically have around 10,000 dimensions, this number is  the  size  of  the  dictionary  one  has  to  build  on  the  basis  of  the  words  extracted  from  a document. Image Data: Let’s say if pixels are to be used as feature for a 128x128 image then, there would be 16,384 dimensions within that image.      Genomic Data: For instance, Parkinson’s disease case-control data comprises of 408,803 Single-Nucleotide polymorphisms  (SNPs) on the other Alzheimer’s has 380,157 SNPs.  Curse of Dimensionality: This happens when it is applied to machine learning algorithms to high- dimension data. Unique problems are faced in machine learning when  analyzing and organizing data within high-dimensional spaces, when the dimensionality tends to increase, the volume of the space increase very quickly making the available data sparse making it problematic as there isn’t sufficient data locally.  Concentration  effect:  Assuming  the  ratio  of  the  variance  of  the  length  of  any  vector  point converges to zero and combining it with increasing data dimensionality, this will lead to eradicate the proportional difference and the closest-point distance. Hence, it will reduce the utility of the measure  of  discrimination  among  near  and  far  neighbors,  this  happens  when  dimensionality increases. This problem implies:    KNN Algorithms or Clustering might be meaningless  in high dimensions, on the other  hand they  might  have  patterns  within  high  dimensions.  Hence,  just  better  distance  metrics  are required.    Until  and  unless  better  distance  metrics  are  developed  the  reduction  of  dimensionality  is  required  wherever possible.  Dimensionality Reduction: The process of converting set of data comprising of vast dimensions into data with less dimensions while still ensuring that it conveys similar information in a concise manner.  Eigenvalues and Eigenvectors: They are used for the analysis of linear transformations, where  for a  given  square  A  if  a known  as Eigenvalue. Whereas,  is corresponding eigenvector of A.  number  and  vector  satisfies,  condition:  then  that  is  For finding the Eigenvalues of a matrix A can be done by solving characteristic polynomial within  :   Since, it is already known that  is not wanted, meaning that we  desire the second case,  allowing us to find the eigenvalues for a matrix. Hence, we need to determine the value of  for  where the roots  of polynomial are the eigenvalues of  the which we get, matrix  A.  When  all  eigenvalues  are  obtained,  an  eigenvector  corresponding  to  a  particular  eigenvalue will be obtained by solving out:  Singular  Value  Decomposition:  A  method  of  decomposing  a  matrix  into  3  other  matrices:  . It is the factorization of real or complex matrix where it represents an expansion of the original data within a coordinate system where the covariance matrix is said to be diagonal.  PCA: The objective of PCA is to take n data points in d dimensions, where they might be correlated and are summarized by a new set of uncorrelated axes, these are known as principal components. These axes are linear combinations of the original d dimensions. The initial k components capture as much of the variations possible between the data points. The main objective however, is rigidly rotating the  axes of  t-dimensional axes  to a  new set  of axes  which comprise  of the  following properties:    Ordered in such  a manner that  principal axis-capture the  highest variance, axis  2 the  next  highest and so on, where axis-d comprises of the lowest of them all.    Covariance  among  each  pair  of  the  principal  axes  is  zero,  this  is  known  as   decorrelation  property.  Some other  dimensionality reduction techniques include:  Independent Component  Analysis:  A method  for  the  separation  of  a multivariate  signal  into independent, non-Gaussian components. It  is commonly used for  separating signals which  are mixed together like in a complex sound or an image signal. It’s objective is finding a set of basis functions which capture the  signal’s underlying sources  and then these  functions are used  for separating the signals for its constituent parts.  Nonlinear dimensionality reduction technique: It uses linear  algebra for the identification of patterns within data, where it uses sophisticated mathematical techniques for  the identification and capturing of underlying structure  of data.  tSNE (t-Distributed  Stochastic Neighbor  Embedding): A  technique used  for visualizing  high- dimensional data into 2 or 3 dimensions,  which works by finding out relationships and patterns within the  data  and represent  those  patterns  in a  low-dimensional  space for  the  reduction  of data complexity.  