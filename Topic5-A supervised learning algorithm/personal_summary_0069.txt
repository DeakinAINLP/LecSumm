 Eigenvalues and Eigenvectors: used in the analysis of linear transformations  For a matrix of size, there are eigenvectors and eigenvalue pairs  It is only possible to have only non-zero eigenvalues for the number of non-zero  eigenvalues are equal to the rank of the matrix.  The matrix is always an orthogonal matrix that rotates the coordinates in a way to  de-correlate the data dimensions.  Singular Value Decomposition(SVD)  SVD is a method of decomposing a matrix into three other matrix where is:   a orghogonal matrix   a diagonal matrix with elements   a orthogonal matrix  The SVD represents the expansion of the original data in ta coordinate system where  the covariance matrix is diagonal.  Typical Dimensionality of data: text data, image data, genomics data  Curse of Dimensionality  As the number of dimensions increases, the number of regions grows exponentially,  that makes the data sparse and somehow not useful anymore, some of the intuitions  from low dimensional spaces fail badly in high dimensions.  In high dimensional spaces, most of the training data resides on the surfaces of the  hyper-sphere or corner of the hypercube. The distance from origin to all different  points become similar the curse of dimensionality result in less distinctive distances  in high dimensions. Concentration effect points out relative contrast between near  and far neighbors diminishes as the dimensionality increases.  Principal Component Analysis(PCA)  The goal is to take data points in dimensions, which may be correlated and  summarizes them by the new set of uncorrelated axes which called principle  components or principal axes. Principal components are sorted in descending order  based on captured variance along each axis.  