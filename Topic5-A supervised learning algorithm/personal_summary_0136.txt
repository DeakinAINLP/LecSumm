 The Main points that I’ve covered in the fourth topic are as follows:  ➢  I have understood that why we reduce the dimensionality of data to fit the algorithms ➢  I got the practical exposure towards the dimensionality reduction approach with python  language and the inbuilt functions to implement Principal Component Analysis.  ➢  My Learning summary Based on 4th Topics Lecture was reducing the dimensionality reduction of different data such as text data, image data and genomic data. Through this technique we can reduce the total number of features from huge datasets or from high dimensional data to make clear visualizations. I have practiced and solved the curse of dimensionality in this topic. For the depth of this knowledge, I have tried the activity for finding eigenvectors and eigenvalues of a 3×3 matrix. At the end I got familiar with some similar dimensionality reduction techniques such as t-SNE (t- Distributed Stochastic Neighbour Embedding) to visualise the high Dimensional data in 3 dimension  Knowledge gained with Lecture slides.  In the Overall topic 4 I have gained the Knowledge of understanding the data and reducing the dimensionality to get a clear and understandable visualisation. I have used the scikit learn libraries to load datasets and implement reductions techniques. All the slides have a vast understanding of how to pre-process data before fitting which is so much important to get an appropriate solution to the problems. Hence Principal Component Analysis and as t-SNE (t-Distributed Stochastic Neighbour Embedding)algorithms are best algorithms for high dimensional data as I have implemented these models in this task 4.1.  Hence the Topic two was very useful and interesting by understanding the concept of Dimensionality Reduction Techniques.  