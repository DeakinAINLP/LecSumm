Learning Notes and Summary of Notes 2 Topic 4 provided a solid synopsis on dimensionality with features.  Large  datasets  with  multiple  features  and  dimensions  generate  a  “curse  of  dimensionality”.  This essentially means that the regions and “relationships” between datapoints can become obfuscated and or  spare  with  large  distances  between  them.  Obfuscated  (or  sparse)  and  not  relatable  data  is  not learnable as no discernible pattern exists.  A key point is the factorisation of matrices (datasets). This is known as Singular Value Decomposition or SVD. This is a key element in breaking down and generating a smaller matrix with the “relationships” to be relatable to the larger more complex and dimensioned matrix.  Another key term is PCA or the principal component analysis which facilitates the reduction of dimensions by rotating the “axis” of the matrix. Also could be known as “Orthogonal transformation”  PCA  however  is  an  excellent  tool  for  enabling  visualisation  of  multi-dimensional  data,  e.g  it  is  really difficult to imagine a 4D plot or graph and for data decorrelation.  There  are  other  linear  methods  of  reducing  dimensions  such  as  separating  a  multivariate  signal  into independent components (known as ICA or independent component analysis).  There  are  also  several  non-linear  mathematical  methods  such  as  t-Distributed  Stochastic  Neighbor Embedding (t-SNE) or Umap.  Lastly we learnt about the python implementations which is best covered in Section 4 of this report for reference. For generating code, the Scikit-learn modules are very helpful.  