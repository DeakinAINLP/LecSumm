Highly dimensional data includes text, image, genomic. One example would be keyframe observations used for SLAM in robot navigation. The number of regions increases exponentially with the dimensionality, so the size of the input for the model, required training duration and amount of data required increases making it unworkable. In high dimensions, most of the data is in the corners of the hypercube, and many of the regions have little or no data (sparse). The distance between data points is relatively similar in high dimensions (distance concentration). Finding the principal components which affect the output is one method to reduce the number of dimensions to reduce the effect of this issue. Presence of a few instances which appear frequently as nearest neighbours (hubs) relates to the intrinsic dimensionality of the data. Using PCA will not remove hubs as reducing dimensionality below the intrinsic level would lose too much information, and hubs may still be present which affect the output of clustering approaches. PCA uses the eigenvalues to define the reprojection of data which has maximum variance in the direction of the eigenvectors, without changing the length. This allows redefinition of the axis used and captures the same information in fewer dimensions. To find the eigenvalues of A, solve det(A –λI) = 0，Then substitute these values in for lambda in Au =  λu to find the eigenvectors Singular value decomposition The properties of AAT and ATA guarantee that the eigenvalue decomposition exists, so SVD can be determined for all matrices. The sigma values are unique and ordered in descending order. PCA uses the SVD to determine orthogonal axes (the eigenvectors) for the feature matrix, ordered by the eigenvector values in descending order, so that the first eigenvector has the most influence. This is equivalent to finding the (eigen)vector which maximises the SSD from each point to the origin, using the average distance as the eigenvalue, then repeating until all components have been found. By using PCA and selecting a smaller number of components, some data will be lost, but this is useful for a classifier as it prevents overfitting to noise. A disadvantage of PCA analysis is that it is a linear decomposition method and will not accurately represent non-linear data. ICA is another method for separating out individual signals making up a composite signal, particularly as used in signal analysis for biological systems eg electroencephalogram. It assumes at most one underlying independent signals is gaussian. It then minimises the mutual information, or maximises the difference from Gaussian distribution to separate the signals. There are linear and non-linear versions of ICA. t-Distributed Stochastic Neighbour Embedding is a non-linear method of analysis which maintains the local structure of the data. It computes similarity between the high dimensional data points, and student t distribution in low dimensional space. Then it finds the difference between the two using gradient descent to update the low dimensional points. The hyperparameter is perplexity (exponentiating the cross entropy) uMap is similar but uses a fuzzy graph and compresses it into a lower dimension. The hyperparameter is number of neighbours. It may be faster to compute by not examining every point,and balance local and global structure better providing better insights. 