The topic started off with understanding the issues that come up when working with data in a high dimensional space i.e the curse of dimensionality which arises when applying machine learning algorithms to high dimensional data. To tackle this, comes dimensionality reduction which minimizes the number of features used in machine learning algorithms. In definiton, I understood that dimensinality reduction the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely.  The first dimensionality reduction technique used in the topic's coursework is PCA(Principal Component Analysis). This technique works by taking n data points in d dimensions, which may be correlated, and summarises them by a new set of uncorrelated axes called principal components. Other than this I learnt how to implement PCA using Single Value Decomposition as well as some of its applications for example in facial image analysis.  