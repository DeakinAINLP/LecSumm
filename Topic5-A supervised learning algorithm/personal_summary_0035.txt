This topic we explore  Solving the curese of dimensionality Eigenvalues and eignvectors Princpial component analysis (PCA)  There are many issues that arise when analysing and organising data in high-dimensional spaces.  Text data:  Imagine a News website. If you start crawling the news on the website for a short period of time such as a topic, depending on the number of documents you crawl, it is typical to have more than 10,000  dimensions. This number is the size of the dictionary you have to build based on the words you extracted from the News documents. We need to represent each document based on the words in a dictionary (remember. the feature vector covered in topics 1 and 2: Data representation).  Image data: Imagine we would like to use pixels as features, just an  image would have 4,096  dimensions!  Genomic data: Take Parkinsons disease case-control data as an example. It has 408,803  Single- nucleotide polymorphisms (SNPs) and Alzheimer’s disease has 380,157 SNPs.  At its core the curse of dimensionality states that as the number of dimensions icreases the number of regions grows exponentially. As the regions grows the data becomes sparser and less useful. For example, checking on a neighbouring data point wouldn’t work in a space with many dimensions.  Relative contrast between near and far neighbours diminishes as the dimensionality increases. This is known as the concentration effect of the distance measure.  Clustering or KNN algorithms may be meaningless in high dimensions. However, there might still be patterns in high dimensions. We just need better distance metrics. So Research is needed!  Until we develop better distance metrics, we should aim to reduce the dimensionality where possible.  Solving the curse Dimensionality reduction refers to the process of converting a set of data into a set with fewer dimensions while still maintaining the information.  Real world examples where two variables have linear or noisy-linear relationships  Height and weight: Taller people tend to weigh more than shorter people but there is variation due to muscle mass, bone density. Temperature and air pressure: As the temperature increases, air molecules tend to move faster and spread out, which can lead to a decrease in air pressure. This relationship can be affected by factors like altitude, humidity, and wind.  Eigenvalues and Eigenvectors Eigenvalues and eigenvectors are prominently used in the analysis of linear transformations.  In a nutshell, they help us understand how a matrix (a rectangular array of numbers) behaves when we multiply it by a vector (a column of numbers).  An eigenvector is a special vector that, when multiplied by a matrix, gets stretched (or shrunk) by a certain amount. The amount it gets stretched by is called the eigenvalue. In other words, an eigenvector is a vector that keeps pointing in the same direction even after it's been transformed by a matrix, just stretched or shrunk.  Eigenvalues of a matrix A can be found by solving the characteristic polynomial in  λ  Singular value decomposition Singular value decomposition (SVD) is a method of decomposing a matrix into three other matrices:  X= USVT  Where:  X is a n x d matrix U is a n x d orthogonal matrix S is a d x d diagonal matrix V is a d x d orthogonal matrix  In linear algebra, the SVD is a factorization of a real or complex matrix. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal.  PCA The goal of PCA is to take n datapoint in d dimensions, which may be correlated, and summarises them by a new set of uncorrelated axes. These are called principal compoennts or principal axes they are linear combinations of the original dimensions  Variance  across each variable Data is represented as a cloud of points in a multidimensional space with one axis for each of the variables. The centroid of the points is defined by the mean of each variable  Covariances among variables Covariances is a measure of how changes in one carriable are associate with changes in a second variable.  Covariance Matrix The covariance matrix is a matrix that contains variances of all variables on the diagonal and co- variances among all pairs of variables in the off-diagonal entries  Formulation of PCA and deriving principal components  Standardise the data.  This is important because PCA is sensitive to the scale of the variables. Calculate the covariance matrix of the standardised data. Find the eigen values and eigenvectors of the covariance matrix. The first principal  component is the eigenvector corresponding to the largest eigenvalue, the second principal component is the eigenvector corresponding to the second largest eigenvalue, and so on.  Transform the data into the space of the principal components by multiplying the  standardised data matrix by the matrix of eigenvectors.  The resulting transformed data matrix has the same number of rows as the original data matrix (i.e., the same number of samples), but fewer columns (i.e., the number of principal components we chose to keep). The transformed data matrix can be used for visualization, clustering, classification, or any other analysis that requires a lower-dimensional representation of the data.  Other dimensionality reduction techniques Independent component analysis (ICA) ICA is a method of separating a multivariate signal into independent, non-Gaussian components.  In signal processing, machine learning, and neuroscience, ICA is commonly used to separate signals that are mixed together, such as in a complex sound or image signal. The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then use these functions to separate the signal into its constituent parts.      