In this topic we studied some major points like Eigenvalues and Eigenvectors, singular value decomposition (SVD),  curse of dimensionality and its solution. Also we learned principal error formulation, comparison between components based and projection based dimensionality reduction,  Eigenvalue and Eigenvectors is an important part of dimensionality reduction technique like principal component analysis (PCA), its main goal is to find a set if linearly independent variables that captures most of the data variability. In this first we calculate covariance matrix of data, eigenvectors of this covariance matrix responds to principal components of data and eigenvalues corresponds to the amount of variability explained by each component. The Eigenvectors with the largest eigenvalues represent the directions in which the data varies the most and they provided the most informative dimensions to represent the data in a lower dimensional space. By selecting only the eigenvectors with the largest eigenvalues, one can reduce the dimensionality of the data while preserving most of its variability. This is because the selected eigenvectors represent less important variations in the data.  The number of features or variables present in data are called as dimensionality of data, it could vary widely depending on the type of data, application and the method used in data collection.  Like a dataset of image might have dimensions that corresponds to the number of pixels in the image, which results in high dimensional data with potentially thousands or millions of dimensions, whereas a data of survey res ponse might have relatively small number of dimensions. It is common for dataset in most fields to have anywhere to have dozen to thousands of dimensions.  SVD is a matrix factorization technique that decomposes a given matrix into three matrices, which can be  used to analyze the properties of original matrix and solving problems like linear algebra, single processing and machine learning. Data compression, image processing, collaborative filtering and principal component analysis are some important applications of SVD, It is a widely used technique in data analysis and machine learning due to its flexibility, efficiency and ability to capture the essential structure of high dimensiona l data.  The curse of dimensionality reduction can be explained as the complexities and  limitations that arise when attempting to reduce the dimensionality of high dimensional data to a lower di mensional representation, there are chances of loosing some information during this process. There are some best practices to be done so as to mitigate this problem, like choosing appropriate technique, evaluating the result while dimension reduction through visualization and clustering technique, optimizing hyper parameters, considering non-linear techniques etc.  Principal error formulation is a way to formulate PCA as an optimization problem, where the goal is to find the principal components that minimize the construction error.        