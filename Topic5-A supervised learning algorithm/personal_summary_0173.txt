In this we started off with dimensionality in data that is when machine learning algorithms are used on highly dimensional data, The Curse of Dimensionality occurs.  When  assessing  and  arranging  data  in  high-dimensional  areas, machine learning presents a set of particularly challenging issues. The volume of the space grows so quickly as the dimensionality rises that the accessible data become sparse. Given the dearth of local data, this is quite problematic. We even got to know about the Dimensionality Reduction is  required  to  avoid  The  Curse  of  Dimensionality. Dimensionality reduction is the technique of reducing the number of dimensions in a piece of data while maintaining the ability to concisely communicate the same information.  Eigenvalues and Eigenvectors are prominently used in the analysis of  linear  transformations  and  then  about  the  Singular  value decomposition (SVD) is a method of decomposing a matrix into three  other  matrices.  Then  PCA  By  using  a  new  set  of uncorrelated  axes,  PCA  aims  to  summarise  n  data  points  in  d dimensions  that  may  be  correlated.  We  got  to  know  about formulation of PCA and deriving principal components. Then we used PCA to perform with eigenvalue decomposition.  Implementation of PCA and using SVD to perform PCA and also we had we  real  world  example  like  using  PCA  in  facial  image  analysis.  The procedure outlined was to Create  the  data's  covariance  matrix.  Find  the fundamental eigenvectors  that  best  capture  the  data.  When  using  K  principle eigenvectors,  determine  the  energy  required  to  maintain  the appearance  of  faces.  Data  projection  using  only  K  principle eigenvectors and K axis of variation preservation.        There are other dimensionality reduction techniques like independent component  analysis  that  is    multivariate  signal  can  be  divided  into independent, non-Gaussian components using the ICA technique. ICA is  frequently  used  in  signal  processing,  machine  learning,  and neuroscience  to  separate  mixed  signals,  such  as  in  a  complicated sound  or  image  signal.  In  order  to  decompose  a  signal  into  its component pieces, ICA aims to identify a set  of basis functions that capture  the  signal's  underlying  sources.  Nonlinear  dimensionality reduction techniques In contrast to linear techniques, which employ linear algebra to find patterns in data, nonlinear techniques use more complex  mathematical  methods  to  recognise  and  capture  the underlying structure of the data. For the purpose of visualising high- dimensional data in two or three dimensions, the t-SNE (t-Distributed Stochastic  Neighbour  Embedding)  technique  is  utilised.  In  order  to simplify the data, it works by identifying patterns and relationships in the data and then portraying those  patterns in a lower-dimensional space.  We  learnt  how  to  set  up  the  required  packages  and  we  had some  hands  on  coding  experience  on  all  the  topic  which  was discussed. Quiz Snapshot:      