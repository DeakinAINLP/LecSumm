This topic’s content ties into the previous topic as it involves clustering. The focus of this content is the issues that arise from datasets with a high dimensionality and how eigenvectors and eigenvalues as well as principal component analysis can help us solve this curse. We also dive into the Python implementation of this. What is a dimension in data? Unfortunately for the Interstellar fan in me, we aren’t referring to the existence of further planes of reality represented in the form of a childhood library to interact with ones past self. Instead, a dimension in the context of data is a feature or variable. For example, a dataset that includes the gender of participants is one dimensional. However, a dataset including the gender and height of participants is two dimensional, and one including the gender, height and weight is three dimensional. As seen, it is important to have dimensions within data to help provide useful information, however, too much data can lead to processing and storage issues. This is known as the “Curse of Dimensionality”. The Curse of Dimensionality Once again, we aren’t discussing the script for what could be a pretty good rip off horror sequel to Interstellar. What we are referring to is the issues that arise when working with datasets that have a high level of dimensionality. As the number of dimensions in a dataset increases, we begin to see a higher prevalence of a number of issues. The most predominant of these are – 1.  An increase in sparsity meaning that as there are more dimensions, the datapoints within the dataset tend to grow more sparce within the data space leading to more distance between neighboring datapoints. This makes it challenging to find relationships and patterns within the data as normal distance calculations become fairly meaningless. 2.  There is a huge uptick in the complexity of the computations that need to be performed on the data. Intuitively, if there are more features or variables to process, it will take a computer longer to run the full process. However, when dealing with extremely high dimensional datasets, the dimensionality can increase exponentially to the point that certain analysis is not feasible to run or is even impossible. It becomes harder to visualize the data as the number of dimensions grows. It is impossible to plot more than three dimensions on a screen and as such, plotting high dimensional data can be impossible. It becomes a lot easier to overfit the model to the data as given the uniqueness of the data, the model may fit the data too closely leading to poor performance when the model is used to identify relationships in other data. How to solve this The main way to solve this curse, is to try and remove variables in order to reduce the dimensionality. This may seem impossible as how could we change the data? However, it is important to determine if all the variables in the data are actually important. For example, there could be variables within the data which are exactly the same which leads to redundancy. Or, certain variables could be grouped together. Eigenvalues and Eigenvectors Eigenvalues and Eigenvectors are two of my least favourite things, but (rather annoyingly), they are very widely used in the analysis of linear transformations. Eigenvalues and eigenvectors help in that they are useful for transforming the data through the use of eigenvectors and selecting which dimensions are important to keep through the use of the corresponding eigenvalues. We see the implementation of this in non-square matrices through the use of Singular Value Decomposition (SVD). Eigenvalues and eigenvectors can only be found for square matrices, as such an alternative matrix factorization technique is needed for non-square matrices which represent the majority of datasets. In SVD, the primary matrix is decomposed into three matrices of which two are orthogonal matrices and one is a diagonal matrix. These are then used to find eigenvectors and subsequent eigenvalues which can then be used for other processes. Principal Component Analysis Principal Component Analysis (PCA) is a dimensionality reduction process that is similarly based to SVD. The goal of PCA is of course to identify the most important components from a dataset to help represent the dataset in a more manageable way. Components are a linear combination of the original features from the dataset that are designed to capture as much of the original variation of the dataset as possible. PCA can be performed by first performing SVD on the data and then sorting the singular values obtained from the SVD. These values represent the variance of the components and as such we select the k largest values which in turn give us the first k principal components. This can then be used to project the data in a 2D manner. Independent Component Analysis The final part of the content touches on Independent Component Analysis (ICA). ICA is another dimensionality reduction technique used in machine learning, however, it differs from PCA in that it assumes that the source data is statistically independent whereas PCA assumes it is uncorrelated. Further, the main goal of ICA is to find a set of basis vectors from which the data can be represented as a linear combination of independent components. Given this, ICA is more useful when the goal is to separate mixed data sources into the original components that make them up without any prior knowledge of the dataset. 