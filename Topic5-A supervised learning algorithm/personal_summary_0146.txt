In topic 4, we delved into the topic of dimensionality in data and explored various aspects related to it.  We started by understanding the concept of the curse of dimensionality, which refers to the challenges and limitations that arise when working with high-dimensional data. To tackle this issue, we explored techniques for dimensionality reduction, with a focus on Principal Component Analysis (PCA).  We learned about eigenvalues and eigenvectors, as well as singular value decomposition (SVD) as a fundamental part of PCA. We then dived into the formulation and derivation of principal components in PCA. Moving on, we implemented PCA using Python and explored an example of applying PCA in facial image analysis.  Additionally, we discussed the setup and practical considerations for performing PCA. Finally, we examined the relationship between dimensionality, correlation, and the curse of dimensionality, and explored how PCA can be used to remove correlation in data.  Throughout the topic, we gained insights into the challenges posed by high-dimensional data and learned how to effectively apply PCA for dimensionality reduction and correlation removal.  