topic 4 covered Dimensionality in data basically refers to the number of features in a dataset and how that coincides with the dimensionality. This concept of dimensionality is relevant in the vector space model, but also the fact that the greater the dimension the more difficult the data is to work with, notably in-terms of computing requirements – referred to as the curse of dimensionality. This can be overcome using dimensionality reduction, which involves reducing the number of dimensions while trying to maintain as much information as possible. There are two main categories of dimensionality reduction: feature selection and feature extraction. Eigenvalues and eigenvectors linear algebraic concept used for decomposing matrix into parts this is important for dimensionality reduction such PCA. SVD the technique generally adopted by PCA for actually decomposing a matrix into singular values, this uses eigenvalues and eigenvectors. PCA this was the key topic for this topics content. PCA is the technique covered and implemented to reduce a datasets dimensionality. The gist of it involves finding the directions of maximum variance and projecting it in to a new subspace with equal or ideally fewer dimensions than the original dataset. 