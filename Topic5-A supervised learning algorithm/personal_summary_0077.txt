 Introduction to Topic 4  ● Solving the curse of dimensionality ● Eigenvalues and eigenvectors ● Principal Component Analysis (PCA) ● Python programming: implementing PCA.  4.2 Dimensionality in data  Let us look at some typical dimensions of the data we’re dealing with:  Text data: Imagine a News website. If you start crawling the news on the website for a short period of time such as a topic, depending on the number of documents you crawl, it is typical to have more than 10,000 dimensions. This number is the size of the dictionary you have to build based on the words you extracted from the News documents. We need to represent each document based on the words in a dictionary (remember. the feature vector covered in topics 1 and 2: Data representation).  Image data: Imagine we would like to use pixels as features, just an 64x64 image would have 4,096 dimensions!  Genomic data: Take Parkinsons disease case-control data as an example. It has 408,803 Single-nucleotide polymorphisms (SNPs) and Alzheimer’s disease has 380,157 SN  4.3 Curse of dimensionality  The Curse of Dimensionality arises when applying machine learning algorithms to highly-dimensional data.  In machine learning we face unique problems when analysing and organising data in high-dimensional spaces. When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This is really problematic since there isn’t enough data locally.  Figure. 1D, 2D, 3D space data  Consider the figure above, first we observed some points in 1D data in 4 regions (20 divided by 5). Then these points are transferred into 2D space, into a region space. In the next step the points are in a 3D space with regions. What would happen when we get to 100 dimensions  At its core, the curse of dimensionality, dictates that as the number of dimensions increases, the number of regions grows exponentially. As the number of regions grows and space increases each data point has more and more room. That makes our data sparse and somehow not useful anymore. For example, what if we would like to check on a neighbour of a data point? Clearly, there wouldn’t be neighbours nearby!  Think about describing any human being. If we use number of arms or legs it includes many other animals. So we need to add another dimension: an upright body posture will differentiate us from many animals. Must breath air will exclude a few. What if we want to differentiate one human from another human? Are these features enough? No! We have to add many more dimensions such as height, weight, skin colour, hair colour, hair type and many more. Now imagine we want to identify each individual on earth. How many descriptors would need to be added? Can we handle such complexity and the computational load of enough dimensions to define a human being?  This example shows why we need to add more dimensions to describe any object, how quickly the dimensionality increases and how complex the system grows.  Also some intuitions drawn from low dimensional spaces fail badly in high dimensions.  Example In high dimensions, most of the volume of the unit sphere is very close to its surface.  Figure. Unit sphere  Figure. Simple illustration of curse of dimensionality.  Also it is obvious that the curse of dimensionality result in less distinctive distances in in high dimensions. So given a point in high dimensions, the relative distance between points far from it and close from it, becomes negligible. The following figure illustrates this point.  Figure. Euclidean distance was used to generate this graph, similar results hold for other distances in terms of distinctiveness.  There is an analysis regarding this problem; it has been called the concentration effect.  This problem can imply that:  > Clustering or KNN algorithms may be meaningless in high dimensions. However, there might still be patterns in high dimensions. We just need better distance metrics. So Research is needed! > Until we develop better distance metrics, we should aim to reduce the dimensionality where possible.  VIDEO: https://youtu.be/JMmuVyDZ_XA  4.4 Solving the curse  4.5 Eigenvalues and Eigenvectors  4.6 Singular value decomposition  4.7 Preliminaries  `  PCA: decorrelation  The main objective of PCA is to rigidly rotate the axes of  t−  dimensional axes to a new set of axes (called principal axes) that have the following properties:  ■ ■ ■ ■  Ordered such that principal axis- captures the highest variance, axis-2 captures the next highest variance, …. , and axis −d  has the lowest variance  Covariance among each pair of the principal axes is zero (the principal axes are uncorrelated i.e. they are orthogonal to each other). This is called decorrelation property  https://youtu.be/FgakZw6K1QQ  4.8 Formulation of PCA and deriving principal components  4.9 Implementation of PCA  N < d = 100 images (n) size 64x64 = 4096 … n < d  SVD:  4.10 Example of using PCA in facial image analysis  Example of using PCA in facial image analysis In this video we learn about a real-world example of how PCA is used. Many researches have used PCA for reducing dimensionality in face recognition problems. In this example, each image in the data set is represented by a vector of size 1024 (e.g. each image is represented by 1024 pixels).  The following steps are followed in the process described in the video.  Generate the covariance matrix for data  Find principle eigen vectors that represent the data  Calculate face image preservation of energy when k principle eigenvectors are used Projecting data back after preserving only k axis of variation (using only k principle eigenvectors) You will also see how the final image quality is effected by different k values and why selecting a smaller value for k can be beneficial for a classifier.  4.11 Other dimensionality reduction techniques  Masters content…  4.12 Setup  Prac Script https://colab.research.google.com/drive/1uiqA9l5pmKNGjqUPdahZmlsTsRoOkASf?usp=s haring  4.13 Python Practical independent and correlated data  Prac Script https://colab.research.google.com/drive/1uiqA9l5pmKNGjqUPdahZmlsTsRoOkASf?usp=s haring  When working with random numbers (i.e. using np.random function), the numbers generated by Python (or any other language) are pseudo-random (the values are computed using a deterministic algorithm and probability plays no real role). For debugging purposes, to get a  repeatable sequence of random numbers, we have to set the seed of random number generation to a specific number.  4.14 Visualisation of correlated and uncorrelated data VIDEO  Prac Script https://colab.research.google.com/drive/1uiqA9l5pmKNGjqUPdahZmlsTsRoOkASf?usp=s haring  4.15 Curse of Dimensionality and PCA to Remove Correlation in Data  Prac Script https://colab.research.google.com/drive/1uiqA9l5pmKNGjqUPdahZmlsTsRoOkASf?usp=s haring  data.csv  The output would look like this:  We can see that two columns in data are correlated. Our goal is to remove this correlation by projecting (representing) this data onto a new set of axis (principal components).  We now proceed with Implementing PCA using the following steps:  1. normalise the data 2. compute the covariance matrix of data 3. compute the eigenvectors (U) and eigenvalues (S) of the covariance matrix.  linalg.eig  Here, the first column represents the first eigen-vector U1, and the second column represents U2. These are the principal components. Notice that eigenvalues S1 and S2 are arranged in decreasing order: S1 > S2. Hence U1 is the direction that captures maximum variation in our given data. U2 is the next direction of variation.  What does PCA offer? So now we found out the principal components (U) the set of axis that capture the maximum variation in data. What can we do this this now?  We can do the following: 1. Decorrelation: Project our data onto U to get decorrelated data 2. Dimensionality Reduction: Reduce U to contain only those axis that contain maximum information. Project our data onto this reduced U to get new data with reduced dimensionality.  Measuring ‘reconstruction error’  How much information did we lose after dimensionality reduction? To measure this, we calculate the reconstruction error of our data. Reconstruction error, is calculated as the square root of sum of squared errors of each data point. Essentially, this becomes the distance between the original data point and the reconstructed data point.  For a better visualisation consider the figure below. The blue dots are the original data points, and the red dots are the reconstructed data points after dimensionality reduction using PCA. The dotted lines shows the distance between each original and reconstructed data point. So reconstruction error becomes the sum of these distances.  You can get a formal definition or watch a video illustrating a simple example if you need more information.  4.16 PCA using Inbuilt Functions in Python  PCA using Inbuilt Functions in Python Here we demonstrate how to use the inbuilt function PCA() in the sklearn.decomposition package.  We start by reading in a data file that contains 5 dimensions or features, download this CSV, add it to your data store and rename it. As with the previous example, we normalise the data, perform PCA and measure the reconstruction error in the recovered data.  Lets begin by reading the given data.  this CSV  