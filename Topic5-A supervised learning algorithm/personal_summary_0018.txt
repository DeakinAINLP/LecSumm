Lesson Overview – Dimensionality Reduction  Curse of dimensionality describes the difficulties arising when working with a set of data that has a high number of dimensions.  While the number of dimensions in a data set can increase exponentially to  ensure  meaningful  and  reliable  results  can  be  obtained;    the  number  of  sparse  areas  can  also increase  and  thus  reducing  the  efficiency  and  meaning  of  each  data  point.    Additionally,  distance concentration can be a problem when working with data with high dimensionality; as the distance between  each  data  point  becomes  more  uniform,  the  meaning  of  each  value  diminishes. Furthermore, presence of hubs is a third curse of dimensionality, describing the occurrence where the few  instances  which  are  occurring  more  frequently  as  nearest  neighbours  of  other  instances.    To overcome  the  curse  of  dimensionality  it  is  important  that  dimensionality  reduction  is  performed, dimensionality  reduction  is  a  process  of  reducing  the  number  of  dimensions  through  different transformation process while ensuring that it still conveys similar information. Eigenvalues and eigenvectors are often used to reduce the number of dimensions in the analysis of linear  transformations.    Eigenvalues  are  considered  as  the  scalar  factor  by  which  the  linear transformation is scaled, and eigenvectors are the direction in which a linear transformation move without changing its direction.  The formula for eigenvalue and eigenvector can be written as Av = λv, where A is the matrix, v is the eigenvector and λ (lambda) is the eigenvalue. Singular value decomposition (SVD) is a technique in linear algebra to decompose a matrix into three separate matrices.  SVD is given by formula X = USVT , where U is a nxd orthogonal matrix, S is a dxd diagonal matrix and V is a dxd orthogonal matrix.  Matrix S would have the same dimension as Matrix X where the numbers diagonal are the singular values of matrix X arranged in descending order while all other values in the matrix are zero.  Matrix U contains the left eiganvector of matrix calculated using  XXT  arranged  in  descending  order.    Lastly,  matrix  V  contains  the  right  eiganvector  of  matrix calculated  using  formula  XTX.    Principal  component  Analysis  (PCA)  was  also  introduced  during  this topic’s learning, PCA is a method which can be used to reduce the dimensionality of a large data set into smaller ones which still contain valuable information after the transformation.  PCA aims to take data points in a large number of dimensions, which may be correlated, and summaries them into a new set of uncorrelated axes.  SVD can be used to implement PCA.  Independent  component  analysis  (ICA)  is  used  to  separate  multivariate  signal  into  additive  sub- components,  ICA is often used to separate mixed signals such as medical imaging, sounds and images. ICA  can  only  be  done  under  the  assumptions  that  the  components  are  independent  and  do  not  conform to the gaussian  distribution.  When signals are separated, it is often easier to identify special features and patterns which may previously be hidden. Distinction between PCA and ICA is that PCA aims to compress information, but IC aims at separating information. In  addition  to  the  linear  techniques  mentioned  above,  more  sophisticated  methods  are  used  to identify  patterns  in  data.    Two  techniques  were  covered  this  topic,  first  of  such  techniques  is  t-distributed Stochastic Neighbor Embedding( t-SNE) which allows the visualisation of high-dimensional data  in  a  lower-dimensional  space,  namely  2D  and  3D.    Another  technique  for  dimensionality reduction for non-linear data is called Uniform Manifold Approximation and Projection (uMAP). Lastly, learning this topic also covered various dimensionality reduction algorithms.  