Dimensionality The size of the dimensions is the number of attributes. The dimensions captured for the various data types will be different. For example, image data, the dimensions for a 64x64 image would be 4,096 dimensions. This would also be an example of high dimensional data.  Curse of dimensionality As the dimensionality increases, the volume of the space increases which causes the data to become sparse. Sparsity in data refers to a high volume of possible combinations remaining unobserved. The space between points in high-dimensional spaces becomes less useful because the value of the distance increase in uniformity.  Solving the cure Dimensionality reduction is the process of converting a set of data that has vast dimensions into  a  data  set  with  fewer  dimensions.  For  example,  if  the  data  is  numerical  and  there  are duplicate features, redundancy can be removed by only using one of the features to represent all duplicates.  Eigenvalues and Eigenvectors Used in the analysis of linear transformation e.g., rotation and scaling. Eigenvector is a non-zero vector that when multiplied by a matrix, a scalar of itself is returned which is the eigenvalue. The equation is  entries. A positive covariance suggests that the two variables will move together in the same direction whereas a negative covariance indicates that the two variables will move in opposing directions. If two variables have no relation, then the covariance will be 0.  Principal Component Analysis (PCA) Take correlated n datapoints in d dimensions and summarise them by a new set of principal components (uncorrelated axes). This will reduce the dimensionality of the data set. It is the orthogonal transformation of data into its principal components. SVD can be used to perform PCA, with a formula of 𝑌 = 𝑈𝑆𝑉𝑇.  Independent component analysis (ICA) Used to separate mixed signals within a complex sound or image signal.  