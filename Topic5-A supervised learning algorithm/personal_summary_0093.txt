4.1P  Summary:  4.2/ 4.3:  Common datasets may contain a massive number of dimensions. Problems arise when attempting to analysing this data, as the dimensionality increases, the volume of the space does also, creating high sparsity between points.  This can be expressed as the curse of dimensionality, that as the number of dimensions increases, the number of regions increases exponentially.  4.4:  Dimensionality can be reduced in linear sets by transformation using a projection vector.  4.5:  Eigenvalues and eigenvectors  For a square matrix, A, if a number 位 and a vector u satisfy Au = 位u then 位 is an eigenvalue, and u is the eigenvector of A.  The Eigenvalue decomposition of a matrix represents it in terms of its eigenvalues and eigenvectors.  Eigenvalues of A can be found by solving the characteristic polynomial in 位.  4.6:  Singular value decomposition (SVD) can decompose a matrix into three other matrices. It expands the original data in a coordinate system.  4.7:  Principal Component Analysis (PCA) is designed to take n data points in d dimensions, which may or may not be correlated, and summarise them with a new set or uncorrelated axes.  These axes are referred to as principal components/ axes. They are linear combinations of the d dimensions.  Data is represented as a points in multidimensional space, with each axis correlating to a variable. The centroids for the points are defined by the mean of the variable.  The variance of each variable is the average squared deviation of its n values around the mean of the variable.  Covariance is the measure of how changes in one variable influence changes in another variable, this represents linear correlation. The covariance matrix contains all variances of all variables on the diagonal and the covariances among all pairs of variables in the off-diagonal entries.  4.9:  SVD can be used to implement PCA. If U is a n * d orthogonal matrix, implementation can take the form of U matrix * data matrix.  4.10:  PCA can be used in face image data via generating the covariance matrix for data, finding principle eigenvectors that represent it, calculating face image preservation of energy when k principle eigenvectors are used and projecting data back after preserving only k axis of variation.  4.11:  Independent Component Analysis (ICA), separates multivariate signals into independent, non-Gaussian components. This is commonly used to separate signals mixed together,  