Following I have summarised the knowledge gained by the reading contents of this topic.  ➢  Dimensionality in Data : Data dimensionality refers to the number of features or  variables used to describe each instance or observation in a dataset. That is, the number of columns or attributes in the dataset. High-dimensional data refers to datasets that have a large number of characteristics or variables for the number of observations. High-dimensional data can present significant data analysis challenges such as increased computational complexity, overfitting, and difficulties in visualizing and interpreting data.  ➢  Eigen Values and Eigen Vectors :  In data analysis and machine learning, eigenvalues and eigenvectors are frequently employed for tasks including dimensionality reduction, feature extraction, and clustering. For instance, principle component analysis (PCA) uses the eigenvalues of the covariance matrix of a data set to represent the percentage of variance explained by each direction, while the corresponding eigenvectors of the covariance matrix identify the direction of maximum variation in the data.  ➢  Singular Value Decomposition : SVD matrix factorization methods are widely used in  both linear algebra and data analysis. Split the matrix A into three parts:  A = UΣV^T  where is a diagonal matrix with non-negative real numbers on the diagonal and U and V are orthogonal matrices. Application areas of SVD in data analysis include feature extraction, dimensionality reduction, data compression, and matrix approximation. It also utilizes various machine learning algorithms such as collaborative filtering, image processing, and natural language processing.  