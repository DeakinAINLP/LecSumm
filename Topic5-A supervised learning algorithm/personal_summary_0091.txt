4.1 Main points and summaries    Dimensionality in data is shown to be typically in text data, image data and genomic data.   The curse of dimensionality is introduced and is shown as a problem whereas more  dimensions are introduced into data, the region and spaces increase, and each data point will have more room. This makes our data sparse and not useful anymore to interpret and use. In order to solve the Curse of Dimensionality we need to reduce the number of dimensions with Dimensionality Reduction. This can be done whilst making sure the data expresses the same information as it did before the reduction.      Eigenvalues and Eigenvectors are introduced in 4.5 and are mainly used for the analysis of  linear transformations. It shows how to find and calculate the eigenvalues and eigenvectors in examples.    Singular value decomposition is another method which is shown and is quite useful. It  deconstructs a matrix into three other matrices and tells us some things about the data.   Principal Component Analysis is introduced as a dimensionality reduction method which is  done by changing a large set of variables into a smaller set but keeping the same information.    We are shown how to do PCA with eigenvalue decomposition in steps.   Implementation of PCA is shown in several different ways in 4.9. We can use SVD to perform PCA also and it is shown in steps.    4.11 introduces another dimensionality reduction method which is called ICA, short for  independent component analysis. This method basically just separates a multivariate signal into different independent ones.    4.12 gets us started on setting up Python to start our PCA algorithms.   4.13 tries to get us to understand what the code given does in each certain case, it starts  with a simple 1D case, moves onto a 2D case with uncorrelated data and then the same but with correlated data.    4.15 introduces PCA to remove correlation in data within Python. In order to implement PCA to our data we must first normalize the data with the code given, calculate the covariance matrix of the normalized data we got and then compute the eigenvectors and eigenvalues of the covariance matrix. After this we can now decorrelate and reduce the dimensionality afterwards.    4.16 shows us how we can use PCA with the inbuilt functions in Python to make things easier. The same steps apply as the previous module, but it is done with less code.   