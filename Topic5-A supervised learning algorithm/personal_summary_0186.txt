Stacey Cranston 220029132 SIT720 Postgraduate  Pass Task 4  Key Learning:    Data analysis and organization in high-dimensional spaces pose many challenges. For instance, text data from a news website may have over 10,000 dimensions, image data using pixels as features may have 4,096 dimensions, and genomic data for diseases like Parkinson's and Alzheimer's can have hundreds of thousands of dimensions.   The Curse of Dimensionality refers to the challenges that arise when applying machine learning algorithms to high-dimensional data, as the number of dimensions increases, the  available  data  become  sparse  and  the  number  of  regions  grows  exponentially, leading  to  less  distinctive  distances  in  high  dimensions.  This  can  lead  to  the concentration effect, where the relative contrast between near and far neighbours diminishes  as  the  dimensionality  increases,  making  clustering  or  KNN  algorithms meaningless in high dimensions, and highlighting the need for better distance metrics or reducing dimensionality where possible.    Dimensionality Reduction is the process of converting high-dimensional data into a lower dimensional representation while preserving important information. This can be  achieved  by  identifying  the  direction  of  maximum  variance  in  the  data  and projecting the data onto that direction, as shown in various examples.    Eigenvalues and eigenvectors are used in the analysis of linear transformations, where an eigenvalue is a number, and an eigenvector is a corresponding vector. The number of nonzero eigenvalues of a matrix is less than or equal to the size of the matrix and can be found by solving the characteristic polynomial.    Singular value decomposition (SVD) is a matrix factorization method that decomposes a matrix into three other matrices. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal and consists of finding the eigenvalues and eigenvectors of the matrix.    Principal  Component  Analysis  (PCA)  is  a  technique  used  to  summarize  a  set  of correlated  data  points  into  a  new  set  of  uncorrelated  axes  called  principal components.  These  principal  components  are  linear  combinations  of  the  original dimensions and are ordered based on the amount of variance they capture. PCA aims to  rigidly  rotate  the  original  axes  to  new  principal  axes  that  are  uncorrelated  and capture the highest variance.    PCA is a technique used to summarise high dimensional data by projecting it onto a new set of uncorrelated axes called principal components. The principal components are  determined  by  finding  the  eigenvectors  of  the  covariance  matrix,  with  the  first principal  component  capturing  the  highest  variance  and  subsequent  components capturing decreasing variances. Implementation of PCA using SVD for cases where the number of data points is less than the number of dimensions. The connection between SVD and EVD and how PCA can  be  used  for  dimensionality  reduction  by  projecting  data  onto  the  eigenvector corresponding to the highest eigenvalue.     PCA  in  facial  image  analysis,  where  PCA  is  used  for  reducing  dimensionality  in  face recognition  problems  by  generating  the  covariance  matrix,  finding  principle  eigen vectors, calculating face image preservation of energy and projecting data back after preserving only axis of variation. ICA  is  a  method  to  separate  mixed  signals  into  independent,  non-Gaussian components by finding a set of basis functions that capture the underlying sources. t-SNE is  a  nonlinear  dimensionality  reduction  technique  that  uses  complex mathematical  techniques  to  identify  and  capture  the  underlying  structure  of  high- dimensional data and represent it in a lower-dimensional space.    uMap is a technique for dimensionality reduction that is similar to tSNE. It has some advantages  over  tSNE,  including  faster  computation  time,  better  handling  of  large datasets, and the ability to preserve global structure in the data while still identifying local  patterns.  Additionally,  uMap  offers  more  flexibility  in  terms  of  parameter settings, allowing for greater control over the resulting visualization.  