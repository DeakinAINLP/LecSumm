Within this module, I am able to understand the key concepts of dimensionality, its curse when implementing machine learning, and how it can be solved, eigenvalues and eigenvectors and the concepts and implementations of  principal component analysis(PCA). Within this, I am able to identify the curses of dimensionality through high-dimensional data and can understand and implement algorithms that can assist on solving the dimensionality curse, done through utilising Dimensionality reduction techniques, allowing massive amounts of data to reduce the number of dimensions while still retaining the accuracy of its datasets.  I am also able to understand on the use of two important dimensionality reduction techniques known as SVD and PCA, with the goal of PCA being to summarise the data points and dimensions, and to rotate the dimensional axis with optimal variance. Furthermore, this module had given me an short overview of three other dimensionality reduction techniques. One of them is Independent component analysis (ICA), in which it is commonly used to separate signals within a complex sound or image. There are also Non-linear dimensionality reduction techniques such as t-SNE and uMap that uses linear algebra for identifying patterns in data.  Through the understanding of dimensionality reduction techniques, I am able to implement the PCA dimensionality reduction technique within a massive dataset using python, can calculate the variance within the dataset, and can furthermore determine the correlation of the 3 components.  