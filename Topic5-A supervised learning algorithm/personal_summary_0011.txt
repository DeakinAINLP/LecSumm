I can certainly feel overwhelmed looking at a huge data set and trying to make sense of it – and it seems in machine learning that computers can have that struggle too. This topic was an interesting look and reducing the amount of complexity in a data set by pruning away dimensions or features of a dataset and to produce more meaningful results and inferences. I learned that a dataset can be suffering from the “curse of dimensionality” when there is too dimensions to a dataset creating sparse data that is difficulty to derive any useful meaning from. There are many ways to solve it such as feature reduction, t-Distributed Stochastic Neighbor Embedding, and Umap - we mostly focused on Principal Component Analysis. With PCA it attempts to reduce the number of variables while retaining as much variance or data as possible. I’m still finding it difficult to wrap my head around Eigenvalues and Eigenvectors… the names likely make it seem more complicated than they really are. I will endeavour to watch some YouTube videos summarising them as I do find that a good way to listen to different explanations of a concept. The pass task was interesting to investigate a more ‘real’ dataset and investigate how to work with them. It felt a lot more practical and good to catch a glimpse into how machine learning can used to work in the real world. I found the visualisation of correlated and uncorrelated data video was great to understand how correlations work, was well explained and interesting, and really help me to solidify my understanding in that area. The practical examples of the Python code and explanations were well written and it was easy to follow and understand how the code was working and what it was doing. 