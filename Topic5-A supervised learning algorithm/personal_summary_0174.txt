Machine learning (Pass Task- Topic 4)  There are many issues that arise when analyzing and organizing data in high-dimensional spaces.  Curse of Dimensionality  The Curse of Dimensionality arises when applying machine learning algorithms to highly- dimensional data. With high-dimensional data, there are an exponentially growing number of possible feature combinations or configurations. As a result, obtaining a representative sample of the data and determining significant patterns or correlations between the attributes become more challenging.  Also, When the dimensionality increases, the volume of the space increases so fast that the available data become sparse.  Dimensionality Reduction  The Curse of Dimensionality calls for Dimensionality Reduction. Dimensionality reduction refers to the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely.  Principal Component Analysis  The goal of PCA is to take ‘n’ data points in ‘d’ dimensions, which may be correlated, and summarizes them by a new set of uncorrelated axes. The uncorrelated axes are called principal components or principal axes.  The principal components are arranged according to how much variance they can account for, with the first principal component accounting for the greatest amount of variance, the second principal component accounting for the next-largest amount of variance, and so on. Each major component creates a new foundation for the data space by linearly combining the original features.  Eigenvalues and Eigenvectors  For a given square matrix A if a number µ and a vector ú satisfy the condition Au=µu then µ is called an eigenvalue and u is the corresponding eigenvector of A.       Eigen values of the Matrix  Since vector ‘u’ cannot be zero,  By solving the above equation, the eigen values can be identified.      Singular Value Decomposition  It is a matrix factorization method. SVD decomposes a matrix into three matrices:  Formulation of PCA and deriving principal components  Step 1: standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis.  Step2: Covariance Matrix creation. This describes how the input data set's variables differ from the mean in relation to one another, or in other words, it determines whether there is a link between them. Because variables can occasionally be highly connected to the point where they include redundant data. We compute the covariance matrix in order to find these associations.  Step3: Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components.  principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set.  Because the eigenvectors of the covariance matrix are actually the directions of the axes where there is the most variance (most information), which we call principal components, eigenvectors and eigenvalues are what are responsible for all the magic described above. The variance held by each Principal Component is indicated by the eigenvalues, which are simply the coefficients associated to the eigenvectors.  Other Dimensionality reduction techniques.  Independent component Analysis (ICA)  Through the use of mathematics, ICA seeks to identify a linear transformation matrix that may be applied to separate a collection of observed signals (X) into separate components (S). By maximizing the estimated components' statistical independence, one can derive the matrix W:      S = WX  where X is the matrix of the observed signals and S is the matrix of independent components.  Nonlinear dimensionality reduction Technique  In order to reduce the dimensionality of high-dimensional data while maintaining the fundamental structure and relationships between data points, nonlinear dimensionality reduction (NLDR) approaches are used. NLDR methods can capture nonlinear interactions between data points, in contrast to linear dimensionality reduction methods like PCA.  T-SNE (t-distributed Stochastic Neighbor Embedding) is one well-liked NLDR method. t-SNE preserves the local structure and relationships between data points while projecting high- dimensional data into a low-dimensional space (often 2D or 3D). It functions by employing a probability distribution to characterize the similarity between two data points in a high- dimensional space and a low-dimensional space. t-SNE is frequently used to visualize high- dimensional data and spot patterns or clusters within it.  