 This topic the topic of the module was dimensionality reduction. When we work with data, we work with several kinds of data such as text data, image data etc.  The amount of data collected is huge, which increases the dimensionality of data, this leads us into the curse of dimensionality in computational intelligence. It dictates that as the number of regions grows exponentially, the space between data points increases, making the data sparse and somehow not useful anymore. To solve the curse of dimensionality, we need to use dimensionality reduction techniques. It refers to the process of converting a set of data having vast dimensions into data with fewer dimensions while making sure it conveys similar information concisely.  Then, we learned about eigen values and eigen vectors. It is simply put when a vector is transformed from to 2d to 3d. The vectors which stay in the same positions are called the above. Eigen vector simply put is a vector which stays in the same position. Then we went into the mathematical explanation of the topic.  Then, we learned about singular value decomposition. The idea of singular value decomposition is to go from a normal vector space using matrix multiplication to a new n-dimensional vector space. The name for the rotation matrix is unitary transformation. SVD simply put is a method of decomposing a matrix into three other matrices. Then we were asked to watch a video which explained it in detail.  Then, we looked at PCA which stands for principal component analysis. To simply put, we have data which has a lot, but we can represent data only in 3d space. So, we perform PCA on the data. It helps represent data in 2d space. It helps us decide which feature is more important for describing the data. PCA at the end of the day a dimensionality reduction technique. It is used for feature extraction, data visualization and data compression. Then, we learned how to perform PCA using python.  