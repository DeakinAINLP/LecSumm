Review of topic 4  This topic we explored the topic of dimension reduction.  First we covered the question of “just what exactly is a dimension in data?” Then we noted that the more dimensions the data being worked with has, the more difficult it can be for us to make predictions based on that data. This is termed “the curse of dimensionality”. Hence if we were able to reduce the dimension of interest it would be advantageous to us.  The process of reducing the number of dimensions we work with is termed “Dimensionality Reduction”. It is important that if we do reduce the number of dimensions that the integrity of the data is maintained.  From there we moved on to investigate various techniques that we could use to try and successfully reduce the number of dimensions that might be present in the data that we are working with. Again, it was another busy topic, with a lot of ground covered.  Dimensionality in Data  Dimensions are a fundamental property of data. They are the attributes of the data that we are interested in analysing. In some cases the number of dimension that we have to deal with can be   very high. Our analysis can be complicated by the fact some some of these dimensions can be inter- related. For example, in health data there is a strong correlation between weight and blood pressure.  Further examples of data with high dimensions given in the course notes are:   Text: using the words of each document being analysed as a feature vector. If the documents are only written in English, there are 171,000 words currently in use (‘How many words are in the English language? | English Live’ n.d.). So the number of dimensions when working with documents can grow very large, very quickly.    Images: we forget just how many pixels make up an image. A simple 4K image (3840 * 2160) has well over 8 million pixels in it. A dataset with many images would thus have many dimensions were pixels used as features.   Genomic data: By any measure we use, genomic data is large. A typical gene expression  dataset may contain the measurements of thousands of genes across multiple samples. Each gene will represent a dimension, and the measurements can be seen as a vector for each dimension. Given the size of a gene, we are guaranteed to have issues with the dimensions of genetic data.  The Curse of Dimensionality  With added dimensions comes an increased difficulty in both understanding and making predictions based on the data.  As an example, consider predicting bacteria locations: our model might be reasonably accurate in the two dimensions of a petri dish. But were we to add volume as a dimension, e.g.: predicting the location of the bacteria in a jar of fluid, there will be an increase in the predictive space. This modelling might still be possible, but it is a far more complex task. And this is only extending the model to three dimensions! As extra dimensions in the data are added to our model, our human ability to visualise, think in, and comprehend any patterns in the data breaks down.  As extra dimensions are added, each of our data points becomes further from any other. This means that our data is becoming sparser (more dispersed) as the number of dimensions grows: making patterns in the data harder to discern.  A side effect this increasing sparseness is that the distance between data points becomes less and less useful as a metric for finding meaningful relationships between them. This is because the distances between data points is becoming more and more uniform as the number of dimensions increase. The reason is shown by the following equation:  .  The difference between the maximum and minimum distances becomes negligible as the number of dimensions grow. This effect is termed the “concentration effect”. This concentration effect means that clustering or nearest neighbour systems become less usable as the number of dimensions increase, as they are based on distance.   To summarise: as the number of dimensions increase, the structure of the data within those dimensions becomes increasingly difficult to discover. If we want to use distance based algorithms we need to reduce the number of dimensions that we examine. Thus one of the key challenges is to reduce the number of dimensions: without loosing any relevant information that may be contained by the data points.  Note that as the number of dimensions increase, the data becomes concentrated in a few regions, forming clusters. Certain data points can then start to appear as the nearest neighbours of different clusters with a surprising frequency. If this happens, it makes identifying the clusters and any dimensional reduction harder.  Solving the Curse of Dimensionality  Reducing the number of dimensions is termed “Dimensionality Reduction”. This unit will only examine linear dimensionality reduction. This is a technique whereby the data samples are projected onto a subspace that has lower dimensionality.  An example of a real world data that has a linear relationship would be hight and weight: the larger a human is, the heavier they are likely to be. On the other hand, a relationship between age and income would be noisy: older people tend to earn more, but there are many other factors at play when it comes to earnings.  Eigenvalues and Eigenvectors  An eigenvector is a vector, that when multiplied by a matrix, produces a scalar multiple of itself (the result stays on the same span as the original). The value of the scalar multiple is known as the eigenvalue. More formally, given the square matrix A, and its corresponding eigenvector u, we can compute the product Au to obtain a scaled version of u, with the scaling factor λ:  Au = λu  Some useful heuristics:    For a square matrix of size d, there will be d number of eigenvectors and eigenvalue pairs.   The number of nonzero eigenvalues will be equal to the rank of the matrix   The number of nonzero eigenvalues will not exceed d.  Eigenvalue decomposition of a square matrix expresses that matrix as the product of three matrices: a diagonal matrix of the eigenvalues, the inverse matrix of the eigenvectors and a matrix of the eigenvectors. In english: a square matrix A can be expressed as a linear combination of its eigenvectors, with the eigenvectors scaled by their corresponding eigenvalues.  We can rearrange the formula Au = λu as Au – λu = 0.  We can rewrite u as uI (u times the identity matrix I). This allows us to rewrite the formula as:  Au – λuI = 0  As matrix vector products have the distributive property, we can then write:   (A – λI)u = 0 where (A – λI) is a matrix. Further, the determinant of (A – λI) must be 0.  Singular Value Decomposition  A drawback of eigenvalue decomposition is that it can only be applied to square matrices. Another decomposition method, termed Singular Value Decomposition (SVD), can be applied to any rectangular matrix. In SVD, the source matrix is decomposed into 3 different matrices:  A = UΣVT  With:   A being an n × d matrix   U is a n × d orthogonal matrix    S is a d × d diagonal matrix, with the elements   V is a d × d orthogonal matrix  Principal Component Analysis  Principal Component Analysis (PCA) is a technique in which the dimensionality of a data set is reduced, but as much of the variation of information in the data set is retained as is possible. The initial possibly correlated data points are transformed into a new set of uncorrelated data points, called the principal components.  PCA identifies the directions of the maximum variance in the data, and projects the data into these directions. The first principal component captures most of the variance in the data, the second principal component captures the second-most variance, the third principal component the third- most variance (Kumar 2023), and so on. The last principal component will have the least amount of variance. The number of principal components is equal to the number of variables in the dataset.  By retaining only the top few principal components we can the reduce the dimensionality of the dataset, but still retain most of the important information. The covariance amongst each pair of principal axes is zero.  PCA can be done via Eigen Value Decomposition: the eigenvectors of a covariance matrix of the data are used to project the higher dimensional data onto lower dimensional subspaces.  Explained variance is the statistical measure of much variation in a dataset can be attributed to each of the principal components. It indicates how much of the total variance is captured by each component. This gives us a way of ranking the components in order of importance, as the larger the explained variance for a principal component is, the more important that component is.  Independent Component Analysis  A multivariate signal is a signal that contains multiple components that have been measured simultaneously over time or space. As an example, stock prices and interest rates are multivariate data sets. These datasets are typically high-dimensional and complex.   Independent Component Analysis (ICA) is a statistical technique that can be used to decompose a multivariate signal into independent non-Gaussian components (components that don’t follow a normal distribution). Non-Gaussian components are chosen as the decomposition target, by ICA as they are not likely to be correlated with other components in the data. The identification of non- Gaussian components can be difficult and computationally expensive.  The underlying assumption of ICA is that the observed signal can be represented as a linear combination of independent components:  X = AS  Where X is the signal, S is the vector of independent components and A is a matrix describing how the components are mixed together.  This makes the goal of ICA to best estimate S and A, given the observed signal X.  Non-linear Dimensionality Reduction Technique  Non-linear dimensionality reduction techniques are used when the dimensionality of the data is not linearly separable. Not being linearly separable means that the data can’t be separated by a straight line. An example of such data would be the sound of different musical instruments playing the same tune. The notes they play are the same, but the sound of each instrument is very different to the human ear. There are several non-linear reduction techniques.  t-Distributed Stochastic Neighbour Embedding  One is called t-Distributed Stochastic Neighbour Embedding (t-SNE). In this unsupervised non- linear technique, probability distributions are created over pairs of high dimensional objects (P), and pairs of low dimensional objects (Q). The difference between the two probability distributions is then iteratively minimised. The measure of the difference used is the Kullback-Leibler (KL) divergence. This measures the amount of information lost when using Q to approximate P.  Uniform Manifold Approximation and Projection  Uniform manifold approximation and projection for dimension reduction (uMap) is a non-linear dimensionality reduction technique that is similar to t-SNE. uMap represents the data has a high- dimensional graph, with each data point being a node, with edges giving the relationship between the nodes. Then a low-dimensional graph is created that preserves the relationships, but with fewer dimensions.  uMap is faster and more scalable than t-SNE, meaning that it is better suited to large datasets. It can also be applied to a wider range of data types including some that can be difficult to handle in other dimensionality reduction techniques.  