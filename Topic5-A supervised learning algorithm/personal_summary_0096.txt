● Dimensionality in Data, e.g: news article text words, image pixels, genome  Single-nucleotide polymorphisms (SNPs)  ● Other examples: social media posts, financial time series, e-commerce data, retail  transaction records, surveillance videos, computer network/system access logs, music audio sample values or frequencies  ● In high dimensional spaces, most of the training data resides in the corners of the  hypercube defining the feature space  ● Concentration effect: relative contrast between near and far neighbours diminishes as  the dimensionality increases  ● Until we develop better distance metrics, we should aim to minimize dimensionality  where possible  ● High dimensional data space is sparsely populated ● Distance values become less and less useful the higher the dimensions (distance  concentration)  ● Higher data dimensionality => higher presence of data hubs ● PCA, ISOMAP, etc are not very good at reducing the problems associated with the data  hubs  ● Intrinsic dimensionality of the data set = the meaningful (+ valuable) dimensionality ● Dimensionality Reduction: obtaining fewer dimensions whilst still ensuring that similar  information is maintained  ● Non-linear dimensionality reduction: kernel principal component analysis ● Eigenvalues: det(A - ● Singular Value Decomposition (SVD): X = USVT, A = U V* (  I) = 0 λ  Σ  >=  σ σ 1  2  ) >=  0 ... >=  σ 𝑝  ● PCA via Eigenvalue Decomposition:  ○ Compute data covariance matrix C ○ Perform Eigenvalue decomposition (EVD): C = UDUT ○ Reduce data dimension  ● When samples (n) < datapoints (d), the number of nonzero eigenvalues of data  covariance matrix is less than or equal to n => using SVD can reduce the computations  to O(  3 ) or less 𝑛  ● A similar problem to image recognition where reducing the dimensionality of a problem is  critical for its implementation: voice recognition  ● Independent component analysis (ICA): method of separating a multivariate signal into independent, non-Gaussian components - to separate signals that are mixed together, such as in a complex sound or image signal ● Nonlinear dimensionality reduction techniques:  ○ t-Distributed Stochastic Neighbor Embedding (t-SNE) - used to visualise  high-dimensional data in 2 or 3 dimensions. It works by finding patterns and relationships in the data and then representing those patterns in a lower-dimensional space to reduce the complexity of the data  ○ Uniform Manifold Approximation and Projection (UMAP) - advanced version of  tSNE  