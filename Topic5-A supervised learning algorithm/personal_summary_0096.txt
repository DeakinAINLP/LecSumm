â— Dimensionality in Data, e.g: news article text words, image pixels, genome  Single-nucleotide polymorphisms (SNPs)  â— Other examples: social media posts, financial time series, e-commerce data, retail  transaction records, surveillance videos, computer network/system access logs, music audio sample values or frequencies  â— In high dimensional spaces, most of the training data resides in the corners of the  hypercube defining the feature space  â— Concentration effect: relative contrast between near and far neighbours diminishes as  the dimensionality increases  â— Until we develop better distance metrics, we should aim to minimize dimensionality  where possible  â— High dimensional data space is sparsely populated â— Distance values become less and less useful the higher the dimensions (distance  concentration)  â— Higher data dimensionality => higher presence of data hubs â— PCA, ISOMAP, etc are not very good at reducing the problems associated with the data  hubs  â— Intrinsic dimensionality of the data set = the meaningful (+ valuable) dimensionality â— Dimensionality Reduction: obtaining fewer dimensions whilst still ensuring that similar  information is maintained  â— Non-linear dimensionality reduction: kernel principal component analysis â— Eigenvalues: det(A - â— Singular Value Decomposition (SVD): X = USVT, A = U V* (  I) = 0 Î»  Î£  >=  Ïƒ Ïƒ 1  2  ) >=  0 ... >=  Ïƒ ğ‘  â— PCA via Eigenvalue Decomposition:  â—‹ Compute data covariance matrix C â—‹ Perform Eigenvalue decomposition (EVD): C = UDUT â—‹ Reduce data dimension  â— When samples (n) < datapoints (d), the number of nonzero eigenvalues of data  covariance matrix is less than or equal to n => using SVD can reduce the computations  to O(  3 ) or less ğ‘›  â— A similar problem to image recognition where reducing the dimensionality of a problem is  critical for its implementation: voice recognition  â— Independent component analysis (ICA): method of separating a multivariate signal into independent, non-Gaussian components - to separate signals that are mixed together, such as in a complex sound or image signal â— Nonlinear dimensionality reduction techniques:  â—‹ t-Distributed Stochastic Neighbor Embedding (t-SNE) - used to visualise  high-dimensional data in 2 or 3 dimensions. It works by finding patterns and relationships in the data and then representing those patterns in a lower-dimensional space to reduce the complexity of the data  â—‹ Uniform Manifold Approximation and Projection (UMAP) - advanced version of  tSNE  