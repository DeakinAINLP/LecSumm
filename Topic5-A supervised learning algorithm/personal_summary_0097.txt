Dimensionality in Data  -  Text Data - Image Data -  Genomic Data  Curse of dimensionality arises when applying machine learning algorithms to high-dimensional data.  -  When the dimensionality increases, the volume of the space increases so fast that the available  data becomes sparse  o  As the number of dimensions increases, the number of regions grows exponentially, as the number of regions increases and space increases and data point has more room; making our data sparse and harder to check on neighbors of data points.  o  Overfitting may occur due to increased complexity as the model may fit the training data too closely resulting in poor performance on new data in addition to underfitting where the model may be too simple to capture patterns in the data.  o  High dimensional data also requires more computational resources resulting in more time  to process  o  Due to the spareness of high-dimensional data the algorithm may have trouble  determining between important and unimportant features as data points may have many near zero values  Solving the Curse  -  Too many variable can be a problem – some are irrelevant and can be removed for our purpose.  Dimensionality Reduction  -  Process of converting a set of data having vast dimensions into data with fewer dimensions while  still preserving the fundamental information  An individuals weight vs height may have a noisy-linear relationship as there would be some degree of a positive correlation between the two as taller people would generally weigh more and as such a potential dimension reduction approach may be to dimensionally reduce the height variable as weight is more variable and influenced by other factors such as exercise routine, diet and genetics. This approach maintains the important information while simplifying the model.  Eigenvalues and Eigenvectors   Singular Value decomposition  -  Method of decomposing a matrix into three other matrices:  -  Where:  o  SVD is a factorization of a real or complex matrix in linear algebra. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal  “When you manipulate a vector with a matrix you're doing nothing more than rotating and stretching”  Preliminaries  -  Goal of PCA is to take n data points in d dimensions, which may be correlated and summaries  them by a new set of uncorrelated axes.  -  Principal components or principal axes are uncorrelated axes that are linear combinations of the  original d dimensions. The first k components capture as much of the variance among the data points as possible  Variance across each variable  -  Data is represented as a cloud of points in a multidimensional space with one axis for each of the  avariables.  -  The centroid of the points is defined by the mean of each variable -  The variance of each variable j is the average squared deviation of its n values around the mean of  that variable  Covariances among variables  -  Covariance is a measure of how changes in one variable are associated with changes in a second variable; degree to which the variables are linearly correlated is represented by their co-variances  -  Covariance matrix  -  A matrix that contains variances of all variables on the diagonal and co-variances among all pairs  of variables in the off-diagonal entries  -  -  PCE: decorrelation  -  PCA’s main objective is to rigidly rotate the axes of t – dimensional axes to a new set of axes that  have the following properties:  o  Ordered such that principal axis – captures the highets variance, axis 2 captures the next  highest variance and axis d has the lowest variance  o  Covariance among each pair of the principal axis is zero; this is called decorrelation  property  Formulation of PCA and deriving principal components  -  Data is on a new axis whose direciton is specified by a d- dimensional vector u  -  We assume u to be unit length vector  interested in direciton of maximum variance  due to only being  -  Each data point x can be project on the vector u to create a new coordination as  so  the variance of the data project on u is:  -  -  Now we want to find out the direction so that the variance  is maximised. Recall that we also  assume  -  By putting it together we want to find:  - -  For solving this problem we introduce a Lagrange multiplier and change the problem into an  unconstrained maximization problem:  o  -  Good way to find maximums ro minimums is to find out where the slope of the function  (derivative) is equal to zero. Taking the derivative w.r.t u and setting it to zero we obtain:  -  This is an eigenvalue problem, where eigenvector. U is known as the first principal component  is the largest eigenvalue of C and u is the corresponding  For u2.. Ud  -  Next set of axes u2... ud can be found incrementally  by finding a direction that maximizes the  variance and is orthogonal to all the principal axes found so far  -  The directions have to be orthogonal since we want them to be uncorrelated -  Therefore the pincipal axes can be collectively written using the Eigenvector matrix U = [u1,  u2,….. Ud] n the order of decreasing eigenvalues of the covariance matrix C   PCA via Eigen Value Decomposition  How to perform PCA with eigenvalue decomposition:  -  Compute data convariance matrix C  -  Perform Eigen value decomposition -  Reduced dimension data is given by:  - -  Y which is n x k matrix is the reduced dimension data from d dimensions to k dimension.  -  Achieve this by multiplying  the decreasing order of eigenvalues  PCA: Minimum Error formulation  which will result in top k Eigen vectors in  -  Analysing PCA from another perspective -  Alternative formulation of PCA based on projection error minimization; suppose we project our data on k dimensions from d dimensions; losses occur due to losing some features in data (k < d) but the error we have while using PCA’s best K dimensions in terms of least square error, is the minimum possible error we can have.  o  Let us consider a set of new axes u1... ud in such a way that they are mutually orthogonal.  I.e.,  if we project a point such as x on u1.... ud to get  new coordinates such as  o  So for all d dimensions we can write this as  o  If we would like to minimze the mean square error due to projection in new k dimension,  we have: top k eigenvectors of covariance are the optimal solutions.  which  we can find that once again  o  Implementation of PCA  PCA for data where n < d  -  There are cases when the number of data points n is less than the number of dimensions d;  n < d e.g., 100 images in 64 x 64 dimensions; n = 100 and d = 64 x 64 = 4096  In this case the number of nonzero eigenvalues of data matrix is less than or equal to n If we use Eigen Value Decomposition (EVD) on the covariance matrix of size d x d, we need to perform calculations of the order of O(d^3) - TOO EXPENSIVE In such cases, SVD can  reduce the computations to O(n^3) or less  Using SVD for PCA  -  We can use SVD to perform PCA given any n x d matrix Y its Singular Value Decomposition  (SVD) is given as:  Where:  Now if Y is mean-centred version of X then the covariance of Y is:  Remember that  , therefore:  -  First major axis is the direction of the largest variance direction -  Second major axis is the direction of the second largest variance -  If we calculat the values of these two axis we can find the projected X data by multipling X and U   Example of using PCA in facial image analysis  Many researches have used PCA for reducing dimensionality in face recognition problems  Steps following in the process:  -  Generate the covariance matrix for data -  Find principle eigen vectors that represent the data -  Calculate face image preservation of energy when K principle eigenvectors are used -  Projecting data back after preserving only K axis of variation (using only K principle  eigenvectors)  Independent component analysis (ICA)  -  Method of separating a multivariate signal into independent, non-gaussian components -  In signal processing, machine learning and neuroscience, ICA is commonly used to separate signals that are mixed together (complex sound or image signal)  -  Goal is to find a set of basis functions that capture the signal’s underlying sources, then use these  functions to separate the signal into its constituent parts  Main objective of PCA is orthogonal transformation of given data into its principal components, here, principle component = axis of maximum variation  Python setup  -  Apply Principal Component Analysis in pracs for two purposes:  -  Data decorrelation -  Dimensionality reduction (summarisation)  We will perform PCA in different ways  -  Perform PCA using linear algebra techniques learnt in class on 2d dataset  -  Look at the inbuilt functions in python to perform PCA  Python Practical independent and correlated data  In the following practical:  -  Learn how to generate data from a specified distribution in one or more dimensions -  Demonstration purposes will use normal distribution with a given mean and standard deviation -  When generating data in more than one dimension, we will consider two scenarios:  o  Generate data from each dimension independently o  Generate data from different dimensions such that dimensions of data are correlated up to  a specified value  1D case  The following code is written to generate the samples from a one-dimensional normal distribution with mean zero (0) and standard deviation one (1); the data x follows a normal distribution specified by density  function:  In this example we  -  Generate samples from a 1D gaussian distribution with mean zero and standard deviation one -  From the generated samples, we empirically compute the mean, standard deviation; also plot the  histogram  -  Store the data to file “normalData1.csv” writing each sample in a separate line  