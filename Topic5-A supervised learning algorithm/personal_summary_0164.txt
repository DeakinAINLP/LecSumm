TOPIC FOUR: DIMENTIONALITY REDUCTION  ●  Dimensionality in data refers to how many attributes a dataset has. Example of data set with high dimensional data is microarrays,  Curse of dimensionality occurs when machine language  ● which measure arrays. ● algorithms are applied to highly dimensional data. The more dimensions you add to a data set the more difficult it becomes to predict certain quantities. Solving curse, it involves reduction of dimension which means to ● simplify understanding of data either numerically or visually. To reduce dimension, combination of related data using multidimensional scaling to identify similarities is the key concept.  Analysis of linear transformations using eigenvalues and  ● eigenvectors  Eigenvectors are by definition nonzero while eigenvalues may be  ● equal to zero.  ●  The variance across each variable as centroid of the points is defined by  Covariance among variables whereby covariance a measure of how  How to calculate eigenvalues and eigenvectors. Concept of singular value decomposition as it represents an ● expansion of the original data in a coordinate system where the covariance matrix is diagonal. ● the mean of each variable. ● changes in one variable are associated with changes in a second variable. ● deriving principal components, how to perform PCA with eigenvalue decomposition as well as formulation of PCA based on projecting error minimization. ● given dataset that is normalizing the origin dataset, calculating covariance matrix between the normalized data points, calculating the eigenvalues and eigenvectors for the covariance matrix. ● facial image analysis.  Using python implementation on a real dataset and using PCA in  Implementation of PCA by finding the principal components from a  Learning on the formulation of Principal Component Analysis PCA and  Learning Independent Component Analysis since it is a method of  ● separating a multivariate signal into independent, non-Gaussian components as a dimensional reduction technique.  ●  ●  Using python in Independent and correlated data.  Learning the curse of dimensionality and PCA in removing correlated data.  Concept of t-distributed stochastic neighbour embedding as t-SNE converts  ● similarities between data point to joint probabilities and try to minimize the Kullback- Leibler divergence between the joint possibilities of the low-dimensional embedding and the high-dimensional data.  