 The Curse of Dimensionality emerges when applying machine learning algorithms to highly- dimensional data.  In machine learning we face unique issues when analyzing and sorting out data in high- dimensional spaces. When the dimensionality increases, the volume of the space increases so quick that the accessible data become inadequate.  In high dimensional spaces, most of the preparing data resides within the corners of the hypercube defining the feature space.  The Curse of Dimensionality calls for Dimensionality Reduction. Dimensionality reduction refers to the method of converting a set of data having endless dimensions into data with less dimensions whereas still making sure that it conveys comparative information concisely.  Search for some real world examples where two variables have linear or noisy-linear relationships. Which dimensions do you think you should select to reduce dimensionality?    The relationship between a person's age and their salary: In common, as individuals get older, their pay tends to extend, and this relationship can be modeled as a linear function.  Eigenvalues and eigenvectors are noticeably utilized within the analysis of linear transformations.  You can try finding eigenvectors and eigenvalues of a 3 √ó 3 matrix such as  ùêµ =   [  7 ‚àí9 ‚àí2 19  0 ‚àí3 3 0 ‚àí8  ]    Eigenvalue: -2 0 1 0    Eigenvector: [  ]  The objective of PCA is to require n data points in d dimensions, which may be related, and outlines them by a new set of uncorrelated axes.  The uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original d dimensions. The first k components capture as much of the variation among the data points as possible.  The covariance matrix is a matrix that contains variances of all variables on the diagonal and co-variances among all pairs of variables in the off-diagonal entries.  ICA is a strategy of separating a multivariate signal into independent, non-Gaussian components.  The goal of ICA is to discover a set of basis functions that capture the signal's basic sources, then utilize these functions to separate the signal into its constituent parts.  Nonlinear methods, as restricted to linear techniques, which use linear algebra to recognize patterns in data, use more advanced mathematical techniques to identify and capture the fundamental structure of the data.  The technique of t-SNE (t-Distributed Stochastic Neighbor Embedding) is utilized to visualise high-dimensional data in two or three dimensions.  uMap is a new technique for dimensionality reduction which works almost the same as tSNE. However, it has some added advantages over tSNE.  Finally we practiced the codes related to this task with many examples and did a lot of practice to get familiar with this module. Lastly we completed the quiz and the pass activity of this module to complete the task.  