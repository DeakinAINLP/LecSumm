   Topic 4 starts off with introducing the existence of data with high dimensions which makes  it hard to analyse and organise,    The Curse of dimensionality occurs when we have to take the highly dimensional data and  pass it through the machine learning algorithms.    As the number of dimensions increase, the number of regions grow exponentially, and our  data is much more sparse and not as useful.    The concentration effect is defined as the process in which the relative contrast between  the far and near neighbours reduces as the dimensionality keeps on rising.    To solve the Curse of dimensionality, we require a process called dimensionality reduction  which is defined as a process of translating a set of data with high dimensions into a given  data with less dimensions while ascertaining that it will provide very similar information  but in a more concise manner.    The concepts of Eigenvalues and Eigenvectors are also introduced.    For a specific square matrix X if a number λ and a vector u is able to fulfil the condition  of Xu = λu, we can then dictate that λ is an eigenvalue while u is the equivalent  eigenvector of matrix X.    Singular value decomposition through which we can decompose a matrix into 3 separate  matrices.      The main goal of principal component analysis (PCA) is to fit n data points into d  dimensions which could be correlated and computes a summary of them using a new set  of uncorrelated axes.    To measure the covariances among variables, we can use the following formula:    The main aim of PCA is to strictly rotate the axes of t-dimensional axes to a new set of  axes where the principal axes are orthogonal to each other and they are ordered in such a  way that the main axis captures the highest variance and the second axis gets the next  highest variance and so on.    In the content, it is also explained on how we perform PCA with eigenvalue  decomposition as shown below:      The content also talk about how PCA is used in facial image analysis with first  generating the covariance matrix, finding the principle of eigen vectors, calculating face  image preservation of energy in the case where we use K principle eigenvectors and    finally project the data back after preserving the energy when K principle eigenvectors  are being used.  Reflection  Topic 4 has been more focused on showcasing how even if we have high-dimensional data, that  doesn’t mean that we cannot make them useful and use them to our advantage. There are ways as  explained  above,  using  which  we  can  translate  the  data  into  less  dimensions  but  while  still  preserving the content and keeping it meaningful enough for further processing.  Quiz          