his topic we have focused on the following: The curse of dimensionality Eigenvalues and eigenvectors Principal component analysis (PCA) Python programming: implementing PCA Dimensionality in Data: In fact, every feature in data has one dimension. So, when we are talking about an issue or case with high number of features, in fact we are referring to high dimensionality issues. This problem can happen everywhere. In text data which can be thousands of dimensions. When we process images, based on the resolution of the photos, we might have thousands of dimensions. Curse of Dimensionality: This issue happens when we apply ML algorithms to highly dimensional data. As each dimension takes one space and the number of spaces increase (with increasing the number of features/dimensions) then we face with the issue of sparse in data. As you can see in the figure above, with increasing the number of dimensions, the number of spaces grow exponentially. When we work with the data that suffers from curse of dimensionality, the mean of distance between points in space increase; and the standard variance decrease. This is the problem that we call the concentration effect. Dimensionality Reduction: The curse of dimensionality calls for Dimensionality Reduction. It refers to the process of converting a set of data with a vast dimensions/features into a data with fewer dimensions/features while making sure that it coveys similar information concisely. Eigenvalue and Eigenvector: Eigenvalues and eigenvectors are used in the analysis of linear transformation.We call it Eigenvalue Decomposition of matrix A. The matrix U is called a full Eigenvector matrix. We use the following equation to calculate the Eigen values and Eigenvectors. Singular Value Decomposition (SVD): SVD is a method of decomposing a matrix into three other matrice.In linear algebra, the SVD is a factorization of a real or complex matrix. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. PCA: The goal of PCA is to take n data points in d dimensions, which may be correlated, and summarises them by a new set of uncorrelated axes. The uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original d dimensions. The first k components capture as much of variation (or variance) among the data points as possible. To calculate PCA, we first need to find the variance across the variables,Then measuring the covariances among the variables. To put it simply, covariance is a measure of how changes in one variable are associated with changes in a second variable.In PCA method, when we project the data on a new axis, we are only interested in direction of maximum variance. Minimum Error Formulation: Another factor that we can is analysing PCA based on projection error minimization. So we want to minimise the errors by the formula below: Independent Component Analysis (ICA): ICA is a method of separating a multivariate signal into independent, non-Gaussian components. It uses to separate signals that are mixed together, such as in complex sound or image signal. The goal of ICA is to find a set of basis functions that capture the signal`s underlying sources, then use these functions to separate the signal into its constituent parts. Nonlinear Dimensionality Reduction Technique: PCA is a linear technique to find a pattern in data; however there are techniques and methods that use non-linear techniques to identify and capture the underlying structure of the data. One of these nonlinear techniques is t-SNE (t-Distribution Stochastic Neighbour Embedding) which is used to visualise high dimensional data in two or three dimensions. T-SNE coverts similarities between data point to joint probabilities and try to minimize the Kuulback-Leibler divergence between the joint possibilities of the low-dimensional embedding and the high-dimensional data. uMAP: it`s also another nonlinear technique which is new and works almost close to t-SNE. UMAP is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. It provides a very general framework for approaching manifold learning and dimension reduction.  (Refer to: https://umaplearn.readthedocs.io/en/latest/how_umap_works.html) 