Single value decomposition  Singular value decomposition (SVD) is a technique for decomposing a matrix into three other matrices: a matrix U, which is orthogonal, a diagonal matrix Σ, and another matrix V, which is also orthogonal. SVD is used to transform data into a new coordinate system where the covariance matrix is diagonal.  (Implementation of PCA) Can you describe this step in different words? Student Discussion what you think this means to your fellow students.  In some dimensionality problems, the number of data points can often times be less than the number of dimensions, This means that the number of nonzero eigenvalues of the data's covariance matrix is less than or equal to the number of data points. If we use the eigenvalue decomposition on a covariance matrix, it can sometimes be quite computationally expensive. As an alternative, singular value decomposition can be used reduce the computations to a lower order. SVD can be used to perform a PCA, which involves multiplying the SVD matrix by the data matrix. This will result in a set of projected data that is uncorrelated, meaning there are no correlations among the projected data. The first dimension or feature has a higher value, which means it is more important than the second one. If we decide to drop one of the dimensions and use dimensionality reduction by PCA, we should choose the eigenvector corresponding to the eigenvalue with the highest value and then project all data on that axis. The mean square error based on this approximation would be the sum of the remaining eigenvalues.  Think of another similar problem where reducing the dimensionality of a problem is critical for its implementation.  Recommendation systems which are used in platforms such as YouTube and Netflix, often have to crawl large datasets to analyze specific user preferences. The problem is, these datasets are notorious for being high-dimensional, making it computationally difficult to pull specific patterns and come up with a result for the recommendation. Dimensionality reduction techniques such as PCA can be easily used in the reduction of the dataset size, while still preserving the important information about user preferences.  tSNE Summary  One nonlinear technique to dimensionality reduction is known as the t-Distributed Stochastic Neighbor Embedding technique. It is used to visualize a high-dimensional dataset in a low-dimensional space. t- SNE works by modeling the pair similarity between data points in high-dimensional space and then attempts to preserve this similarity as accurately as possible in a lower-dimensional representation. It is able to accomplish this by minimizing a cost function that compares each distributions and the similarities between the original and projected data points, using a t-distribution to model the similarities in the lower-dimensional space.                         Provide summary of your reading list – external resources, websites, book chapters, code libraries, etc.  I used a few different resources for this task. These included the module content to understand some of the concepts taught, such as the mathematics behind dimensionality reduction techniques. I used Youtube, stackoverflow and some other websites in the implementation of the code and how to use the libraries such as SKlearn for the dimensionality reduction techniques like PCA.  Reflect on the knowledge that you have gained by reading contents of this topic with respect to machine learning.  Dimensionality reduction is a powerful technique in machine learning because it allows us to transform complex, high-dimensional datasets into simpler, more manageable representations that retain the essential information of the original data. By reducing the dimensionality of a dataset, we can overcome the curse of dimensionality and improve the performance of machine learning models.  Principal Component Analysis and Singular Value Decomposition are two commonly used dimensionality reduction techniques that enable us to identify and extract the most important patterns and relationships in a dataset. Both techniques allow us to identify the key factors that explain the variability in the data and reduce the dimensionality of the dataset while minimizing information loss. This makes them useful tools in a wide range of machine learning applications, including image and speech recognition, natural language processing, and data visualization.  Overall, dimensionality reduction through techniques like PCA and SVD helps to overcome the challenges posed by high-dimensional data, making it easier to analyze, visualize, and interpret the data while improving the performance and efficiency of machine learning algorithms.                Test        