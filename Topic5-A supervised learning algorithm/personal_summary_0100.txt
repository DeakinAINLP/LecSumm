Overview In topic 4, we focused on the following learning goals:    Solving the curse of dimensionality   Eigenvalues and eigenvectors   Principal component analysis   Python programming: implementation  Dimensionality in Data There are some issues when it comes to analyzing and organizing data in high-dimensional spaces. For example:  Text data:  -  When crawling a News website, you might end up with 10,000 - dimensions documents. And this will be the size of dictionary you’ll have to build.  Image data:  -  A 64x64 image would have 4096 dimensions  Genomic data  -  Parkinsons disease case-control data as an example, has  408,801.  Curse of Dimensionality The curse of dimensionality arises when applying machine learning algorithms to highly-dimensional data.  When dimensionality increases, the volume of the space increases fast that the available data become sparse. This is problematic!  The curse of dimensionality dictates that as the number of dimensions increases, the number of regions grows exponentially.  When describing any human being, if we use number of arms or legs it includes many other animals. So we need to add another dimension like an “upright body posture”. But if went to differentiate one human from another human, we have to add many more dimensions such as height, weight, skin color, etc. Now imagine we want to identify each individual on earth, how many descriptors would we need to add?  In high dimensions, most of the volume of the unit sphere is very close to its surface.  The curse of dimensionality result in less distinctive distances in high dimensions. So given a point in high dimensions, the relative distance between points fat from it and close to it, becomes negligible.  The figure illustrates this.  There is an analysis regarding this problem; it has been called the concentration effect.  Concentration Effect If we assume the ratio of the variance of the length of any point vector converges to zero with increasing data dimensionality, the proportional difference between the farthest-point distance and the closest-point distance vanishes.  So it reduces the utility of the measure to discriminate between near and fat neighbors. Relative contrast between near and fat neighbors diminishes as the dimensionality increases.  This problem implies:  -  Clustering or KNN algorithms may be meaningless in high dimensions. However, there might still be patterns in high dimensions. We just need better distance metrics. So research is needed.  -  Until we develop better distance metrics, we should aim to  reduce the dimensionality where possible.  Solving the Curse The Curse of Dimensionality calls for Dimensionality Reduction to solve the problem. Dimensionality Reduction refers to the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely.  Singular Values Decomposition (SVD) A method of decomposing a matrix into three other matrices.  (check section 4.6)  Preliminaries Variance across each variable  Data is represented as a cloud of points in multidimensional space with one axis for each of the variables. The centroid of the points is defined by the mean of each variable. The variance of each variable is the average squared deviation of its n values around the mean of that variable.  Covariance among variables  Simply put, covariance is a measure of how changes in one variable are associated with changes in a second variable.  Covariance matrix  The covariance matrix is a matrix that contains variances of all variables on the diagonal and co-variance among all pairs of variables in the off-diagonal entries. It can be written as:  The main objective of PCA is to rigidly rotate the axes of t- dimensional axes to a new set of axes that have the following properties  -  Ordered such that principal axis captures the highest variance, axis-2 captures the next highest variance,…, and axis-d has the lowest variance  -  Covariance among each pair of the principal axes is zero  Implementation of PCA Suppose we have 100 images in 64x64 dimensions, n=100,  d= 64x64=4096.  -  In this case, the number of nonzero eigenvalues of data  covariance matrix is less than or equal to n.  -  If we use eigen value decomposition (EVD) on the covariance matrix of size dxd, we need to perform computations of the order of O(d^3). This may be too expensive.  -  In such case, SVD can reduce computations to O(n^3) or less.  Independent component analysis  ICA is a method of separating a multivariate signal into independent, non-Guassian components. In signal processing, machine learning and neuroscience, ICA is commonly used to separate signals that are mixed together, such as in a complex sound or image signal. The goals of ICA is to find a set of basis functions that capture the signal’s underlying sources then use these functions to separate the signal into its constituent parts.  Nonlinear dimensionality reduction technique  This technique uses more complicated mathematical techniques to identify and capture the underlying structure of the data.  uMap  a new technique for dimensionality reduction which works almost the same as TSNE  