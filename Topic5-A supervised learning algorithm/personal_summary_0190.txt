In  this  topic  I  learned  the  very  interesting  topics,  such  as  Curse  of  Dimensionality,  Dimensionality Reduction, Singular value decomposition, PCA, Implementation of PCA.  Curse of Dimensionality: Curse of Dimensionality is a set of problems arise when we are working with high-dimensional data. The dimension of a dataset refers to number of attributes or features that exist in  a  dataset.  A  dataset  with  many  attributes.  i.e.,  100  or  more  than  hundred  is  referred  to  high dimensional data. Some of the diﬃculties that come with high dimensional data during analysing or visualising the data to ﬁnd the patterns for the machine learning models. The diﬃculties related to training  to  machine  learning  models  due  to  this  high  dimensional  data  known  as  curse  of dimensionality [1].  Dimensionality  Reduction:  The  reduction  of  attributes  from  the  dataset  known  as  dimensionality reduction.  In  this  topic  also,  I  learned  about  SVD  (Singular  Value  Decomposition)  and  this  is  method  of decomposing a matrix into three other matrices.  X =USV T  I came to know about PCA (Principal Component Analysis) is as dimensionality reduction technique is used for to reduce the dimensionality of large data sets by transforming a large set of variables into smaller one that still contains most of the information in large set.  There is another term it’s called covariance that tells the correlations between the variables. If the covariance is positive then, the two variables increase or decrease together i.e., correlated and if the covariance  is  negative  then,  one  increases  and  other  decreases  i.e.,  inversely  correlated.  The eigenvectors and eigenvalues that we need to compute from the covariance of matrix to determine the Principal Components of the data.  The Principal Components are new variables that are constructed as linear combinations or mixtures of initial variables and these new variables combined in that way, so they are uncorrelated and most of the information within initial variables is squeezed or compressed into ﬁrst components [2].  Answer to Python question 3:  As  shown  above,  each  component  is  perfectly  correlated  with  itself.  However,  the  oﬀ-diagonal elements are all very close to 0, thus indicating that there is very little correlation between any pair of components. All the three components are largely orthogonal and are not strongly correlated with each other.  