 Learning objective  - solving the curse of dimensionality - eigenvalues and eigenvectors -  principal Component Analysis (PCA) -  Python programming : implementing PCA.  Learning summary  -  Dimensionality in data  Take an example of pixels in a picture as features, a 64x64 image would have 4096  dimensions.  -  Curse of dimensionality  The Curse of Dimensionality arises when applying machine learning algorithms to highly-  dimensional data.  To be more specific about the definition, the curse of dimensionality dictates that as the number of dimensions increases, the number of regions grows exponentially. As the number of regions grows and space increases each data point has more and more room. That makes our data sparse and somehow not useful anymore.  Therefore, using clustering or KNN algorithms may be meaningless in high dimensions, the solution to this is the need of better distance metrics. The aim of this is to reduce the dimensionality where possible.  -  Solving the curse  To handle the redundant or variables, the Curse of Dimensionality calls for  Dimensionality Reduction. Dimensionality reduction refers to the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely.  -  Eigenvalues and Eigenvectors  Eigenvalues and eigenvectors are prominently used in the analysis of linear  transformations.  For a given square matrix A if a number 位 and a vector u satisfy the condition Au = 位u  then 位 is called an eigenvalue and u is the corresponding eigenvector of A.  Finding eigenvalues and eigenvectors  Eigenvalues of a matrix A can be found by solving the characteristic polynomial  -  Singular value decomposition  in 位  Singular value decomposition (SVD) is a method of decomposing a matrix into three  other matrices: X=USVT  Where:   Preliminaries  The goal of PCA is to take n data points in d dimensions, which may be correlated, and  summarize them by a new set of uncorrelated axes.  The uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original d dimensions. The first k components capture as much of the variation (or variance) among the data points as possible.  Variance across each variable: Data is represented as a cloud of points in a  multidimensional space with one axis for each of the variables. The centroid of the points is defined by the mean of each variable. The variance of each variable j  is the average squared deviation of its n values around the mean of that variable.  Covariances among variables: covariance is a measure of how changes in one variable are associated with changes in a second variable. Degree to which the variables are linearly correlated is represented by their co-variances.  Covariance Matrix: The covariance matrix is a matrix that contains variances of all variables on the diagonal and co-variances among all pairs of variables in the off-diagonal entries.  PCA: decorrelation: The main objective of PCA is to rigidly rotate the axes of d- dimensional axes to a new set of axes (called principal axes) that have the following properties.  -  Implementation of PCA  Using SVD for PCA: We can use SVD to perform PCA. As you have seen before in previous  section, given any d*d matrix Y its Singular Value Decomposition (SVD)is given as: Y=USVT where  U is an n*d orthogonal matrix (same as U in previous section) S is a d*d diagonal matrix with elements V is a d*d orthogonal matrix  -  Other dimensionality reduction techniques  o  Independent component analysis (ICA): ICA is a method of separating a multivariate signal into independent, non-Gaussian components.  Nonlinear dimensionality reduction technique: Nonlinear techniques, as opposed to linear techniques, which use linear algebra to identify patterns in data, use more sophisticated mathematical techniques to identify and capture the underlying structure of the data.  uMap: uMap is a new technique for dimensionality reduction which works almost the  same as tSNE. However, it has some added advantages over tSNE.     