Curse of dimensionality describes the situation where the amount of data needed to fully cover the feature space grows exponentially as the number of features or dimensions of a dataset rises. This can cause a number of issues with machine learning, including overfitting, visualisation challenges, and an increase in computing complexity. Eigenvalues and eigenvectors are concepts from linear algebra that are commonly used in data analysis and machine learning. When converted by a matrix, an eigenvector is a vector that maintains its original direction but can be scaled by a scalar quantity known as an eigenvalue. Principal component analysis and other dimensionality reduction methods frequently employ eigenvectors and eigenvalues (PCA). A matrix is divided into three matrices using the SVD matrix factorization technique, which can be applied to a variety of tasks, including data compression, noise reduction, and data visualisation. In PCA and other dimensionality reduction methods, it is frequently utilised. A dimensionality reduction method called PCA reduces the amount of variation in high-dimensional data while converting it into a lower-dimensional space. By identifying a group of major components that accurately represent the most significant patterns in the data, it achieves this. An optimisation problem that aims to optimise the variance of the data projected onto a lower-dimensional space can be used to define PCA and derive principal components. The eigenvectors of the original data's covariance matrix are then used to derive the principal components.  