Dimensionality Reduction  Dimensionality in Data refers to many varying aspects data can come in(text, image genomic etc) and the associated dilemma of how such dimensionality is organised and represented.  It doesn’t refers just to different types of data but also how we categorise the attributes data of each type.  Think simply the difference in difficulty of drawing a 1D image such as line, to a box shape (2D), then to a cube (3D) – each dimension adds a layer of difficulty in representation.  The difficulty  grows  exponentially  as  dimension  increase  and  is  known  as  the  “curse  of dimensionality”.  As the number of dimensions in a dataset increases, the amount of data required to cover the space between data points grows exponentially. This can cause a sparsity problem, where there may be insufficient data to make accurate predictions or identify patterns in the data.  In addition, the curse of dimensionality can lead to overfitting, where a model becomes too complex to generalize well to new data. This is because with high-dimensional data, there are many more possible models and it becomes more challenging to choose the best one- this aspect is known as distance concentration.  An example of noisy linear relationship is the relationship between a person's age and their income. Generally, people tend to earn more as they gain more experience and progress in their  careers.  However,  this  relationship  can  be  noisy  due  to  factors  such  as  job  changes, career transitions, and variations in earning potential across different industries. In this case, selecting either age or income could be a good option for reducing dimensionality.  Generally,  dimensions  that  have  strong  linear  or  non-linear  relationships  with  the  target variable are most informative, while dimensions that are weakly correlated or redundant can be discarded.  Principal Component Analysis – Approaches  Eigenvalues and Eigenvectors  In machine learning, eigenvectors are used in principal component analysis (PCA) to reduce the dimensionality of data by selecting a subset of the most informative features.  So eigenvalues and eigenvectors are used to analyze and transform matrices.  An eigenvector of a square matrix is a vector that, when multiplied by the matrix, produces a scalar multiple of itself. This scalar multiple is called the eigenvalue of the matrix. In other words, an eigenvector is a special vector that points in a direction that is only scaled by the matrix and not rotated or otherwise transformed.  The eigen value is the value of the eigenvectors when it is “squished” flat  Singular value decomposition Singular  value  decomposition  (SVD)  is  a  matrix  factorization  method  that  decomposes  a matrix into three components: a diagonal matrix of singular values, a matrix of left singular vectors, and a matrix of right singular vectors. SVD has many applications in fields such as signal processing, data analysis, and image compression  The rest of the module covered PCA application in python with various activities.  2. Provide summary of your reading list – external resources, websites, book chapters, code libraries, etc. For this unit I read the provided materials, Python libraries and did the activities throughout the module in a Jupyter notebook.  I  also  supplemented  my  reading  with  Code  Academy  Machine  Learning  course  Learn  the Basics of Machine Learning | Codecade my And some additional YouTube videos: Basics of the Curse of Dimensionality https://youtu.be/JMmuVyDZ_XA Eigenvectors and Eigenvalues https://youtu.be/PFDu9oVAE-g  3. Reflect on the knowledge that you have gained by reading contents of this topic with respect to machine learning. This topic I found the concepts of dimensionality reduction techniques and their applications in machine learning very interesting. I was unfamiliar with Principal Component Analysis (PCA) and how it uses Eigenvectors and Eigenvalues to reduce the dimensionality of a dataset. I can see that this concept would be be useful in solving problems with large datasets by reducing the  number  of  features  while  retaining  the  most  important  information.  Given  that  high- dimensional  datasets  are  common  in  today's  data-driven  world,  managing  the  number  of features can improve the performance of machine learning algorithms.  Practically, I can see it importance in image recognition, as images can have many pixels, each of which is a feature. Dimensionality reduction techniques can be used to reduce the number of pixels by identifying the most important features, while retaining the overall information contained in the image.  