   Topic 4 - Dimensionality Reduction  Curse of Dimensionality.  It states that as the number of dimensions increases, the number of regions grows exponentially. As the  number of regions grows and space increases each data point has more room that makes the data sparse  and not useful.  Concentration Effect.  Relative contrast between near and far neighbours diminishes as the dimensionality increases. This is  known as the concentration effect of the distance measure.  The implication of this is that clustering or KNN algorithms become meaningless in high dimensions.  Hence we should aim to reduce the dimensionality where possible.  Dimensionality Reduction.  Its the process of converting a set of data having vast dimensions into data with fewer dimensions while  still making sure that it conveys similar information concisely.  Eigenvalues and Eigenvectors.  For a given square matrix A if a number lambda and a vector u satisfy the condition Au = A.lambda then lambda is called an eigenvalue and u is the corresponding eigenvector of  A .  Singular Value Decomposition.  It is a method of decomposing a matrix into three other matrices. It represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal.  PCA  The goal of PCA is to take n data points in d dimensions, which may be correlated and summarises them by a new set of principal axes. These axes are linear combinations of the original d dimensions.  Variance across each variable. The variance of each variable is the average squared deviation of its n values around the mean of that variable.  Covariances among variables. It is a measure of how changes in one variable are associated with changes in a second variable.  Objective of PCA The main objective of PCA is to rigidly rotate the axes of t-dimensional axes to a new set of axes that have the following properties:  Ordered such that principal axis - captures the highest variance, axis-2 captures the next highest variance and so on  Covariance among each pair of the principal axes is zero. This is called decorrelation property.  https://www.evernote.com/client/web?login=true#?an=true&n=9cecc7f4-4f43-45ba-e514-696d6fab3f2b&  1/2     An important application of PCA is in facial image analysis.  Independent Component Analysis  It is a method of separating a multivariate signal into independent, non-Gaussian components. In signal  processing, machine learning and neuroscience, ICA is used to separate signals that are mixed together.  The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then uses  these functions to separate the signal into its constituent parts.  Nonlinear dimensionality reduction technique.  The technique t-SNE (t-Distributed Stochastic Neighbour Embedding) is used to visualise high-  dimensional data in two or three dimensions. It works by finding patterns and relationships in the data  and then representing those patterns in a lower-dimensional space to reduce the complexity of the data.  https://www.evernote.com/client/web?login=true#?an=true&n=9cecc7f4-4f43-45ba-e514-696d6fab3f2b&  2/2   