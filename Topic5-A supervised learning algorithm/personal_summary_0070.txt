Evidence of learning Dimensionality The curse of dimensionality It is very common in machine learning to run into cases where the number of variables or dimensions to analyse becomes very large. This significantly impacts the performance of model training.  Furthermore, in higher-dimensional spaces the data quickly becomes very sparse.  The curse of dimensionality is that as the number of dimensions increases, the number of regions grows exponentially.  Note: Some intuitions drawn from low-dimensional spaces fails in high dimensions. For example, in lower dimensional space a circle/sphere of radius r occupies a lower fraction of space as a square/cube with the same radius but in higher dimensions the hypersphere begins to occupy close to 100% of the hypercube of the same radius.  Features of high dimensionality that are problematic in machine learning:  -  -  In high dimensional space most of the training data resides in the corners of the hypercube defining the feature space. The distance of two points in higher dimensional space becomes negligible as more dimensions are added. This is known as the concentration effect.   The concentration effect As the number of dimensions increase:  - -  the average distance between points will increase and the variation in distance between points will decrease. This is shown in the above graph by the standard deviation decreasing.  This can imply that:  -  Clustering or KNN may be meaningless in high dimensions, however there may be some patterns in high dimensions. If that is the case, then designing better distance metrics to compare objects can overcome the concentration effect.  -  We should aim to reduce dimensionality where possible until better distance metrics can be  developed for our problem.  Dimensionality reduction Where possible, the number of dimensions should be reduced through dimensionality reduction techniques.  Dimensionality reduction refers to converting a set of data with many dimensions into a set with lower dimensions while ensuring it conveys similar information concisely.  For example, if all objects along two dimensions have the same value in both dimensions, we can discard one dimension:   Other dimensionality reduction techniques:  1.  Principal component analysis (PCA): In short this involves identifying and removing the  2.  features of least variance in a dataset using singular value decomposition. Independent component analysis (ICA): A method used on multivariate signals to separate it into its constituent non-Gaussian components.  Linear algebra & statistics prerequisite concepts Eigenvalues and eigenvectors An eigenvector of a square matrix is a non-zero vector that, when multiplied by the matrix, produces a scalar multiple of itself.  The scalar multiple is called the eigenvalue.  Every square matrix has at least one eigenvector and eigenvalue but may have multiple.  Eigenvectors can be found by solving the characteristic equation of the matrix, or by using iterative methods like the power method or the QR algorithm.  Eigenvectors are used in machine learning to reduce the number of dimensions in the data in techniques such as principle component analysis (PCA).  Finding the eigenvalues of a matrix 1.  Start with an n x n matrix A. 2.  Subtract ŒªI from A, where Œª is a scalar and I is the n x n identity matrix. 3.  Find the determinant of the resulting matrix. 4.  Set the determinant equal to 0 to get the characteristic equation. 5.  Solve the characteristic equation to find the eigenvalues of the matrix.   That is, solve the following equation for Œª:  For example, for a 2x2 matrix:  det(ùê¥ ‚àí ùúÜùêº) = 0  det (  ùëé ‚àí Œª ùëê  ùëè ùëë ‚àí Œª  ) = 0  (ùëé ‚àí Œª)(d ‚àí Œª) ‚àí bc = 0  (ùëéùëë ‚àí ùëéŒª ‚àí dŒª + Œª2) ‚àí ùëèùëê = 0  Then solve for Œª. It may be necessary to use the quadratic equation to solve for Œª.  Linear combinations A linear combination is a sum of scalar multiples of vectors. For example, if we have two vectors u and v then a linear combination would be:  ùëéùë¢ + ùëèùë£  Where a and b are scalar values. This combines two vectors into a new vector. The coefficients are used to change the proportion of which each component vector contributes to the new vector.  Basis and vector space A basis for a vector space is a set of linearly independent vectors that can be used to express any vector in the vector space as a linear combination of the basis vectors. For example, in two- dimensional space the basis vectors by default are:  1 0  and  0 1  This makes up the 2x2 identity matrix. The vector  3 2  a linear combination:  can be made from these two basis vectors using  In a different vector space (i.e. the basis vectors are different) this same vector would be expressed as a linear combination of the basis vectors in terms of different scalars. For example:    In other words, different vector spaces can be thought of as different perspectives to describe the same vectors.  Relevant: https://www.youtube.com/watch?v=P2LTAUO1TdA  Linear transformations in relation to vector space We can convert a vector expressed in one vector space into another using linear transformations, similarly to how we convert different units of measurement in the real world (e.g.: pounds to kilograms).  Variance and covariance Variance in statistics is a measure of how spread-out values in a dataset are from the mean of the set. It is the sum of the squared differences between each value in the dataset and the mean of the dataset. It is defined by this formula:  Covariance is a measure of how two variables in a dataset are related to each other. If two variables have a positive covariance, it means they tend to increase and decrease together. If they have a negative covariance it means that while one variable increases the other tends to decrease. If the covariance is 0 it means they have no relationship to each other. Covariance between two variables is defined by this formula:    The covariance matrix contains the variances of all variables along its diagonal and the covariances of all pairs of variables in the off-diagonals. It is defined as:  Or:  Singular value decomposition Singular value decomposition is a method used to decompose a matrix into three other matrices  The matrices U and V are both orthonormal, i.e: their columns are vectors that all point to directions that are perpendicular to each other and are all at the unit length.  The columns of matrix U are called the left singular vectors while the columns of matrix V are called the right singular vectors. These vectors represent the directions which have the most variance in the dataset.  They are found by choosing the vector that describes the direction of the most variance in the dataset, followed by another vector that describes the direction maximum variance that is perpendicular to the first vector, followed by more vectors that are perpendicular to the existing vectors and describe the maximum variance in the remaining directions.  This process is continued until all the vectors have been chosen which will be the rank of the matrix (the size of the largest square matrix that can fit in the matrix).      The singular values are found by finding the square root of the eigenvalues of either AAT or ATA. These singular values are then arranged along the main diagonal in descending order.  Principle component analysis The goal of PCA is to rotate the dataset so that each axis is in line with the directions which have the most variance.  SVD is used in PCA to identify the directions with the most variance.  The steps for PCA are as follows:  1.  Standardize the data to have a mean of 0 and a variance of 1.    This is important because it ensures that each feature has equal importance in the  PCA analysis.  2.  Compute the covariance matrix of the standardized data.    This is necessary to obtain the covariance matrix, which is used to compute the  eigenvectors and eigenvalues.  3.  Perform eigen decomposition on the covariance matrix to obtain the eigenvectors and  eigenvalues.    This involves computing the eigenvectors and eigenvalues of the covariance matrix, which represent the directions and magnitudes of maximum variance in the data.  4.  Sort the eigenvectors by their corresponding eigenvalues in descending order.    This is important because it sorts the eigenvectors by their importance, allowing us  to choose the most significant directions.  5.  Choose the first k eigenvectors with the largest eigenvalues to form a new matrix.    This involves selecting the k eigenvectors with the largest eigenvalues to form a new  matrix, where k is the number of principal components we want to retain.  6.  Project the standardized data onto the new matrix to obtain the principal components.    This involves projecting the standardized data onto the new matrix to obtain the principal components, which are the new features that capture the maximum variance in the data.  7.  Optionally, calculate the proportion of the total variance explained by each principal  component and choose an appropriate number of components to retain.    This is optional but useful for deciding how many principal components to retain. We can look at the proportion of the total variance explained by each principal component and choose a cut-off based on how much variance we want to retain.  Independent component analysis Statistical independence Two random variables are said to be statistically independent if the probability of one variable occurring does not affect the probability of another variable occurring.  Statistical measures used in ICA Kurtosis: A measure of the shape of the probability distribution, used to identify non-Gaussian distributions.  Negentropy: A measure of the amount of information in a probability distribution, used to identify distributions that are as independent as possible.  ICA In ICA it is assumed that the dataset being analysed is made up of multiple independent non- Gaussian components that are combined using linear combination. The goal of ICA is to separate the data into a set of underlying independent components which can then be analysed separately.  ICA finds components that are statistically independent from one another by assuming that the observed data is a linear combination of multiple components. ICA tries to identify the components by iteratively estimating the components and the mixing matrix, using statistical measures such as kurtosis or negentropy to find the components that are most independent.  