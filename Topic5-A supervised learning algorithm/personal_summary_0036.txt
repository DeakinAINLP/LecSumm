This topic’s unit, Dimensionality Reduction, introduced the concept of high dimensionality datasets and the problems that they introduce into data analysis. The unit then expands into methods of factoring matrices, including Eigen value decomposition and Singular value decomposition with a focus on the mathematics, before returning to how they are applied in Principal Component Analysis to reduce the dimensionality of a dataset and how to achieve this in Python.  Datasets can contain large number of features, for example text, images or genomic data. Each feature in a dataset has its own associated ‘dimension’, i.e., a plane on which it is represented. These datasets are said to have high dimensionality. As the number of dimensions increases the dimensional space of all dimensions together increases. This increase in dimensional space means leaves a large amount of empty space (where no data has been plotted). In these regions the model cannot accurately compare data. With large number of features, a lot of the features do not have a large variance between the data, this means that measurement between these features becomes uniform reducing the difference between datapoints and increasing the difficulty of comparing data.  Dimensionality reduction is a technique that we can apply to these datasets to reduce the number of features, keeping the ones with the most variation, so they are easier to analyze. Dimensionality reduction is the application of mathematical techniques to reduce the number of dimensions in a dataset while attempting to keep the most important information about the dataset.  The unit introduced Eigenvalue decomposition (EVD), a matrix factorization technique used in linear transformation that can only be applied to square matrices, decomposing the matrix into eigenvectors and eigenvalues. Eigenvalues are scalar values that represent how an eigenvector is scaled. Eigenvectors are matrices that when multiplied by a square matrix, results in a scalar multiple of the original vector.  While Singular Value Decomposition (SVD) is matrix factorization technique that can be applied to any matrix and decomposes a matrix into singular vectors and values. It is used to reduce the size of the data while preserving the important features of the original matrix.  After covering EVD and SVD in depth the unit moves onto Principal component analysis (PCA). PCA is a statistical technique to reduce the dimensionality of a dataset. It aims to identify the principal components, components with a highest amount of variation in the data. PCA can be performed using EVD or SVD, when using EVD the eigenvectors are the principal components and when using SVD they are the singular vectors.  The unit briefly touches on another dimensionality reduction technique for linear transformation, independent component analysis (ICA) and a couple of techniques for non-linear transformation, Nonlinear dimensionality reduction technique, and Uniform Manifold Approximation and Projection (uMap). The unit site also includes some examples of implementing PCA using Python libraries.  