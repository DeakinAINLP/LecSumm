 In the topic four online class we mainly learnt about dimensionality reduction, which is the  transformation of data from a high-dimensional space into a low-dimensional space so that the  low-dimensional representation shows some meaningful properties of the original data.  We  first  learnt  about  some  basic  concepts  on  eigenvalues  and  eigenvectors,  which  are  prominently used in the analysis of linear transformations, and singular value decomposition  (SVD),  which  is  a  method  of  decomposing  a  matrix  into  three  other  matrices.  Under  eigenvalues and eigenvectors lesson, we talked about how to find eigenvalues and eigenvectors  and under singular value decomposition (SVD) we learnt the way of calculating the SVD. Then  we  discussed  typical  dimension  types  of  the  data  we  are  dealing  with,  what  curse  of  dimensionality refers to and why high dimensional data is problematic for machine learning,  what concentration effect means. We also talked about solving the curse of dimensionality and  our lecturer explained an example for it.  In the second half of the online class, we learnt about Principal Component Analysis (PCA).  Under  preliminaries  of  PCA,  we  concentrated  on  the  equations  to  calculate  variance  across  each variable, co-variances among variables, covariance matrix and then the decorrelation of  PCA. Next, we went through the formulation of PCA and deriving principal components using  different methods such as PCA via Eigen value decomposition, minimum error formulation  etc. After that, we talked about implementation of PCA. There are several alternative ways of  implementing PCA and we learnt some of them such as PCA for data where ùëõ<ùëë, using SVD  for  PCA  etc.  Finally,  we  briefly  discussed  some  other  components  vs  projection-based  dimensionality reduction techniques such as Independent component analysis (ICA) and t-SNE  (t-Distributed Stochastic Neighbor Embedding).  In the programming part of this unit, we learnt how to use Python to visualize correlated and  uncorrelated data, curse of dimensionality and PCA to remove correlation in data, implement  PCA using in-built functions in python, decide the number of principal components to keep for  dimensionality  reduction  and  how  to  calculate  the  reconstruction  error  of  data  after  PCA.  Finally we learnt how to perform t-Distributed Stochastic Neighbour Embedding in Python.  