 Dimensionality Reduction+ There are types of dimensionality of data such as text-data, image-data and genomic data  Curse of dimensionality 2:25pm   It arises when applying machine learning algorithms to highly-dimensional data  When the dimensionality increases, the volume of the space increases so fast  that the available data become sparse or scattered then the model won’t be able to find the proper pattern and bad evaluation   Increase in dimensionality will increase exponentially the fraction of volume. If we want to precisely describe any object, we will need to increase the dimensions or add descriptors of that specific object, which in turn will make the system more complex.  Concentration effect 2:33pm   The notion of distance becomes less and less useful (reaches to zero) as the dimensionality increases.    This problem can imply that K-means nearest neighbors or clustering algorithms are not useful and are meaningless in higher dimensions. This will require better distance metrics to find any patterns in those high dimensions. If we don’t not have better distance metrics, we will need to reduce the dimensionality whenever possible.  Solving the curse of dimensionality 2:35pm  If variables are correlated, then they are redundant and can be removed.  Using Dimensionality reduction will convert a set of data that contains vast dimensions to a set of data with fewer dimensions while maintaining identical or similar information. eigenvalues and eigenvectors 2:03pm   Are used in the analysis of linear transformations which is projecting the data  point into a new set of data point  Changing the format of the data into a new form or shape, which eigenvalues and  eigenvectors will be used to help do that.  Using formula Au = λu. Where λ is defined as an eigenvalue and u is an   eigenvector that corresponds to the square matrix of A. For a matrix A of size d x d, there will be d eigenvectors and eigenvalue pairs, for example a matrix with 2 x 2 will have 2 eigenvectors and eigenvalue pairs.  Eigenvector is the column matrix U The transpose of matrix U is called a eigenvalue decomposition of matrix A Orthogonal every u vector is independent and uncorrelated, angle is 90 degrees Matrix U is an orthogonal matrix that rotates the coordinates in a way to  de-correlate the dimensionality.  Singular value decomposition (SVD) 2:20pm  We use SVD when it is not a square matrix.   Learnt that this is a method of decomposing a matrix into three other matrices:  𝑇 With a formula of 𝑋  =  𝑈  *  𝑆  *  𝑉 Where X is a 𝑛  *  𝑑 Where U is a 𝑛  *  𝑑 Where S is a 𝑑  *  𝑑 variance Where V is a  𝑑  *  𝑑  orthogonal matrix  matrix orthogonal matrix that’s the same as the eigenvector diagonal matrix that is the sigma, which is the   Learnt that the SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal When calculating the SVD, we will have to find:   The eigenvalues and eigenvectors of  𝑇 𝑋  *  𝑋 𝑇 Columns of V will be the eigenvectors of 𝑋 *  𝑋 𝑇 Columns of U will be the eigenvectors of 𝑋  *  𝑋  and  𝑇 *  𝑋 𝑋  Singular values in S are square roots of eigenvalues from  𝑇 𝑋  *  𝑋  or  𝑇 *  𝑋 𝑋   The diagonal entries of the S matrix are the singular values and are arranged in descending order which always contains real numbers  Principal Component Analysis (PCA) 2:44pm  Preliminaries 2:45pm     Take n data points in 𝑑 dimensions where it could be correlated and then summarize them by a new set of uncorrelated axes which are called principal components or principal axes. These uncorrelated axes are linear combinations of the original 𝑑 dimensions. These uncorrelated axes are sorted in descending order based on the variance along the axes.  We compare the dimension vectors u1 and u2 to find which has the  maximum variance direction and choose that vector   In this example we see that there are two dimension vectors of u1 and u2, we see that the vector u1 has the maximum variance direction while u2 is orthogonal, which contains the lowest variance.  We prioritize the one that has the maximum variance, therefore we  choose u1 over u2.   Formulation of PCA 2:48pm   In the previous notes above from preliminaries. We will project the data on a new axis, using the direction that is specified by a d-dimensional vector u1 which has the maximum variance direction.  We will assume that u1 is the unit length vector,  ||𝑢1||  =  1  or  𝑇 *  𝑢1  =  1 𝑢1  Since that  ||𝑢1||  =  1  𝑇 *  𝑢1  =  1 𝑢1 u1 is orthogonal and thus satisfies the orthogonality Then each data point  or  can be projected on the vector u1 to create a  𝑋 𝑖  are the same, this means that  new coordinate of  1  =  𝑢 𝑌 𝑖 1 Therefore we will find the variance of the data projected on u1 as:  , where  1 𝑌 𝑖  𝑇 *  𝑚𝑒𝑎𝑛(𝑋)     Essentially, we would like to find the maximized variance.  The final formula to finding the maximum variance is to find the slope of the function (derivative) is equal to zero, where:    This is similar to the eigenvalue problem, where λ is the largest eigenvalue of C and u1 is the corresponding eigenvector. u1 is known as the first principal component. The next set of axes of u2,....,  can be found in a decreasing order  𝑈 𝑑  incrementally from the eigenvector of u1 orthogonal matrix  PCA: Minimum Error Formulation 2:56pm  We are trying to minimize the error rate in out new k dimensions from the  original d dimensions   Implementation of PCA 2:59pm     If the number of data points (n) is less than the number of dimensions d, where the number of nonzero eigenvalues of the data covariance matrix is less than or equal to n. This will mean that the data covariance matrix will have no rank and can not be inverted, which makes implementing the standard approach of PCA difficult. If we used Eigen Value Decomposition(EVD) on the covariance matrix, it would be too expensive as we need to perform high computations from the d dimensions of the order of O(d^3) where d is the number of dimensions.  So then we used SVD to rescue the computations from O(d^3) to O(n^3)  where n is the number of data points Other dimensionality reduction techniques 3:02pm   t-SNE (t-Distributed Stochastic Neighbor Embedding)    Is a nonlinear dimensionality reduction technique where it turns a high-dimensional data into a 2D or 3D dimensional space which can then be used to visualize the dataset in a reduced complexity. This can be done by using the probability of two points being neighbors in both the high-dimensional and 2D or 3D dimensional space, and then minimizing the difference between the two probability distributions.  UMAP (Uniform Manifold Approximation and Projection)    Is a nonlinear dimensionality reduction technique that is similar to t-SNE where it turns a high-dimensional data into a 2D or 3D dimensional space. The difference is that UMAP uses a graph-based approach to model the data.   Independent Component Analysis (ICA)  A technique used to separate a multivariate signal into independent, non-Gaussian components. Essentially it is to identify underlying independent components that are mixed together to form the observed signal and gain an understanding of it.  Python programming : Implementing PCA.  Learnt how to create data from each dimension independently and data that are correlated. Learnt how to visualize correlated and uncorrelated data. Learnt how to use PCA to remove the correlation of a data. Learnt how to measure the reconstruction error when applying PCA. Learnt how to use PCA on a dataset with higher dimensions Learnt how to use t-SNE on a dataset with higher dimensions  For t-SNE, I have used this website to better understand how t-SNE worked https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a 3a293108d1 https://towardsdatascience.com/t-distributed-stochastic-neighbor-embedding-t-sn e-bb60ff109561 For UMAP, I have used this website to better understand how UMAP worked https://towardsdatascience.com/tsne-vs-umap-global-structure-4d8045acba17 https://www.youtube.com/watch?v=eN0wFzBA4Sc    