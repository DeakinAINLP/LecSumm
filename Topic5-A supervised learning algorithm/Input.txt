Independent component analysis (ICA) ICA is a method of separating a multivariate signal into independent, non-Gaussian components. In signal processing, machine learning, and neuroscience, ICA is commonly used to separate signals that are mixed together, such as in a complex sound or image signal. The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then use these functions to separate the signal into its constituent parts.  To learn more about ICA, please refer to the following resources: References: Comon, Pierre (1994): "Independent Component Analysis: a new concept?", Signal Processing, 36(3):287–314 (The original paper describing the concept of ICA) Hyvärinen, Aapo, and Erkki Oja. "Independent component analysis: algorithms and applications." Neural networks 13.4-5 (2000): 411-430.       The following topic is optional for SIT307 students.  SIT720 student must complete the following topic.   Nonlinear dimensionality reduction technique  Nonlinear techniques, as opposed to linear techniques, which use linear algebra to identify patterns in data, use more sophisticated mathematical techniques to identify and capture the underlying structure of the data. The technique of t-SNE (t-Distributed Stochastic Neighbor Embedding) is used to visualise high-dimensional data in two or three dimensions. It works by finding patterns and relationships in the data and then representing those patterns in a lower-dimensional space to reduce the complexity of the data. References: “Visualizing High-Dimensional Data Using t-SNE” van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008) “t-Distributed Stochastic Neighbor Embedding” van der Maaten, L.J.P. uMap uMap is a new technique for dimensionality reduction which works almost the same as tSNE. However, it has some added advantages over tSNE. To understand and learn more about uMap, please refer to the following resources.  References: McInnes, Leland, John Healy, and James Melville. "Umap: Uniform manifold approximation and projection for dimension reduction." arXiv preprint arXiv:1802.03426 (2018). Curse of Dimensionality The Curse of Dimensionality arises when applying machine learning algorithms to highly-dimensional data. In machine learning we face unique problems when analysing and organising data in high-dimensional spaces. When the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This is really problematic since there isn’t enough data locally. Figure. 1D, 2D, 3D space data Consider the figure above, first we observed some points in 1D data in 4{"version":"1.1","math":"\(4\)"} regions (20{"version":"1.1","math":"\(20\)"} divided by 5{"version":"1.1","math":"\(5\)"}). Then these points are transferred into 2D space, into a 16{"version":"1.1","math":"\(16\)"} region space. In the next step the points are in a 3D space with 64{"version":"1.1","math":"\(64\)"} regions. What would happen when we get to 100{"version":"1.1","math":"\(100\)"}  dimensions? At its core, the curse of dimensionality, dictates that as the number of dimensions increases, the number of regions grows exponentially. As the number of regions grows and space increases each data point has more and more room. That makes our data sparse and somehow not useful anymore. For example, what if we would like to check on a neighbour of a data point? Clearly, there wouldn’t be neighbours nearby! Think about describing any human being. If we use number of arms or legs it includes many other animals. So we need to add another dimension: an upright body posture will differentiate us from many animals. Must breath air will exclude a few. What if we want to differentiate one human from another human? Are these features enough? No! We have to add many more dimensions such as height, weight, skin colour, hair colour, hair type and many more. Now imagine we want to identify each individual on earth. How many descriptors would need to be added? Can we handle such complexity and the computational load of enough dimensions to define a human being? This example shows why we need to add more dimensions to describe any object, how quickly the dimensionality increases and how complex the system grows. Also some intuitions drawn from low dimensional spaces fail badly in high dimensions. Example In high dimensions, most of the volume of the unit sphere is very close to its surface. Figure. Unit sphere Now, consider a sphere with radius r=1{"version":"1.1","math":"\(r=1\)"} in d−{"version":"1.1","math":"\(d-\)"}dimensional space. Let’s compute the fraction of the volume that is betweenr=1−ϵ{"version":"1.1","math":"\(r=1-\epsilon\)"} and r=1{"version":"1.1","math":"\(r=1\)"}.   The volume of a sphere in d−{"version":"1.1","math":"\(d-\)"}dimension is given by vd(r)=kdrd{"version":"1.1","math":"\(v_d(r) = k_dr^d\)"}. So the desired fraction is:  vd(1)−vd(1−ϵ)vd(1)=1−(1−ϵ)d{"version":"1.1","math":"\frac{v_d(1) - v_d(1-\epsilon)}{v_d(1)} = 1-(1-\epsilon)^d"} Now let’s extend our dimensions into higher numbers. You can see from the following figure, the exponential growth of the fraction of volume based on ϵ{"version":"1.1","math":"\(\epsilon\)"}, which is a fairly small constant. Figure. Simple illustration of curse of dimensionality. This shows that the volume of the hypersphere tends towards zero and fraction of volume vd(1)−vd(1−ϵ)vd(1){"version":"1.1","math":"\(\frac{v_d(1) - v_d(1-\epsilon)}{v_d(1)}\)"} tends to grow to 100%{"version":"1.1","math":"\(100 \%\)"} of the volume as the dimensionality tends to infinity, whereas the volume of the surrounding hypercube remains constant. This surprising and rather counter-intuitive observation partially explains the problems associated with the curse of dimensionality in classification: In high dimensional spaces, most of the training data resides in the corners of the hypercube defining the feature space. Also it is obvious that the curse of dimensionality result in less distinctive distances in in high dimensions. So given a point in high dimensions, the relative distance between points far from it and close from it, becomes negligible. The following figure illustrates this point. Figure. Euclidean distance was used to generate this graph, similar results hold for other distances in terms of distinctiveness. There is an analysis regarding this problem; it has been called the concentration effect. Concentration effect Let us assume the ratio of the variance of the length of any point vector (denoted by ∥∥Xd∥∥{"version":"1.1","math":"\(\|\|X_d\|\|\)"} with the length of the mean point vector (denoted by E[∥Xd∥]{"version":"1.1","math":"\(E[\|X_d\|]\)"}) converges to zero with increasing data dimensionality. The proportional difference between the farthest-point distance Dmax{"version":"1.1","math":"\(D_{max}\)"} and the closest-point distance Dmin{"version":"1.1","math":"\(D_{min}\)"} (the relative contrast) vanishes: if  limd→∞var(||Xd||E[||Xd||])=0{"version":"1.1","math":"if\ \ lim_{d\rightarrow \infty} var(\frac{||X_d||}{E[||X_d||]}) = 0"} Then we can say: Dmax−DminDmin→0{"version":"1.1","math":"\frac{D_{max} - D_{min}}{D_{min}} \rightarrow 0"} So it reduces the utility of the measure to discriminate between near and far neighbours. Relative contrast between near and far neighbours diminishes as the dimensionality increases. This is known as the concentration effect of the distance measure. This problem can imply that: Clustering or KNN algorithms may be meaningless in high dimensions. However, there might still be patterns in high dimensions. We just need better distance metrics. So Research is needed! Until we develop better distance metrics, we should aim to reduce the dimensionality where possible. Activity This is an additional video, hosted on YouTube. Eigenvalues and Eigenvectors Eigenvalues and eigenvectors are prominently used in the analysis of linear transformations. For a given square matrix A{"version":"1.1","math":"\(A\)"} if a number λ{"version":"1.1","math":"\(\lambda\)"} and a vector u{"version":"1.1","math":"\(u\)"} satisfy the condition Au=λu{"version":"1.1","math":"\(Au = \lambda u\)"}  then λ{"version":"1.1","math":"\(\lambda\)"} is called an eigenvalue and u{"version":"1.1","math":"\(u\)"} is the corresponding eigenvector of  A{"version":"1.1","math":"\(A\)"} . Is it always possible, at least for certain λ{"version":"1.1","math":"\(\lambda\)"} and  u{"version":"1.1","math":"\(u\)"}, to have matrix multiplication be the same as just multiplying the vector by a constant? For a matrix  A{"version":"1.1","math":"\(A\)"} of size  d×d{"version":"1.1","math":"\(d\times d\)"}, there are d{"version":"1.1","math":"\(d\)"} eigenvectors and eigenvalue pairs. It is only possible to have only k{"version":"1.1","math":"\(k\)"} (which is less than or equal to d{"version":"1.1","math":"\(d\)"}) nonzero eigenvalues for  A{"version":"1.1","math":"\(A\)"}. Also the number of nonzero eigenvalues are equal to the rank of the matrix. If U=[u1,u2,...,ud]{"version":"1.1","math":"\(U=[u_1,u_2,...,u_d]\)"} are the d{"version":"1.1","math":"\(d\)"} eigenvectors of matrix  A{"version":"1.1","math":"\(A\)"} and  λ1,...,λd{"version":"1.1","math":"\(\lambda_1,...,\lambda_d\)"} are the corresponding eigenvalues, then we have: Au1=λ1u1,  Au2=λ2u2,  ...,  Aud=λdud{"version":"1.1","math":"Au_1 = \lambda_1 u_1, \ \ Au_2 = \lambda_2 u_2,\ \ ..., \ \ Au_d = \lambda_d u_d"} This can be collectively written as: AU=U[λ100⋮⋮⋮0…λd]{"version":"1.1","math":"AU = U\begin{bmatrix} \lambda_1& 0 & 0 \\[0.3em] \vdots& \vdots & \vdots \\[0.3em] 0&\ldots & \lambda_d \\[0.3em] \end{bmatrix}"} The matrix U{"version":"1.1","math":"\(U\)"} is always orthogonal which means  uiTuj=0{"version":"1.1","math":"\(u_i^T u_j = 0\)"} if i≠j{"version":"1.1","math":"\(i\neq j\)"} and 1{"version":"1.1","math":"\(1\)"} otherwise. It is obvious that  UT=U−1{"version":"1.1","math":"\(U^T = U^{-1}\)"}, therefore we have: A=UDUT{"version":"1.1","math":"A = UDU^T"} We also call this Eigenvalue Decomposition of matrix A{"version":"1.1","math":"\(A\)"}. The matrix U{"version":"1.1","math":"\(U\)"} is called a full eigenvector matrix. The matrix U{"version":"1.1","math":"\(U\)"} is always an orthogonal matrix, that rotates the coordinates in a way to de-correlate the data dimensions. When applied as a linear transformation to a vector, every orthogonal matrix will either rotate or reflect the vector without changing its length. Finding Eigenvalues and Eigenvectors Eigenvalues of a matrix A{"version":"1.1","math":"\(A\)"} can be found by solving the characteristic polynomial in λ{"version":"1.1","math":"\( \lambda\)"}: Au=λu{"version":"1.1","math":"Au = \lambda u"} Au−λu=0{"version":"1.1","math":"Au - \lambda u = 0"} (A−λI)u=0{"version":"1.1","math":"(A - \lambda I)u = 0"} Since we already know that we do not want u=0→{"version":"1.1","math":"\(u=\vec{0}\)"} this means that we want the second case. Knowing this will allow us to find the eigenvalues for a matrix. Therefore we will need to determine the values of λ{"version":"1.1","math":"\( \lambda\)"} for which we get, det(A−λI)=0{"version":"1.1","math":"det(A-\lambda I) = 0 "} The roots of the polynomial are the eigenvalues of the matrix A{"version":"1.1","math":"\(A\)"} . Once all the eigenvalues are obtained, a eigenvector corresponding to a particular eigenvalue can be obtained by solving: Au1=λ1u1{"version":"1.1","math":"Au_1 = \lambda_1 u_1"} Finding eigenvalues and eigenvectors: Example Let us work on an example of finding Eigenvalues and Eigenvectors. Consider: A=[225−1]{"version":"1.1","math":"A = \begin{bmatrix} 2 & 2 \\[0.3em] 5 & -1 \\[0.3em] \end{bmatrix}"} As we have explained the first step is to find  det(A−λI){"version":"1.1","math":"\(det(A-\lambda I)\)"}: det(A−λI)=det([225−1]−λ[1001]){"version":"1.1","math":"det(A - \lambda I) = det \Big(\begin{bmatrix} 2 & 2 \\[0.3em] 5 & -1 \\[0.3em] \end{bmatrix} -\lambda\begin{bmatrix} 1 & 0 \\[0.3em] 0 & 1 \\[0.3em] \end{bmatrix} \Big)"} =det([225−1]−[λ00λ])=|2−λ25−1−λ|{"version":"1.1","math":"= det \Big(\begin{bmatrix} 2 & 2 \\[0.3em] 5 & -1 \\[0.3em] \end{bmatrix} -\begin{bmatrix} \lambda & 0 \\[0.3em] 0 & \lambda \\[0.3em] \end{bmatrix} \Big) = \begin{vmatrix} 2-\lambda & 2 \\[0.3em] 5 & -1-\lambda \\[0.3em] \end{vmatrix}"} =(2−λ)(−1−λ)−10=λ2−λ−12{"version":"1.1","math":"= (2-\lambda)(-1-\lambda)-10 = \lambda^2 - \lambda - 12"} The eigenvalues of A{"version":"1.1","math":"\(A\)"} are the solutions of the quadratic equation λ2−λ−12=0{"version":"1.1","math":"\(\lambda^2 - \lambda - 12=0\)"}  namely λ1{"version":"1.1","math":"\(\lambda_1\)"} = −3 and λ2{"version":"1.1","math":"\(\lambda_2\)"} = 4. Now lets move to the next step , finding eigenvectors. We work with  λ=−3{"version":"1.1","math":"\(\lambda = -3\)"}. The equation Au=λu{"version":"1.1","math":"\(Au = \lambda u\)"} becomes Au=−3u{"version":"1.1","math":"\(Au = -3u\)"}. Assume u=[u1u2]{"version":"1.1","math":"\(u = \begin{bmatrix} u_1 \\[0.3em] u_2 \\[0.3em] \end{bmatrix}\)"} then using matrix A{"version":"1.1","math":"\(A\)"} from above, we would have: Au=[225−1][u1u2]=[2u1+2u25u1−u2]{"version":"1.1","math":"Au = \begin{bmatrix} 2 & 2 \\[0.3em] 5 & -1 \\[0.3em] \end{bmatrix} \begin{bmatrix} u_1 \\[0.3em] u_2 \\[0.3em] \end{bmatrix} = \begin{bmatrix} 2u_1+2u_2 \\[0.3em] 5u_1-u_2 \\[0.3em] \end{bmatrix}"} While −3u{"version":"1.1","math":"\(-3u\)"} is: −3u=[−3u1−3u2]{"version":"1.1","math":"\(-3u = \begin{bmatrix} -3u_1 \\[0.3em] -3u_2 \\[0.3em] \end{bmatrix}\)"}  By setting these two equal, we get: [2u1+2u25u1−u2]=[−3u1−3u2]{"version":"1.1","math":"\begin{bmatrix} 2u_1+2u_2 \\[0.3em] 5u_1-u_2 \\[0.3em] \end{bmatrix}=\begin{bmatrix} -3u_1 \\[0.3em] -3u_2 \\[0.3em] \end{bmatrix}"} which will result in:  {5u1=−2u2u1=−25u2{"version":"1.1","math":"\begin{cases} 5u_1 = -2u_2\\ u_1 = -\frac{2}{5}u_2 \end{cases}"} This means that, while there are infinitely many nonzero solutions (solution vectors) of the equation Au=−3u{"version":"1.1","math":"\(Au = -3u\)"}, they all satisfy the condition that the first entry  u1{"version":"1.1","math":"\(u1\)"} is −25{"version":"1.1","math":"\(-\frac{2}{5}\)"} times the second entry  u2{"version":"1.1","math":"\(u_2\)"}. Thus all solutions of this equation can be characterized by: [2t−5t]=t[2−5]{"version":"1.1","math":"\begin{bmatrix} 2t \\[0.3em] -5t \\[0.3em] \end{bmatrix} = t\begin{bmatrix} 2 \\[0.3em] -5 \\[0.3em] \end{bmatrix}"} where t{"version":"1.1","math":"\(t\)"} is any real number. The nonzero vectors  u{"version":"1.1","math":"\(u\)"} that satisfy Au=−3u{"version":"1.1","math":"\(Au = −3u\)"} are called eigenvectors associated with the eigenvalue  λ=−3{"version":"1.1","math":"\(\lambda = -3\)"}. One such eigenvector is: u1=[2−5]{"version":"1.1","math":"u_1 = \begin{bmatrix} 2 \\[0.3em] -5 \\[0.3em] \end{bmatrix}"} Similarly, we can find eigenvectors associated with the eigenvalue λ=4{"version":"1.1","math":"\(\lambda = 4\)"} by solving Au=4u{"version":"1.1","math":"\(Au = 4u\)"}: [2u1+2u25u1−u2]=[4u14u2]{"version":"1.1","math":"\begin{bmatrix} 2u_1+2u_2 \\[0.3em] 5u_1-u_2 \\[0.3em] \end{bmatrix}=\begin{bmatrix} 4u_1 \\[0.3em] 4u_2 \\[0.3em] \end{bmatrix}"} Like the λ=−3{"version":"1.1","math":"\(\lambda = -3\)"} , we can find the the set of eigenvectors associated with λ=4{"version":"1.1","math":"\(\lambda = 4\)"} which is:  u2=[11]{"version":"1.1","math":"u_2 = \begin{bmatrix} 1 \\[0.3em] 1 \\[0.3em] \end{bmatrix}"} Activity You can try finding eigenvectors and eigenvalues of a  3×3{"version":"1.1","math":"\(3\times 3\)"} matrix such as B=[70−3−9−23190−8]{"version":"1.1","math":"\(B = \begin{bmatrix} 7 & 0& -3\\[0.3em] -9 & -2& 3\\[0.3em] 19 & 0& -8\\[0.3em] \end{bmatrix}\)"}. Share your answer in the discussion forum. In this video we learn about a real-world example of how PCA is used. Many researches have used PCA for reducing dimensionality in face recognition problems. In this example, each image in the data set is represented by a vector of size 1024 (e.g. each image is represented by 1024 pixels). The following steps are followed in the process described in the video. Generate the covariance matrix for data Find principle eigen vectors that represent the data Calculate face image preservation of energy when K{"version":"1.1","math":"\(K\)"} principle eigenvectors are used Projecting data back after preserving only K{"version":"1.1","math":"\(K\)"} axis of variation (using only K{"version":"1.1","math":"\(K\)"} principle eigenvectors) You will also see how the final image quality is effected by different k{"version":"1.1","math":"\(k\)"} values and why selecting a smaller value for k{"version":"1.1","math":"\(k\)"} can be beneficial for a classifier. View transcript SPEAKER: In this tutorial, we're going to learn about real-world example of PCA. Many researchers have used PCA for reducing dimensionality in face recognition problems. After reduce the dimensionality of data in face images, you can use any classifier, such as KNN, to classify images of faces. These are the face images of 100 people. And each image size is 32 time 32. For each image, we would generate a vector of 32 time 32, 1,024, dimensional vector by rasterizing. So basically, we are representing each image by 1,024 pixels. We can also represent this data in form of a matrix. As you know, we have 100 images. And each of them has 1,024 dimensions. So this is the form of the matrix. As the first step of PCA, we need to find a covariance matrix. As we already know, the covariance matrix of this data would be a matrix of the size 1,024 times 1,024, which looks like this one. Now, if we find the principal eigenvectors and the corresponding eigenvalues, you can see there are only 100 non-zero eigenvalues. As you can see in here, these are the number of eigenvalues. So the eigenvalue number 1 has this value. Number 2 has this value. And the last one is getting close to 0. And the rest of eigenvalues are 0. So it means this 100 principal eigenvectors are kind of responsible for representing the whole data. And we can escape the other parts of data since they are not really informative. Now you may ask, how much information would we preserve by reducing 1,024 dimensions into only K dimensions? In image processing, we call this preservation of energy, or information on the image. As we can see here, the face image preservation of energy if only K principal eigenvectors are used is a plot like this. Let's say we are going to use 100 eigenvectors, finally. But what if we only use the top 10 of this principal eigenvectors? As we can see, if you use the 0 to 10 principal eigenvectors, we are going to have around 10% of the information regarding the images. But if we increase that, let's say, from 10 to 20 principal eigenvectors, we are going to have 20% of the information. But as we increase this more, let's say, when we are using 70 to 80 principal eigenvectors, we can gain around 80% of information. And finally, if we use all these 100 eigenvectors, we are going to have all their information. Now if we plot these eigenvectors after resizing the images, they look like-- something like this. You may be able to discern some sort of features out of this images. We can also project each of the data vector xi using the K principal eigenvectors. We can say x dash i equals to xi times the matrix U, which matrix U is the matrix of principal eigenvectors. And here, we say-- choose all the rows, but only use K first columns. It means we select all rows, but only five columns of the matrix of principal eigenvectors. So in here, we say here we want to only use five first principal eigenvectors. Then we are projecting xi to x dash i with only five principal eigenvectors. To check what happened to our original data after preserving all the K axes of variance, we can project the data back into the original dimension. So we can use x dash i and multiply that to the transpose of this U matrix we already had. Don't forget in here, the rows and columns are changed. Even though the data which has been projected back is in original 1,024 dimensions-- but this data is only used K dimensional subspaces rather than all dimensions. In the pictures in the right side, we can see the original face images, though in the left side, you can see the reconstructed images by only using five principal axes of variance. As you can see, in here, we are losing some details which are clear in these images, but they're not clear in this one. Now, if we increase the number of principal axes, you can see-- we can now-- now we are getting more details in the images of faces. The most interesting part is sometimes, we intentionally want to do this. And the reason is we don't want to get too much into the noises. Let's say you're working in a company, and one day, you don't shave before going to work. And you have some sort of beard on your face. Should this system let you in or not? Of course it should. So by training your system on some noisy unrelated things, you may lose the generality of your system. So you have to be careful. Sometimes, we need to use these sort of data rather than the original data with too much details. Activity Formulation of PCA and deriving principal components Let us say that we first project the data on a new axis, whose direction is specified by a d−{"version":"1.1","math":"\(d-\)"} dimensional vector u1{"version":"1.1","math":"\(u_1\)"}. Since we are only interested in direction of maximum variance, we assume u1{"version":"1.1","math":"\(u_1\)"} to be a unit length vector, i.e. ∥u1∥=1{"version":"1.1","math":"\(\|u_1\| = 1\)"}, or u1Tu1=1{"version":"1.1","math":"\(u_1^Tu_1 = 1\)"}. Now each data point xi{"version":"1.1","math":"\(x_i\)"} can be projected on the vectorui{"version":"1.1","math":"\(u_i\)"} to create a new co-ordinate as yi1=u1Tx¯{"version":"1.1","math":"\(y_{i1} = u_1^T\bar{x}\)"}. So the variance of the data projected on ui{"version":"1.1","math":"\(u_i\)"} is: (1n−1)∑i=1n(u1Txi−u1Tx¯)2{"version":"1.1","math":"\Big( \frac{1}{n-1}\Big) \sum_{i=1}^{n} (u_1^Tx_i - u_1^T\bar{x})^2"} As we have shown before, the mean of the new data is y¯=u1Tx¯{"version":"1.1","math":"\(\bar{y} = u_1^T\bar{x}\)"} and the variance of the projected data is: (1n−1)∑i=1n(u1Txi−u1Tx¯)2{"version":"1.1","math":"\Big( \frac{1}{n-1}\Big) \sum_{i=1}^{n} (u_1^Tx_i - u_1^T\bar{x})^2"} =u1T[(1n−1)∑i=1n(xi−x¯)(xi−x¯)T]u1=u1TCu1{"version":"1.1","math":"= u_1^T \Big[ \Big( \frac{1}{n-1}\Big) \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T \Big]u_1 = u_1^TCu_1"} Now as we know we would like to find out the direction so that the variance u1TCu1{"version":"1.1","math":"\(u_1^TCu_1\)"} is maximised. Recall that we also assume u1Tu1=1{"version":"1.1","math":"\(u_1^Tu_1 = 1\)"}.  By putting it together we want to find:  {maxx u1TCu1s.t.   u1Tu1=1{"version":"1.1","math":"\begin{cases} \max_{x} \ u_1^TCu_1\\ s.t.\ \ \ u_1^Tu_1 = 1 \end{cases}"} For solving this problem we introduce a Lagrange multiplier and change the problem into an unconstrained maximisation problem:  maxx u1TCu1+λ1(1−u1Tu1){"version":"1.1","math":"\max_{x} \ u_1^TCu_1 + \lambda_1(1-u_1^Tu_1)"} If you want to find maximums or minimums a good way to get started is to find out where the slope of the function (derivative) is equal to zero. Taking derivative w.r.t. ui{"version":"1.1","math":"\(u_i\)"} and setting it to zero we obtain: Cu1=λ1u1{"version":"1.1","math":"Cu_1 = \lambda_1u_1"} This is an eigenvalue problem, where λ1{"version":"1.1","math":"\(\lambda_1\)"} is the largest eigenvalue of C{"version":"1.1","math":"\(C\)"} and ui{"version":"1.1","math":"\(u_i\)"} is the corresponding eigenvector. ui{"version":"1.1","math":"\(u_i\)"} is known as the first principal component. Now what about u2,...,ud{"version":"1.1","math":"\(u_2,...,u_d\)"}? Next set of axes u2,...,ud{"version":"1.1","math":"\(u_2,...,u_d\)"} can be found incrementally by finding a direction that maximizes the variance and is orthogonal to all the principal axes found so far. The directions have to be orthogonal since we want them to be uncorrelated (Why uncorrelated?). Therefore, the principal axes can be collectively written using the Eigenvector matrix U=[u1,u2,...,ud]{"version":"1.1","math":"\(U = [u_1,u_2,...,u_d]\)"} n the order of decreasing eigenvalues of the covariance matrix C{"version":"1.1","math":"\(C\)"}.  The question which arises here is what if we project the data using all d{"version":"1.1","math":"\(d\)"} principal components? Well in this case we just doing de-correlation but no dimensionality reduction. However, if we project data on only top k{"version":"1.1","math":"\(k\)"} principal components such that  k≤d{"version":"1.1","math":"\(k \leq d\)"} we achieve dimensionality reduction and each new dimension is also uncorrelated of other dimensions. PCA via Eigen Value Decomposition Now we’ll see how to perform PCA with eigenvalue decomposition. It is fairly easy: Compute data covariance matrix C{"version":"1.1","math":"\(C\)"}  Perform Eigen value decomposition (EVD){"version":"1.1","math":"\((EVD)\)"} as C=UDUT{"version":"1.1","math":"\(C = UDU^T\)"} Reduced dimension data is given by: So as you can see, Y{"version":"1.1","math":"\(Y\)"} which is n×k{"version":"1.1","math":"\(n\times k\)"} matrix, is the reduced dimension data from d{"version":"1.1","math":"\(d\)"} dimensions to k{"version":"1.1","math":"\(k\)"} dimension. And we will achieve this by multiplying X{"version":"1.1","math":"\(X\)"} and U{"version":"1.1","math":"\(U\)"}: Xn×d Ud×k{"version":"1.1","math":"\(X^{n\times d} \ U^{d\times k}\)"} which will result in top k{"version":"1.1","math":"\(k\)"} Eigen vectors in the decreasing order of eigenvalues. PCA: Minimum Error Formulation We would like to analyse PCA from another perspective. This is an alternative formulation of PCA based on projection error minimization. Suppose we project our data on k{"version":"1.1","math":"\(k\)"} dimensions from d{"version":"1.1","math":"\(d\)"} dimensions. Obviously losses incurred due to losing some features in data (k<d{"version":"1.1","math":"\(k < d\)"}). But the error we have while using PCA’s best k{"version":"1.1","math":"\(k\)"}  dimensions in terms of least square error, is the minimum possible error we can have. Let us consider a set of new axes u1,...,ud{"version":"1.1","math":"\(u_1,...,u_d\)"} in such a way that they are mutually orthogonal, i.e. uiTuj=1{"version":"1.1","math":"\(u_i^Tu_j = 1\)"} if i=j{"version":"1.1","math":"\(i=j\)"} otherwise 0{"version":"1.1","math":"\(0\)"}. Now if we project a point such as xi{"version":"1.1","math":"\(x_i\)"} on u1,...,ud{"version":"1.1","math":"\(u_1,...,u_d\)"} to get new coordinates as yij=xiTuj{"version":"1.1","math":"\(y_{ij} = x_{i}^{T}u_j\)"}.  So for all d{"version":"1.1","math":"\(d\)"} dimensions we can write this as xi=∑j=1dyijuj{"version":"1.1","math":"\(x_i =\sum_{j=1}^{d} y_{ij} u_{j}\)"} or xi=∑j=1kyijuj+∑j=k+1dyijuj{"version":"1.1","math":"\(x_i = \sum_{j=1}^{k} y_{ij} u_{j} + \sum_{j=k+1}^{d} y_{ij} u_{j}\)"} as two separated terms. If we would like to minimize the mean square error due to projection in new k{"version":"1.1","math":"\(k\)"} dimension, we have: minu1,...,uk1n∑i=1n||xi−∑j=1kyijuj||2{"version":"1.1","math":"\(\min_{u_1,...,u_k} \frac{1}{n} \sum_{i=1}^{n} ||x_i - \sum_{j=1}^{k} y_{ij}u_{j}||^2\)"} which we find that once again top k{"version":"1.1","math":"\(k\)"} eigenvectors of covariance are the optimal solutions. Activity The youtube resources in the see also section below will help you explore this topic further if you would like to know more. Share any questions or responses in the Student Discussion SEE ALSO PCA: DIRECTION OF GREATEST VARIANCEPrincipal Component Analysis (PCA) reduces the dimensionality of the data by selecting directions along which our data has the largest variance. Picking the direction of greatest variance preserves the distances in the original space (far-away points in the original space are less likely to end up very close to each other) Implementation of PCA There are several alternative ways of implementing PCA. PCA for data where n<d{"version":"1.1","math":"\(n < d\)"} There are cases when the number of data points (n{"version":"1.1","math":"\(n\)"})  is less that number of dimensions d{"version":"1.1","math":"\(d\)"} i.e. n<d{"version":"1.1","math":"\(n < d\)"} Consider the following scenario: say we have 100{"version":"1.1","math":"\(100\)"} images in 64×64{"version":"1.1","math":"\(64\times64\)"} dimensions, n=100{"version":"1.1","math":"\(n = 100\)"} and d=64×64=4096{"version":"1.1","math":"\(d=64\times64 = 4096\)"}. In this case, the number of nonzero eigenvalues of data covariance matrix is less than or equal to n{"version":"1.1","math":"\(n\)"}. If we use Eigen Value Decomposition (EVD) on the covariance matrix of size d×d{"version":"1.1","math":"\(d \times d\)"}, we need to perform computations of the order of O(d3){"version":"1.1","math":"\(O(d^3)\)"}. This may be too expensive! In such cases, SVD can reduce the computations to O(n3){"version":"1.1","math":"\(O(n^3)\)"} or less. Using SVD for PCA We can use SVD to perform PCA. As you have seen before in previous section, given any n×d{"version":"1.1","math":"\(n \times d\)"} matrix Y{"version":"1.1","math":"\(Y\)"} its Singular Value Decomposition (SVD)is given as:  Y=USVT{"version":"1.1","math":"\(Y = USV^T\)"} Where: U{"version":"1.1","math":"\(U\)"} is a n×d{"version":"1.1","math":"\(n\times d\)"} orthogonal matrix (same as U{"version":"1.1","math":"\(U\)"} in previous section) S{"version":"1.1","math":"\(S\)"} is a d×d{"version":"1.1","math":"\(d\times d\)"} diagonal matrix with elements S(i,i)=σi{"version":"1.1","math":"\(S(i,i) = \sigma_i\)"}  V{"version":"1.1","math":"\(V\)"} is a  d×d{"version":"1.1","math":"\(d\times d\)"} orthogonal matrix Now if Y{"version":"1.1","math":"\(Y\)"} is mean-centred version of X{"version":"1.1","math":"\(X\)"} then the covariance of Y{"version":"1.1","math":"\(Y\)"} is: (n−1)C=YYT=US(VTV)SUT=USISUT=US2UT{"version":"1.1","math":"\((n-1)C = YY^T = US(V^TV)SU^T = USISU^T = US^2U^T\)"} Remember that VTV=I{"version":"1.1","math":"\(V^TV = I\)"}, therefore:  C=U(s2n−1)UT=UDUT{"version":"1.1","math":"\(C = U(\frac{s^2}{n-1})U^T = UDU^T\)"} What we just did is that we created a connection among SVD and EVD. Also U{"version":"1.1","math":"\(U\)"} of SVD is same as U{"version":"1.1","math":"\(U\)"} of EVD. Therefore, the singular vectors of SVD are the same as Eigenvectors of EVD and D=s2n−1{"version":"1.1","math":"\(D = \frac{s^2}{n-1}\)"} We have the relation λd=σd2n−1{"version":"1.1","math":"\(\lambda_d = \frac{\sigma_d^2}{n-1}\)"}. If you do not want to use EVD you can just use SVD and get the matrix S{"version":"1.1","math":"\(S\)"} or get the singular values and then compute the eigenvalues (λd=σd2n−1{"version":"1.1","math":"\(\lambda_d = \frac{\sigma_d^2}{n-1}\)"}). This gives the things we need to perform PCA. Remember performing PCA is nothing but multiplying U{"version":"1.1","math":"\(U\)"}  matrix by the data matrix. Consider the following figure as a real-world example. Figure. Illustration of projected data (right) and original data (left) The first major axis is the direction of largest variance direction. The second major axis is the direction of the second largest variance. If we calculate the values of these two axis in Figure 1 and call them u1{"version":"1.1","math":"\(u_1\)"} andu2{"version":"1.1","math":"\(u_2\)"} we can find the projected X{"version":"1.1","math":"\(X\)"}  data by multiplying X{"version":"1.1","math":"\(X\)"} and U{"version":"1.1","math":"\(U\)"} (right image of Figure). As you can see in the figure, the projected data lost its correlated form and looks uncorrelated. Remember in this example we did not perform any dimensionality reduction. Both projected data and the original data had 2{"version":"1.1","math":"\(2\)"} dimensions. So in this particular example, we used PCA to de-correlate the dimensions. In this example the covariance matrix of the original is: C=[0.65920.25380.25380.9864]{"version":"1.1","math":"C = \begin{bmatrix} 0.6592 & 0.2538 \\[0.3em] 0.2538 & 0.9864 \\[0.3em] \end{bmatrix}"} Which as you can see captures some relations and correlations among the dimensions. On the other hand, the covariance matrix of the projected data is: C=[1.1247000.5208]{"version":"1.1","math":"C = \begin{bmatrix} 1.1247 & 0 \\[0.3em] 0 & 0.5208 \\[0.3em] \end{bmatrix}"} Which illustrates two important points: There are no correlations among the projected data C(i,j)=0;i≠j{"version":"1.1","math":"\(C(i,j) = 0; \quad i\neq j\)"}. The first dimension or feature has a higher value which means it is more important than the second one C(1,1)>C(2,2){"version":"1.1","math":"\(C(1,1) > C(2,2)\)"}. Now if we decide to drop one of the dimensions and use dimensionality reduction by PCA, we should choose the eigenvector corresponding to the eigenvalue =1.1247{"version":"1.1","math":"\(= 1.1247\)"} and then project all data on that axis. Mean square error based on this approximation would be the sum of the remaining eigenvalues. PCA using Inbuilt Functions in Python Here we demonstrate how to use the inbuilt function PCA() in the sklearn.decomposition package. We start by reading in a data file that contains 5 dimensions or features, download this CSV, add it to your data store and rename it. As with the previous example, we normalise the data, perform PCA and measure the reconstruction error in the recovered data. Lets begin by reading the given data. Code example #1 # Read the data import pandas as pddata = pd.read_csv('data/train_wbcd.csv').dropna() The output would look like this: (98,32) Step 1: Data Normalization Our data consists of 98 data points and 32 dimensions (features). Next step is to normalise the data. We use the inbuilt function called scale() function from sklearn.preprocessing to do this. Code example #2 #normalize our data data_norm=data.copy()mu = data_norm.iloc[:,2:].mean(axis=0) # mean of each colsigma = data_norm.iloc[:,2:].std(axis=0)  # std dev of each coldata_norm.iloc[:,2:]=(data_norm.iloc[:,2:]-mu)/sigma Step 2: Implement PCA We use the inbuilt function to perform PCA. Here, n_components specify the number of principal components to use. We start by using all the principal components. Code example #3 #perform PCA using sklearn PCA implementation from sklearn.decomposition import PCApca = PCA(n_components=2)Xnorm=data_norm.iloc[:,2:].copy().valuespca.fit(Xnorm) Dimensionality Reduction: How to choose the dimensions to keep To put it differently, how can we choose the number of principal components to retain? We can decide this by looking the variance captured by each principle component. Code example #4 #The amount of variance that each PC explains #The amount of variance that each PC explainsvar= pca.explained_variance_ratio_print(var) The output would look like this: [0.50414964 0.16306662] Here, we see that the first component captures around 50.41% variance, second component captures around 16.31% variance and so on. To make it much easier, we can calculate the cumulative variance: Code example #5 #Cumulative Variance explains var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)print(var1)plt.plot(var1)plt.xlabel("Principal components")plt.ylabel("Variance captured") The output would look like this: [50.41 66.72] Text(0, 0.5, 'Variance captured') So, if k is the number of principal components, we see that k=1{"version":"1.1","math":"\(k=1\)"} captures around 50.41% variance, k=2{"version":"1.1","math":"\(k=2\)"} (the first 2{"version":"1.1","math":"\(2\)"} components together) capture around 66.72% variance and so on. Since k=2{"version":"1.1","math":"\(k=2\)"} captures more than 60% variance in our data, lets add the third component. Code example #6 pca = PCA(n_components=3)Zred = pca.fit_transform(Xnorm)print(Zred.shape) The output would look like this: (98,3) Step 3: Measuring ‘reconstruction error’ We can recreate our original data (Xrec) from the reduced data (Zred) using the inverse_transform() function, and calculate the reconstruction error as before. Code example #7 # Reconstruct our data Xrec = pca.inverse_transform(Zred) print(Xrec.shape) The output would look like this: (98, 30) Code example #8 # Measure the reconstruction error rec_error = np.linalg.norm(Xnorm-Xrec, 'fro')/np.linalg.norm(Xnorm, 'fro') print(rec_error) The output would look like this: 0.4859375839859015 Influence of Dimensionality Reduction on Reconstruction error Let us see how dropping the dimensionality of data affects the reconstruction error. We perform PCA using increasing number of principal components, and measure the reconstruction error in each case. Code example #9 # vary principal components from 1 to 5n_comp = range(1,nDims+1)print(n_comp) The output would look like this: [1 to 31] Code example #10 # Initialize vector of rec_error rec_error = np.zeros(len(n_comp)+1) for k in n_comp:     pca = PCA(n_components=k)     Zred = pca.fit_transform(Xnorm)     Xrec = pca.inverse_transform(Zred)     rec_error[k] = np.linalg.norm(Xnorm-Xrec, 'fro')/np.linalg.norm(Xnorm, 'fro')     # print("k={}, rec_error={}".format(k, rec_error[k])) rec_error = rec_error[1:] #we started recording from index 1, so drop index 0 #Visualize the change in error plt.plot(n_comp,rec_error) plt.xlabel('No of principal components (k)') plt.ylabel('Reconstruction Error') The output would look like this: k=1, rec_error=0.7041664292871995 k=2, rec_error=0.5768741133033028 k=3, rec_error=0.4859375839859015 k=4, rec_error=0.42116146026231194 k=5, rec_error=0.36045374675457575 k=6, rec_error=0.30913128269060297 k=7, rec_error=0.27125793238867757 k=8, rec_error=0.23850958714153778 k=9, rec_error=0.20993574805189139 k=10, rec_error=0.18009067168555537 k=11, rec_error=0.15895469219138864 k=12, rec_error=0.14204872660421053 k=13, rec_error=0.12491660595890769 k=14, rec_error=0.10762651261855655 k=15, rec_error=0.09293005293332164 k=16, rec_error=0.08154504058386751 k=17, rec_error=0.07229131841987832 k=18, rec_error=0.06475153142280245 k=19, rec_error=0.05717366047807598 k=20, rec_error=0.04920331546237984 k=21, rec_error=0.0414355944971561 k=22, rec_error=0.03500391231126629 k=23, rec_error=0.029208658945960503 k=24, rec_error=0.023546878335901463 k=25, rec_error=0.01776517524045024 k=26, rec_error=0.012569695037471464 k=27, rec_error=0.005900246502294497 k=28, rec_error=0.003314694313492582 k=29, rec_error=0.0014227150677471412 k=30, rec_error=9.424658158893498e-16 Text(0, 0.5, 'Reconstruction Error') Activity You are now be able to understand: How to perform PCA on any data Decide the number of principal components to keep for dimensionality reduction Calculate the reconstruction error of our data after PCA. Preliminaries The goal of PCA is to take n{"version":"1.1","math":"\(n\)"} data points in d{"version":"1.1","math":"\(d\)"} dimensions, which may be correlated, and summarises them by a new set of uncorrelated axes. The uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original d{"version":"1.1","math":"\(d\)"} dimensions. The first k{"version":"1.1","math":"\(k\)"} components capture as much of the variation (or variance) among the data points as possible. But first, lets see how to define variance across each variable. Variance across each variable Data is represented as a cloud of points in a multidimensional space with one axis for each of the variables. The centroid of the points is defined by the mean of each variable. The variance of each variable j{"version":"1.1","math":"\(j\)"}  is the average squared deviation of its n{"version":"1.1","math":"\(n\)"} values around the mean of that variable: Cjj=1n−1∑i=1n(xij−x¯j)2{"version":"1.1","math":"C_{jj} = \frac{1}{n-1}\sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2"} Covariances among variables To put it simply, covariance is a measure of how changes in one variable are associated with changes in a second variable. Degree to which the variables are linearly correlated is represented by their co-variances: Covariance Matrix The covariance matrix is a matrix that contains variances of all variables on the diagonal and co-variances among all pairs of variables in the off-diagonal entries. It can be written as: C=(1n−1)∑i=1n(xi−x¯)(xi−x¯)T{"version":"1.1","math":"C = \Big( \frac{1}{n-1}\Big)\sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T"} Let us say yi=xi−x¯{"version":"1.1","math":"\(y_i = x_i - \bar{x}\)"}, and [y1T⋮ynT]{"version":"1.1","math":"\(\begin{bmatrix} y_1^T \\[0.3em] \vdots\\[0.3em] y_n^T\\[0.3em] \end{bmatrix}\)"}, then C=(1n−1YTY){"version":"1.1","math":"\(C = \Big(\frac{1}{n-1} Y^T Y \Big)\)"} PCA: decorrelation The main objective of PCA is to rigidly rotate the axes of t−{"version":"1.1","math":"\(t-\)"}dimensional axes to a new set of axes (called principal axes) that have the following properties: Ordered such that principal axis- captures the highest variance, axis-2 captures the next highest variance, …. , and axis−d{"version":"1.1","math":"\(-d\)"}  has the lowest variance Covariance among each pair of the principal axes is zero (the principal axes are uncorrelated i.e. they are orthogonal to each other). This is called decorrelation property Activity Python Practical: independent and correlated data It’s time to try another practical Python demonstration. Open your Jupyter Notebook via the Anaconda Navigator or the command line. Copy the example code into your Jupyter Notebook and Run it after copying each section of code. Try to understand what each part of the code is doing as you go. In this practical, we will learn how to generate data from a specified distribution in one and more dimensions. For demonstration purposes, we will use a normal distribution with a given mean and standard deviation. When generating data in more than one dimension, we will consider two scenarios: generate data from each dimension independently, generate data from different dimensions such that dimensions of data are correlated up to a specified value. Let’s start with a single dimension (1D) space. 1D Case The following code is written to generate the samples from a one dimensional normal distribution with mean zero (0) and standard deviation one (1). The data x{"version":"1.1","math":"\(x\)"} follows a normal distribution specified by density function: f(x)=1(2π)exp(−x22){"version":"1.1","math":"\(f(x)=\frac{1}{\sqrt(2\pi)}\text{exp}\left(-\frac{x^2}{2}\right)\)"}  In this example, we: Generate samples from a 1D Gaussian distribution with mean zero and standard deviation one From the generated samples, we empirically compute the mean, standard deviation; also plot the histogram Store the data to a file “normalData1.csv” writing each sample in a separate line. Lets start by initialising the necessary variables. Note: When working with random numbers (i.e. using np.random function), the numbers generated by Python (or any other language) are pseudo-random (the values are computed using a deterministic algorithm and probability plays no real role). For debugging purposes, to get a repeatable sequence of random numbers, we have to set the seed of random number generation to a specific number. Code example #1 # import the necessary modules here  # to save us from writing some very complex maths code from matplotlib import pyplot as plt import numpy as np import pandas as pd # For repeatability of results we'll use the same seed #  to produce a (fairly) random number np.random.seed(1) # define the parameters of  the Gaussian probability distribution to try # a lower value of 0 and upper value of 1 gives the standard normal curve mu = 0.0 sigma = 1.0 numDims = 1   # variable that holds the number of dimensions numSamples = 1000    # variable that holds the number of samples We now generate a matrix X = [numSamples x numDims] of random numbers taken from a standard Gaussian distribution. Then, print the mean and sample deviation of each dimension. # generate the samples from the Gaussian distribution X = np.random.normal(mu,sigma,[numSamples,numDims]) # compute the mean and standard deviation of the generated samples empiricalMean   = np.mean(X) empiricalStdDev = np.std(X) print("empirical mean    = ", empiricalMean) print("empirical std dev = ", empiricalStdDev) The output would look like this: empirical mean    =  0.03881247615960185 empirical std dev =  0.9810041339322116 Now lets plot the histogram of our data. It should look like a standard normal distribution reflecting the standard lower value of mu = 0 and an upper value of sigma = 1. Code example #2 # plot the histogram of the data (easy to see in 1 dimension) numBins = 30 freq, bins, ignored = plt.hist(X, numBins, density=True) #print bins plt.bar(bins[:numBins], freq) plt.show() print(len(freq)) The output would look like this: 30 As a last step, lets save the data we created to a csv file. Code example #3 # store the data in a csv file my_dataframe = pd.DataFrame(X) #converting numpy to dataframe my_dataframe.to_csv("normalData1.csv") # check that the file has been created in the local folder # end of task-1 2D case: uncorrelated data Use a new cell in Jupyter for this code. In this example, we generate the samples from a two dimensional normal distribution with mean [0,0]T{"version":"1.1","math":"\(\left[0,0\right]^T\)"} and covariance matrix I2×2{"version":"1.1","math":"\(\textbf{I}_{2\times2}\)"}.  The data x=[x1,x2]{"version":"1.1","math":"\(\textbf{x}=[x_1,x_2]\)"} follows a normal distribution specified by density function: f(x)=12πexp(−xTx2){"version":"1.1","math":"\(f(\textbf{x})=\frac{1}{2\pi}\text{exp}\left(-\frac{\textbf{x}^T\textbf{x}}{2}\right)\)"}. In the following code we: generate samples from a 2D Gaussian distribution with zero mean vector and identity covariance. empirically compute the mean and covariance; then plot the data. store the data to a file “normalData2.csv” writing each 2D sample in a separate line. Code example #4 Lets start by initialising the necessary variables. # For repeatability of results np.random.seed(1) # define the parameters of a Gaussian distribution numDims = 2 mu = np.tile(0.0,numDims) covmat = np.identity(numDims) numSamples = 10000 We can generate a (2D in this case) multivariate Gaussianas as follows: # generate samples from 2D normal distribution X = np.random.multivariate_normal(mu,covmat,numSamples) print("matrix size =" , np.shape(X)) The output would look like this: matrix size = (10000, 2) # visualize the data fig, ax = plt.subplots() ax.plot(X[:,0],X[:,1], '.') ax.set_xlabel('Dim 1') ax.set_ylabel('Dim 2') ax.set_title('Scatterplot of the two dimensions') ax.axis('equal') The output would look like this: (-4.04766798811529, 4.559345566815589, -3.8253155037953013, 4.400761642087505) Code example #5 We can now verify the mean and standard deviation for each of the dimensions we created: # compute the mean and standard deviation of the generated samples empirical_Mean = X.mean(0) empirical_CovMat = np.cov(X.T)# X.T is the transpose of X as cov function requires data on columns print("Empirical mean       = ", empirical_Mean) print("Empirical Covariance = ", empirical_CovMat) The output would look like this: Empirical mean       =  [0.01764318 0.00156489] Empirical Covariance =  [[ 1.01075632 -0.01609859]  [-0.01609859  0.9892676 ]] Code example #6 Finally, lets plot a 2D histogram for our generated data. Does it relate with the scatter plot above? # plot the histogram of the data numBins = 40 plt.hist2d(X[:,0],X[:,1], bins=numBins) plt.colorbar() plt.show() #store the data in a csv file my_dataframe = pd.DataFrame(X) #converting numpy to dataframe my_dataframe.to_csv("normalData2.csv") # end of task-2 The output would look like this: 2D case: correlated data Generate samples from a 2D Gaussian distribution with zero mean vector and arbitrary covariance. empirically compute the mean and covariance Plot the data. store the data to a file “normalData3.csv” writing each 2D sample in a separate line. Code example #7 The following code is written to generate the samples from a two dimensional normal distribution with mean [0,0]T{"version":"1.1","math":"\(\left[0,0\right]^T\)"} and covariance matrix Σ=[1ρρ1]{"version":"1.1","math":"\(\Sigma=\left[\begin{array}{cc}1 & \rho\\\rho & 1\end{array}\right]\)"}.  The data x=[x1,x2]{"version":"1.1","math":"\(\textbf{x}=[x_1,x_2]\)"} follows a normal distribution specified by density function:  f(x)=1(2π)D/2|Σ|1/2exp(−xTΣ−1x2){"version":"1.1","math":"\(f(\textbf{x})=\frac{1}{\left(2\pi\right)^{D/2}|\Sigma|^{1/2}}\text{exp}\left(-\frac{\textbf{x}^{T}\Sigma^{-1}\textbf{x}}{2}\right)\)"} withD=2{"version":"1.1","math":"\(D=2\)"}. Again, we start by initializing the necessary variables: Code example #8 # import the necessary modules here from matplotlib import pyplot as plt import numpy as np import pandas as pd# For repeatability of results np.random.seed(1) # define the parameters of Gaussian numDims = 2 mu = np.tile(0.0,numDims) rho = 0.7 covmat = [[1,rho],[rho,1]] print(covmat) numSamples = 10000 The output would look like this: [[1, 0.7], [0.7, 1]] Code example #9 We can generate samples from a 2D normal distribution using the code: # generate samples from 2D normal distribution X = np.random.multivariate_normal(mu,covmat,numSamples) print("matrix size =" , np.shape(X)) The output would look like this: matrix size = (10000, 2) Code example #10 Lets plot the data for visualization purposes. # visualize the data plt.plot(X[:,0],X[:,1], '.') plt.axis('equal'); plt.show() The output would look like this: Code example #11 Now, lets calculate the empirical mean and standard deviation # compute the mean and standard deviation of the generated samples empirical_Mean = X.mean(0) empirical_CovMat = np.cov(X.T)# X.T is the transpose of X as cov function requires data on columns print("empirical mean = ", empirical_Mean) print("empirical Covariance = ", empirical_CovMat) The output would look like this: empirical mean =  [-0.01687229 -0.01566013] empirical Covariance =  [[0.99603632 0.71075273]  [0.71075273 1.01902971]] Code example #12 Lets plot the histogram for the data. How will it be related to the scatterplot? # plot the histogram of the data numBins = 40 plt.hist2d(X[:,0],X[:,1], bins=numBins) plt.colorbar() plt.show() #store the data in a csv file my_dataframe = pd.DataFrame(X) #converting numpy to dataframe my_dataframe.to_csv("normalData3.csv") Setup The main objective of PCA is orthogonal transformation of given data into its principal components. Here, principle component = axis of maximum variation. You can get a math free introduction and introdution to PCA from this blog. However, you should also try to read and understand this detailed and clear mathematical treatment of PCA Before we start anything, let’s set-up our Python environment. We can import our packages and then, add them to our python environment. Code example #1 We do that with the following Python code: # import the necessary modules here from matplotlib import pyplot as plt import numpy as np import pandas as pd from sklearn import metrics In subsequent practicals, we will apply Principle Component analysis (PCA) algorithm for two purposes: data decorrelation dimensionality reduction (summarisation). We will also perform PCA in different ways: Example 1: We will perform PCA using linear algebra techniques learnt in class on a 2D dataset. Example 2: We will look at the inbuilt functions in python to perform PCA. In the next two (2) lessons (also called steps) we will see these two different ways of implementing PCA using Python. Activity Singular value decomposition Singular value decomposition (SVD) is a method of decomposing a matrix into three other matrices: X=USVT{"version":"1.1","math":"X = USV^T"} Where: X{"version":"1.1","math":"\(X\)"} is a n×d{"version":"1.1","math":"\(n\times d\)"} matrix U{"version":"1.1","math":"\(U\)"} is a n×d{"version":"1.1","math":"\(n\times d\)"}orthogonal matrix (same as U{"version":"1.1","math":"\(U\)"} in previous section) S{"version":"1.1","math":"\(S\)"} is a d×d{"version":"1.1","math":"\(d\times d\)"} diagonal matrix with elements S(i,i)=σi{"version":"1.1","math":"\(S(i,i) = \sigma_i\)"} V{"version":"1.1","math":"\(V\)"} is a d×d{"version":"1.1","math":"\(d\times d\)"} orthogonal matrix In linear algebra, the SVD is a factorization of a real or complex matrix. The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. The diagonal elements of S{"version":"1.1","math":"\(S\)"}, σi{"version":"1.1","math":"\(\sigma_i\)"}'s are called singular values of the matrix X{"version":"1.1","math":"\(X\)"}. The number of nonzero singular values is less than or equal to min(n,d){"version":"1.1","math":"\(min(n,d)\)"} and is also equal to the rank of matrix X{"version":"1.1","math":"\(X\)"}. Calculating the SVD consists of finding the eigenvalues and eigenvectors of XXT{"version":"1.1","math":"\(XX^T\)"} and XTX{"version":"1.1","math":"\(X^TX\)"}. The eigenvectors of XTX{"version":"1.1","math":"\(X^TX\)"} make up the columns of V{"version":"1.1","math":"\(V\)"}, the eigenvectors of XXT{"version":"1.1","math":"\(XX^T\)"} make up the columns of U{"version":"1.1","math":"\(U\)"}. Also, the singular values in S{"version":"1.1","math":"\(S\)"} are square roots of eigenvalues from XXT{"version":"1.1","math":"\(XX^T\) "} or XTX{"version":"1.1","math":"\(X^TX\)"}. The singular values are the diagonal entries of the S{"version":"1.1","math":"\(S\)"} matrix and are arranged in descending order. The singular values are always real numbers. If the matrix A{"version":"1.1","math":"\(A\)"} is a real matrix, then U{"version":"1.1","math":"\(U\)"} and V{"version":"1.1","math":"\(V\)"} are also real. Activity Solving the Curse  In some problems, there are too many variables. But, are all variables important? If not, then some of them are irrelevant for our purpose and can be removed. If all variables are numeric, what if they are correlated? If they are exactly the same, this means redundancy! Can we group them together? The ‌Curse of Dimensionality calls for Dimensionality Reduction. Dimensionality reduction refers to the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely. Let’s look at an example of dimensionality reduction. Example 1 As you can see in the following figure, we have a 2D data. Figure. Representation of a sample data in 2D We can take a subset of this data such as Z{"version":"1.1","math":"\(Z\)"} which looks like this:  X=[1.191.191.231.232.432.43]{"version":"1.1","math":"X = \begin{bmatrix} 1.19 & 1.19\\[0.3em] 1.23 & 1.23\\[0.3em] 2.43 & 2.43 \end{bmatrix}"} At first glance, the first thing comes to mind is the first and second features of these data points are the same. So why not just to use only one of these features? Well, this is the whole concept of dimensionality reduction. As you saw in the above figure, we can transform this data points on the red arrow as the only dimension. For this we define a projection vector such as: [0.50.5]{"version":"1.1","math":"\(\begin{bmatrix} 0.5 \\[0.3em] 0.5 \\[0.3em] \end{bmatrix}\)"} then by multiplying data points and projection vector we will have: [1.19,1.19][0.50.5]=1.19{"version":"1.1","math":"\begin{bmatrix} 1.19, 1.19 \\[0.3em] \end{bmatrix} \begin{bmatrix} 0.5 \\[0.3em] 0.5 \\[0.3em] \end{bmatrix} = 1.19"} [1.23,1.23][0.50.5]=1.23{"version":"1.1","math":"\begin{bmatrix} 1.23, 1.23 \\[0.3em] \end{bmatrix} \begin{bmatrix} 0.5 \\[0.3em] 0.5 \\[0.3em] \end{bmatrix} = 1.23"} [2.43,2.43][0.50.5]=2.43{"version":"1.1","math":"\begin{bmatrix} 2.43, 2.43 \\[0.3em] \end{bmatrix} \begin{bmatrix} 0.5 \\[0.3em] 0.5 \\[0.3em] \end{bmatrix} = 2.43"} ⇒X′=[1.191.232.43]{"version":"1.1","math":"\Rightarrow X^{\prime} = \begin{bmatrix} 1.19\\ 1.23 \\ 2.43 \\ \end{bmatrix}"} So X′=[1.191.232.43]{"version":"1.1","math":"\(X^{\prime} = \begin{bmatrix} 1.19\\ 1.23 \\ 2.43 \\ \end{bmatrix}\)"}  is the projected data into a single dimension (the red arrow in the figure). As a result we reduced one dimension of this 2D data. If you notice the formation of the data, you can see that [0.50.5]{"version":"1.1","math":"\(\begin{bmatrix} 0.5 \\[0.3em] 0.5 \\[0.3em] \end{bmatrix}\)"} is also the direction of maximum variance in data!. We will later use this fact in other methods. As opposed to the previous example, what if the data is not exactly on the red arrow? Is the direction of maximum data variance the same? Example 2 The following figure is an example where data points lie on a noisy line. Figure. Example of data points lie on a noisy line. As you can see in the figure, the u1{"version":"1.1","math":"\(u_1\)"} dimension vector, points towards the direction of the highest variance and theu2{"version":"1.1","math":"\(u_2\)"} dimension vector, points towards the highest variance in the subspace, orthogonal to the u1{"version":"1.1","math":"\(u_1\)"} vector. Thus, projecting onto maximum variance direction (u1{"version":"1.1","math":"\(u_1\)"}) in the figure above) means capturing more variance and results in capturing more information to analyse. Example 3 There are also some examples in which the points lie on noisy curves and shapes (see the following figure). Figure. Example of data points lie on a noisy curve. In this unit, we will confine ourselves to linear dimensionality reduction problems only. There are also machine learning methods that can deal with nonlinear problems as well (such as kernel principal component analysis). Activity Curse of Dimensionality import matplotlib.pyplot as plt import seaborn as sns def euclidean_distance(p1, p2):     p1, p2 = np.array(p1), np.array(p2) #Ensure p1/p2 are NumPy Arrays     return np.sqrt(np.sum(np.square(p2-p1))) %matplotlib inline sns.set_style('darkgrid') avg_distances = [] for n in range(2, 100):     avg_distances.append(np.mean([euclidean_distance(np.random.randint(low=-100, high=100, size=n), [0 for i in range(n)]) for p in range(500)])) plt.figure(figsize=(10,10)) plt.plot(range(2,100), avg_distances,'bs-') plt.plot( np.diff(avg_distances),'ro-') plt.xlabel('Number of dimensions') plt.ylabel('Euclidean Distance') plt.show() Curse of dimensionality: Using PCA to Remove Correlation in Data As a first step, let's try to apply PCA in removing correlations from simple 2D data. This data has one direction of large variation and one of smaller variation. Let’s assume our data is stored in the file data.csv. Let's read this and plot it. Code example #1 We do that with the following Python code: data = pd.read_csv('data/data.csv', delimiter=",", header=None).values print(data.shape) The output would look like this: (50, 2) Code example #2 Let’s visualise this data. plt.plot(data[:,0],data[:,1], '.', markersize=14) plt.axis('equal'); plt.title('Original Data') plt.show() The output would look like this: We can see that two columns in data are correlated. Our goal is to remove this correlation by projecting (representing) this data onto a new set of axis (principal components). We now proceed with Implementing PCA using the following steps: normalise the data compute the covariance matrix of data compute the eigenvectors (U) and eigenvalues (S) of the covariance matrix. Step 1: Normalising the data But first, before doing PCA, we have to normalise the data. For this we subtract mean value of each feature from the dataset, and scaling each dimension so that they are in the same range. Code example #3 mu = data.mean(axis=0) # mean of each col sigma = data.std(axis=0)  # std dev of each col Xnorm = (data - mu)/sigma print (Xnorm[0:5,:]) The output would look like this:     [[-0.52327626 -1.59279926]      [ 0.46383434  0.84036357]      [-1.14836881 -0.58315168]      [-1.05407533 -1.27072671]      [-0.98397954 -0.81658765]] Step 2: Calculate the covariance matrix of normalised data If m{"version":"1.1","math":"\(m\)"} is the number of training data, calculate the covariance matrix as: ∑=1mXnormTXnorm{"version":"1.1","math":"\(\sum=\frac{1}{m}Xnorm^{T}Xnorm\)"} Code example #4 # Covariance matrix of normalized data m = len(Xnorm) covmat = np.dot(Xnorm.T, Xnorm)/m  print(covmat) The output would look like this:     [[1.        0.7355261]      [0.7355261 1.       ]] Step 3: Calculate the eigenvectors and eigenvalues of the covariance matrix Now, compute the eigenvalues (S{"version":"1.1","math":"\(S\)"}) and eigenvectors (U{"version":"1.1","math":"\(U\)"}) of this covariance matrix. The eigenvectors (U{"version":"1.1","math":"\(U\)"})  become the principal components. We use linalg.eig() function from numpy: to compute the eigenvalues and eigenvectors of a square array. Code example #5 S,U = np.linalg.eig(covmat) print('Eigen values: {}'.format(S)) print('Eigen vectors:') print(U) The output would look like this:     Eigen values: [1.7355261 0.2644739]     Eigen vectors:     [[ 0.70710678 -0.70710678]      [ 0.70710678  0.70710678]] Here, the first column represents the first eigen-vector U1, and the second column represents U2. These are the principal components. Notice that eigenvalues S1 and S2 are arranged in decreasing order: S1 > S2. Hence U1 is the direction that captures maximum variation in our given data. U2 is the next direction of variation. What does PCA offer? So now we found out the principal components (U{"version":"1.1","math":"\(U\)"}) the set of axis that capture the maximum variation in data. What can we do this this now? We can do the following: 1. Decorrelation: Project our data onto U{"version":"1.1","math":"\(U\)"} to get decorrelated data 2. Dimensionality Reduction: Reduce U{"version":"1.1","math":"\(U\)"} to contain only those axis that contain maximum information. Project our data onto this reduced  to get new data with reduced dimensionality. Decorrelation Code example #6 # Z contains uncorrelated data   Z = np.dot(Xnorm,U) Lets visualize the data before and after PCA. # 2 plots in one row fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,5))  # added size of each figs (width, height) fig.subplots_adjust(wspace=0.2) # leave some space between figs # plot for original data  axs[0].scatter(data[:,0], data[:,1]) axs[0].set_title("Original Data") # plot for uncorrelated data after PCA axs[1].scatter(Z[:,0], Z[:,1]) axs[1].set_title("Data after PCA") The output would look like this:     Text(0.5,1,'Data after PCA') Dimensionality Reduction To reduce the dimensionality of our 2D data to 1D, we remove the principle component that captures the least variation. Our principle components, which are the eigen vectors of the covariance matrix are: U[:,0] and U[:,1]. By projecting our data Xnorm onto just U[:,0], we get a reduced Z in 1D. In general, we decide to keep k{"version":"1.1","math":"\(k\)"} eigenvectors in U{"version":"1.1","math":"\(U\)"} that captures maximum variation. Then our reduced data Znorm becomes: XnormM×M×UreducedM×k=ZM×k{"version":"1.1","math":"\(\text{Xnorm}_{M\times M} \times \text{Ureduced}_{M\times k}=\text{Z}_{M\times k}\)"} In this case, k=1{"version":"1.1","math":"\(k=1\)"}. Code example #7 k = 1 # number of principal components to retain Ured =  U[:,0:k] # choose the first k principal components #project our data Xnorm onto Ured Zred = np.dot(Xnorm,Ured)  print(Zred.shape) print(Ured.shape) The output would look like this:     (50, 1)     (2, 1) Code example #8 #recover our Xnorm data from Zred Xrec = np.dot(Zred, Ured.T) print(Xrec.shape) #Visualize the recovered data fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,5))  # added size of each figs (width, height) fig.subplots_adjust(wspace=0.2) # leave some space between figs # plot for Xnorm  axs[0].scatter(Xnorm[:,0], Xnorm[:,1]) axs[0].set_title("Normalised Original Data") # plot for Xrec axs[1].scatter(Xrec[:,0], Xrec[:,1]) axs[1].set_title("Recovered data after dimensionality reduction") The output would look like this:     (50, 2)     Text(0.5,1,'Recovered data after dimensionality reduction') Measuring ‘reconstruction error’ How much information did we lose after dimensionality reduction? To measure this, we calculate the reconstruction error of our data. Reconstruction error, is calculated as the square root of sum of squared errors of each data point. Essentially, this becomes the distance between the original data point and the reconstructed data point. For a better visualisation consider the figure below. The blue dots are the original data points, and the red dots are the reconstructed data points after dimensionality reduction using PCA. The dotted lines shows the distance between each original and reconstructed data point. So reconstruction error becomes the sum of these distances. > Figure. title Math formula In mathematical terms, given our data Xnorm (MxN matrix) and reconstructed data Xrec (MxN matrix), where M=50{"version":"1.1","math":"\(M=50\)"} data points and N=2{"version":"1.1","math":"\(N = 2\)"} dimensions, the reconstruction error is calculated as:   Reconstruction Error=∑i=1M∑j=1N(Xnormij−Xrecij)2{"version":"1.1","math":"\(\text{Reconstruction Error}=\sum_{i=1}^{M}\sum_{j=1}^{N}(\text{Xnorm}_{ij}-\text{Xrec}_{ij})^{2}\)"} Frobenius Norm To make this error term between  0{"version":"1.1","math":"\(0\)"} and 1{"version":"1.1","math":"\(1\)"},  we divide it by the Frobenius norm of the original data Xnorm. Frobenius norm of a matrix is defined as the square root of the sum of the absolute squares of its elements. You can get a formal definition or watch a video illustrating a simple example if you need more information. In python, frobenius norm is implemented in linear algebra package of numpy. You can call it using linalg.norm(, 'fro') Code example #9 rec_err = np.linalg.norm(Xnorm-Xrec, 'fro')/np.linalg.norm(Xnorm, 'fro') print("The reconstruction error is: {}".format(rec_err)) The output would look like this:     The reconstruction error is: 0.36364398984 In this video we practise some of the Python code that was introduced in the previous step. You will see the following code examples from the previous lesson being used: 2D uncorrelated data (Code example #4) 2D correlated data (Code example #8) View transcript SPEAKER: In this tutorial, we will show you how to visualise correlated and uncorrelated data. A set of two or more random variables is called uncorrelated if each pair of them are uncorrelated. If two variables are uncorrelated, there is no linear relationship between them. Now back into the code. At first, we're going to define a number of dimensions of the data we're going to generate. So I have defined a number of dimensions in here as 2, and the variable is numDims. Then we are going to define a mu and variance or covariance for our data. So the mu in here is np.tile 0 and number of dimensions. np.tile simply repeats the numbers of elements in an array. So in here it's going to repeat 0 two times. Also, the covariance matrix is defined as np.identity, with the input of number dimensions. So this is going to create a two-dimensional identity matrix, which the diagonal values are 1, and the non-diagonal values are 0. So basically, when non-directional values are 0, it indicates there should not be any correlation among the dimensions. Then we are going to define the number of samples as 5,000 times number of dimensions. Then we are going to generate the samples out of a 2-D dimensional normal distribution. So x equals np.random, which creates random variables, dot multivariate_normal, which that these samples are from 2-D normal distribution. This function will have three inputs-- mu, covariance matrix, and number of samples. So remember, if you want to generate random numbers based on 2-D normal distribution, or normal distribution, you should have these three inputs-- the mu, and the covariance. Also, the number of samples. Then we are going to create a figure and a set of subplots. Also, we're going to plot x-- the data points we created-- the random data points. We are going to set some labels as dimension 1 and dimension 2 for this figure we created. Also, we're going to set some titles for our plot. Then we are going to set equal scalings-- equal scaling by changing limits-- for the plotting of the data. Now if we're on this code, we are going to see these data points. So this is the first dimension, and this is the second dimension. And as you can, see there is no correlation among these data points. Because we can't interfere by choosing the values of one dimension, the other one is going to change. Now what if we would like to generate correlated data, and not uncorrelated data. So as you may guess we should change this covariance matrix, because right now the covariance matrix, which is responsible for indicating an amount of correlation between these dimensions, has only 0 values in non-diagonal elements. So I'm going to define a value such as row. 