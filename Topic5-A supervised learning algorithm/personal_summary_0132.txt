1.1  Dimensionality in Data For example image data text data genomic data。  1.2  Curse of Dimensionality As  dimensionality  increases,  the  available  data  becomes  sparse  due  to  the  increase  in  spatial volume.  And  some  of  the  conclusions  drawn  in  low-dimensional  spaces  do  not  apply  to  high- dimensional spaces. Relative contrast between near and far neighbors diminishes as the dimensionality increases. There is a lack of better high-dimensional distance metrics, so dimensionality reduction is one way to solve the problem.  1.3  Solving the curse Converting a set of data with huge dimensions into data with fewer dimensions. Kernel principal component analysis can deal with nonlinear problems。  1.4  Eigenvalues and Eigenvectors Eigenvalues and eigenvectors are prominently used in the analysis of linear transformations.  1.5  Singular value decomposition A method of decomposing a matrix into three other matrices  1.6  Preliminaries (PCA) Summarize potentially relevant data points at the dimensional level by a new set of uncorrelated axes. The main objective is to obtain the highest variance for the principal axis and to obtain a covariance of 0 between each pair of principal axes.  1.7  The mathematical principles of PCA The mathematical derivation process of PCA. Implementation of PCA。  1.8  Implementation of PCA Clustering  can  be  evaluated  by  comparing  cluster  performance  with  known  clusters  (external assessment)  and  determining  whether  clusters  follow  certain  intrinsic  assumptions  (internal assessment).  1.9  Example of using PCA in facial image analysis How to use PCA for face image analysis step by step  1.10 ICA , t-SNE and uMap ICA is a method for separating multivariate signals into independent, non-Gaussian components. The goal is to find a set of basis functions that m describe the underlying source of the signal and then use these functions to separate the signal into its components. t-SNE  uses  more  sophisticated  mathematical  techniques  to  identify  and  capture  the  underlying structure of the data. The principle is to look for patterns and relationships in the data and then represent these patterns in a low-dimensional space to reduce the complexity of the data. The principle of uMap is essentially the same as that of t-SNE. When dealing with large data sets, uMap has a clear advantage in terms of block running speed and small memory footprint.  1.11   Implementation of PCA and t-SNE algorithms using python  