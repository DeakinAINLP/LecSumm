 The main point of this topic’s content was to learn about dimensionality reduction techniques  and why we need them. The motivation for using these techniques is essentially to help reduce  dimensionality of a given data set which may have a lot of features. The disadvantage of having high  number of features is that we can run into overfitting. Another way to explain the issue with  dimensionality is as we increase it, we increase the sparsity of the data, and we may not have enough  data to “fill” missing regions which can impact the model as it cannot find proper patterns using the  current distance measuring metrics. However, despite this, we sometimes need the high dimensionality  to allow us to distinguish different things. An example from the unit site included increasing the number  of features to distinguish humans from other animals or humans from humans.  To solve the issue of dimensionality, one way we can do this is to look at the features and see if  they are needed or not (and thus remove if not needed). An approach that we can use if we still need  features and cannot remove them is  to use dimensionality reduction where we can transform the data  into a new projection vector (or new axis). To my understanding, the choice of projection vector will  depend on the eigen vector and value with the largest variance as this will be the vector direction that  will contain the most information or in other words, retain the variance that allows us to distinguish data  points in the newly transformed data. In contrast, if we use the eigenvector and value with the minimum  variance, then we will likely lose a lot of information from the original dimension. Ultimately, all of this  can be beneficial to clustering algorithms to help find patterns in the data and creating clusters. For  example, using the iris data from sklearn, where we are expecting to see three species of flowers and  therefore, should expect three clusters, if we try and perform a clustering algorithm, the performance  and quality will be quite poor and two species in particular a very close to each other. However, when we  reduce the dimensionality, we can see at least two very clear clusters now. One of the clusters in  particular, while might not be easily distinguishable, has vastly improved from the original dimension  where it was completely indistinguishable and had a lot of overlapping data points (more features  probably required to improve this further). Our way of performing dimensionality reduction is through  the use of PCA. We can perform PCA through techniques such as Singular Value Decomposition or Eigen  Value Decomposition.  As an extra topic for SIT307 students, we have something known as Independent Component  Analysis. From my understanding from the unit site,  we use ICA to separate mixed signals that may be  related to sound or images with the overall goal of finding underlying/hidden sources.  