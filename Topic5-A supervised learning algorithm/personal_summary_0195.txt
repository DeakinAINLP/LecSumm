Data Dimensionality Reduction  The number of dimensions of our data greatly affects the organisation and analysis of data. With higher numbers of dimensions there is exponential increase in the amount of space between data points. As an example consider a dataset containing values between 0 and 20. On a one dimensional graph, these would be plotted on a line and organised into sectors of 5 (0 to 5, 5 to 10 etc). On a two- dimensional graph, these would be organised on a graph with 16 sectors or 5 x 5. One a three dimensional graph, these would be organised onto a graph with 64 regions of 5 x 5 x 5. With this much growth from an increase of only two dimensions, we can see the problems this would cause if we encountered data with say 100 dimensions. This increase in the number of sectors and so the amount of space between he data points means that the datapoints are more sparse and makes it more difficult to compare data points with neighbours.  Results from clustering data with high dimensionality can be meaningless to us without the ability to differentiate between near and far neighbours. While there may still be patterns to provide useful insight in such data, more research is needed before our metrics can match the problem. Our solution in the meantime is to reduce the dimensionality of our data as much as we can.  In simple terms, we reduce the dimensionality by removing or combining fields in the data where applicable where the data will still portray the correct meaning.  One such method to reduce the dimensionality of the data is Principal Component Analysis (PCA). This can be summarised into five steps:    Standardising the data   Compute a covariance matrix to identify correlations in the data   Compute the eigenvectors and eigenvalues of the covariance matrix to identify principal  components    Create a feature vector to determine which components to keep   Reset the data along the principal components axes  The goal of PCA is to take n data points in d dimensions, which may be correlated, and summarises them by a new set of uncorrelated axes. These uncorrelated axes are called principal components and the first of these capture as much of the variation in the data as possible. By finding the axes  with the most variance, we find the direction that the data is spread out the most. These directions then act as our new x and y axes and allow us to graph the same data in a more meaningful way.      