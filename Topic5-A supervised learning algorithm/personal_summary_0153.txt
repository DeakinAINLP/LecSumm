1)  Eigen values and eigen vectors are used for linear transformations 2)  Number of nonzero eigen values are equal to a rank of a matrix 3)  Singular value decomposition is a method to decompose a matrix into three other matrices 4)  If we increase dimensionality then volume of the space increases and data becomes sparse 5)  With increasing dimesionality variances, mean of distances decreases 6)  Relative contrast between near and far neighbours diminishes as the dimensionality increases 7)  The goal of PCA is to take data points in dimensions, which may be correlated, and summarizes  them by a new set of uncorrelated axes  8)  The uncorrelated axes are called principal components or principal axes  Part 2: Summary of Reading material:  Dimensionality decrease is the most common way of lessening the quantity of elements (or aspects) in a dataset while holding however much data as could be expected. This should be possible for different reasons, for example, to decrease the intricacy of a model, to work on the presentation of a learning calculation, or to make it simpler to picture the information. There are a few procedures for dimensionality decrease, including head part examination (PCA), solitary worth disintegration (SVD), and straight discriminant investigation (LDA). Every procedure utilizes an alternate strategy to extend the information onto a lower-layered space while protecting significant data.  https://www.geeksforgeeks.org/dimensionality-reduction/  Principal Component analysis (PCA): PCA is an unsupervised linear dimensionality decrease and information representation method for extremely high layered information. As having high layered information is exceptionally difficult to acquire bits of knowledge from adding to that, it is computationally escalated. The primary thought behind this method is to decrease the dimensionality of information that is profoundly connected by changing the first arrangement of vectors to another set which is known as Principal component.  t-SNE is likewise a solo non-direct dimensionality decrease and information perception procedure. The number related behind t-SNE is very perplexing however the thought is basic. It inserts the focuses from a higher aspect to a lower aspect attempting to save the neighborhood of that point.  https://www.geeksforgeeks.org/difference-between-pca-vs-t-sne/  As there are however many principal components as there are factors in the information, head parts are built in such a way that the main principal components represents the biggest conceivable change in the informational index  https://builtin.com/data-science/step-step-explanation-principal-component-analysis  Part 3: The Curse of Dimensionality arises when applying machine learning algorithms to highly-dimensional data. It decides that as the number of dimensions increases, the number of regions grows exponentially. Dimensionality decrease alludes to the most common way of changing over a bunch of information having huge aspects into information with less aspects while as yet ensuring that it passes on comparative data briefly. There are many dimensionality reduction techniques like PCA, ICA, t SNE etc  