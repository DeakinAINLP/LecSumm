 Summary: Dimensionality Reduction  1.  Important mathematical concepts:  â—‹  Variance (S2) of each dimension (spread of values from the variable mean [ğ‘¥Ì…]) (cid:3041) (cid:3533)(ğ‘¥(cid:3036) âˆ’ ğ‘¥Ì…)(cid:2870) (cid:3036)(cid:2880)(cid:2869)  1 ğ‘› âˆ’ 1  ğ‘†(cid:2870) =  â—‹  Covariance is the degree to which 2 variables are linearly correlated.  ğ‘ğ‘œğ‘£(ğ‘–, ğ‘—) / ğ‘ˆ(cid:3036)(cid:3037) =  1 ğ‘› âˆ’ 1  (cid:3041)  (cid:3533) (ğ‘¥(cid:3036)(cid:3040) âˆ’   ğ‘¥Ì…(cid:3036))(ğ‘¥(cid:3037)(cid:3040) âˆ’  ğ‘¥Ì…(cid:3037)) (cid:3040)(cid:2880)(cid:2869)  Positive value: both dimensions increase together.  i. ii.  Negative value: one dimension increase and one decrease. iii. iv.  Covariance Matrix (C) of a 3 dimensions dataset:  Zero: the two dimensions are independent of each other.  â—  â—‹  Eigenvalues and Eigenvectors  i.  Used in the analysis of linear transformations. Au = Î»u (Eigen decomposition theorem): ii.  â—  A (square matrix [d x d]) â—  u = vector (Eigenvector of A that is d-dimension/transformation  matrix)  â—  Î» (lambda) = number (Eigenvalue of A) ğ‘¢(cid:2869) ğ‘¢(cid:2870)  ğ´(cid:2869)(cid:2869) ğ´(cid:2870)(cid:2869) (cid:3428) ğ´(cid:2869)(cid:2870) ğ´(cid:2870)(cid:2870)  (cid:4675) = Î» x (cid:4674)  ğ‘¢(cid:2869) ğ‘¢(cid:2870)  (cid:3432) x (cid:4674)  (cid:4675),  ğ‘’ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’: (cid:4672)  2 3 2 1  (cid:4673) Ã— (cid:4672)  3 2  iii.  UT = U-1 = A = UDUT  (cid:4673) = (cid:4672)  (cid:4673) = 4 Ã— (cid:4672)  12 8  3 2  (cid:4673)  â—  This is called the Eigenvalue Decomposition (UDUT) of  matrix A.  â—  Matrix U is always orthogonal (perpendicular / ui  Tuj = 0 if i â‰  j  and 1) that rotates the coordinates in a way to de-correlate the data dimensions.  iv.  Steps to get Eigenvalues and Eigenvectors with a given A Matrix. â—  Find Eigenvalue(Î») by det(A - Î»I) = 0, I is an Identity Matrix â—  Then find the Eigenvector by inset Î» back to Au = Î»u.  â—‹  Singular value decomposition (SVD)  i.  ii. iii.  A method to decompose a matrix (X) that is not square (m x n) in three (3) other matrices. It factorises a real or complex matrix. X = USVT  â—  U = a (m x m) orthogonal matrix = â€œRotatingâ€  â—  S = a (m x n) diagonal matrix with elements of singular values  in descending order (S(i, i) = Î´i) = â€œStretchingâ€  â—  V = a (n x n) orthogonal matrix = â€œRotatingâ€  (cid:3429)  ğ‘‹(cid:2869)(cid:2869) ğ‘‹(cid:2870)(cid:2869) ğ‘‹(cid:2869)(cid:2870) ğ‘‹(cid:2870)(cid:2870) ğ‘‹(cid:2869)(cid:2871) ğ‘‹(cid:2870)(cid:2871)  (cid:3433) = (cid:3429)  ğ‘ˆ(cid:2869)(cid:2869) ğ‘ˆ(cid:2870)(cid:2869) ğ‘ˆ(cid:2871)(cid:2869) ğ‘ˆ(cid:2869)(cid:2870) ğ‘ˆ(cid:2870)(cid:2870) ğ‘ˆ(cid:2871)(cid:2870) ğ‘ˆ(cid:2869)(cid:2871) ğ‘ˆ(cid:2870)(cid:2871) ğ‘ˆ(cid:2871)(cid:2871)  (cid:3433) Ã— (cid:3429)  ğ‘ (cid:2869) 0 0  0 ğ‘ (cid:2870) 0  (cid:3433) Ã— (cid:3428)  ğ‘‰(cid:2869)(cid:2869) ğ‘‰(cid:2870)(cid:2869) ğ‘‰(cid:2869)(cid:2870) ğ‘‰(cid:2870)(cid:2870)  (cid:3432)  iv.  Steps to find SVD:  â—  Finding the Eigenvalues (XXT) and Eigenvectors (XTX) â—  XTX make-up columns V. â—  XXT make up columns U. â—  S is sqrt of XXT or XTX.  2.  Typical Dimensionality in data  â—‹  Text data Image data â—‹ â—‹  Genomic Data  3.  Curse of Dimensionality  â—‹  Increasing dimensionality leads to:  Volume of the space increases exponentially.  i. ii.  Data become sparse. vd(r) = kdrd, r = 1 - Îµ iii. iv.  High dimensional space:  â—  Training data resides on the surfaces of the hypersphere or  corners of the hypercube.  â—  Causes the distance from the origin to all different points  becomes similar (less distinctive).  â—  Presence of hubs  a.  A few instances appear surprisingly frequently as  nearest neighbours of other neighbours.  v.  Concentration Effect (Distance Concentration):  â—  The proportional difference between the farthest point distance  Dmax and the closest point distance Dmin vanishes.  â—  Ratio of the variance of the length of any point vector (||Xd||)  and length of the mean point vector (E||Xd||) converges to zero with increasing data dimensionality. Implies:  â—  a.  Clustering or KNN algorithms may be meaningless in  high dimensions.  b.  Main aim is to reduce the dimensionality where  possible.  â—‹  Solutions: i. ii. iii.  Too many variables: remove those that are irrelevant. All numeric, if correlated (redundancy) - group them together. Features are not the same.  â—  Choose the feature with the highest variance.      iv.  Linear Dimensionality reduction/Linear transformation methods/Matrix Factorization  â—  Refers to the process of converting a dataset of dimension Q into dimension R where R < Q ensures similar information contents. If feature 1 = feature 2,  â—  a.  Transform the data with a projection vector into a single  dimension.  b.  ğ‘ƒğ‘Ÿğ‘œğ‘—ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ =   (cid:4674)  0.5 0.5  (cid:4675)  â—  Principal Component Analysis (PCA)  a.  Utilises Variance, Covariance, Lagrange multiplier,  partial derivative, Eigen Value Decomposition (EVD) or SVD in mathematical concepts.  b.  Take n data points in d dimensions, which may be  correlated and summarises them by a new set of uncorrelated axes (Principal components) that are linear combinations of the original d dimensions.  c.  Main Objectives:  i.  Ordered such that principal axis-captures the highest variance, axis-2 captures the next highest variance, â€¦, and axis-d has the lowest variance (principal components are sorted in descending order based on captured variance along each axis).  ii.  Covariance among each pair of the principal axes is zero (Decorrelation property).  d.  Geometric Rationale:  a = distance of point to origin is fixed b = Minimize the distance to best fitting line. c = Maximize the distance from projected point to the origin (0,0)  ii.  i.  Objects are represented as a cloud of n points in a multidimensional space with an axis for each d dimension. The centroid of the points is defined by the mean of each variable (Mean centred [ğ‘¥Ì…] =0). Assumption of maximum variance for unit length vector (u1) to be ||u1|| = 1 = u1 The new coordinates for yi1= u1 e.  Steps of PCA (Maximisation) via EVD:  Tu1 Tx  iv.  iii.  i.  Compute data covariance matrix (C). ii. iii.  Reduced the dimension of data to Y (k-  Perform EVD as C = UDUT  dimension by selecting the top k eigenvector. ğ‘Œ(cid:3041)Ã—(cid:3038) = ğ‘‹(cid:3041)Ã—(cid:3031) âˆ™ ğ‘ˆ(cid:3031)Ã—(cid:3038)  1.  Ynxk = Data in k-dimension 2.  Xnxk = Original data in d-dimension 3.  Udxk = Top k Eigen vectors in the decreasing order of Eigenvalues  f.  PCA: Minimum Error Formulation/Projection error  minimization/minimal reconstruction error    i.  Uses the minimise mean square error (b) to find  top k eigenvectors of covariance.  g.  Limitations:  i.  Reducing dimensions using PCA changes the  distances of our data. (Preserves large pairwise distance better than small pairwise distance).  â—  Independent Component Analysis  a.  The goal is to find a linear representation of non-  Gaussian data so that the components are statistically independent.  â—  Locally Linear Embedding  v.  Non-Linear Dimension Reductions/Neighbour Graphs â—  Kernal principal component analysis â— â—  Stochastic Neighbour Embedding (SNE) â€“ Hinton and Roweis  ISOMAP  (2002) t-Distributed Stochastic Neighbour Embedding (T-SNE)  â—  a.  Works by finding patterns and relationships in the data  and then representing those patterns in a lower- dimensional space to reduce the complexity (by a fixed perplexity) of the data.  â—  uMap (Uniform Manifold Approximation and Projection for  dimension reduction.  a.  Combination of Nerve Theorem, UMAP Adjunction, b.  Assumption:  i.  Data is uniformly distributed on the manifold  (Riemannian Metric). The manifold is locally connected.  ii.  c.  Implementation  i.  Find (approximate) nearest neighbours. 1.  RP-Trees and NN-Descend  ii.  Optimize the layout subquadratically. 1.  SGD and Negative sampling  