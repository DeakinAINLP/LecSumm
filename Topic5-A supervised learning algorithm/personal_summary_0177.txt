In this topic we learnt about Dimensionality Reduction. Eigen values and Eigen vectors are components used in analysis of linear transformation for dimensionality reduction. Linear transformation is the transformation of the data into a different format. For a given square matrix A, if it satisfies Au=λu, then λ is called Eigen value and u is the Eigen vector. For a matrix of size d*d, there are d Eigen vectors and Eigen value pairs We also learnt how to find Eigen vector and Eigen values of a given square matrix A in this topic.  Different type of data have different dimensions.  It can go to huge numbers thus resulting in huge number of features. Dimensionality reduction can be used to get a lower set of features and to avoid over fitting due to huge amount of irrelevant data. Dimensionality has few curses as well. With increasing dimensionality data becomes sparse. Moreover, variance among mean of distances between data points reduces with an increment in dimensionality and it means distance metrics in clustering becomes meaning less. Concentration effect is one of the main curse of increasing dimensionality. To solve this issue we can use dimensionality reduction by removing irrelevant data and find redundant features and remove them.  Principal Component Analysis or PCA is a widely used method for dimensionality reduction of datasets. In PCA, the first principal component captures the most significant amount of variance in the data. The second principal component captures the next most significant amount of variance, and so on. Each principal component is orthogonal to the previous one, and the total variance of the data is equal to the sum of the variances of all the principal components.  Eigen value decomposition and Minimum Error Formulation are two techniques of PCA to perform dimensionality reduction. Implementation of PCA will be difficult when number of data points is less than the number of dimensions, hence Eigen value decomposition is not preferable to use in such scenarios and SVD can be used. Singular value decomposition (SVD) is a technique used to decompose a matrix into three other matrices. Furthermore, PCA cannot be used for dimensionality reduction in case of nonlinear data. Other techniques like ICA, t-SNE, UMAP etcetera is useful in such cases.  