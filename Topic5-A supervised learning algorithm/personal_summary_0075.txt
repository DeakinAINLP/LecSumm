Last topic, we have come around the one concept of unsupervised Machine  Learning, that was Clustering. This topic, we focus on Dimensionality Reduction. In this topic, we covered about a major issue with dataset with numerous features; this is famously called as the “Curse of Dimensionality”.  It states that as the number of features in a dataset increase, the regions around these features also get to increase exponentially. Therefore, instances in these dimensions also tend to decrease. As a result, the instances in data in these hyper-dimensions would become insignificant. As a result of these datasets, our models can become very complex, bulky, and they would take a lot of time to process. We also have learned about the concentration affect where we assume the ratio of the variance of length of any point vector with the length of main point vector converges to zero with increasing dimensions. As a result, the utility of the measure to discriminate between near and far neighbours also becomes meaningless. This also implies that algorithms like clustering or cannons can become meaningless in high dimensions and unless we develop better metrics for distance, we should aim to reduce the dimensionality where possible.  The solution for this huge problem is an algorithm called “Dimensionality Reduction”.  This is a very famous algorithm, known for reducing high dimensions into lower number of dimensions. This Algorithm is known as Principal Component Analysis, because when the algorithm reduces the high number of dimensions into lower number of dimensions, they are called principal components or principal axes.  Our algorithm is also known to be based on a mathematical concept from linear  algebra called Singular Value Decomposition (SVD) and Eigenvectors and Eigenvalues. We took a further dive into other mathematical concepts, related to the PCA, like variance and covariance among variables, vectors, unit vectors, matrices, and covariance matrices. In fact, how PCA works is done in three major steps: First Compute the Covariance matrix, then perform the Eigen value decomposition, and then find the reduced dimensional data by multiplying the original data in d-dimensions with the Top k Eigenvectors in the decreasing order of the Eigenvalues. PCA can be also analysed from another perspective and that is based on ‘projection error minimization’. This topic, we also learned a well-known use of the PCA in facial Analysis.  We also learned about some of the other known dimensionality reduction techniques, which includes ICA, t-SNE, and uMap. The last two techniques can be used as Non-Linear Dimensionality Reduction Techniques. We also programmed PCA in Python with two purposes; data correlation and dimensionality summarisation.  