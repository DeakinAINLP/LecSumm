SIT720 Machine Learning Task 4.1  Student Name: Jason Lian Student ID: 214082118 Email: shijiel@deakin.edu.au  Topic 4 Learning Summary Dimensionality Reduction: Techniques to reduce the number of features in a dataset, thereby making it more manageable and easier to visualise while retaining as much information as feasible.  Principal Component Analysis (PCA): A linear dimensionality reduction technique that projects data onto the most relevant directions (principal components) in a dataset. PCA aids in identifying pa(cid:425)erns, reducing noise, and compressing data with minimal loss of information.  Singular Value Decomposition (SVD): A matrix decomposition technique used in principal component analysis (PCA) that identiﬁes a dataset's principal components. SVD can also be used to reduce dimensionality, reduce noise, and compress data.  t-Distributed Stochastic Neighbour Embedding (t-SNE): A non-linear dimensionality reduction technique that preserves local structures in high- dimensional data by minimising the divergence between probability distributions of pairwise similarities. t-SNE is particularly eﬀective at visualising complex data relationships.  Uniform Manifold Approximation and Projection (UMAP): A non-linear technique for dimensionality reduction that seeks to preserve both local and global data structures. UMAP is more computationally eﬀective than t-SNE and can be used for general dimensionality reduction and visualisation tasks.  Eigenvalues and Eigenvectors: Essential PCA components for determining new feature matrices. Eigenvalues represent the quantity of variance each principal component explains, whereas eigenvectors deﬁne the directions of these components.  Variance and Correlation: Comprehending the variance explained by principal components and the correlation between them facilitates the evaluation of the eﬃcacy of dimensionality reduction techniques and the eﬀect on the dataset.  Deterministic Algorithms: Algorithms, such as PCA, that generate the same output for the same input data, as opposed to stochastic algorithms, which incorporate randomness and can generate various outputs for the same input.  