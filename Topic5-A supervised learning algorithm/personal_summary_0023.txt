Dimensionality in Data    Text data:  Imagine a News website. If you start crawling the news on the website for a short period of time such as a topic, depending on the number of documents you crawl, it is typical to have more than 10,000 dimensions. This number is the size of the dictionary you have to build based on the words you extracted from the News documents. We need to represent each document based on the words in a dictionary (remember. the feature vector covered in topics 1 and 2: Data representation). Image data: Imagine we would like to use pixels as features, just an 64×64 image would have 4,096 dimensions!     Genomic data: Take Parkinsons disease case-control data as an example. It has 408,803  Single-  nucleotide polymorphisms (SNPs) and Alzheimer’s disease has 380,157 SNPs.  Curse of Dimensionality  The Curse of Dimensionality arises when applying machine learning algorithms to highly-dimensional data.    When the dimensionality increases, the volume of the space increases so fast that the available  data become sparse.    At its core, the curse of dimensionality, dictates that as the number of dimensions increases, the  number of regions grows exponentially.    As the number of regions grows and space increases each data point has more and more room.   That makes our data In high dimensional spaces, most of the training data resides in the corners of the hypercube defining the feature space. the curse of dimensionality result in less distinctive distances in in high dimensions. So given a point in high dimensions, the relative distance between points far from it and close from it, becomes negligible.  Concentration effect    Relative contrast between near and far neighbours diminishes as the dimensionality  increases. This is known as the concentration effect of the distance measure.  This problem can imply that:  Clustering or KNN algorithms may be meaningless in high dimensions. However, there might still be patterns in high dimensions. We just need better distance metrics. So Research is needed!  Until we develop better distance metrics, we should aim to reduce the dimensionality where possible.    The Curse of Dimensionality calls for Dimensionality Reduction. Dimensionality reduction refers to the process of converting a set of data having vast dimensions into data with fewer dimensions while still making sure that it conveys similar information concisely.  Eigenvalues and Eigenvectors  Eigenvalues and eigenvectors are prominently used in the analysis of linear transformations.   In Mathematics, an eigenvector corresponds to the real non zero eigenvalues which point in the direction stretched by the transformation whereas eigenvalue is considered as a factor by which it is stretched. In case, if the eigenvalue is negative, the direction of the transformation is negative.    For every real matrix, there is an eigenvalue. Sometimes it might be complex. The existence of the eigenvalue for the complex matrices is equal to the fundamental theorem of algebra.    Eigenvalues are the special set of scalars associated with the system of linear equations.   Eigenvectors are the vectors (non-zero) that do not change the direction when any linear  transformation is applied. It changes by only a scalar factor.  Singular value decomposition  Singular value decomposition (SVD) is a method of decomposing a matrix into three other matrices.  Using SVD, we can determine the rank of the matrix, quantify the sensitivity of a linear system to numerical error, or obtain an optimal lower-rank approximation to the matrix.  Preliminaries  The goal of principal component analysis (PCA) is to take n data points in d dimensions, which may be correlated, and summarises them by a new set of uncorrelated axes. The uncorrelated axes are called principal components or principal axes. These axes are linear combinations of the original d dimensions.  Variance across each variable  Data is represented as a cloud of points in a multidimensional space with one axis for each of the variables. The centroid of the points is defined by the mean of each variable  Covariances among variables  To put it simply, covariance is a measure of how changes in one variable are associated with changes in a second variable.  Covariance Matrix  The covariance matrix is a matrix that contains variances of all variables on the diagonal and co- variances among all pairs of variables in the off-diagonal entries.  PCA: decorrelation  The main objective of PCA is to rigidly rotate the axes of d−dimensional axes to a new set of axes (called principal axes) that have the following properties:     Ordered such that principal axis- captures the highest variance, axis-2 captures the next highest  variance, …. , and axis−d  has the lowest variance    Covariance among each pair of the principal axes is zero (the principal axes are uncorrelated i.e.  they are orthogonal to each other). This is called decorrelation property.  Formulation of PCA and deriving principal components  1.  Standardize the range of continuous initial variables 2.  Compute the covariance matrix to identify correlations 3.  Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal  components  4.  Create a feature vector to decide which principal components to keep 5.  Recast the data along the principal components axes  PCA: Minimum Error Formulation  In the Minimum Error Formulation, PCA is defined as the linear projection that minimises the average projection cost (mean squared error) between the data points and their projection .  Implementation of PCA  We can use SVD to perform PCA.  Independent component analysis (ICA)  ICA is a method of separating a multivariate signal into independent, non-Gaussian components.  In signal processing, machine learning, and neuroscience, ICA is commonly used to separate signals that are mixed together, such as in a complex sound or image signal. The goal of ICA is to find a set of basis functions that capture the signal's underlying sources, then use these functions to separate the signal into its constituent parts.  