SUMMARY OF THE MAIN POINTS FOR TOPIC 4 LEARNING This topics topic is dimensionality reduction (DR) & covered methods including PCA, ICA, t-SNE & UMAP.  The main motivation for using DR is of course overcoming the curse of dimensionality. There are multiple   aspects   to   the   problems   DR   can   cause   and   the   significance   of   these   effects   depend somewhat on the problem domain i.e. data mining vs ML vs optimisation etc, but some of the main issues are:   More dimensions requires more computation and an exponentially larger solution space to  traverse.   Data becomes more sparse making distinguishing distances increasingly difficult i.e. near and far become harder to differentiate, and more generally, it becomes increasingly difficult to determine similarity or difference among data points.   Exponentially   more   data   is   required   to   make   characterisations   of   a   group   that   is meaningfully distinguishable from other groups, which is a problem in and of itself but can also lead to outliers becoming more influential.  One way to deal with the problems associated with high dimensional data is to reduce the number of dimensions. Of course the problem then becomes determining how to discard dimensions and still maintain the fundamental nature of the data and the relationships between data points in it.  One of the most common ways to reduce the dimension of a data set is Principle Component Analysis (PCA). This method computes a new basis for the data in such a way that the basis vectors (coordinate axes) align with the main directions of variance in the data. First find a vector that encapsulates   the   largest   amount   of   variance   in   the   data,   then   find   an   orthogonal   vector   which encapsulates the next largest amount of variance in the data and so on. The data can then be re- oriented with reference to this new basis and since variance is a proxy for information, we can trivially   eliminate   dimensions   associated   with   the   least   information   loss   by   choosing   the   basis vectors with the smallest amount of variance. We can thereby make a well informed decision about which dimensions to discard on the basis of how much information we are prepared to discard to achieve our lower dimensional version of the data set.  PCA is an eigen-decomposition method which can be defined as a special case of SVD. It involves finding the eigenvalues and eigenvectors of a data sets covariance matrix. The eigenvectors can be used to re-project the data onto a lower dimensional space; by discarding some eigenvectors. The eigenvalues represent the amount of “information” that the corresponding eigenvector contains, with larger eigenvalues indicating a higher level of information in the direction of that eigenvector.  Independent Component Analysis (ICA) is a blind source separation method. That is, it is a method capable of taking a linear combination/mixture of independent signals and decomposing the mixture into components that are once again independent i.e. unmixing the signal. The actual method of determining the independence of components depends on the particular version of ICA of which there are many. The two most common methods use Mutual Information (MI) and the so called “Gaussianity”  i.e.  how  much  something  fits  a   gaussian  model.  I’m  not  familiar  with  the  latter version   but   the   MI   version   is   relatively   straight   forward   to   describe:   Random   variables   are independent if they have zero MI so formulate components in such a way as to minimise the MI between them. This can be achieved for instance via an optimisation scheme that minimises a cost function which describes some form of MI between components.  Unlike   PCA,   ICA  has   no   inherent   properties   that   are   useful   for   the   purpose   of   dimensionality reduction;   that   I’m   aware   of.   It   is   often   suggested   to   run   PCA  to   determine   the   number   of  components to keep, but that suggests there is an equivalence or at least relationship between the variance and independence properties in the data, and if that is true, then why bother running ICA after PCA, and if there is no equivalence or relationship then the choice that results results from PCA should have no bearing on the choice for ICA.  t-SNE and UMAP are two non-linear dimensionality reduction methods; that are mainly useful for visualisation.   I   don’t   pretend   to   really   understand   the   mathematics   of   these   methods   but   in summary, they attempt to create a probabilistic description of the likelihood that each data point would “consider” other data points to be their “neighbour”. This allows the higher dimensional data to be re-projected onto a 2D or 3D space using the likely neighbour information to decide where in the lower dimensional space data points should be located so as to maintain the local structure from the higher dimensional space and possibly some of the global structure i.e. if data points were highly likely to be neighbours in the higher dimensional space, they remain highly likely to be neighbours in the lower dimensional space.  With extensive knowledge of their limitations and the effect of their parameters and the critical role that initialisation plays in the results, these methods seem to be useful as an exploratory tool that can   act   as   a   guide   for   deciding   which   features   in   a   data   set   might   be   worth   exploring   for relationships using other analysis tools/methods. However, while t-SNE and UMAP are clearly popular, widely used and apparently useful, it seems there is considerable evidence to question the relevance   and/or   validity   of   the   results   they   produce   as   there   is   no   guarantee   they   have   not essentially invented relationships and structure that does not exist (see my reading references).    