 This topic we learnt concepts regarding dimensionality reduction, covering the following:  Curse of Dimensionality  Increasing the number of dimensions in a dataset can negatively degrade a machine learning algorithm by increasing the complexity or affecting performance. This is a result of the exponential growth of a feature space, making it more difficult for machine learning algorithms to effectively learn from the available data points.  Dimensionality Reduction  Dimensionality reduction provides techniques to reduce the impact of the curse of dimensionality. These techniques convert high-dimensional datasets into a smaller dimensional dataset while ensuring the data conveys similar information.  Singular Value Decomposition (SVG)  This is a method to decompose a matrix into three other matrices: X = USVT  Where:    U is an orthogonal matrix.   S (“Sigma”) is a diagonal matrix.   V is the transpose of the orthogonal matrix.  By breaking down the matrix into the above matrices, SVG allows the most important features to be identified to help reduce the complexity of high-dimensional datasets.  Principle Component Analysis (PCA)  In similar fashion, the PCA is another technique which aims to transform high-dimensional data into lower-dimensional representation of the original dataset. This technique uses the eigenvalues and eigenvectors of the data’s covariance matrix, which corresponds to the new orthogonal axes (“principal components”) in the transformed space. This assists in identifying which features have the most impact in a dataset.  SVG can be used for PCA, where the columns of matrix V represent the principal components which are the new axes in the transformed space. The data can then be projected onto the new axes by multiplying the standardised matrix by a subset of the columns of matrix V (principal components).  Independent Component Analysis (ICA)  This technique is used to separate multivariate signals into its underlying independent sources. Unlike PCA, which aims to capture the maximum variance, ICA focuses on maximising the independence of the components.  t-Distributed Stochastic Neighbour Embedding (t-SNE)  This is a non-linear dimensionality reduction technique which is effective at visualising high- dimensional data in a lower-dimensional space (i.e. 2D or 3D). This technique maintains the local  structure of the data by preserving the distance between the nearby points while allowing the separation of well-separated clusters.  Uniform Manifold Approximation and Projection (uMap)  This is a similar technique to t-SNE however has some advantages such as faster runtime and more consistent preservation of local and global data structures.  