Main points covered in Topic 4  1.  Data Sparsity  a.  The sparsity of the data increases with the increase in the dimensions. For a 1-D space, for features of 50, the regions are 10, but for 2-D, the regions are 100 and the increase in the dimensions exponentially increases the space and the data will have  more  and  more  room  for  it.  This  is  referred  to  the  term,  ‘curse  of dimensionality.’  2.  Challenges in high dimensions  a.  High  computational  complexity,  overfitting  and  difficulty  in  getting  meaningful insights  are  a  few  challenges.  As  the  dimensions  increase,  the  volume  of  the feature space also increases exponentially.  b.  The  solve  this  problem,  there  are  several  techniques.  The  main  idea  is  PCA (Principal  component  analysis).  This  happens  through  feature  selection, extraction,  and  dimensionality  reduction.  Dimensionality  reduction  techniques aim  to  reduce  the  number  of  features  in  a  dataset  while  retaining  the  most important information.  c.  As it is impossible to draw a 4-dimensional plot, PCA converts a meaningful plot in  2D for visualization. PCA  3.  PCA – Principal Component Analysis  a.  Step 1 - Center the data (This is done by finding the mean of the data and centering  it to the origin)  b.  Step 2 – Find PC1 (Find the best fitting line going through the origin) the distances from the line to the datapoint can be found and the maximum sum of squared distances is identified as the PC1.  c.  Step 3 – Post identifying PC1, PC2 is a perpendicular line to it.  In practice the number of PCs is either number of variables or number of samples, whichever is smaller.  d.  Step 4 – After all the PCs are found, eigen values are found. The eigen values here are the means of all distances. Where SS stands for sum of squared distances e.  Step 5 – Finally, choose the 2 components with the highest eigen values and plot  the 2D plot. 4.  Eigenvalues and Eigenvectors:  a.  Eigenvalues and eigenvectors are concepts in linear algebra that are used in PCA and  other  dimensionality  reduction  techniques.  An  eigenvector  is  a  non-zero vector that, when multiplied by a matrix, results in a scalar multiple of itself. The scalar multiple is known as the eigenvalue. Eigenvectors and eigenvalues are used to determine the principal components in PCA, which is ultimately the variance.  5.  t-SNE:  a.  t-SNE (t-Distributed Stochastic Neighbor Embedding) is a technique for visualizing high-dimensional data in a low-dimensional space. Unlike PCA, which focuses on preserving  global  structure  in  the  data,  t-SNE  is  designed  to  preserve  local structure in the data. t-SNE works by modeling the probability of two points being neighbors in high-dimensional space and then minimizing the difference between the  high-dimensional  and  low-dimensional  probabilities.  t-SNE  has  become  a popular technique for visualizing high-dimensional data, particularly in the field of machine learning.  Reflection on the knowledge gained.  Dimensionality refers to the number of features or attributes that describe each observation in a  dataset.  Datasets  with  a  high  number  of  dimensions  are  commonly  found  pretty  much everywhere, but these high-dimensional datasets can be difficult to analyze and understand.One of  the  main  challenges  of  high-dimensional  data  is  the  curse  of  dimensionality.  The  curse  of dimensionality  describes  the  difficulties  that  arise  when  working  with  high-dimensional  data, such as overfitting, increased computational complexity, and difficulty in identifying meaningful patterns  in  the  data.  As  the  number  of  dimensions  in  a  dataset  increases,  the  feature  space becomes exponentially larger, which makes it difficult to identify patterns in the data.To solve the curse of dimensionality, there are several techniques. Out of which, feature selection involves selecting a subset of the most informative features from a dataset. Feature extraction transforms the original features into a new set of features that capture the most important information in the data. Dimensionality reduction techniques aim to reduce the number of features in a dataset while retaining the most important information.  One common technique for this is PCA, which involves transforming the original features into a new  set  of  features  known  as  principal  components.  The  principal  components  are  linear combinations  of  the  original  features  that  capture  the  most  variance  in  the  data.  The  first principal component is the linear combination of features that captures the most variance, and subsequent  principal  components  capture  the  remaining  variance  in  the  data.  The  principal components are derived by finding the eigenvectors and eigenvalues of the covariance matrix of the original features. Eigenvectors and eigenvalues are concepts in linear algebra that are used in PCA and other dimensionality reduction techniques. An eigenvector is a non-zero vector that, when multiplied by a matrix, results in a scalar multiple of itself. The scalar multiple is known as the eigenvalue. Eigenvectors and eigenvalues are used to determine the principal components in PCA.  Another  technique  for  visualizing  high-dimensional  data  is  t-SNE  (t-Distributed  Stochastic Neighbor  Embedding).  t-SNE,  or  t-Distributed  Stochastic  Neighbor  Embedding,  is  a  machine learning technique used for visualizing high-dimensional data in a lower-dimensional space. The goal of t-SNE is to represent complex, high-dimensional data in a way that is easy to visualize and   understand.  Unlike  other  dimensionality  reduction  techniques,  such  as  PCA,  which  focus  on preserving global structure in the data, t-SNE is designed to preserve local structure in the data. t-SNE works by modeling the probability of two points being neighbors in high-dimensional space and  then  minimizing  the  difference  between  the  high-dimensional  and  low-dimensional probabilities. This allows t-SNE to preserve both local and global structure in the data. One of the main advantages of t-SNE is its ability to visualize complex, high-dimensional datasets in a way that is easy to interpret.  