This topic, we covered dimensionality in data and how the curse of dimensionality can be a problem for machine learning. We also learned about techniques like PCA and SVD to reduce dimensionality and solve the curse.  My reading list included various web pages on these topics, as well as code libraries like NumPy and Scikit-learn.  Through this topic's content, I gained a better understanding of the challenges posed by high-dimensional data and the importance of dimensionality reduction techniques.  Using eigenvectors for dimensionality reduction.    Given a matrix D contains some data points   𝐷𝑇𝐷 to obtain a square matrix   Obtain eigen-vectors of  𝐷𝑇𝐷, which gives the Eigen matrix E   DE will transform the data points to the new coordinate space  𝐷𝑇𝐷 and D’s covariance matrix have the same Eigenvectors  Power Iteration makes advantage of the property of Eigenvector.   𝑀𝑣 = 𝜆𝑣. The transformed vector stops changing direction once it is an eigenvector.  Given eigenvector find the corresponding eigenvalue:  𝑥𝑇𝑀𝑥 = 𝜆, 𝑤ℎ𝑒𝑟𝑒 𝑥 𝑖𝑠 𝑎 𝑢𝑛𝑖𝑡 𝑒𝑖𝑔𝑒𝑛𝑣𝑒𝑐𝑡𝑜𝑟  𝑥𝑇𝑀𝑥 = 𝑥𝑇𝜆𝑥 = 𝜆𝑥𝑇𝑥 = 𝜆 ∗ 1 = 𝜆   (x is a unit vector, 𝑥𝑇𝑥 = 1)  Find second eigenvector by eliminating the influence of the first eigenvector – the first eigenvector is an eigenvector in M2 with eigenvalue=0.  𝑀∗ = 𝑀 − 𝜆1𝑥𝑥𝑇  𝑀∗𝑥 = (𝑀 − 𝜆1𝑥𝑥𝑇)𝑥 = 𝑀𝑥 − 𝜆1𝑥𝑥𝑇𝑥 = 𝑀𝑥 − 𝜆1𝑥 = 0  Like PCA, SVD is another form of matrix analysis that can reduces the dimension. SVD defines a small number of “concepts”. By eliminating the least important concepts, we can obtain a smaller representation.     