This topic, we covered dimensionality in data and how the curse of dimensionality can be a problem for machine learning. We also learned about techniques like PCA and SVD to reduce dimensionality and solve the curse.  My reading list included various web pages on these topics, as well as code libraries like NumPy and Scikit-learn.  Through this topic's content, I gained a better understanding of the challenges posed by high-dimensional data and the importance of dimensionality reduction techniques.  Using eigenvectors for dimensionality reduction.    Given a matrix D contains some data points   ğ·ğ‘‡ğ· to obtain a square matrix   Obtain eigen-vectors of  ğ·ğ‘‡ğ·, which gives the Eigen matrix E   DE will transform the data points to the new coordinate space  ğ·ğ‘‡ğ· and Dâ€™s covariance matrix have the same Eigenvectors  Power Iteration makes advantage of the property of Eigenvector.   ğ‘€ğ‘£ = ğœ†ğ‘£. The transformed vector stops changing direction once it is an eigenvector.  Given eigenvector find the corresponding eigenvalue:  ğ‘¥ğ‘‡ğ‘€ğ‘¥ = ğœ†, ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘¥ ğ‘–ğ‘  ğ‘ ğ‘¢ğ‘›ğ‘–ğ‘¡ ğ‘’ğ‘–ğ‘”ğ‘’ğ‘›ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ  ğ‘¥ğ‘‡ğ‘€ğ‘¥ = ğ‘¥ğ‘‡ğœ†ğ‘¥ = ğœ†ğ‘¥ğ‘‡ğ‘¥ = ğœ† âˆ— 1 = ğœ†   (x is a unit vector, ğ‘¥ğ‘‡ğ‘¥ = 1)  Find second eigenvector by eliminating the influence of the first eigenvector â€“ the first eigenvector is an eigenvector in M2 with eigenvalue=0.  ğ‘€âˆ— = ğ‘€ âˆ’ ğœ†1ğ‘¥ğ‘¥ğ‘‡  ğ‘€âˆ—ğ‘¥ = (ğ‘€ âˆ’ ğœ†1ğ‘¥ğ‘¥ğ‘‡)ğ‘¥ = ğ‘€ğ‘¥ âˆ’ ğœ†1ğ‘¥ğ‘¥ğ‘‡ğ‘¥ = ğ‘€ğ‘¥ âˆ’ ğœ†1ğ‘¥ = 0  Like PCA, SVD is another form of matrix analysis that can reduces the dimension. SVD defines a small number of â€œconceptsâ€. By eliminating the least important concepts, we can obtain a smaller representation.     